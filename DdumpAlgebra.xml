<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.8/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.8/ http://www.mediawiki.org/xml/export-0.8.xsd" version="0.8" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <base>http://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.24wmf7</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Mathematics Subject Classification</title>
    <ns>0</ns>
    <id>3008091</id>
    <revision>
      <id>596843118</id>
      <parentid>594480955</parentid>
      <timestamp>2014-02-24T00:17:35Z</timestamp>
      <contributor>
        <username>Mark viking</username>
        <id>17698045</id>
      </contributor>
      <comment>/* Applied mathematics / other */ Added sections 60 and 62 from MSC 2010</comment>
      <text xml:space="preserve" bytes="9895">{{math|}}The '''Mathematics Subject Classification (MSC)''' is an alphanumerical [[classification scheme]] collaboratively produced by staff of and based on the coverage of the two major mathematical reviewing databases, [[Mathematical Reviews]] and [[Zentralblatt MATH]]. It is used by many mathematics [[Academic journal|journals]], which ask authors of [[research papers]] and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2010.

== Structure ==

The MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.

The first level is represented by a two digit number, the second by a letter, and the third by another two digit number. For example:

* '''53''' is the classification for [[differential geometry]]
* '''53A''' is the classification for classical differential geometry
* '''53A45''' is the classification for [[Euclidean vector|vector]] and [[tensor]] analysis

=== First level ===
At the top level 64 mathematical disciplines are labeled with a unique 2 digit number. As well as the typical areas of mathematical research, there are top level categories for &quot;[[History]] and [[Biography]]&quot;, &quot;[[Mathematics Education]]&quot;, and for the overlap with different sciences. [[Physics]] (i.e. mathematical physics) is particularly well represented in the classification scheme with a number of different categories including:
* [[Fluid mechanics]]
* [[Quantum mechanics]]
* [[Geophysics]]
* [[Optics]] and [[electromagnetic theory]]

All valid MSC classification codes must have at least the first level identifier.

=== Second level ===

The second level codes are a single letter from the Latin alphabet. These represent specific areas covered by the first level discipline. The second level codes vary from discipline to discipline.

For example, for differential geometry, the top-level code is '''53''', and the second-level codes are:
* '''A''' for classical differential geometry
* '''B''' for local differential geometry
* '''C''' for global differential geometry
* '''D''' for sympletic geometry and contact geometry

In addition the special second level code &quot;-&quot; is used for specific kinds of materials. These codes are of the form:

* '''53-00''' General reference works (handbooks, dictionaries, bibliographies, etc.)
* '''53-01''' Instructional exposition (textbooks, tutorial papers, etc.)
* '''53-02''' Research exposition (monographs, survey articles)
* '''53-03''' Historical (must also be assigned at least one classification number from Section 01)
* '''53-04''' Explicit machine computation and programs (not the theory of computation or programming)
* '''53-06''' Proceedings, conferences, collections, etc.

The second and third level of these codes are always the same - only the first level changes. It is not valid to put '''53-''' as a classification, either '''53''' on its own, or better yet a more specific code should be used.

=== Third level ===

Third level codes are the most specific, usually corresponding to a specific kind of mathematical object or a well-known problem or research area.

The third-level code '''99''' exists in every category and means ''none of the above, but in this section''

== Using the scheme ==

The AMS recommends that papers submitted to its journals for publication have one primary classification and one or more optional secondary classifications. A typical MSC subject class line on a research paper looks like

MSC Primary 03C90; Secondary 03-02;

==History==
{{expand section|date=January 2014}}
According to the American Mathematical Society help page about MSC,&lt;ref&gt;[http://www.ams.org/mathscinet/help/field_help.html#mscp]&lt;/ref&gt; the MSC has been revised a number of times since 1940, but the original classification of older items has not been reclassified. This can sometimes make it difficult to search for older works dealing with particular topics. Changes at the first level involved the subjects with (present) codes 03, 08, 12-20, 28, 37, 51, 58, 74, 90, 91, 92.

== Relation to other classification schemes ==

For physics papers the [[Physics and Astronomy Classification Scheme]] is often used. Due to the large overlap between mathematics and physics research it is quite common to see both '''PACS''' and '''MSC''' codes on research papers, particularly for multidisciplinary journals and repositories such as the [[arXiv]].

The [[ACM Computing Classification System]] is a similar hierarchical classification scheme for [[computer science]]. There is some overlap between the AMS and ACM classification schemes, in subjects related to both mathematics and computer science, however the two schemes differ in the details of their organization of those topics.

The classification scheme used on the arXiv is chosen to reflect the papers submitted. As arXiv is multidisciplinary its classification scheme does not fit entirely with the MSC, ACM or PACS classification schemes. It is common to see codes from one or more of these schemes on individual papers.

==First-level areas==
The top level subjects under the MSC are, grouped here by common area names that are not part of the MSC,:

===General/foundations===
*00: General (Includes topics such as [[recreational mathematics]], [[philosophy of mathematics]] and [[mathematical model]]ing.)
*01: [[History of mathematics|History]] and [[List of mathematicians|biography]]
*03: [[Mathematical logic]] and [[Foundations of mathematics|foundations]], including [[model theory]], [[computability theory]], [[set theory]], [[proof theory]], and [[algebraic logic]]

===Discrete mathematics/algebra===
*05: [[Combinatorics]]
*06: [[Order theory]] 
*08: General [[algebraic system]]s
*11: [[Number theory]]
*12: [[Field theory (mathematics)|Field theory]] and [[polynomial]]s 
*13: [[Commutative ring]]s and [[commutative algebra|algebras]]
*14: [[Algebraic geometry]]
*15: [[Linear algebra|Linear]] and [[multilinear algebra]]; [[Matrix (mathematics)|matrix theory]] 
*16: [[Associative ring]]s and [[associative algebra]]s
*17: [[Non-associative ring]]s and [[non-associative algebra]]s
*18: [[Category theory]]; [[homological algebra]]
*19: [[K-theory]]
*20: [[Group theory]] and generalizations 
*22: [[Topological group]]s, [[Lie group]]s, and analysis upon them

===Analysis===
*26: [[Real function]]s, including [[derivative]]s and [[integral]]s
*28: [[Measure (mathematics)|Measure]] and [[Integral|integration]]
*30: [[Complex function]]s, including [[approximation theory]] in the [[complex number|complex domain]]
*31: [[Potential theory]] 
*32: [[Several complex variables]] and [[analytic space]]s
*33: [[Special functions]]
*34: [[Ordinary differential equation]]s 
*35: [[Partial differential equation]]s 
*37: [[Dynamical system]]s and [[ergodic theory]] 
*39: [[Difference equation]]s and [[functional equation]]s 
*40: [[Sequence]]s, [[series (mathematics)|series]], [[summability]] 
*41: [[Approximation theory|Approximations]] and [[Expansion (approximation theory)|expansions]]
*42: [[Harmonic analysis]], including [[Fourier analysis]], [[Fourier transform]]s, [[trigonometric approximation]], [[trigonometric interpolation]], and [[orthogonal function]]s
*43: Abstract [[harmonic analysis]] 
*44: [[Integral transform]]s, [[operational calculus]] 
*45: [[Integral equation]]s   
*46: [[Functional analysis]], including [[infinite-dimensional holomorphy]], [[integral transform]]s in [[distribution space]]s
*47: [[Operator theory]] 
*49: [[Calculus of variations]] and [[optimal control]]; [[Optimization (mathematics)|optimization]] (including [[geometric integration theory]])

===Geometry and topology===
*51: [[Geometry]] 
*52: [[Convex geometry]] and [[discrete geometry]]
*53: [[Differential geometry]] 
*54: [[General topology]] 
*55: [[Algebraic topology]] 
*57: [[Manifold]]s
*58: [[Global analysis]], [[analysis on manifolds]] (including [[infinite-dimensional holomorphy]])

===[[Applied mathematics]] / other===
*60 [[Probability theory]], [[stochastic processes]]
*62 [[Statistics]]
*65 [[Numerical analysis]]
*68 [[Computer science]]
*70 [[Mechanics]] (including [[particle mechanics]])
*74 [[Mechanics of deformable solids]] 
*76 [[Fluid mechanics]]
*78 [[Optics]], [[electromagnetic theory]] 
*80 Classical [[thermodynamics]], [[heat transfer]]
*81 [[Quantum mechanics|Quantum theory]]
*82 [[Statistical mechanics]], structure of matter
*83 [[Theory of relativity|Relativity]] and [[gravitational theory]], including [[relativistic mechanics]] 
*85 [[Astronomy]] and [[astrophysics]]
*86 [[Geophysics]]
*90 [[Operations research]], [[mathematical programming]] 
*91 [[Game theory]], [[mathematical economics|economics]], [[mathematical sociology|social]] and [[mathematical psychology|behavioral sciences]]
*92 [[Biology]] and other [[natural science]]s
*93 [[Systems theory]]; control, including [[optimal control]]
*94 [[Information]] and [[communication]], [[electrical network|circuits]]
*97 [[Mathematics education]]

==See also==
* [[Areas of mathematics]]
* [[Mathematical knowledge management]]

==Notes==

{{reflist}}

==External links==
*[http://msc2010.org/mscwiki/index.php?title=MSC2010 Mathematics Subject Classification 2010] The site where the MSC 2010 revision was carried out publicly in an MSCwiki.  A view of the whole scheme and the changes made from MSC2000, as well as PDF files of the MSC and ancillary documents are there.  A personal copy of the MSC in [[TiddlyWiki]] form can be had also. 
*The [[American Mathematical Society]] page on [http://www.ams.org/msc/ the Mathematics Subject Classification].  
*[http://www.math.niu.edu/~rusin/known-math/index/beginners.html Description] of the MSC by Dave Rusin.

[[Category:Fields of mathematics]]
[[Category:Classification systems]]</text>
      <sha1>mv4tg43tw9baovyeqi0khsmv36ikacn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Areas of mathematics</title>
    <ns>0</ns>
    <id>1337579</id>
    <revision>
      <id>611542272</id>
      <parentid>611542200</parentid>
      <timestamp>2014-06-04T15:40:44Z</timestamp>
      <contributor>
        <username>RC711</username>
        <id>21410501</id>
      </contributor>
      <minor/>
      <comment>/* Other mathematical sciences */ typo</comment>
      <text xml:space="preserve" bytes="19483">[[Mathematics]] has become a vastly diverse subject over [[History of mathematics|history]], and there is a corresponding need to categorize the different '''areas of mathematics'''. A number of different classification schemes have arisen, and though they share some similarities, there are differences due in part to the different purposes they serve. In addition, as mathematics evolves, these classification schemes must evolve as well to account for newly created areas or newly discovered links between different areas. Classification is made more difficult by some subjects, often the most active, which straddle the boundary between different areas.

A traditional division of mathematics is into [[pure mathematics]], mathematics studied for its intrinsic interest, and [[applied mathematics]], mathematics which can be directly applied to real world problems.&lt;ref&gt;For example the [[Encyclopædia Britannica Eleventh Edition]] groups its mathematics articles as Pure, Applied, and Biographies. See
[http://en.wikisource.org/w/index.php?title=User:Tim_Starling/ScanSet_PNG_demo&amp;vol=29&amp;page=ED9A945].&lt;/ref&gt;
This division is not always clear and many subjects have been developed as pure mathematics to find unexpected applications later on. Broad divisions, such as [[discrete mathematics]] and [[computational mathematics]], have emerged more recently.

==Classification systems==
*The [[Mathematics Subject Classification]] (MSC) is produced by the staff of the review databases [[Mathematical Reviews]] and [[Zentralblatt MATH]]. Many mathematics journals ask authors to label their papers with MSC subject codes. The MSC divides mathematics into over 60 areas, with further subdivisions within each area.
*In the [[Library of Congress Classification]], mathematics is assigned the subclass QA within the class Q (Science). The LCC defines [[Library of Congress Classification:Class Q -- Science#QA Mathematics|broad divisions]], and individual subjects are assigned specific numerical values.
*The [[List of Dewey Decimal classes#500 – Science|Dewey Decimal Classification]] assigns mathematics to division 510, with subdivisions for [[Algebra]] &amp; [[number theory]], [[Arithmetic]], [[Topology]], [[Analysis]], [[Geometry]], [[Numerical analysis]], and [[Probabilities]] &amp; [[applied mathematics]].
*The [http://arxiv.org/archive/math Categories within Mathematics] list is used by the [[Arxiv]] for categorising [[preprints]]. It is more modern than MSC and so includes things like [[quantum algebra]].
*The [[International Mathematical Union|IMU]] uses its [http://www.mathunion.org/activities/icm/icm-2010-program-structure/ programme structure] for organizing the lectures at its four-yearly [[International Congress of Mathematicians|ICM]]. One of its top-level sections that MSC doesn't have is [[Lie theory]].
*The [[ACM Computing Classification System]] includes a couple of mathematical [http://portal.acm.org/ccs.cfm?part=author&amp;coll=portal&amp;dl=GUIDE categories] F. Theory of Computation, and G. Mathematics of Computing.
*[[MathOverflow]] has a [http://mathoverflow.net/tags tag system].
*Mathematics book publishers such as [[Springer Science+Business Media|Springer]] ([http://www.springer.com/mathematics?SGWID=0-10042-0-0-0 subdisciplines]), [[Cambridge University Press|Cambridge]] ([http://www.cambridge.org/gb/knowledge/other_subject/item1521/?site_locale=en_GB Browse Mathematics and statistics]) and the [[American Mathematical Society|AMS]] ([http://www.ams.org/bookstore/textbooks subject area]) use their own subject lists on their websites to enable customers to browse books or filter searches by subdiscipline, including topics such as [[mathematical biology]] and [[mathematical finance]] as top-level headings.
*Schools and other educational bodies have [[syllabus]]es.
*Research institutes and university mathematics departments often have sub-departments or study groups. e.g. [[Society for Industrial and Applied Mathematics|SIAM]] has [http://siam.org/activity/ activity groups] for its members.
*Wikipedia uses a [[:Category:Mathematics]] system on its articles, and also has a [[list of mathematics lists]].

==Major divisions of mathematics==
===Foundations===
;[[Recreational mathematics]]: From [[magic square]]s to the [[Mandelbrot set]], numbers have been a source of amusement and delight for millions of people throughout the ages. Many important branches of &quot;serious&quot; mathematics have their roots in what was once a mere puzzle and/or game.
;[[History of mathematics|History]] and [[List of mathematicians|biography]]: The history of mathematics is inextricably intertwined with the subject itself. This is perfectly natural: mathematics has an internal organic structure, deriving new theorems from those that have come before. As each new generation of mathematicians builds upon the achievements of our ancestors, the subject itself expands and grows new layers, like an onion.
;[[Mathematical logic]] and [[Foundations of mathematics|foundations]], including [[set theory]]: Mathematicians have always worked with logic and symbols, but for centuries the underlying laws of logic were taken for granted, and never expressed symbolically. '''Mathematical logic''', also known as '''symbolic logic''', was developed when people finally realized that the tools of mathematics can be used to study the structure of logic itself. Areas of research in this field have expanded rapidly, and are usually subdivided into several distinct departments.
;;[[Model theory]]
:: Model theory studies mathematical [[structure (mathematical logic)|structures]] in a general framework. Its main tool is [[first-order logic]].
;;[[Set theory]]
:: A [[Set (mathematics)|set]] can be thought of as a collection of distinct things united by some common feature. Set theory is subdivided into three main areas. [[Naive set theory]] is the original set theory developed by mathematicians at the end of the 19th century. [[Axiomatic set theory]] is a rigorous [[axiom]]atic theory developed in response to the discovery of serious flaws (such as [[Russell's paradox]]) in naive set theory.  It treats sets as &quot;whatever satisfies the axioms&quot;, and the notion of collections of things serves only as motivation for the axioms. [[Internal set theory]] is an axiomatic extension of set theory that supports a [[logically consistent]] identification of ''illimited'' (enormously large) and ''infinitesimal'' (unimaginably small) elements within the [[real number]]s. See also [[List of set theory topics]].
;;[[Proof theory]] and [[constructive mathematics]]
:: '''Proof theory''' grew out of [[David Hilbert]]'s ambitious program to formalize all the proofs in mathematics. The most famous result in the field is encapsulated in [[Gödel's incompleteness theorems]]. A closely related and now quite popular concept is the idea of [[Turing machines]]. '''Constructivism''' is the outgrowth of [[L.E.J. Brouwer|Brouwer]]'s unorthodox view of the nature of logic itself; constructively speaking, mathematicians cannot assert &quot;Either a circle is round, or it is not&quot; until they have actually exhibited a circle and measured its roundness.
===Arithmetics===
The study of quantity.

===Algebra===
The study of structure begins with [[number]]s, first the familiar [[natural number]]s and [[integer]]s and their [[arithmetic]]al operations, which are recorded in [[elementary algebra]]. The deeper properties of these numbers are studied in [[number theory]]. The investigation of methods to solve equations leads to the field of [[abstract algebra]], which, among other things, studies [[ring (mathematics)|rings]] and [[field (mathematics)|field]]s, structures that generalize the properties possessed by everyday numbers. Long standing questions about [[compass and straightedge]] construction were finally settled by [[Galois theory]]. The physically important concept of [[vector (geometric)|vector]]s, generalized to [[vector space]]s, is studied in [[linear algebra]].

; [[Order theory]]: Any set of real numbers can be written out in ascending order. Order Theory extends this idea to sets in general. It includes notions like [[lattice (order)|lattices]] and ordered [[algebraic structure]]s. See also the [[order theory glossary]] and the [[list of order topics]].

; General [[algebraic system]]s: Given a [[Set (mathematics)|set]], different ways of combining or relating members of that set can be defined. If these obey certain rules, then a particular algebraic structure is formed. [[Universal algebra]] is the more formal study of these structures and systems.

; [[Number theory]]: Number theory is traditionally concerned with the properties of integers. More recently, it has come to be concerned with wider classes of problems that have arisen naturally from the study of integers. It can be divided into elementary number theory (where the integers are studied without the aid of techniques from other mathematical fields); [[analytic number theory]] (where [[calculus]] and [[complex analysis]] are used as tools); [[algebraic number theory]] (which studies the algebraic numbers  - the roots of [[polynomial]]s with integer [[coefficient]]s); [[geometric number theory]]; [[combinatorial number theory]]; [[Transcendence theory|transcendental number theory]]; and [[computational number theory]]. See also the [[list of number theory topics]].

; [[Field theory (mathematics)|Field theory]] and polynomials: Field theory studies the properties of [[Field (mathematics)|fields]]. A field is a mathematical entity for which addition, subtraction, multiplication and division are [[well-defined]].  A polynomial is an expression in which constants and variables are combined using only addition, subtraction, and multiplication.

; [[Commutative ring]]s and [[commutative algebra|algebras]]: In [[ring theory]], a branch of abstract algebra, a commutative ring is a ring in which the multiplication operation obeys the [[commutative operation|commutative law]]. This means that if a and b are any elements of the ring, then a×b=b×a.  Commutative algebra is the field of study of commutative rings and their ideals, modules and algebras.  It is foundational both for [[algebraic geometry]] and for algebraic number theory. The most prominent examples of commutative rings are [[polynomial ring|rings of polynomials]].
===Analysis===
Within the world of mathematics, '''[[Mathematical analysis|analysis]]''' is the branch that focuses on change: [[Derivative|rates of change]], [[Integral|accumulated change]], and multiple things changing relative to (or independently of) one another.

Modern analysis is a vast and rapidly expanding branch of mathematics that touches almost every other subdivision of the discipline, finding direct and indirect applications in topics as diverse as [[number theory]], [[cryptography]], and [[abstract algebra]]. It is also the language of science itself and is used across [[chemistry]], [[biology]], and [[physics]], from [[astrophysics]] to [[X-ray crystallography]].
&lt;!--
;[[Dynamical system]]s: The study of the solutions to the [[equations of motion]] of systems that are primarily [[mechanics|mechanical]] in nature; although this ranges from [[planetary orbit]]s through the behaviour of [[electronic circuit]]s to the solutions of [[partial differential equation]]s that arise in biology.  Much of modern research is focused on the study of [[chaotic system]]s. See also the [[list of dynamical system topics]] --&gt;

===Combinatorics===
[[Combinatorics]] is the study of finite or discrete collections of objects that satisfy specified criteria. In particular, it is concerned with &quot;counting&quot; the objects in those collections ([[enumerative combinatorics]]) and with deciding whether certain &quot;optimal&quot; objects exist ([[extremal combinatorics]]). It includes [[graph theory]], used to describe inter-connected objects (a graph in this sense is a network, or collection of connected points). See also the [[list of combinatorics topics]], [[list of graph theory topics]] and [[glossary of graph theory]].  A ''combinatorial flavour'' is present in many parts of [[problem-solving]].

===Geometry and topology===
[[Geometry]] deals with spatial relationships, using fundamental qualities or [[axiom]]s. Such axioms can be used in conjunction with mathematical definitions for points, straight lines, curves, surfaces, and solids to draw logical conclusions.  See also [[List of geometry topics]]

;[[Convex geometry]] and [[discrete geometry]]: Includes the study of objects such as [[polytopes]] and [[polyhedra]]. See also [[List of convexity topics]]

;Discrete or [[combinatorial geometry]]: The study of geometrical objects and properties that are [[discrete mathematics|discrete]] or [[combinatorial]], either by their nature or by their representation. It includes the study of shapes such as the [[Platonic solids]] and the notion of [[tessellation]].

;[[Differential geometry]]: The study of geometry using calculus, and is very closely related to [[differential topology]]. Covers such areas as [[Riemannian geometry]], [[curvature]] and [[differential geometry of curves]]. See also the [[glossary of differential geometry and topology]].

;[[Algebraic geometry]]: Given a [[polynomial]] of two real [[Variable (mathematics)|variables]], then the points on a plane where that function is zero will form a curve. An [[algebraic curve]] extends this notion to polynomials over a [[field (mathematics)|field]] in a given number of variables. Algebraic geometry may be viewed as the study of these curves. See also the [[list of algebraic geometry topics]] and [[list of algebraic surfaces]].

;[[Topology]]: Deals with the properties of a figure that do not change when the figure is continuously deformed. The main areas are point set topology (or [[general topology]]), [[algebraic topology]], and the topology of [[manifold]]s, defined below.

;[[General topology]]: Also called ''point set topology''. Properties of [[topological space]]s. Includes such notions as [[open set|open]] and [[closed set|closed]] [[Set (mathematics)|sets]], [[compact space]]s, [[continuous function]]s, [[limit of a sequence|convergence]], [[separation axiom]]s, [[metric space]]s, [[dimension theory]].  See also the [[glossary of general topology]] and the [[list of general topology topics]].

;[[Algebraic topology]]: Properties of algebraic objects associated with a topological space and how these algebraic objects capture properties of such spaces.  Contains areas like [[homology theory]], [[cohomology theory]], [[homotopy theory]], and [[homological algebra]], some of them examples of [[functor]]s.  Homotopy deals with [[homotopy group]]s (including the [[fundamental group]]) as well as [[simplicial complexes]] and [[CW complexes]] (also called ''cell complexes'').  See also the [[list of algebraic topology topics]].

;[[Manifold]]s: A manifold can be thought of as an ''n''-[[dimension]]al generalization of a [[surface]] in the usual 3-dimensional [[Euclidean space]]. The study of manifolds includes  differential topology, which looks at the properties of differentiable functions defined over a manifold.  See also [[complex manifold]]s.

===Applied mathematics===
====Probability and statistics====
See also [[glossary of probability and statistics]]
*[[Probability theory]]: The mathematical theory of [[Statistical randomness|random]] phenomena. Probability theory studies [[random variable]]s and [[event (probability theory)|event]]s, which are mathematical abstractions of [[determinism|non-deterministic]] events or measured quantities. See also [[:Category:probability theory]], and the [[list of probability topics]].
**[[Stochastic process]]es: An extension of probability theory that studies collections of random variables, such as [[time series]] or [[random field|spatial processes]]. See also [[List of stochastic processes topics]], and [[:Category:Stochastic processes]].

*[[Statistics]]: The science of making effective use of numerical [[data]] from experiments or from populations of individuals. Statistics includes not only the collection, analysis and interpretation of such data, but also the planning of the collection of data, in terms of the design of [[Survey sampling|surveys]] and [[experimental design|experiments]]. See also the [[list of statistical topics]] and [[:Category:Statistics]].
&lt;!-- &lt;ref&gt;Dodge, Y. (2003) ''The Oxford Dictionary of Statistical Terms'', OUP. ISBN 0-19-920613-9&lt;/ref&gt; --&gt;

====Computational sciences====
;[[Numerical analysis]]: Many problems in mathematics cannot in general be solved exactly. Numerical analysis is the study of [[iterative method]]s and [[algorithms]] for approximately solving problems to a specified error bound. Includes [[numerical differentiation]], [[numerical integration]] and [[numerical methods]]; c.f. [[scientific computing]]. See also [[List of numerical analysis topics]]
;[[Computer algebra]]: This area is also called '''symbolic computation''' or '''algebraic computation'''. It deals with exact computation, for example with integers of arbitrary size, polynomials or elements of finite fields. It includes also the computation with non numeric mathematical objects like polynomial [[ideal (ring theory)|ideals]] or series.

====Physical sciences====
;[[Mechanics]]: Addresses what happens when a real physical object is subjected to forces. This divides naturally into the study of rigid solids, deformable solids, and fluids, detailed below.

;[[Particle mechanics]]: In mathematics, a particle is a [[point-like]], perfectly rigid, solid object. Particle mechanics deals with the results of subjecting particles to forces. It includes [[celestial mechanics]]—the study of the motion of celestial objects.

;[[Mechanics of deformable solids]]: Most real-world objects are not point-like nor perfectly rigid. More importantly, objects change shape when subjected to forces. This subject has a very strong overlap with [[continuum mechanics]], which is concerned with continuous matter. It deals with such notions as [[stress (physics)|stress]], [[Strain (materials science)|strain]] and [[Elasticity (physics)|elasticity]]. See also [[continuum mechanics]].

;[[Fluid mechanics]]: [[Fluid]]s in this sense includes not just [[liquid]]s, but flowing [[gas]]es, and even [[solid]]s under certain situations. (For example, dry [[sand]] can behave like a fluid). It includes such notions as [[viscosity]], [[turbulent flow]] and [[laminar flow]] (its opposite).  See also [[fluid dynamics]].

====Other mathematical sciences====
*[[Operations research]](OR), also known as operational research, provides optimal or near-optimal solutions to complex problems. OR uses [[mathematical model]]ing, [[statistics|statistical analysis]], and [[mathematical optimization]].
*[[Mathematical programming]] (or mathematical optimization) minimizes (or maximizes) a real-valued function over a domain that is often specified by constraints on the variables. Mathematical programming studies these problems and develops [[iterative method]]s and [[algorithm]]s for their solution.

==See also==

*[[Glossary of areas of mathematics]]
*[[Outline of mathematics]]

==Notes==
{{reflist}}

== External links ==
*[http://www.math.niu.edu/~rusin/known-math/index/tour_div.html The Divisions of Mathematics]


{{Areas of mathematics}}

{{DEFAULTSORT:Areas Of Mathematics}}
[[Category:Fields of mathematics| ]]</text>
      <sha1>nawwwoy4z7ti0oqxyqiirfbqpblb859</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Outline of mathematics</title>
    <ns>0</ns>
    <id>33543938</id>
    <revision>
      <id>611160303</id>
      <parentid>605720855</parentid>
      <timestamp>2014-06-02T00:30:43Z</timestamp>
      <contributor>
        <username>Sardanaphalus</username>
        <id>427947</id>
      </contributor>
      <comment>/* External links */ template names, category, indices</comment>
      <text xml:space="preserve" bytes="5823">{{MathTopicTOC}}
The following outline is provided as an overview of and topical guide to mathematics:

'''[[Mathematics]]''' – the search for fundamental truths in pattern, quantity, and change. For more on the relationship between [[mathematics]] and [[science]], refer to the article on [[science#Mathematics|science]].

==Nature of mathematics==
*[[Definitions of mathematics]] – Mathematics has no generally accepted definition. Different schools of thought, particularly in philosophy, have put forth radically different definitions, all of which are controversial.
*[[Philosophy of mathematics]] – its aim is to provide an account of the nature and methodology of mathematics and to understand the place of mathematics in people's lives.

===Mathematics is===
*an [[academic discipline]] – branch of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part), and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties to which their practitioners belong.
*a [[formal science]] – branch of knowledge concerned with the properties of formal systems based on definitions and rules of inference. Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world.

===General reference===
====Classification systems====
*[[List of Dewey Decimal Classes#500-599 – Science|Mathematics in the Dewey Decimal Classification system]]
&lt;!--*[[Mathematics in the Library of Congress Classification system]]--&gt;
*''[[Mathematics Subject Classification]]'' – alphanumerical classification scheme collaboratively produced by staff of and based on the coverage of the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH.

====Reference databases====
*''[[Mathematical Reviews]]'' – journal and online database published by the American Mathematical Society (AMS) that contains brief synopses (and occasionally evaluations) of many articles in mathematics, statistics and theoretical computer science.
*''[[Zentralblatt MATH]]'' – service providing reviews and abstracts for articles in pure and applied mathematics, published by Springer Science+Business Media. It is a major international reviewing service which covers the entire field of mathematics. It uses the Mathematics Subject Classification codes for organizing their reviews by topic.

==Subjects==
===Quantity===
[[Mathematics#Quantity|Quantity]] –
*[[Arithmetic]] –
*[[Natural number]]s –
*[[Integer]]s –
*[[Rational number]]s –
*[[Real number]]s –
*[[Complex number]]s –
*[[Hypercomplex number]]s –
*[[Infinity]] –

===Structure===
[[Mathematics#Structure|Structure]] –
*[[Abstract algebra]] –
*[[Linear algebra]] –
*[[Number theory]] –
*[[Order theory]] –
*[[Function (mathematics)]] –

===Space===
[[Mathematics#Space|Space]] –
*[[Geometry]] –
*[[Algebraic geometry]] –
*[[Trigonometry]] –
*[[Differential geometry]] –
*[[Topology]] –
*[[Fractal geometry]] –

===Change===
[[Mathematics#Change|Change]] –
*[[Calculus]] –
*[[Vector calculus]] –
*[[Differential equation]]s –
*[[Dynamical system]]s –
*[[Chaos theory]] –
*[[Mathematical analysis|Analysis]] –

===Foundations and philosophy===
[[Foundations of mathematics]] –
*[[Philosophy of mathematics]] –
*[[Category theory]] –
*[[Set theory]] –
*[[Type theory]] –

===Mathematical logic===
[[Mathematical logic]] –
*[[Model theory]] –
*[[Proof theory]] –
*[[Recursion theory]] –
*[[Set theory]] –
*[[Type theory]] –

===Discrete mathematics===
[[Discrete mathematics]] –
*[[Combinatorics]]
*[[Theory of computation]]
*[[Cryptography]]
*[[Graph theory]]

===Applied mathematics===
[[Applied mathematics]] –
*[[Mathematical physics]] –
*[[Mechanics|Analytical mechanics]] –
*[[Fluid mechanics|Mathematical fluid dynamics]] –
*[[Numerical analysis]] –
*[[Mathematical optimization]] –
*[[Probability]] –
*[[Physics]] –
*[[Statistics]] –
*[[Mathematical economics]] –
*[[Financial mathematics]] –
*[[Game theory]] –
*[[Mathematical biology]] –
*[[Cryptography]] –
*[[Operations research]] –
*[[Information theory]] –
*[[Control theory]] –
*[[Dynamical system]]s –

==History==
{{main|History of mathematics}}
*[[Babylonian mathematics]]
*[[Egyptian mathematics]]
*[[Indian mathematics]]
*[[Greek mathematics]]
*[[Chinese mathematics]]
**[[Abacus]]
*[[History of the Hindu-Arabic numeral system]]
*[[Islamic mathematics]]
*[[Japanese mathematics]]
*[[History of algebra]]
*[[History of geometry]]
*[[History of mathematical notation]]
*[[History of trigonometry]]
*[[History of writing numbers]]

==Psychology==
*[[Mathematics education]]
*[[Numeracy]]
*[[Numerical Cognition]]
*[[Subitizing]]
*[[Mathematical anxiety]] 
*[[Dyscalculia]]
*[[Acalculia]]
*[[Ageometresia]]
*[[Number sense]]
*[[Numerosity adaptation effect]]
*[[Approximate number system]]
*[[Mathematical maturity]]

==Influential mathematicians==
See [[:Category:Lists of mathematicians|Lists of mathematicians]]

==See also==
*[[Lists of mathematics topics]]
*[[Areas of mathematics]]
*[[Glossary of areas of mathematics]]

== External links ==
{{Sister project links|Mathematics}}
*[http://www.maa.org/BLL/toc.htm MAA Reviews – The Basic Library List – Mathematical Association of America]
*[http://www.math.ucdavis.edu/~saito/books.html Naoki's Recommended Books, compiled by Naoki Saito, U. C. Davis]
*[http://www.math.cornell.edu/~hatcher/Other/topologybooks.pdf A List of Recommended Books in Topology, compiled by Allen Hatcher, Cornell U.]


{{Areas of mathematics}}
{{Outlines}}

[[Category:Mathematics| ]]
[[Category:Fields of mathematics| ]]
[[Category:Outlines|Mathematics]]</text>
      <sha1>q01ylbxo4hfkbtm8rhw38bwq9mah0tm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Canonical form</title>
    <ns>0</ns>
    <id>515096</id>
    <revision>
      <id>597896816</id>
      <parentid>597879438</parentid>
      <timestamp>2014-03-03T01:56:36Z</timestamp>
      <contributor>
        <username>Anita5192</username>
        <id>13220696</id>
      </contributor>
      <comment>/* References */ Separated notes and references into two sections.</comment>
      <text xml:space="preserve" bytes="9392">{{refimprove|date=December 2007}}
{{hatnote|For other senses of &quot;canonical&quot; in mathematics, see [[Canonical (disambiguation)#Mathematics]]}}
In [[mathematics]] and [[computer science]], a '''canonical''', '''normal''', or '''standard'''  '''form''' of a [[mathematical object]] is a standard way of presenting that object as a [[mathematical expression]]. The distinction between &quot;canonical&quot; and &quot;normal&quot; forms varies by subfield. In most fields, a canonical form specifies a ''unique'' representation for every object, while a normal form simply specifies its form, without the requirement of uniqueness.

The canonical form of a [[positive integer]] in [[decimal representation]] is a finite sequence of digits that does not begin with zero.

More generally, for a class of objects on which an [[equivalence relation]] (which can differ from standard notions of equality, for instance by considering different forms of equal objects to be nonequivalent) is defined, a '''canonical form''' consists in the choice of a specific object in each class. For example, [[row echelon form]] and [[Jordan normal form]] are canonical forms for matrices.

In [[computer science]], and more specifically in [[computer algebra]], when representing mathematical objects in a computer, there are usually many different ways to represent the same object. In this context, a '''canonical form''' is a representation such that every object has a unique representation. Thus, the equality of two objects can easily be tested by testing the equality of their canonical forms. However canonical forms frequently depend on arbitrary choices (like ordering the variables), and this introduces difficulties for testing the equality of two objects resulting on independent computations. Therefore, in computer algebra, ''normal form'' is a weaker notion: A '''normal form''' is a representation such that zero is uniquely represented. This allows to test equality by putting the difference of two objects in normal form (see [[Computer algebra#Equality]]).

'''Canonical form''' can also mean a [[differential form]] that is defined in a natural (canonical) way; [[#Differential forms|see below]].

In [[computer science]], data that has more than one possible representation can often be canonicalized into a completely unique representation called its '''canonical form'''.  Putting something into canonical form is [[canonicalization]].&lt;ref&gt;The term 'canonization' is sometimes incorrectly used for this.&lt;/ref&gt;

==Definition==
Suppose we have some set ''S'' of objects, with an [[equivalence relation]]. A '''canonical form''' is given by designating some objects of ''S'' to be &quot;in canonical form&quot;, such that every object under consideration is equivalent to exactly one object in canonical form. In other words, the canonical forms in ''S'' represent the equivalence classes, once and only once. To test whether two objects are equivalent, it then suffices to test their canonical forms for equality.
A canonical form thus provides a [[classification theorem]] and more, in that it not just classifies every class, but gives a distinguished (canonical) representative.

In practical terms, one wants to be able to recognize the canonical forms. There is also a practical, algorithmic question to consider: how to pass from a given object ''s'' in ''S'' to its canonical form ''s''*? Canonical forms are generally used to make operating with equivalence classes more effective. For example in [[modular arithmetic]], the canonical form for a residue class is usually taken as the least non-negative integer in it. Operations on classes are carried out by combining these representatives and then reducing the result to its least non-negative residue.
The uniqueness requirement is sometimes relaxed, allowing the forms to be unique up to some finer equivalence relation, like allowing reordering of terms (if there is no natural ordering on terms).

A canonical form may simply be a convention, or a deep theorem.

For example, polynomials are conventionally written with the terms in descending powers: it is more usual to write ''x''&lt;sup&gt;2&lt;/sup&gt; + ''x'' + 30 than ''x'' + 30 + ''x''&lt;sup&gt;2&lt;/sup&gt;, although the two forms define the same polynomial. By contrast, the existence of [[Jordan canonical form]] for a matrix is a deep theorem.

==Examples==
Note: in this section, &quot;[[up to]]&quot; some equivalence relation E means that the canonical form is not unique in general, but that if one object has two different canonical forms, they are E-equivalent.

=== Linear algebra ===
{| class=&quot;wikitable&quot;
|-
! Objects
! ''A'' is equivalent to ''B'' if:
! Normal form
! Notes
|- 
| [[Normal matrix|Normal]] matrices over the [[complex numbers]]
| &lt;math&gt;A=U^* B U&lt;/math&gt; for some [[unitary matrix]] ''U'' 
| [[Diagonal matrices]] (up to reordering)
| This is the [[Spectral theorem]]
|- 
| Matrices over the complex numbers
| &lt;math&gt;A=U B V^*&lt;/math&gt; for some [[unitary matrix|unitary matrices]] ''U'' and ''V''
| Diagonal matrices with real positive entries (in descending order)
| [[Singular value decomposition]] 
|- 
| Matrices over an [[algebraically closed field]]
| &lt;math&gt;A=P^{-1} B P&lt;/math&gt; for some [[invertible]] matrix ''P'' 
| [[Jordan normal form]] (up to reordering of blocks)
|- 
| Matrices over an [[algebraically closed field]]
| &lt;math&gt;A=P^{-1} B P&lt;/math&gt; for some [[invertible]] matrix ''P'' 
| [[Weyr canonical form]] (up to reordering of blocks)
|-
| Matrices over a field
| &lt;math&gt;A=P^{-1} B P&lt;/math&gt; for some [[invertible]] matrix ''P''
| [[Frobenius normal form]]
|-
| Matrices over a [[principal ideal domain]]
| &lt;math&gt;A=P^{-1} B Q&lt;/math&gt; for some [[invertible]] Matrices ''P'' and ''Q''
| [[Smith normal form]]
| The equivalence is the same as allowing invertible elementary row and column transformations
|- 
| Finite-dimensional vector spaces over a field ''K''
| ''A'' and ''B'' are isomorphic as vector spaces
| &lt;math&gt;K^n&lt;/math&gt;, ''n'' a non-negative integer
|}

=== Classical logic ===
{{main|Canonical form (Boolean algebra)}}
* [[Negation normal form]]
* [[Conjunctive normal form]]
* [[Disjunctive normal form]]
* [[Algebraic normal form]]
* [[Prenex normal form]]
* [[Skolem normal form]]

=== Functional analysis ===
{| class=&quot;wikitable&quot;
|-
! Objects
! ''A'' is equivalent to ''B'' if:
! Normal form
|-
| [[Hilbert spaces]]
| ''A'' and ''B'' are isometrically isomorphic as Hilbert spaces
| &lt;math&gt;\ell^2(I)&lt;/math&gt; [[Hilbert space#Sequence spaces|sequence spaces]] (up to exchanging the index set ''I'' with another index set of the same [[cardinality]])
|- 
&lt;!-- please double-check this one --&gt;
| Commutative &lt;math&gt;C^*&lt;/math&gt;-algebras with unit
| ''A'' and ''B'' are isomorphic as &lt;math&gt;C^*&lt;/math&gt;-algebras
| The algebra &lt;math&gt;C(X)&lt;/math&gt; of continuous functions on a [[compact space|compact]] [[Hausdorff space]], up to [[homeomorphism]] of the base space.
|}

=== Number theory ===
* [[canonical representation of a positive integer]]
* canonical form of a [[continued fraction]]

=== Algebra ===
{| class=&quot;wikitable&quot;
|-
! Objects
! ''A'' is equivalent to ''B'' if:
! Normal form
|-
| Finitely generated ''R''-modules with ''R'' a [[principal ideal domain]]
| ''A'' and ''B'' are isomorphic as ''R''-modules
| [[Structure theorem for finitely generated modules over a principal ideal domain|Primary decomposition (up to reordering) or invariant factor decomposition]]
|}

=== Geometry ===
*The equation of a line: ''Ax''&amp;nbsp;+&amp;nbsp;''By''&amp;nbsp;=&amp;nbsp;''C'', with  ''A&lt;sup&gt;2&lt;/sup&gt;''&amp;nbsp;+&amp;nbsp;''B''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1 and ''C''&amp;nbsp;≥&amp;nbsp;0

*The equation of a circle: &lt;math&gt;(x - h)^2 + (y - k)^2 = r^2\,&lt;/math&gt;

By contrast, there are alternative forms for writing equations. For example, the equation of a line may be written as a [[linear equation]] in point-slope and slope-intercept form.

=== Mathematical notation ===

Standard form is used by many mathematicians and scientists to write extremely [[large numbers#Standardized system of writing very large numbers|large numbers]] in a more concise and understandable way.

=== Set theory ===
* [[Cantor normal form#Cantor normal form|Cantor normal form]] of an [[ordinal number]]

=== Game theory ===
* [[Normal form game]]

=== Proof theory ===
* [[Normal form (natural deduction)]]

===Rewriting systems===
* In an [[abstract rewriting system]] a normal form is an irreducible object.
=== Lambda calculus===
* [[Beta normal form]] if no beta reduction is possible; Lambda calculus is a particular case of an abstract rewriting system.

=== Dynamical systems ===
* [[Normal form (bifurcation theory)|Normal form of a bifurcation]]

===Graph theory===
{{main|Graph canonization}}

===Differential forms===
Canonical [[differential form]]s include the [[canonical one-form]] and [[canonical symplectic form]], important in the study of [[Hamiltonian mechanics]] and [[symplectic manifold]]s.

=== Computation ===
* [[Data normalization]]

==See also==
* [[Canonical class]]
* [[Normalization (disambiguation)]]
* [[Standardization]]

==Notes==
&lt;references/&gt;

==References==
*{{citation | last=Shilov | first=Georgi E. | title=Linear Algebra | editor-last=Silverman | editor-first=Richard A. | date=1977 | publisher=Dover | isbn=0-486-63518-X | year=1977}}.

[[Category:Algebra]]
[[Category:Concepts in logic]]
[[Category:Mathematical terminology]]
[[Category:Formalism (deductive)]]

[[de:Normalform]]
[[nl:Normaalvorm]]</text>
      <sha1>b9kt2jj0tf3ktymyct46tmyikgjopfy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Indeterminate (variable)</title>
    <ns>0</ns>
    <id>1460235</id>
    <revision>
      <id>611089226</id>
      <parentid>610644354</parentid>
      <timestamp>2014-06-01T15:24:15Z</timestamp>
      <contributor>
        <ip>216.239.78.22</ip>
      </contributor>
      <comment>I improved the grammar.</comment>
      <text xml:space="preserve" bytes="2748">In [[mathematics]], and particularly in formal [[algebra]], an '''indeterminate''' or '''indeterminant''' is a symbol that is treated as a variable, but does not stand for anything else but itself and is used as a placeholder in objects such as [[polynomial]]s and [[formal power series]]. In particular it does not designate a constant or a [[parameter]] of the problem, it is not an unknown that could be solved for, and it is not a [[variable (mathematics)|variable]] designating a function argument or being summed or integrated over; it is not any type of [[bound variable]].

==Polynomials==
A polynomial in an indeterminate ''X'' is an expression of the form &lt;math&gt;a_0 + a_1X + a_2X^2 + \ldots + a_nX^n&lt;/math&gt;, where the ''a''&lt;sub&gt;''i''&lt;/sub&gt; are called the [[coefficient]]s of the polynomial. Two such polynomials are equal only if the corresponding coefficients are equal.&lt;ref&gt;Herstein, Section 3.9&lt;/ref&gt; In contrast, two polynomial functions in a variable ''x'' may be equal or not depending on the value of ''x''. 

For example, the functions
:&lt;math&gt;f(x)=2+3x, g(x)=5+2x&lt;/math&gt;
are equal when ''x''=3 and not equal otherwise. But the two polynomials 
:&lt;math&gt;2+3X, 5+2X&lt;/math&gt;
are unequal since 2 does not equal 5 and 3 does not equal 2. In fact 
:&lt;math&gt;2+3X=a+bX \,&lt;/math&gt;,
does not hold ''unless'' ''a''&amp;nbsp;=&amp;nbsp;2 and ''b''&amp;nbsp;=&amp;nbsp;3. This is because ''X'' is not, and does not designate, a number.

The distinction is subtle since a polynomial in ''X'' can be changed to a function in ''x'' by substitution. But the distinction is important because information may be lost when this substitution is made. Working in [[Modular arithmetic|modulo 2]]:
:&lt;math&gt;0-0^2=0,\ 1-1^2=0,\,&lt;/math&gt;
so the polynomial function ''x''−''x''&lt;sup&gt;2&lt;/sup&gt; is identically equal to 0 for ''x'' having any value in the modulo 2 system. But the polynomial ''X''-''X''&lt;sup&gt;2&lt;/sup&gt; is not the zero polynomial since the coefficients, 0, 1 and −1, are not all zero.

==Formal power series==
A formal power series in an indeterminate X is an expression of the form ''a''&lt;sub&gt;0&lt;/sub&gt;+''a''&lt;sub&gt;1&lt;/sub&gt;''X''+''a''&lt;sub&gt;2&lt;/sub&gt;''X''&lt;sup&gt;2&lt;/sup&gt;+…. This is similar to the definition of a polynomial except that an infinite number of the coefficients may be nonzero. Unlike the [[power series]] encountered in calculus, questions of convergence are irrelevant. So power series that would diverge for values of ''x'', such as 1+''x''+2''x''&lt;sup&gt;2&lt;/sup&gt;+6''x''&lt;sup&gt;3&lt;/sup&gt;+…+''n''!''x''&lt;sup&gt;''n''&lt;/sup&gt;+…, are allowed.

==See also==
*[[Indeterminate system]]
*[[Polynomial]]
*[[Formal power series]]

==References==
{{PlanetMath attribution|title=indeterminate|id=6444}}
{{reflist}}

*I.N. Herstein ''Topics in Algebra'', Wiley (1975)
[[Category:Algebra]]</text>
      <sha1>dfug9cty6mhz1zsa4a860996zvc2bc3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Topological module</title>
    <ns>0</ns>
    <id>1616141</id>
    <revision>
      <id>507832383</id>
      <parentid>507830451</parentid>
      <timestamp>2012-08-17T13:05:47Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Unreferenced}}</comment>
      <text xml:space="preserve" bytes="1353">{{unreferenced|date=August 2012}}
In [[mathematics]], a '''topological module''' is a [[module (algebra)|module]] over a [[topological ring]] such that [[scalar multiplication]] and addition are [[continuous function (topology)|continuous]].

== Examples ==

A [[topological vector space]] is a topological module over a [[topological field]].

An [[abelian group|abelian]] [[topological group]] can be considered as a topological module over '''Z''', where '''Z''' is the [[ring of integers]] with the [[discrete topology]].

A topological ring is a topological module over each of its [[subring]]s.

A more complicated example is the ''I''-adic topology on a ring and its modules. Let ''I'' be an [[ideal (ring theory)|ideal]] of a ring ''R''. The sets of the form {{nowrap|''x'' + ''I''&lt;sup&gt;''n''&lt;/sup&gt;}}, for all ''x'' in ''R'' and all positive integers ''n'', form a [[base (topology)|base]] for a topology on ''R'' that makes ''R'' into a topological ring. Then for any left ''R''-module ''M'', the sets of the form {{nowrap|''x'' + ''I''&lt;sup&gt;''n''&lt;/sup&gt;''M''}}, for all ''x'' in ''M'' and all positive integers ''n'', form a base for a topology on ''M'' that makes ''M'' into a topological module over the topological ring ''R''.

== See also ==
*[[Linear topology]]

[[Category:Algebra]]
[[Category:Topology]]

{{topology-stub}}
{{algebra-stub}}</text>
      <sha1>o03mxhe3estjxfgwmqfnvd05kln2a0d</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Inverse element</title>
    <ns>0</ns>
    <id>45199</id>
    <revision>
      <id>578016331</id>
      <parentid>574022951</parentid>
      <timestamp>2013-10-20T19:54:12Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* See also */ [[Latin square property]]</comment>
      <text xml:space="preserve" bytes="13939">In [[abstract algebra]], the idea of an '''inverse element''' generalises concepts of a [[additive inverse|negation (sign reversal)]] in relation to [[addition]], and a [[Multiplicative inverse|reciprocal]] in relation to [[multiplication]]. The intuition is of an element that can 'undo' the effect of combination with another given element. While the precise definition of an inverse element varies depending on the algebraic structure involved, these definitions coincide in a [[Group (mathematics)|group]].

The word 'inverse' is derived from {{lang-la|[[wikt:inversus|inversus]]}} that means 'turned upside down', 'overturned'.

== Formal definitions ==
=== In a unital magma ===
Let &lt;math&gt;S&lt;/math&gt; be a [[Set (mathematics)|set]] with a [[binary operation]] &lt;math&gt;*&lt;/math&gt; (i.e., a [[Magma (algebra)|magma]]). If &lt;math&gt;e&lt;/math&gt; is an [[identity element]] of &lt;math&gt;(S,*)&lt;/math&gt; (i.e., ''S'' is a unital magma) and &lt;math&gt;a*b=e&lt;/math&gt;, then &lt;math&gt;a&lt;/math&gt; is called a '''left inverse''' of &lt;math&gt;b&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; is called a '''right inverse''' of &lt;math&gt;a&lt;/math&gt;. If an element &lt;math&gt;x&lt;/math&gt; is both a left inverse and a right inverse of &lt;math&gt;y&lt;/math&gt;, then &lt;math&gt;x&lt;/math&gt; is called a '''two-sided inverse''', or simply an '''inverse''', of &lt;math&gt;y&lt;/math&gt;. An element with a two-sided inverse in &lt;math&gt;S&lt;/math&gt; is called '''invertible''' in &lt;math&gt;S&lt;/math&gt;. An element with an inverse element only on one side is '''left invertible''', resp. '''right invertible'''. If all elements in ''S'' are invertible, ''S'' is called a [[Quasigroup|loop]].

Just like &lt;math&gt;(S,*)&lt;/math&gt; can have several left identities or several right identities, it is possible for an element to have several left inverses or several right inverses (but note that their definition above uses a ''two-sided'' identity &lt;math&gt;e&lt;/math&gt;). It can even have several left inverses ''and'' several right inverses.

If the operation &lt;math&gt;*&lt;/math&gt; is [[associative]] then if an element has both a left inverse and a right inverse, they are equal. In other words, in a [[monoid]] every element has at most one inverse (as defined in this section). In a monoid, the set of (left and right) invertible elements is a [[group (mathematics)|group]], called the [[group of units]] of &lt;math&gt;S&lt;/math&gt;, and denoted by &lt;math&gt;U(S)&lt;/math&gt; or ''H''&lt;sub&gt;1&lt;/sub&gt;.

A left-invertible element is left-[[cancellation property|cancellative]], and analogously for right and two-sided.

=== In a semigroup ===
{{main|Regular semigroup}}
The definition in the previous section generalizes the notion of inverse in group relative to the notion of identity. It's also possible, albeit less obvious, to generalize the notion of an inverse by dropping the identity element but keeping associativity, i.e. in a [[semigroup]].

In a semigroup &lt;math&gt;S&lt;/math&gt; an element ''x'' is called '''(von Neumann) regular''' if there exists some element ''z'' in S such that ''xzx'' = ''x''; ''z'' is sometimes called a '''pseudoinverse'''. An element ''y'' is called (simply) an '''inverse''' of ''x'' if ''xyx'' = ''x'' and ''y'' = ''yxy''. Every regular element has at least one inverse: if ''x'' = ''xzx'' then it is easy to verify that ''y'' = ''zxz'' is an inverse of ''x'' as defined in this section. Another easy to prove fact: if ''y'' is an inverse of ''x'' then ''e'' = ''xy'' and ''f'' = ''yx'' are [[idempotent element|idempotent]]s, that is ''ee'' = ''e'' and ''ff'' = ''f''. Thus, every pair of (mutually) inverse elements gives rise to two idempotents, and ''ex'' = ''xf'' = ''x'', ''ye'' = ''fy'' = ''y'', and ''e'' acts as a left identity on ''x'', while ''f'' acts a right identity, and the left/right roles are reversed for ''y''. This simple observation can be generalized using [[Green's relations]]: every idempotent ''e'' in an arbitrary semigroup is a left identity for ''R&lt;sub&gt;e&lt;/sub&gt;'' and right identity for ''L&lt;sub&gt;e&lt;/sub&gt;''.&lt;ref&gt;Howie, prop. 2.3.3, p. 51&lt;/ref&gt; An intuitive description of this is fact is that every pair of mutually inverse elements produces a local left identity, and respectively, a local right identity.

In a monoid, the notion of inverse as defined in the previous section is strictly narrower than the definition given in this section. Only elements in [[Green's_relations#The_H_and_D_relations|''H''&lt;sub&gt;1&lt;/sub&gt;]] have an inverse from the unital magma perspective, whereas for any idempotent ''e'', the elements of ''H''&lt;sub&gt;e&lt;/sub&gt; have an inverse as defined in this section. Under this more general definition, inverses need not be unique (or exist) in an arbitrary semigroup or monoid. If all elements are regular, then the semigroup (or monoid) is called regular, and every element has at least one inverse. If every element has exactly one inverse as defined in this section, then the semigroup is called an [[inverse semigroup]]. Finally, an inverse semigroup with only one idempotent is a group. An inverse semigroup may have an [[absorbing element]] 0 because 000 = 0, whereas a group may not.

Outside semigroup theory, a unique inverse as defined in this section is sometimes called a '''quasi-inverse'''. This is generally justified because in most applications (e.g. all examples in this article) associativity holds, which makes this notion a generalization of the left/right inverse relative to an identity.

=== ''U''-semigroups ===

A natural generalization of the inverse semigroup is to define an (arbitrary) unary operation ° such that (''a''°)° = ''a'' for all ''a'' in ''S''; this endows ''S'' with a type {{langle}}2,1{{rangle}} algebra. A semigroup endowed with such an operation is called a '''''U''-semigroup'''. Although it may seem that ''a''° will be the inverse of ''a'', this is not necessarily the case. In order to obtain interesting notion(s), the unary operation must somehow interact with the semigroup operation. Two classes of ''U''-semigroups have been studied:

* '''''I''-semigroups''', in which the interaction axiom is ''aa''°''a'' = ''a''
* '''[[Semigroup with involution|*-semigroups]]''', in which the interaction axiom is (''ab'')° = ''b''°''a''°. Such an operation is called an [[involution (mathematics)|involution]], and typically denoted by ''a''*

Clearly a group is both an ''I''-semigroup and a *-semigroup. Inverse semigroups are exactly those semigroups that are both ''I''-semigroups and *-semigroups. A class of semigroups important in semigroup theory are [[completely regular semigroup]]s; these are ''I''-semigroups in which one additionally has ''aa''° = ''a''°''a''; in other words every element has commuting pseudoinverse ''a''°. There are few concrete examples of such semigroups however; most are [[completely simple semigroup]]s. In contrast, a class of *-semigroups, the [[*-regular semigroup]]s, yield one of best known examples of a (unique) pseudoinverse, the [[Moore–Penrose inverse]]. In this case however the involution ''a''* is not the pseudoinverse. Rather, the pseudoinverse of ''x'' is the unique element ''y'' such that ''xyx'' = ''x'', ''yxy'' = ''y'',   (''xy'')* = ''xy'', (''yx'')* = ''yx''. Since *-regular semigroups generalize inverse semigroups, the unique element defined this way in a *-regular semigroup is called the '''generalized inverse''' or '''Penrose–Moore inverse'''. In a *-regular semigroup ''S'' one can identify a special subset of idempotents F(''S'') called a [[Semigroup with involution#P-systems|P-system]]; every element ''a'' of the semigroup has exactly one inverse ''a''* such that ''aa''* and ''a''*''a'' are in F(''S''). The P-systems of Yamada are based upon the notion of regular *-semigroup as defined by Nordahl and Scheiblich. 

== Examples ==
All examples in this section involve associative operators, thus we shall use the terms left/right inverse for the unital magma-based definition, and quasi-inverse for its more general version.

=== Real numbers ===
Every [[real number]] &lt;math&gt;x&lt;/math&gt; has an [[additive inverse]] (i.e. an inverse with respect to [[addition]]) given by &lt;math&gt;-x&lt;/math&gt;. Every nonzero real number &lt;math&gt;x&lt;/math&gt; has a [[multiplicative inverse]] (i.e. an inverse with respect to [[multiplication]]) given by &lt;math&gt;\frac 1{x}&lt;/math&gt; (or &lt;math&gt;x^{-1}&lt;/math&gt;). By contrast, [[0 (number)|zero]] has no multiplicative inverse, but it has a unique quasi-inverse, 0 itself.

=== Functions and partial functions ===
A function &lt;math&gt;g&lt;/math&gt; is the left (resp. right) [[inverse function|inverse of a function]] &lt;math&gt;f&lt;/math&gt; (for [[function composition]]), if and only if &lt;math&gt;g \circ f&lt;/math&gt; (resp. &lt;math&gt;f \circ g&lt;/math&gt;) is the [[identity function]] on the [[domain of a function|domain]] (resp. [[codomain]]) of &lt;math&gt;f&lt;/math&gt;. The inverse of a function &lt;math&gt;f&lt;/math&gt; is often written &lt;math&gt;f^{-1}&lt;/math&gt;, but [[inverse function#Note on notation|this notation is sometimes ambiguous]]. Only [[bijection]]s have two-sided inverses, but ''any'' function has a quasi-inverse, i.e. the [[full transformation monoid]] is regular. The monoid of [[partial functions]] is also regular, whereas the [[symmetric inverse semigroup|monoid of injective partial transformations]] is the prototypical inverse semigroup.

=== Galois connections ===
The lower and upper adjoints in a (monotone) [[Galois connection]], ''L'' and ''G'' are quasi-inverses of each other, i.e. ''LGL'' = ''L'' and ''GLG'' = ''G'' and one uniquely determines the other. They are not left or right inverses of each other however.

=== Matrices ===
A [[square matrix]] &lt;math&gt;M&lt;/math&gt; with entries in a [[field (mathematics)|field]] &lt;math&gt;K&lt;/math&gt; is invertible (in the set of all square matrices of the same size, under [[matrix multiplication]]) if and only if its [[determinant]] is different from zero. If the determinant of &lt;math&gt;M&lt;/math&gt; is zero, it is impossible for it to have a one-sided inverse; therefore a left inverse or right inverse implies the existence of the other one. See [[invertible matrix]] for more.

More generally, a square matrix over a [[commutative ring]] &lt;math&gt;R&lt;/math&gt; is invertible [[if and only if]] its determinant is invertible in &lt;math&gt;R&lt;/math&gt;.

Non-square matrices of [[full rank]] have several one-sided inverses:&lt;ref&gt;[http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/VideoLectures/detail/lecture33.htm MIT Professor Gilbert Strang Linear Algebra Lecture #33 – Left and Right Inverses; Pseudoinverse.]&lt;/ref&gt;
* For &lt;math&gt;A:m\times n \mid m&gt;n&lt;/math&gt; we have a left inverse: &lt;math&gt; \underbrace{ (A^{T}A)^{-1}A^{T} }_{ A^{-1}_\text{left} } A = I_{n} &lt;/math&gt;
* For &lt;math&gt;A:m\times n \mid m&lt;n&lt;/math&gt; we have a right inverse: &lt;math&gt; A \underbrace{ A^{T}(AA^{T})^{-1} }_{ A^{-1}_\text{right} } = I_{m} &lt;/math&gt;

The right inverse can be used to determine the least norm solution of ''Ax'' = ''b''.

No rank-deficient matrix has any (even one-sided) inverse.  However, the [[Moore–Penrose pseudoinverse]] exists for all matrices, and coincides with the left or right (or true) inverse when it exists.

As an example of matrix inverses, consider:

: &lt;math&gt;A:2\times 3 =
  \begin{bmatrix}
    1 &amp; 2 &amp; 3 \\
    4 &amp; 5 &amp; 6
  \end{bmatrix}
&lt;/math&gt;

So, as ''m'' &lt; ''n'', we have a right inverse. &lt;math&gt;A^{-1}_\text{right} = A^{T}(AA^{T})^{-1}&lt;/math&gt;

: &lt;math&gt;\begin{align}
  AA^{T} &amp;= \begin{bmatrix}
              1 &amp; 2 &amp; 3 \\
              4 &amp; 5 &amp; 6
            \end{bmatrix}\cdot
            \begin{bmatrix}
              1 &amp; 4\\
              2 &amp; 5\\
              3 &amp; 6
            \end{bmatrix} =  
            \begin{bmatrix}
              14 &amp; 32\\
              32 &amp; 77
            \end{bmatrix} \\
  (AA^{T})^{-1} &amp;= \begin{bmatrix}
                     14 &amp; 32\\
                     32 &amp; 77
                   \end{bmatrix}^{-1} = \frac{1}{54}
                   \begin{bmatrix}
                      77 &amp; -32\\
                     -32 &amp; 14
                   \end{bmatrix} \\
  A^{T}(AA^{T})^{-1} &amp;= \frac{1}{54}
                   \begin{bmatrix}
                     1 &amp; 4\\
                     2 &amp; 5\\
                     3 &amp; 6
                   \end{bmatrix}\cdot
                   \begin{bmatrix}
                      77 &amp; -32\\
                     -32 &amp; 14
                   \end{bmatrix} = \frac{1}{18}
                   \begin{bmatrix}
                     -17 &amp; 8\\
                      -2 &amp; 2\\
                      13 &amp; -4
                   \end{bmatrix} = A^{-1}_\text{right}
\end{align}&lt;/math&gt;

The left inverse doesn't exist, because

: &lt;math&gt;
  A^{T}A = \begin{bmatrix}
             1 &amp; 4\\
             2 &amp; 5\\
             3 &amp; 6
           \end{bmatrix} \cdot
           \begin{bmatrix}
             1 &amp; 2 &amp; 3 \\
             4 &amp; 5 &amp; 6
           \end{bmatrix} =
           \begin{bmatrix}
             17 &amp; 22 &amp; 27 \\
             22 &amp; 29 &amp; 36\\
             27 &amp; 36 &amp; 45
           \end{bmatrix}
&lt;/math&gt;

which is a singular matrix, and cannot be inverted.

== See also ==
*[[loop (algebra)]]
*[[division ring]]
*[[unit (ring theory)]]
*[[Latin square property]]

==Notes==
{{Reflist}}

==References==
* M. Kilp, U. Knauer, A.V. Mikhalev, ''Monoids, Acts and Categories with Applications to Wreath Products and Graphs'', De Gruyter Expositions in Mathematics vol. 29, Walter de Gruyter, 2000, ISBN 3-11-015248-7, p. 15 (def in unital magma) and p. 33 (def in semigroup)
*{{cite book|last= Howie|first= John M.|title=Fundamentals of Semigroup Theory|year=1995|publisher=[[Clarendon Press]]|isbn=0-19-851194-9}} contains all of the semigroup material herein except *-regular semigroups.
* Drazin, M.P., ''Regular semigroups with involution'', Proc. Symp. on Regular Semigroups (DeKalb, 1979), 29–46
* Miyuki Yamada, ''P-systems in regular semigroups'', Semigroup Forum, 24(1), December 1982, pp. 173–187
*Nordahl, T.E., and H.E. Scheiblich, Regular * Semigroups, Semigroup Forum, 16(1978), 369–377.

[[Category:Algebra]]
[[Category:Abstract algebra]]
[[Category:Binary operations|*Inverse element]]</text>
      <sha1>3bg1a0t022au68hc25fa87lzbz8nx48</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Distributive lattice</title>
    <ns>0</ns>
    <id>310883</id>
    <revision>
      <id>595771198</id>
      <parentid>595603372</parentid>
      <timestamp>2014-02-16T20:01:57Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>undid previous own edit (restored &quot;citation&quot;) after criticism from David Eppstein</comment>
      <text xml:space="preserve" bytes="13843">{{Refimprove|date=May 2011}}
In [[mathematics]],  a '''distributive lattice''' is a [[lattice (order)|lattice]] in which the operations of [[join and meet]] [[distributivity|distribute]] over each other. The prototypical examples of such structures are collections of sets for which the lattice operations can be given by set [[union (set theory)|union]] and [[intersection (set theory)|intersection]]. Indeed, these lattices of sets describe the scenery completely: every distributive lattice is &amp;ndash; up to [[order isomorphism|isomorphism]] &amp;ndash; given as such a lattice of sets.

==Definition==
As in the case of arbitrary lattices, one can choose to consider a distributive lattice ''L'' either as a structure of [[order theory]] or of [[universal algebra]]. Both views and their mutual correspondence are discussed in the article on [[lattice (order)|lattices]]. In the present situation, the algebraic description appears to be more convenient:

A lattice (''L'',∨,∧) is '''distributive''' if the following additional identity holds for all ''x'', ''y'', and ''z'' in ''L'':

: ''x'' ∧ (''y'' ∨ ''z'') = (''x'' ∧ ''y'') ∨ (''x'' ∧ ''z'').

Viewing lattices as partially ordered sets, this says that the meet operation [[Limit-preserving function (order theory)|preserves]] non-empty finite joins. It is a basic fact of lattice theory that the above condition is equivalent to its [[duality (order theory)|dual]]:&lt;ref&gt;{{cite book| author=Garrett Birkhoff| title=Lattice Theory| year=1967| volume=25| publisher=Am. Math. Soc.| series=Colloquium Publications}}; here: §5-6, p.8-12&lt;/ref&gt;

: ''x'' ∨ (''y'' ∧ ''z'') = (''x'' ∨ ''y'') ∧ (''x'' ∨ ''z'') &amp;nbsp; for all ''x'', ''y'', and ''z'' in ''L''.&lt;ref&gt;For individual elements ''x'', ''y'', ''z'', e.g. the first equation may be violated, but the second may hold; see the N&lt;sub&gt;5&lt;/sub&gt; picture for an example.&lt;/ref&gt;

In every lattice, defining ''p''≤''q'' as usual to mean ''p''∧''q''=''p'', the inequation ''x'' ∧ (''y'' ∨ ''z'') ≥ (''x'' ∧ ''y'') ∨ (''x'' ∧ ''z'') holds as well as its dual inequation ''x'' ∨ (''y'' ∧ ''z'') ≤ (''x'' ∨ ''y'') ∧ (''x'' ∨ ''z''). A lattice is distributive if one of the converse inequations holds, too.
More information on the relationship of this condition to other distributivity conditions of order theory can be found in the article on [[distributivity (order theory)]].

== Morphisms ==

A morphism of distributive lattices is just a lattice homomorphism as given in the article on [[lattice (order)|lattices]], i.e. a function that is compatible with the two lattice operations. Because such a morphism of lattices preserves the lattice structure, it will consequently also preserve the distributivity (and thus be a morphism of distributive lattices).

== Examples ==
[[File:Young's lattice.svg|thumb|[[Young's lattice]]]]
Distributive lattices are ubiquitous but also rather specific structures. As already mentioned the main example for distributive lattices are lattices of sets, where join and meet are given by the usual set-theoretic operations. Further examples include:

* The [[Lindenbaum–Tarski algebra|Lindenbaum algebra]] of most [[logic]]s that support [[logical conjunction|conjunction]] and [[Logical disjunction|disjunction]] is a distributive lattice, i.e. &quot;and&quot; distributes over &quot;or&quot; and vice versa.

* Every [[Boolean algebra (structure)|Boolean algebra]] is a distributive lattice.

* Every [[Heyting algebra]] is a distributive lattice. Especially this includes all [[complete Heyting algebra|locales]] and hence all [[open set]] lattices of [[topological space]]s. Also note that Heyting algebras can be viewed as Lindenbaum algebras of [[intuitionistic logic]], which makes them a special case of the above example.

* Every [[Total order|totally ordered set]] is a distributive lattice with max as join and min as meet.
* The [[natural number]]s form a  distributive lattice ([[Complete_lattice#Complete_semilattices|complete as a meet-semilattice]]) with the [[greatest common divisor]] as meet and the [[least common multiple]] as join.

* Given a positive integer ''n'', the set of all positive [[divisor]]s of ''n'' forms a distributive lattice, again with the greatest common divisor as meet and the least common multiple as join. This is a Boolean algebra if and only if ''n'' is [[Square-free integer|square-free]].

* A [[Riesz space|lattice-ordered vector space]] is a distributive lattice.

*[[Young's lattice]] given by the inclusion ordering of [[Young_diagram#Diagrams|Young diagrams]] representing [[Partition (number theory)|integer partitions]] is a distributive lattice.

== Characteristic properties ==

Various equivalent formulations to the above definition exist. For example, ''L'' is distributive [[if and only if]] the following holds for all elements ''x'', ''y'', ''z'' in ''L'':

: (''x''&lt;math&gt;\wedge &lt;/math&gt;''y'')&lt;math&gt;\vee &lt;/math&gt;(''y''&lt;math&gt;\wedge &lt;/math&gt;''z'')&lt;math&gt;\vee &lt;/math&gt;(''z''&lt;math&gt;\wedge &lt;/math&gt;''x'') = (''x''&lt;math&gt;\vee &lt;/math&gt;''y'')&lt;math&gt;\wedge &lt;/math&gt;(''y''&lt;math&gt;\vee &lt;/math&gt;''z'')&lt;math&gt;\wedge &lt;/math&gt;(''z''&lt;math&gt;\vee &lt;/math&gt;''x'').

Similarly, ''L'' is distributive if and only if

: ''x''&lt;math&gt;\wedge &lt;/math&gt;''z'' = ''y''&lt;math&gt;\wedge &lt;/math&gt;''z'' and ''x''&lt;math&gt;\vee &lt;/math&gt;''z'' = ''y''&lt;math&gt;\vee &lt;/math&gt;''z'' always imply ''x''=''y''.

&lt;gallery caption=&quot;[[Hasse diagrams]] of the two prototypical non-distributive lattices&quot; align=&quot;center&quot;&gt;
Image:M 3 mit Beschriftung.svg|The diamond lattice ''M''&lt;sub&gt;3&lt;/sub&gt;.
Image:N5 annotated.gif|The pentagon lattice ''N''&lt;sub&gt;5&lt;/sub&gt;.
&lt;/gallery&gt;
The simplest ''non-distributive'' lattices are ''M''&lt;sub&gt;3&lt;/sub&gt;, the &quot;diamond lattice&quot;, and ''N''&lt;sub&gt;5&lt;/sub&gt;, the &quot;pentagon lattice&quot;. A lattice is distributive if and only if none of its sublattices is isomorphic to ''M''&lt;sub&gt;3&lt;/sub&gt; or&amp;nbsp;''N''&lt;sub&gt;5&lt;/sub&gt;; a sublattice is a subset that is closed under the meet and join operations of the original lattice. Note that this is not the same as being a subset that is a lattice under the original order (but possibly with different join and meet operations). Further characterizations derive from the representation theory in the next section.

Finally distributivity entails several other pleasant properties. For example, an element of a distributive lattice is [[Lattice (order)#Important_lattice-theoretic_notions|meet-prime]] if and only if it is [[Lattice (order)#Important_lattice-theoretic_notions|meet-irreducible]], though the latter is in general a weaker property. By duality, the same is true for [[Lattice (order)#Important_lattice-theoretic_notions|join-prime]] and [[Lattice (order)#Important_lattice-theoretic_notions|join-irreducible]] elements.&lt;ref&gt;See [[Birkhoff's representation theorem#The partial order of join-irreducibles]].&lt;/ref&gt; If a lattice is distributive, its [[covering relation]] forms a [[median graph]].&lt;ref&gt;{{citation
 | first1 = Garrett | last1 = Birkhoff | authorlink1 = Garrett Birkhoff
 | first2 = S. A. | last2 = Kiss
 | title = A ternary operation in distributive lattices
 | journal = Bulletin of the American Mathematical Society
 | volume = 53 | issue = 1 | year = 1947 | pages = 749–752
 | mr = 0021540 
 | url = http://projecteuclid.org/euclid.bams/1183510977
 | doi = 10.1090/S0002-9904-1947-08864-9}}.&lt;/ref&gt;

Furthermore, every distributive lattice is also [[modular lattice|modular]].

== Representation theory ==

The introduction already hinted at the most important characterization for distributive lattices: a lattice is distributive if and only if it is isomorphic to a lattice of sets (closed under [[Union (set theory)|set union]] and [[Intersection (set theory)|intersection]]). That set union and intersection are indeed distributive in the above sense is an elementary fact. The other direction is less trivial, in that it requires the representation theorems stated below. The important insight from this characterization is that the identities (equations) that hold in all distributive lattices are exactly the ones that hold in all lattices of sets in the above sense.

[[Birkhoff's representation theorem]] for distributive lattices states that every ''finite'' distributive lattice is isomorphic to the lattice of [[Upper set|lower set]]s of the [[Partially ordered set|poset]] of its join-prime (equivalently: join-irreducible) elements. This establishes a [[bijection]] (up to [[isomorphism]]) between the class of all finite posets and the class of all finite distributive lattices. This bijection can be extended to a [[equivalence of categories|duality of categories]] between homomorphisms of finite distributive lattices and [[Monotonic function|monotone function]]s of finite posets. Generalizing this result to infinite lattices, however, requires adding further structure.

Another early representation theorem is now known as [[Stone's representation theorem for distributive lattices]] (the name honors [[Marshall Harvey Stone]], who first proved it). It characterizes distributive lattices as the lattices of [[Compact space|compact]] [[open set|open]] sets of certain [[topological space]]s. This result can be viewed both as a generalization of Stone's famous [[Stone's representation theorem for Boolean algebras|representation theorem for Boolean algebras]] and as a specialization of the general setting of [[Stone duality]].

A further important representation was established by [[Hilary Priestley]] in her [[Priestley's representation theorem for distributive lattice|representation theorem for distributive lattices]]. In this formulation, a distributive lattice is used to construct a topological space with an additional partial order on its points, yielding a (completely order-separated) ''ordered [[Stone's representation theorem for Boolean algebras|Stone space]]'' (or ''[[Priestley space]]''). The original lattice is recovered as the collection of [[clopen set|clopen]] lower sets of this space.

As a consequence of Stone's and Priestley's theorems, one easily sees that any distributive lattice is really isomorphic to a lattice of sets. However, the proofs of both statements require the [[Boolean prime ideal theorem]], a weak form of the [[axiom of choice]].

== Free distributive lattices ==
[[File:Monotone Boolean functions.svg|thumb|360px|Free distributive lattices on zero, one, two, and three generators. The elements labeled &quot;0&quot; and &quot;1&quot; are the empty join and meet, and the element labeled &quot;majority&quot; is (''x'' ∧ ''y'') ∨ (''x'' ∧ ''z'') ∨ (''y'' ∧ ''z'') = (''x'' ∨ ''y'') ∧ (''x'' ∨ ''z'') ∧ (''y'' ∨ ''z'').]]
The [[free object|free]] distributive lattice over a set of generators ''G'' can be constructed much more easily than a general free lattice. The first observation is that, using the laws of distributivity, every term formed by the binary operations &lt;math&gt;\vee&lt;/math&gt; and &lt;math&gt;\wedge&lt;/math&gt; on a set of generators can be transformed into the following equivalent ''normal form'':

: ''M''&lt;sub&gt;1&lt;/sub&gt; &lt;math&gt;\vee&lt;/math&gt; ''M''&lt;sub&gt;2&lt;/sub&gt; &lt;math&gt;\vee&lt;/math&gt; ... &lt;math&gt;\vee&lt;/math&gt; ''M''&lt;sub&gt;''n''&lt;/sub&gt;

where the ''M''&lt;sub&gt;''i''&lt;/sub&gt; are finite meets of elements of ''G''.  Moreover, since both meet and join are [[Commutativity|commutative]] and [[Idempotence|idempotent]], one can ignore duplicates and order, and represent a join of meets like the one above as a set of sets:

: {''N''&lt;sub&gt;1&lt;/sub&gt;, ''N''&lt;sub&gt;2&lt;/sub&gt;, ..., ''N''&lt;sub&gt;''n''&lt;/sub&gt;},

where the ''N''&lt;sub&gt;''i''&lt;/sub&gt; are finite subsets of ''G''. However, it is still possible that two such terms denote the same element of the distributive lattice. This occurs when there are indices ''j'' and ''k'' such that ''N''&lt;sub&gt;''j''&lt;/sub&gt; is a subset of ''N''&lt;sub&gt;''k''&lt;/sub&gt;. In this case the meet of ''N''&lt;sub&gt;''k''&lt;/sub&gt; will be below the meet of ''N''&lt;sub&gt;''j''&lt;/sub&gt;, and hence one can safely remove the ''redundant'' set ''N''&lt;sub&gt;''k''&lt;/sub&gt; without changing the interpretation of the whole term. Consequently, a set of finite subsets of ''G'' will be called ''irredundant'' whenever all of its elements ''N''&lt;sub&gt;''i''&lt;/sub&gt; are mutually incomparable (with respect to the subset ordering); that is, when it forms an [[antichain]] of finite sets.

Now the free distributive lattice over a set of generators ''G'' is defined on the set of all finite irredundant sets of finite subsets of ''G''. The join of two finite irredundant sets is obtained from their union by removing all redundant sets. Likewise the meet of two sets ''S'' and ''T'' is the irredundant version of { ''N''&lt;math&gt;\cup&lt;/math&gt;''M'' | ''N'' in ''S'', ''M'' in ''T''}. The verification that this structure is a distributive lattice with the required [[universal property]] is routine.

The number of elements in free distributive lattices with ''n'' generators is given by the [[Dedekind number]]s. These numbers grow rapidly, and are known only for ''n''&amp;nbsp;≤&amp;nbsp;8; they are
:2, 3, 6, 20, 168, 7581, 7828354, 2414682040998, 56130437228687557907788 {{OEIS|id=A000372}}.
The numbers above count the number of free distributive lattices in which the lattice operations are joins and meets of finite sets of elements, including the empty set. If empty joins and empty meets are disallowed, the resulting free distributive lattices have two fewer elements; their numbers of elements form the sequence
:1, 4, 18, 166, 7579, 7828352, 2414682040996, 56130437228687557907786 {{OEIS|id=A007153}}.

==See also==
* [[Duality theory for distributive lattices]]
* [[Spectral space]]

==References==
{{reflist|2}}

==Further reading==
*{{cite book |last1=Burris |first1=Stanley N. |last2=Sankappanavar |first2=H.P. |title=A Course in Universal Algebra |url=http://www.thoralf.uwaterloo.ca/htdocs/ualg.html |year=1981 |publisher=Springer-Verlag |isbn=3-540-90578-2}}

[[Category:Lattice theory]]
[[Category:Algebra]]</text>
      <sha1>nxmfxsx46wrlg3agmj3wde9j2ovnfxo</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Symmetric difference</title>
    <ns>0</ns>
    <id>262288</id>
    <revision>
      <id>602098228</id>
      <parentid>598975687</parentid>
      <timestamp>2014-03-31T11:49:49Z</timestamp>
      <contributor>
        <username>Evan Aad</username>
        <id>18147469</id>
      </contributor>
      <comment>/* Symmetric difference on measure spaces */ The difference between the measure of two sets.</comment>
      <text xml:space="preserve" bytes="10467">[[File:Venn0110.svg|thumb|
[[Venn diagram]] of &lt;math&gt;~A \triangle B&lt;/math&gt;&lt;br&gt;
The symmetric difference is &lt;br&gt;
the [[Union (set theory)|union]] [[Complement (set theory)#Relative complement|without]] the [[Intersection (set theory)|intersection]]:&lt;br&gt;
[[File:Venn0111.svg|40px]] &lt;math&gt;~\setminus~&lt;/math&gt; [[File:Venn0001.svg|40px]] &lt;math&gt;~=~&lt;/math&gt; [[File:Venn0110.svg|40px]]
]]

In [[mathematics]], the '''symmetric difference''' of two [[Set (mathematics)|sets]] is the set of elements which are in either of the sets and not in their intersection.  The symmetric difference of the sets ''A'' and ''B'' is commonly denoted by
:&lt;math&gt; A\,\triangle\,B\,&lt;/math&gt;
or
:&lt;math&gt;A \ominus B.&lt;/math&gt;
or
:&lt;math&gt;A \oplus B.&lt;/math&gt;

For example, the symmetric difference of the sets &lt;math&gt;\{1,2,3\}&lt;/math&gt; and &lt;math&gt;\{3,4\}&lt;/math&gt; is &lt;math&gt;\{1,2,4\}&lt;/math&gt;. The symmetric difference of the set of all students and the set of all females consists of all male students together with all female non-students.

The [[power set]] of any set becomes an [[abelian group]] under the operation of symmetric difference, with the [[empty set]] as the [[neutral element]] of the group and every element in this group being its own [[inverse element|inverse]]. The power set of any set becomes a [[Boolean ring]] with symmetric difference as the addition of the ring and [[intersection (set theory)|intersection]] as the multiplication of the ring.

== Properties ==
[[File:Venn 0110 1001.svg|thumb|
Venn diagram of &lt;math&gt;~A \triangle B \triangle C&lt;/math&gt;&lt;br&gt;&lt;br&gt;
[[File:Venn 0110 0110.svg|40px]] &lt;math&gt;~\triangle~&lt;/math&gt; [[File:Venn 0000 1111.svg|40px]] &lt;math&gt;~=~&lt;/math&gt; [[File:Venn 0110 1001.svg|40px]]
]]

The symmetric difference is equivalent to the [[union (set theory)|union]] of both [[complement (set theory)|relative complement]]s, that is:

:&lt;math&gt;A\,\triangle\,B = (A \smallsetminus B) \cup (B \smallsetminus A),\,&lt;/math&gt;

and it can also be expressed as the union of the two sets, minus their [[intersection (set theory)|intersection]]:

:&lt;math&gt;A\,\triangle\,B = (A \cup B) \smallsetminus (A \cap B),&lt;/math&gt;

or with the [[Exclusive or|XOR]] operation:

:&lt;math&gt;A\,\triangle\,B = \{x : (x \in A) \oplus (x \in B)\}.&lt;/math&gt;

In particular, &lt;math&gt;A\triangle B\subseteq A\cup B&lt;/math&gt;.

The symmetric difference is [[commutativity|commutative]] and [[associativity|associative]]:

:&lt;math&gt;A\,\triangle\,B = B\,\triangle\,A,\,&lt;/math&gt;
:&lt;math&gt;(A\,\triangle\,B)\,\triangle\,C = A\,\triangle\,(B\,\triangle\,C).\,&lt;/math&gt;

Thus, the repeated symmetric difference is an operation on a [[multiset]] of sets giving the set of elements which are in an odd number of sets.

The symmetric difference of two repeated symmetric differences is the repeated symmetric difference of the [[Multiset#Operations|join]] of the two multisets, where for each double set both can be removed. In particular:

:&lt;math&gt;(A\,\triangle\,B)\,\triangle\,(B\,\triangle\,C) = A\,\triangle\,C.\,&lt;/math&gt;

This implies a sort of [[triangle inequality]]: the symmetric difference of ''A'' and ''C'' is contained in the union of the symmetric difference of ''A'' and ''B'' and that of ''B'' and ''C''. (But note that for the [[diameter]] of the symmetric difference the triangle inequality does not hold.)   

The [[empty set]] is [[identity element|neutral]], and every set is its own inverse:
:&lt;math&gt;A\,\triangle\,\varnothing = A,\,&lt;/math&gt;
:&lt;math&gt;A\,\triangle\,A = \varnothing.\,&lt;/math&gt;

Taken together, we see that the [[power set]] of any set ''X'' becomes an [[abelian group]] if we use the symmetric difference as operation. Because every element in this group is its own inverse, this is in fact a [[vector space]] over the [[finite field|field with 2 elements]] '''Z'''&lt;sub&gt;2&lt;/sub&gt;. If ''X'' is finite, then the [[singleton (mathematics)|singleton]]s form a [[basis (linear algebra)|basis]] of this vector space, and its [[Hamel dimension|dimension]] is therefore equal to the number of elements of ''X''. This construction is used in [[graph theory]], to define the [[cycle space]] of a graph.

Intersection [[distributivity|distributes]] over symmetric difference:
:&lt;math&gt;A \cap (B\,\triangle\,C) = (A \cap B)\,\triangle\,(A \cap C),&lt;/math&gt;
and this shows that the power set of ''X'' becomes a [[ring (mathematics)|ring]] with symmetric difference as addition and intersection as multiplication. This is the prototypical example of a [[Boolean ring]].

Further properties of the symmetric difference:

* &lt;math&gt;A\triangle B=A^c\triangle B^c&lt;/math&gt;, where &lt;math&gt;A^c&lt;/math&gt;,&lt;math&gt;B^c&lt;/math&gt; is &lt;math&gt;A&lt;/math&gt;'s complement,&lt;math&gt;B&lt;/math&gt;'s complement, respectively, relative to any (fixed) set that contains both.
* &lt;math&gt;\left(\bigcup_{\alpha\in\mathcal{I}}A_\alpha\right)\triangle\left(\bigcup_{\alpha\in\mathcal{I}}B_\alpha\right)\subseteq\bigcup_{\alpha\in\mathcal{I}}\left(A_\alpha\triangle B_\alpha\right)&lt;/math&gt;, where &lt;math&gt;\mathcal{I}&lt;/math&gt; is an arbitrary non-empty index set.
* If &lt;math&gt;f : S \rightarrow T&lt;/math&gt; is any function and &lt;math&gt;A, B \subseteq T&lt;/math&gt; are any sets in &lt;math&gt;f&lt;/math&gt;'s codomain, then &lt;math&gt;f^{-1}\left(A \Delta B\right) = f^{-1}\left(A\right) \Delta f^{-1}\left(B\right)&lt;/math&gt;.

The symmetric difference can be defined in any [[Boolean algebra (structure)|Boolean algebra]], by writing
:&lt;math&gt; x\,\triangle\,y = (x \lor y) \land \lnot(x \land y) = (x \land \lnot y) \lor (y \land \lnot x) = x \oplus y.&lt;/math&gt;
This operation has the same properties as the symmetric difference of sets.

==''n''-ary symmetric difference==
As above, the symmetric difference of a collection of sets contains just elements which are in an odd number of the sets in the collection:
:&lt;math&gt;\triangle M = \left\{ a \in \bigcup M: |\{A\in M:a \in A\}| \mbox{ is odd}\right\}&lt;/math&gt;.
Evidently, this is well-defined only when each element of the union &lt;math&gt;\bigcup M&lt;/math&gt; is contributed by a finite number of elements of &lt;math&gt;M&lt;/math&gt;. 

Suppose &lt;math&gt;M=\{M_{1},M_{2}, \ldots , M_{n}\}&lt;/math&gt; is a [[multiset]] and &lt;math&gt;n \ge 2&lt;/math&gt;. Then there is a formula for &lt;math&gt;|\triangle M|&lt;/math&gt;, the number of elements in &lt;math&gt;\triangle M&lt;/math&gt;, given solely in terms of intersections of elements of &lt;math&gt;M&lt;/math&gt;:
:&lt;math&gt;|\triangle M| = \sum_{l=1}^{n} (-2)^{l-1} \sum_{i_{1} \ne i_{2} \ne \ldots \ne i_{l}} |M_{i_{1}} \cap M_{i_{2}} \cap \ldots \cap M_{i_{l}}|&lt;/math&gt;,
where &lt;math&gt;i_{1} \ne i_{2} \ne \ldots \ne i_{l}&lt;/math&gt; is meant to indicate that &lt;math&gt;\{i_{1}, i_{2}, \ldots, i_{l}\}&lt;/math&gt; is a subset of distinct elements of &lt;math&gt;\{1,2,\ldots,n\}&lt;/math&gt;, of which there are &lt;math&gt;\binom{n}{l}&lt;/math&gt;.

==Symmetric difference on measure spaces==
As long as there is a notion of &quot;how big&quot; a set is, the symmetric difference between two sets can be considered a measure of how &quot;far apart&quot; they are.  Formally, if μ is a [[Sigma-finite|σ-finite]] [[measure space|measure]] defined on a [[sigma-algebra|σ-algebra]] Σ, the function
:&lt;math&gt;d(X,Y) = \mu(X\,\triangle\,Y)&lt;/math&gt;
is a [[Pseudometric space|pseudometric]] on Σ.  ''d'' becomes a [[metric space|metric]] if Σ is considered modulo the [[equivalence relation]] ''X'' ~ ''Y'' if and only if &lt;math&gt;\mu(X\,\triangle\,Y) = 0&lt;/math&gt;.  The resulting metric space is [[Separable space|separable]] if and only if [[L^2|L&lt;sup&gt;2&lt;/sup&gt;(μ)]] is separable.

If &lt;math&gt;\mu(X), \mu(Y) &lt; \infty&lt;/math&gt;, we have: &lt;math&gt;|\mu(X) - \mu(Y)| \leq \mu(X\,\triangle\,Y)&lt;/math&gt;. Indeed,
&lt;math&gt;
\begin{align}
|\mu(X) - \mu(Y)| &amp; = |(\mu(X \setminus Y) + \mu(X\cap Y)) - (\mu(X \cap Y) + \mu(Y \setminus X))| \\
&amp; = |\mu(X \setminus Y) - \mu(Y \setminus X)| \\
&amp; \leq |\mu(X \setminus Y)| + |\mu(Y \setminus X)| \\
&amp; = \mu(X \setminus Y) + \mu(Y \setminus X) \\
&amp; = \mu((X \setminus Y) \cup (Y \setminus X)) \\
&amp; = \mu(X\Delta Y)
\end{align}
&lt;/math&gt;

Let &lt;math&gt;S=\left(\Omega,\mathcal{A},\mu\right)&lt;/math&gt; be some measure space and let &lt;math&gt;F,G\in\mathcal{A}&lt;/math&gt; and &lt;math&gt;\mathcal{D},\mathcal{E}\subseteq\mathcal{A}&lt;/math&gt;.

Symmetric difference is measurable: &lt;math&gt;F\triangle G\in\mathcal{A}&lt;/math&gt;.

We write &lt;math&gt;F=G\left[\mathcal{A},\mu\right]&lt;/math&gt; iff &lt;math&gt;\mu\left(F\triangle G\right)=0&lt;/math&gt;. The relation &quot;&lt;math&gt;=\left[\mathcal{A},\mu\right]&lt;/math&gt;&quot; is an equivalence relation on the &lt;math&gt;\mathcal{A}&lt;/math&gt;-measurable sets.

We write &lt;math&gt;\mathcal{D}\subseteq\mathcal{E}\left[\mathcal{A},\mu\right]&lt;/math&gt; iff to each &lt;math&gt;D\in\mathcal{D}&lt;/math&gt; there's some &lt;math&gt;E\in\mathcal{E}&lt;/math&gt; such that &lt;math&gt;D=E\left[\mathcal{A},\mu\right]&lt;/math&gt;. The relation &quot;&lt;math&gt;\subseteq\left[\mathcal{A},\mu\right]&lt;/math&gt;&quot; is a partial order on the family of subsets of &lt;math&gt;\mathcal{A}&lt;/math&gt;.

We write &lt;math&gt;\mathcal{D}=\mathcal{E}\left[\mathcal{A},\mu\right]&lt;/math&gt; iff &lt;math&gt;\mathcal{D}\subseteq\mathcal{E}\left[\mathcal{A},\mu\right]&lt;/math&gt; and &lt;math&gt;\mathcal{E}\subseteq\mathcal{D}\left[\mathcal{A},\mu\right]&lt;/math&gt;.  The relation &quot;&lt;math&gt;=\left[\mathcal{A},\mu\right]&lt;/math&gt;&quot; is an equivalence relationship between the subsets of &lt;math&gt;\mathcal{A}&lt;/math&gt;.

The &quot;symmetric closure&quot; of &lt;math&gt;\mathcal{D}&lt;/math&gt; is the collection of all &lt;math&gt;\mathcal{A}&lt;/math&gt;-measurable sets that are &lt;math&gt;=\left[\mathcal{A},\mu\right]&lt;/math&gt; to some &lt;math&gt;D\in\mathcal{D}&lt;/math&gt;. The symmetric closure of &lt;math&gt;\mathcal{D}&lt;/math&gt; contains &lt;math&gt;\mathcal{D}&lt;/math&gt;. If &lt;math&gt;\mathcal{D}&lt;/math&gt; is a sub-&lt;math&gt;\sigma&lt;/math&gt;-algebra of &lt;math&gt;\mathcal{A}&lt;/math&gt;, so is the symmetric closure of &lt;math&gt;\mathcal{D}&lt;/math&gt;.

&lt;math&gt;F=G\left[\mathcal{A},\mu\right]&lt;/math&gt; iff &lt;math&gt;\left|\mathbf{1}_F-\mathbf{1}_G\right|=0&lt;/math&gt; &lt;math&gt;\left[\mathcal{A},\mu\right]&lt;/math&gt;-a.e.

==See also==
{{col-begin}}
{{col-break}}
* [[Algebra of sets]]
* [[Boolean function]]
* [[Difference (set theory)]]
* [[Exclusive or]]
* [[Fuzzy set]]
{{col-break}}
* [[Logical graph]]
* [[Set theory]]
* [[Symmetry]]
{{col-end}}

==References==
* {{cite book | last=Halmos | first=Paul R. | authorlink=Paul Halmos | title=[[Naive Set Theory (book)|Naive set theory]] | series=The University Series in Undergraduate Mathematics | publisher=van Nostrand Company | year=1960 | zbl=0087.04403 }} 

* {{planetmath reference|id=916|title=Symmetric difference}}
* {{MathWorld |title=Symmetric Difference |urlname=SymmetricDifference}}
* [http://www.encyclopediaofmath.org/index.php/Symmetric_difference_of_sets ''Symmetric difference of sets'']. In [[Encyclopaedia of Mathematics]]

{{Set theory}}

[[Category:Algebra]]
[[Category:Basic concepts in set theory]]
[[Category:Binary operations]]</text>
      <sha1>hivnlg6lurdcxu23cjm8vdt97jfvzd1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Monomial basis</title>
    <ns>0</ns>
    <id>976666</id>
    <revision>
      <id>612519597</id>
      <parentid>543841537</parentid>
      <timestamp>2014-06-11T16:46:49Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <comment>/* top */ ce</comment>
      <text xml:space="preserve" bytes="1283">In [[mathematics]] the '''monomial basis''' of a [[polynomial ring]] is its basis (as [[vector space]] or [[free module]] over the field or ring of coefficients) that consists in the set of all monomials. In fact, a [[polynomial]] may be uniquely written as a [[linear combination]] of [[monomial]]s. 

Univariate polynomials expressed on the monomial basis can be evaluated efficiently using [[Horner's method]].

==Definition==

The '''monomial basis''' for the vector space &lt;math&gt;\Pi_n&lt;/math&gt; of polynomials with degree ''n'' is the [[polynomial sequence]] of monomials

:&lt;math&gt;1,x,x^2,.\ldots,x^n&lt;/math&gt;

The '''monomial form''' of a polynomial &lt;math&gt;p \in \Pi_n&lt;/math&gt; is a linear combination of monomials

:&lt;math&gt;a_0 1 + a_1 x + a_2 x^2 + \ldots + a_n x^n&lt;/math&gt;

alternatively the shorter [[sigma notation]] can be used

:&lt;math&gt;p=\sum_{\nu=0}^n a_{\nu}x^\nu&lt;/math&gt;

==Notes==

A polynomial can always be converted into monomial form by calculating its [[Taylor expansion]] around 0.

==Examples==

A polynomial in &lt;math&gt;\Pi_4&lt;/math&gt;

:&lt;math&gt;1+x+3x^4&lt;/math&gt;

==See also==
*[[Horner's method]]
*[[Polynomial sequence]]
*[[Newton polynomial]]
*[[Lagrange polynomial]]
*[[Legendre polynomial]]
*[[Bernstein form]]
*[[Chebyshev form]]

[[Category:Algebra]]
[[Category:Polynomials]]</text>
      <sha1>s52xx7wqk7aetqjvvqo4wszpi4gfgbt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Polynomial transformations</title>
    <ns>0</ns>
    <id>3881088</id>
    <revision>
      <id>611406010</id>
      <parentid>611405828</parentid>
      <timestamp>2014-06-03T17:21:01Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>oops, no footnotes shouldn't take the same extra param that expert did</comment>
      <text xml:space="preserve" bytes="5354">&lt;!-- Please do not remove or change this AfD message until the issue is settled --&gt;
{{Article for deletion/dated|page=Polynomial transformations|timestamp=20140527181108|year=2014|month=May|day=27|substed=yes|help=off}}
&lt;!-- For administrator use only: {{Old AfD multi|page=Polynomial transformations|date=27 May 2014|result='''keep'''}} --&gt;
&lt;!-- End of AfD message, feel free to edit beyond this point --&gt;
{{multiple issues|1=
{{orphan | date=December 2011}}
{{no footnotes|date=May 2014}}
}}
In [[mathematics]], a '''[[polynomial]]''' is a function of the form &lt;math&gt; a_0 + a_1 x + \cdots + a_{n} x^n &lt;/math&gt;.  '''Polynomial transformations''' are modifications of the polynomial to form related polynomials, possibly of simpler form, typically with roots having a specific form of relationship to the roots of the original.

==Reciprocals of the roots==
Suppose we have some polynomial &lt;math&gt; a_0 + a_1 x + \cdots + a_{n} x^n &lt;/math&gt; and we are to find the sum of the [[Multiplicative inverse|reciprocals]] of the [[nth root|root]]s. Initially we may think that we are faced with the task of finding the roots and then their reciprocals and summing them, but there is an easier way.

For every root &lt;math&gt;r_n&lt;/math&gt; of f(x) the following holds,
&lt;math&gt; f\left( \frac{1}{\frac{1}{r_n}} \right) = 0 &lt;/math&gt;. Looking at this statement further we see that plugging in x for &lt;math&gt; \frac{1}{r_n} &lt;/math&gt; we get a polynomial whose roots are the reciprocals of the roots of f(x). Nevertheless this function will not be a polynomial, as its highest power will be &lt;math&gt; \frac{1}{x^n} &lt;/math&gt;. To turn this into a polynomial all we have to do is multiply it by &lt;math&gt; x^n &lt;/math&gt;. Note the order of the [[coefficient]]s, earlier the polynomial was &lt;math&gt; a_0 + a_1 x + \cdots + a_{n} x^n &lt;/math&gt;, now the coefficients have simply reversed so we get &lt;math&gt; a_{n} + a_{n-1} x + \cdots + a_0 x^n &lt;/math&gt;. The sum of the roots of this polynomial will be &lt;math&gt; a_1 &lt;/math&gt;, which was the product of the roots taken n-1 at a time for the original polynomial.

==Constant multiples of the roots==
To find say a function that has k times the roots of f(x) we can either again find all the roots, multiply then by k and then multiply them together appropriately to find some function for which they are the roots or we can think of it the smart way. We can make the substitution
&lt;math&gt; \frac{x}{2}  = x &lt;/math&gt; into f(x).

==Roots that differ by a constant==

Suppose we have some polynomial &lt;math&gt; a_0 + a_1 x + \cdots + a_{n-1} x^{(n-1)} &lt;/math&gt;  and we want to find some polynomial that has roots that are k units greater than the roots of f(x). One obvious approach would be to find the roots of f(x) and then multiply out a polynomial that has k greater than those roots. Also notice that this can be easily accomplished by thinking of the graph of the function. If it has roots at &lt;math&gt; r_1, r_2 \ldots r_n &lt;/math&gt; than we can simply shift the function over k units which will give us a function that has roots that are k units greater than the roots of f(x). so for ''x'' in ''f''(''x'') substitute ''x''&amp;nbsp;&amp;minus;&amp;nbsp;''k''.

==The pattern==
Suppose we have some one to one onto [[Function (mathematics)|function]] ''m''(''x'') and some function ''f''(''x'') that has roots &lt;math&gt; r_0, r_1, r_2, r_3, \ldots , r_n &lt;/math&gt; and we are faced with the problem of finding a function that has roots

:&lt;math&gt; m(r_0), m(r_1), m(r_2), \ldots, m(r_n). &lt;/math&gt;

Given that the function ''m''(''x'') is both one to one and onto we can find &lt;math&gt; m^{-1}(x) &lt;/math&gt;. Now to find a function that has roots &lt;math&gt; m(r_0), m(r_1), m(r_2), \ldots, m(r_n) &lt;/math&gt; we only have to make the substitution &lt;math&gt; x= m^{-1}(x) &lt;/math&gt;. This function will have roots at &lt;math&gt; m(r_0), m(r_1), m(r_2), \ldots, m(r_n) &lt;/math&gt; because ''f''(''x'') only has roots at &lt;math&gt; r_0, r_1, r_2, r_3, \ldots , r_n &lt;/math&gt; and plugging in &lt;math&gt; m(r_n) &lt;/math&gt; into the function will be the same as pluggiing in &lt;math&gt; r_n &lt;/math&gt;   into f(x) because  &lt;math&gt; m^{-1}(x) &lt;/math&gt; is a function such that &lt;math&gt;  m^{-1}(m(x)) = x &lt;/math&gt;.

==Transformation for equation-solving==

Polynomial transformations have been applied to the simplification of polynomial equations for solution, where possible, by radicals.  Descartes introduced the transformation of a polynomial of degree ''d'' which eliminates the term of degree ''d''−1 by a translation of the roots.  Such a polynomial is termed &quot;depressed&quot;: this already suffices to solve the quadratic by square roots.   In the case of the cubic, [[Tschirnhaus transformation]]s replace the variable by a quadratic function, thereby making it possible to eliminate two terms, and so can be used to eliminate the linear term in a depressed cubic to achieve the solution of the cubic by a combination of square and cube roots.  The Bring–Jerrard transformation, which is quartic in the variable, brings a quintic into &quot;principal&quot; or [[Bring-Jerrard normal form]] with terms of degree 5,1 and zero.

==References==
* {{cite journal | last1=Adamchik | first1=Victor S. | last2=Jeffrey | first2=David J. | title=Polynomial transformations of Tschirnhaus, Bring and Jerrard | zbl=1055.65063 | journal=SIGSAM Bull. | volume=37 | number=3 | pages=90-94 | year=2003 | url=http://www.sigsam.org/bulletin/articles/145/Adamchik.pdf }}

[[Category:Algebra]]</text>
      <sha1>fsylidsnn2r1rc5rjr7limjdfmev79u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>K-theory (physics)</title>
    <ns>0</ns>
    <id>4221033</id>
    <revision>
      <id>590240899</id>
      <parentid>582073529</parentid>
      <timestamp>2014-01-11T17:14:21Z</timestamp>
      <contributor>
        <username>Arch dude</username>
        <id>1763779</id>
      </contributor>
      <comment>/* Open problems */ this use of &quot;polarization&quot; is highly specific to [[geometric quantization]], where it is defined,  and is not any of the links.</comment>
      <text xml:space="preserve" bytes="22388">{{string theory}}

In [[string theory]], '''the K-theory classification''' refers to a conjectured application of ''[[K-theory]]'' (in [[abstract algebra]] and [[algebraic topology]]) to superstrings, to classify the allowed [[Ramond-Ramond field]] strengths as well as the charges of stable D-[[Membrane (M-theory)|branes]].

In [[condensed matter physics]] K-theory has also found important applications, specially in the topological classification of [[topological insulator]]s, superconductors and stable Fermi Surfaces ({{harvtxt|Kitaev|2009}}, {{harvtxt|Horava|2005}}).

== History ==

This conjecture, applied to D-brane charges, was first proposed by {{harvtxt|Minasian|Moore|1997}}.  It was popularized by {{harvtxt|Witten|1998}} who demonstrated that in [[type IIB]] string theory arises naturally from [[Ashoke Sen]]'s realization of arbitrary D-brane configurations as stacks of [[D9-brane|D9]] and anti-D9-branes after [[tachyon condensation]].

Such stacks of branes are inconsistent in a non-torsion [[Kalb-Ramond field|Neveu-Schwarz (NS) 3-form]] background, which, as was highlighted by {{harvtxt|Kapustin|2000}}, complicates the extension of the K-theory classification to such cases. {{harvtxt|Bouwknegt|Varghese|2000}} suggested a solution to this problem: D-branes are in general classified by a [[twisted K-theory]], that had earlier been defined by {{harvtxt|Rosenberg|1989}}.

== Applications ==
{{string theory}}
The K-theory classification of D-branes has had numerous applications.  For example, {{harvtxt|Hanany|Kol|2000}} used it to argue that there are eight species of [[orientifold]] one-plane. {{harvtxt|Uranga|2001}} applied the K-theory classification to derive new consistency conditions for [[Compactification (physics)#Flux compactification|flux compactification]]s.  K-theory has also been used to conjecture a formula for the topologies of [[T-duality|T-dual]] manifolds by {{harvtxt|Bouwknegt|Evslin|Varghese|2004}}.  Recently K-theory has been conjectured to classify the [[spinors]] in [[compactification (physics)|compactification]]s on [[generalized complex manifold]]s.

=== Open problems ===

Despite these successes, [[Ramond-Ramond field|RR fluxes]] are not quite classified by K-theory.  {{harvtxt|Diaconescu|Moore|Witten|2003}} argued that the K-theory classification is incompatible with [[S-duality]] in [[Type II string theory|IIB string theory]].

In addition, if one attempts to classify fluxes on a compact ten-dimensional spacetime, then a complication arises due to the self-duality of the RR fluxes.  The duality uses the [[Hodge star]], which depends on the metric and so is continuously valued and in particular is generically irrational.  Thus not all of the RR fluxes, which are interpreted as the [[Chern character]]s in K-theory, can be rational.  However Chern characters are always rational, and so the K-theory classification must be replaced.  One needs to choose a half of the fluxes to quantize, or a ''polarization'' in the [[geometric quantization]]-inspired language of Diaconescu, Moore, and Witten and later of {{harvtxt|Varghese|Sati|2004}}. Alternately one may use the K-theory of a 9-dimensional [[time]] slice as has been done by {{harvtxt|Maldacena|Moore|Seiberg|2001}}.

==K-theory classification of RR fluxes==

In the classical limit of [[type II string theory]], which is type II [[supergravity]], the [[Ramond-Ramond field|Ramond-Ramond field strengths]] are [[differential forms]].  In the quantum theory the well-definedness of the partition functions of D-branes implies that the RR field strengths obey [[magnetic monopole#Dirac's quantization|Dirac quantization conditions]] when [[spacetime]] is [[compact space|compact]], or when a spatial slice is compact and one considers only the (magnetic) components of the field strength which lie along the spatial directions.  This led twentieth century physicists to classify RR field strengths using [[cohomology]] with integral coefficients.

However some authors have argued that the cohomology of spacetime with integral coefficients is too big.  For example, in the presence of Neveu-Schwarz H-flux or non-spin cycles some RR fluxes dictate the presence of D-branes.  In the former case this is a consequence of the supergravity equation of motion which states that the product of a RR flux with the NS 3-form is a D-brane charge density.  Thus the set of topologically distinct RR field strengths that can exist in brane-free configurations is only a subset of the cohomology with integral coefficients.

This subset is still too big, because some of these classes are related by large gauge transformations.  In QED there are large gauge transformations which add integral multiples of two pi to Wilson loops.  The p-form potentials in type II supergravity theories also enjoy these large gauge transformations, but due to the presence of [[Chern-Simons]] terms in the supergravity actions these large gauge transformations transform not only the p-form potentials but also simultaneously the (p+3)-form field strengths.  Thus to obtain the space of inequivalent field strengths from the forementioned subset of integral cohomology we must quotient by these large gauge transformations.

The [[Twisted K-theory#How to calculate it|Atiyah-Hirzebruch spectral sequence]] constructs twisted K-theory, with a twist given by the NS 3-form field strength, as a quotient of a subset of the [[cohomology]] with integral coefficients.  In the classical limit, which corresponds to working with rational coefficients, this is precisely the quotient of a subset described above in supergravity.  The quantum corrections come from torsion classes and contain mod 2 torsion corrections due to the Freed-Witten anomaly.

Thus twisted K-theory classifies the subset of RR field strengths that can exist in the absence of D-branes quotiented by large gauge transformations. Daniel Freed has attempted to extend this classification to include also the RR potentials using differential K-theory.

==K-theory classification of D-branes==

K-theory classifies D-branes in noncompact spacetimes, intuitively in spacetimes in which we are not concerned about the flux sourced by the brane having nowhere to go.  While the K-theory of a 10d spacetime classifies D-branes as subsets of that spacetime, if the spacetime is the product of time and a fixed 9-manifold then K-theory also classifies the conserved D-brane charges on each 9-dimensional spatial slice.  While we were required to forget about RR potentials to obtain the K-theory classification of RR field strengths, we are required to forget about RR field strengths to obtain the K-theory classification of D-branes.

=== K-theory charge versus BPS charge ===

As has been stressed by [[Petr Hořava (theorist)|Petr Hořava]], the K-theory classification of D-branes is independent of, and in some ways stronger than, the classification of [[Bogomol'nyi Prasad Sommerfield bound|BPS states]].  K-theory appears to classify stable D-branes missed by [[supersymmetry]] based classifications.

For example, D-branes with torsion charges, that is with charges in the order N cyclic group &lt;math&gt;\mathbf Z_N&lt;/math&gt;, attract each other and so can never be BPS.  In fact, N such branes can decay, whereas no superposition of branes that satisfy a Bogomolny bound may ever decay.  However the charge of such branes is conserved modulo N, and this is captured by the K-theory classification but not by a BPS classification.  Such torsion branes have been applied, for example, to model [[Douglas-Shenker strings]] in supersymmetric U(N) [[gauge theory|gauge theories]].

===K-theory from tachyon condensation===

[[Ashoke Sen]] has conjectured that, in the absence of a topologically nontrivial NS 3-form flux, all IIB brane configurations can be obtained from stacks of spacefilling D9 and anti D9 branes via [[tachyon condensation]].  The topology of the resulting branes is encoded in the topology of the gauge bundle on the stack of the spacefilling branes.  The topology of the gauge bundle of a stack of D9s and anti D9s can be decomposed into a gauge bundle on the D9's and another bundle on the anti D9's.  Tachyon condensation transforms such a pair of bundles to another pair in which the same bundle is direct summed with each component in the pair.  Thus the tachyon condensation invariant quantity, that is, the charge which is conserved by the tachyon condensation process, is not a pair of bundles but rather the equivalence class of a pair of bundles under direct sums of the same bundle on both sides of the pair.  This is precisely the usual construction of [[topological K-theory]].  Thus the gauge bundles on stacks of D9's and anti-D9's are classified by topological K-theory.  If Sen's conjecture is right, all D-brane configurations in type IIB are then classified by K-theory.  [[Petr Hořava (theorist)|Petr Horava]] has extended this conjecture to type IIA using D8-branes.

===Twisted K-theory from MMS instantons===

While the tachyon condensation picture of the K-theory classification classifies D-branes as subsets of a 10-dimensional spacetime with no NS 3-form flux, the Maldacena, Moore, Seiberg picture classifies stable D-branes with finite mass as subsets of a 9-dimensional spatial slice of spacetime.

The central observation is that D-branes are not classified by integral homology because Dp-branes wrapping certain cycles suffer from a Freed-Witten anomaly, which is cancelled by the insertion of D(p-2)-branes and sometimes D(p-4)-branes that end on the afflicted Dp-brane.  These inserted branes may either continue to infinity, in which case the composite object has an infinite mass, or else they may end on an anti-Dp-brane, in which case the total Dp-brane charge is zero.  In either case, one may wish to remove the anomalous Dp-branes from the spectrum, leaving only a subset of the original integral cohomology.

The inserted branes are unstable.  To see this, imagine that they extend in time away (into the past) from the anomalous brane.  This corresponds to a process in which the inserted branes decay via a Dp-brane that forms, wraps the forementioned cycle and then disappears.  MMS&lt;ref&gt;[[Juan Maldacena]], [[Greg Moore (physicist)|Gregory Moore]] and [[Nathan Seiberg]]. ''D-Brane Instantons and K-Theory Charges''. http://arxiv.org/abs/hep-th/0108100&lt;/ref&gt; refer to this process as an instanton, although really it need not be instantonic.

The conserved charges are thus the nonanomolous subset quotiented by the unstable insertions.  This is precisely the [[Twisted K-theory#How to calculate it|Atiyah-Hirzebruch spectral sequence]] construction of twisted K-theory as a set.

==Reconciling twisted K-theory and S-duality==

Diaconescu, Moore, and Witten have pointed out that the twisted K-theory classification is not compatible with the [[S-duality]] covariance of type IIB string theory.  For example, consider the constraint on the [[Ramond-Ramond field|Ramond-Ramond 3-form field strength]] G&lt;sub&gt;3&lt;/sub&gt; in the [[Twisted K-theory#How to calculate it|Atiyah-Hirzebruch spectral sequence]] (AHSS):

:&lt;math&gt;  d_3G_3=Sq^3G_3+H\cup G_3=G_3\cup G_3+H\cup G_3=0
&lt;/math&gt;

where d&lt;sub&gt;3&lt;/sub&gt;=Sq&lt;sup&gt;3&lt;/sup&gt;+H is the first nontrivial differential in the AHSS, Sq&lt;sup&gt;3&lt;/sup&gt; is the third [[Steenrod square]] and the last equality follows from the fact that the nth Steenrod square acting on any n-form x is x&lt;math&gt;\cup&lt;/math&gt;x.

The above equation is not invariant under S-duality, which exchanges G&lt;sub&gt;3&lt;/sub&gt; and H.  Instead Diaconescu, Moore, and Witten have proposed the following S-duality covariant extension

:&lt;math&gt;  G_3\cup G_3+H\cup G_3+H\cup H=P
&lt;/math&gt;

where P is an unknown characteristic class that depends only on the topology, and in particular not on the fluxes. {{harvtxt|Diaconescu|Freed|Moore|2007}} have found a constraint on P using the [[ME8|E&lt;sub&gt;8&lt;/sub&gt; gauge theory approach to M-theory]] pioneered by Diaconescu, Moore, and Witten.

Thus D-branes in IIB are not classified by twisted K-theory after all, but some unknown S-duality-covariant object that inevitably also classifies both fundamental strings and [[NS5-brane]]s.

However the MMS prescription for calculating twisted K-theory is easily S-covariantized, as the Freed-Witten anomalies respect S-duality.  Thus the S-covariantized form of the MMS construction may be applied to construct the S-covariantized twisted K-theory, as a set, without knowing having any geometric description for just what this strange covariant object is.  This program has been carried out in a number of papers, such as {{harvtxt|Evslin|Varadarajan|2003}} and {{harvtxt|Evslin|2003a}}, and was also applied to the classification of fluxes by {{harvtxt|Evslin|2003b}}.  {{harvtxt|Bouwknegt|Evslin|Jurco|Varghese|2006}} use this approach to prove Diaconescu, Moore, and Witten's conjectured constraint on the 3-fluxes, and they show that there is an additional term equal to the D3-brane charge. {{harvtxt|Evslin|2006}} shows that the [[Klebanov-Strassler cascade]] of [[Seiberg duality|Seiberg dualities]] consists of a series of S-dual MMS instantons, one for each Seiberg duality.  The group, &lt;math&gt;\mathbf Z_N&lt;/math&gt; of universality classes of the &lt;math&gt;SU(M+N)\times SU(M)&lt;/math&gt; supersymmetric [[gauge theory]] is then shown to agree with the S-dual twisted K-theory and not with the original twisted K-theory.

Some authors have proposed radically different solutions to this puzzle.  For example, {{harvtxt|Kriz|Sati|2005}} propose that instead of twisted K-theory, II string theory configurations should be classified by [[elliptic cohomology]].

==Researchers==
Prominent researchers in this area include [[Ed Witten]], Peter Bouwknegt, Angel Uranga, Emanuel Diaconescu, [[Greg Moore (physicist)|Gregory Moore]], [[Anton Kapustin]], [[Jonathan Rosenberg (mathematician)|Jonathan Rosenberg]], Ruben Minasian, Amihay Hanany, Hisham Sati, [[Nathan Seiberg]], [[Juan Maldacena]], [[Daniel Freed]], and Igor Kriz.

==See also==
*[[Twisted K-theory]]
*[[Ramond-Ramond field]]
*[[Kalb-Ramond field]]
*[[D-brane]]

==Notes==
{{reflist}}

==References==
*{{citation
 | last1 = Bouwknegt | first1 = Peter
 | last2 = Evslin | first2 = Jarah
 | last3 = Jurco | first3 = Branislav
 | last4 = Varghese | first4 = Mathai | authorlink3 = Mathai Varghese
 | last5 = Sati | first5 = Hisham
 | title = Flux Compactifications on Projective Spaces and The S-Duality Puzzle
 | journal = [[Advances in Theoretical and Mathematical Physics]]
 | volume = 10 | year = 2006 | pages = 345–394
 | arxiv = hep-th/0501110|bibcode = 2005hep.th....1110B }}.

*{{citation
 | last1 = Bouwknegt | first1 = Peter
 | last2 = Evslin | first2 = Jarah
 | last3 = Varghese | first3 = Mathai | authorlink3 = Mathai Varghese
 | title = T-Duality: Topology Change from H-flux
 | journal = [[Communications in Mathematical Physics]]
 | volume = 249 | year = 2004 | pages = 383–415
 | doi = 10.1007/s00220-004-1115-6
 | arxiv = hep-th/0306062| issue = 2|bibcode = 2004CMaPh.249..383B }}.

*{{citation
 | last1 = Bouwknegt | first1 = Peter
 | last2 = Varghese | first2 = Mathai | authorlink2 = Mathai Varghese
 | title = D-branes, B-fields and twisted K-theory
 | journal = [[Journal of High Energy Physics]]
 | year = 2000
 | volume = 0003
 | issue = 007
 | doi = 10.1088/1126-6708/2000/03/007
 | pages = 007
 | arxiv = hep-th/0002023|bibcode = 2000JHEP...03..007B }}.

*{{citation
 | last1 = Diaconescu | first1 = Emanuel
 | last2 = Freed | first2 = Daniel S. | authorlink2=Dan Freed
 | last3 = Moore | first3 = Gregory
 | contribution = The M-theory 3-form and E&lt;sub&gt;8&lt;/sub&gt; gauge theory
 | title = Elliptic Cohomology: Geometry, Applications, and Higher Chromatic Analogues
 | editor1-last = Miller | editor1-first = Haynes R.
 | editor2-last = Ravenel | editor2-first = Douglas C.
 | publisher = Cambridge University Press
 | year = 2007 | pages = 44–88
 | arxiv = hep-th/0312069|bibcode = 2003hep.th...12069D }}.

*{{citation
 | last1 = Diaconescu | first1 = Emanuel
 | last2 = Moore | first2 = Gregory
 | last3 = Witten | first3 = Edward | authorlink3 = Edward Witten
 | title = E&lt;sub&gt;8&lt;/sub&gt; Gauge Theory, and a Derivation of K-Theory from M-Theory
 | journal = [[Advances in Theoretical and Mathematical Physics]]
 | volume = 6 | year = 2003 | pages = 1031–1134
 | arxiv = hep-th/0005090|bibcode = 2000hep.th....5090D }}.

*{{citation
 | last = Evslin | first = Jarah
 | title = IIB Soliton Spectra with All Fluxes Activated
 | journal = [[Nuclear Physics B]]
 | volume = 657
 | year = 2003a
 | pages = 139–168
 | doi = 10.1016/S0550-3213(03)00154-8|arxiv = hep-th/0211172 |bibcode = 2003NuPhB.657..139E }}.

*{{citation
 | last = Evslin | first = Jarah
 | title = Twisted K-Theory from Monodromies
 | journal = [[Journal of High Energy Physics]]
 | volume = 0305 | year = 2003b | issue = 030
 | doi = 10.1088/1126-6708/2003/05/030
 | pages = 030
 | arxiv = hep-th/0302081|bibcode = 2003JHEP...05..030E }}.

*{{citation
 | last = Evslin | first = Jarah
 | contribution = The Cascade is a MMS Instanton
 | title = Advances in Soliton Research
 | publisher = Nova Science Publishers
 | year = 2006
 | pages = 153–187
 | arxiv = hep-th/0405210|bibcode = 2004hep.th....5210E }}.

*{{citation
 | last1 = Evslin | first1 = Jarah
 | last2 = Varadarajan | first2 = Uday
 | title = K-Theory and S-Duality: Starting Over from Square 3
 | journal = Journal of High Energy Physics
 | volume = 0303 | year = 2003 | issue = 026
 | doi = 10.1088/1126-6708/2003/03/026
 | pages = 026
 | arxiv = hep-th/0112084|bibcode = 2003JHEP...03..026E }}.

*{{citation
 | last = Hanany | first1 = Amihay
 | last2 = Kol | first2 = Barak | authorlink2 = Barak Kol
 | title = On Orientifolds, Discrete Torsion, Branes and M Theory
 | journal = Journal of High Energy Physics
 | year = 2000
 | volume = 0006
 | issue = 013
 | doi = 10.1088/1126-6708/2000/06/013
 | pages = 013
 | arxiv = hep-th/0003025|bibcode = 2000JHEP...06..013H }}.

*{{citation
 | last = Kapustin
 | first = Anton
 | title = D-branes in a topologically nontrivial B-field
 | journal = [[Advances in Theoretical and Mathematical Physics]]
 | volume = 4
 | year = 2000
 | pages = 127–154
 | arxiv = hep-th/9909089|bibcode = 1999hep.th....9089K }}.

*{{citation
 | last1 = Kriz | first1 = Igor
 | last2 = Sati | first2 = Hisham
 | title = Type IIB String Theory, S-Duality, and Generalized Cohomology
 | journal = Nuclear Physics B
 | volume = 715 | year = 2005 | pages = 639–664
 | doi = 10.1016/j.nuclphysb.2005.02.016
 | arxiv = hep-th/0410293| issue = 3|bibcode = 2005NuPhB.715..639K }}.

*{{citation
 | last1 = Maldacena | first1 = Juan | authorlink1 = Juan Maldacena
 | last2 = Moore | first2 = Gregory
 | last3 = Seiberg | first3 = Nathan | authorlink3 = Nathan Seiberg
 | title = D-Brane Instantons and K-Theory Charges
 | journal = Journal of High Energy Physics
 | volume = 0111 | year = 2001 | issue = 062
 | doi = 10.1088/1126-6708/2001/11/062
 | pages = 062
 | arxiv = hep-th/0108100|bibcode = 2001JHEP...11..062M }}.

*{{citation
 | last1 = Minasian | first1 = Ruben
 | last2 = Moore | first2 = Gregory
 | title = K-theory and Ramond-Ramond charge
 | journal = Journal of High Energy Physics
 | volume = 9711
 | year = 1997
 | issue = 002
 | doi =10.1088/1126-6708/1997/11/002
 | pages = 002
 | arxiv = hep-th/9710230|bibcode = 1997JHEP...11..002M }}.

*{{citation
 | last1 = Olsen | first1 = Kasper
 | last2 = Szabo | first2 = Richard J. | authorlink2 = Richard J. Szabo
 | title = Constructing D-Branes from K-Theory
 | journal = Advances in Theoretical and Mathematical Physics
 | volume = 3 | year = 1999 | pages = 889–1025
 | arxiv = hep-th/9907140|bibcode = 1999hep.th....7140O }}.

*{{citation
 | last = Rosenberg | first = Jonathan
 | title = Continuous-Trace Algebras from the Bundle Theoretic Point of View
 | journal = Journal of the Australian Mathematical Society, Series A
 | volume = 47 | year = 1989 | pages = 368–381
 | url = http://anziamj.austms.org.au/JAMSA/V47/Part3/Rosenberg.html
 | doi = 10.1017/S1446788700033097
 | issue = 03}}.

*{{citation
 | last = Uranga | first = Angel M.
 | title = D-brane probes, RR tadpole cancellation and K-theory charge
 | journal = Nuclear Physics B
 | volume = 598 | year = 2001 | pages = 225–246
 | doi = 10.1016/S0550-3213(00)00787-2
 | arxiv = hep-th/0011048|bibcode = 2001NuPhB.598..225U }}.

*{{citation
 | last1 = Varghese | first1 = Mathai | authorlink1 = Mathai Varghese
 | last2 = Sati | first2 = Hisham
 | title = Some Relations between Twisted K-theory and E&lt;sub&gt;8&lt;/sub&gt; Gauge Theory
 | journal = Journal of High Energy Physics
 | volume = 0403 | year = 2004 | issue = 016
 | doi = 10.1088/1126-6708/2004/03/016
 | pages = 016
 | arxiv = hep-th/0312033|bibcode = 2004JHEP...03..016M }}.

*{{citation
 | last = Witten | first = Edward | authorlink = Edward Witten
 | title = D-Branes and K-Theory
 | journal = Journal of High Energy Physics
 | volume = 9812
 | year = 1998
 | issue = 019
 | doi = 10.1088/1126-6708/1998/12/019
 | pages = 019
 | arxiv = hep-th/9810188|bibcode = 1998JHEP...12..019W }}.

==References (Condensed Matter Physics)==
*{{citation
 | last = Kitaev | first = Alexei | authorlink = Alexei Kitaev
 | title = Periodic table for topological insulators and superconductors
 | journal = Arxiv (cond-mat)
 | year = 2009
 | arxiv = 0901.2686|bibcode = 2009AIPC.1134...22K |doi = 10.1063/1.3149495 }}.

*{{citation
 | last = Horava | first = Petr
 | title = Stability of Fermi Surfaces and K Theory
 | journal = Physical Review Letters
 | volume = 95
 | year = 2005
 | issue = 016405}}.

==Further reading==
An excellent introduction to the [[K-theory]] classification of [[D-branes]] in 10 dimensions via [[Ashoke Sen]]'s conjecture is the original paper &quot;D-branes and K-theory&quot; by [[Edward Witten]]; there is also an extensive review by {{harvtxt|Olsen|Szabo|1999}}.

A very comprehensible introduction to the [[twisted K-theory]] classification of conserved D-brane charges on a 9-dimensional [[timeslice]] in the presence of [[Neveu-Schwarz flux]] is {{harvtxt|Maldacena|Moore|Seiberg|2001}}.

==External links==
*[http://xstructure.inr.ac.ru/x-bin/theme3.py?level=1&amp;index1=296918 K-theory on arxiv.org]

[[Category:String theory]]
[[Category:Algebra]]
[[Category:K-theory]]

[[de:K-Theorie]]
[[fr:K-théorie]]
[[it:K-teoria ritorta]]
[[pt:K-Teoria (Física)]]</text>
      <sha1>qjg5mepr7025spkud4i585fwjt1onyf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Goursat's lemma</title>
    <ns>0</ns>
    <id>5114212</id>
    <revision>
      <id>612491248</id>
      <parentid>541677544</parentid>
      <timestamp>2014-06-11T12:39:46Z</timestamp>
      <contributor>
        <ip>94.112.136.34</ip>
      </contributor>
      <comment>Add note about goursat's integral lemma, used to prove cauchy's integral theorem</comment>
      <text xml:space="preserve" bytes="3839">: ''Not to be confused with [[Goursat's integral lemma]] from [[Complex analysis]]''

'''[[Goursat]]'s lemma''' is an [[algebra]]ic [[theorem]] about [[subgroup]]s of the [[Direct product of groups|direct product]] of two [[Group (mathematics)|groups]]. 

It can be stated as follows.
:Let &lt;math&gt;G&lt;/math&gt;, &lt;math&gt;G'&lt;/math&gt; be groups, and let &lt;math&gt;H&lt;/math&gt; be a subgroup of &lt;math&gt;G\times G'&lt;/math&gt; such that the two [[projection (mathematics)|projections]] &lt;math&gt;p_1: H\rightarrow G&lt;/math&gt; and &lt;math&gt;p_2: H\rightarrow G'&lt;/math&gt; are [[surjective]] (i.e., &lt;math&gt;H&lt;/math&gt; is a [[subdirect product]] of &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;G'&lt;/math&gt;). Let &lt;math&gt;N&lt;/math&gt; be the kernel of &lt;math&gt;p_2&lt;/math&gt; and &lt;math&gt;N'&lt;/math&gt; the [[Kernel (algebra)|kernel]] of &lt;math&gt;p_1&lt;/math&gt;. One can identify &lt;math&gt;N&lt;/math&gt; as a [[normal subgroup]] of &lt;math&gt;G&lt;/math&gt;, and &lt;math&gt;N'&lt;/math&gt; as a normal subgroup of &lt;math&gt;G'&lt;/math&gt;. Then the image of &lt;math&gt;H&lt;/math&gt; in &lt;math&gt;G/N\times G'/N'&lt;/math&gt; is the [[graph of a function|graph]] of an [[isomorphism]] &lt;math&gt;G/N\approx G'/N'&lt;/math&gt;.

An immediate consequence of this is that the subdirect product of two groups can be described as a [[Direct product of groups#Fiber products|fiber product]] and vice versa.

== Proof of Goursat's lemma ==

Before proceeding with the [[Mathematical proof|proof]], &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;N'&lt;/math&gt; are shown to be normal in &lt;math&gt;G \times \{e'\}&lt;/math&gt; and &lt;math&gt;\{e\} \times G'&lt;/math&gt;, respectively.  It is in this sense that &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;N'&lt;/math&gt; can be identified as normal in ''G'' and ''G''', respectively.

Since &lt;math&gt;p_2&lt;/math&gt; is a [[homomorphism]], its kernel ''N'' is normal in ''H''. Moreover, given &lt;math&gt;g \in G&lt;/math&gt;, there exists &lt;math&gt;h=(g,g') \in H&lt;/math&gt;, since &lt;math&gt;p_1&lt;/math&gt; is surjective.  Therefore, &lt;math&gt;p_1(N)&lt;/math&gt; is normal in ''G'', viz:
:&lt;math&gt;gp_1(N)=p_1(h)p_1(N)=p_1(hN)=p_1(Nh)=p_1(N)g&lt;/math&gt;.
It follows that &lt;math&gt;N&lt;/math&gt; is normal in &lt;math&gt;G \times \{e'\}&lt;/math&gt; since
: &lt;math&gt;(g,e')N = (g,e')(p_1(N) \times \{e'\}) = gp_1(N) \times \{e'\} = p_1(N)g \times \{e'\} = (p_1(N) \times \{e'\})(g,e')=N(g,e')&lt;/math&gt;. 

The proof that &lt;math&gt;N'&lt;/math&gt; is normal in &lt;math&gt;\{e\} \times G'&lt;/math&gt; proceeds in a similar manner.

Given the identification of &lt;math&gt;G&lt;/math&gt; with &lt;math&gt;G \times \{e'\}&lt;/math&gt;, we can write &lt;math&gt;G/N&lt;/math&gt; and &lt;math&gt;gN&lt;/math&gt; instead of &lt;math&gt;(G \times \{e'\})/N&lt;/math&gt; and &lt;math&gt;(g,e')N&lt;/math&gt;, &lt;math&gt;g \in G&lt;/math&gt;.  Similarly, we can write &lt;math&gt;G'/N'&lt;/math&gt; and &lt;math&gt;g'N'&lt;/math&gt;, &lt;math&gt;g' \in G'&lt;/math&gt;.

On to the proof. Consider the map &lt;math&gt;H \rightarrow G/N \times G'/N'&lt;/math&gt; defined by &lt;math&gt;(g,g') \mapsto (gN, g'N')&lt;/math&gt;. The image of &lt;math&gt;H&lt;/math&gt; under this map is &lt;math&gt;\{(gN,g'N') | (g,g') \in H \}&lt;/math&gt;.  This [[Relation (mathematics)|relation]] is the graph of a [[well-defined]] function &lt;math&gt;G/N \rightarrow G'/N'&lt;/math&gt; provided &lt;math&gt;gN=N \Rightarrow g'N'=N'&lt;/math&gt;, essentially an application of the [[vertical line test]].

Since &lt;math&gt;gN=N&lt;/math&gt; (more properly, &lt;math&gt;(g,e')N=N&lt;/math&gt;), we have &lt;math&gt;(g,e') \in N \subset H&lt;/math&gt;. Thus &lt;math&gt;(e,g') = (g,g')(g^{-1},e') \in H&lt;/math&gt;, whence &lt;math&gt;(e,g') \in N'&lt;/math&gt;, that is, &lt;math&gt;g'N'=N'&lt;/math&gt;. Note that by symmetry, it is immediately clear that &lt;math&gt;g'N'=N' \Rightarrow gN=N&lt;/math&gt;, i.e., this function also passes the [[horizontal line test]], and is therefore [[injective function|one-to-one]].  The fact that this function is a surjective group homomorphism follows directly.

== References ==

* [[Ken Ribet|Kenneth A. Ribet]] (Autumn 1976), &quot;[[Galois]] [[Group action|Action]] on Division Points of [[Abelian Variety|Abelian Varieties]] with Real Multiplications&quot;, ''[[American Journal of Mathematics]]'', Vol. 98, No. 3, 751–804.

[[Category:Algebra]]
[[Category:Lemmas]]
[[Category:Articles containing proofs]]</text>
      <sha1>izjduavour35xqytvfmqyoyb64cl30z</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Conjugate (algebra)</title>
    <ns>0</ns>
    <id>7523925</id>
    <revision>
      <id>589625432</id>
      <parentid>589440804</parentid>
      <timestamp>2014-01-07T17:09:09Z</timestamp>
      <contributor>
        <username>Quondum</username>
        <id>12331483</id>
      </contributor>
      <comment>Undid revision 589440804 by [[Special:Contributions/120.28.150.232|120.28.150.232]] – unexplained change</comment>
      <text xml:space="preserve" bytes="1182">{{About|binomial conjugates in algebra||Conjugate (disambiguation)}}
In [[algebra]], a '''conjugate''' is a [[binomial]] formed by negating the second term of a binomial. The conjugate of {{nowrap|''x'' + ''y''}} is {{nowrap|''x'' − ''y''}}, where ''x'' and ''y'' are [[real number]]s. If ''y'' is [[imaginary number|imaginary]], the process is termed [[complex conjugation]]: the [[complex conjugate]] of {{nowrap|''a'' + ''bi''}} is {{nowrap|''a'' − ''bi''}}, where ''a'' and ''b'' are real.

== Differences of squares ==

{{main|Difference of two squares}}
In a [[commutative ring]], an expression of the form

:&lt;math&gt; a^2-b^2 &lt;/math&gt;

can be factored to give

:&lt;math&gt; (a+b)(a-b) &lt;/math&gt;

where one factor is the conjugate of the other. This can be useful when trying to [[Rationalisation (mathematics)|rationalize]] a denominator containing radicals (√).

==See also==
*[[Conjugate element (field theory)]]

==External links==
*[http://www.mathwords.com/r/rationalizing_the_denominator.htm Rationalizing the Denominator] from Mathwords.com
*[http://www.blc.edu/fac/rbuelow/common/glossarya-m.htm#conjugate Math glossary] from Bethany Lutheran College

[[Category:Algebra]]</text>
      <sha1>l1zyhwremuxv73fdckclp1u7nmpdc8t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Immanant of a matrix</title>
    <ns>0</ns>
    <id>6466838</id>
    <revision>
      <id>596083467</id>
      <parentid>572557730</parentid>
      <timestamp>2014-02-18T20:59:58Z</timestamp>
      <contributor>
        <username>Vieque</username>
        <id>20650448</id>
      </contributor>
      <minor/>
      <comment>Disambiguated: [[Schur function]] → [[Schur polynomial]]</comment>
      <text xml:space="preserve" bytes="2218">:''Immanant redirects here; it should not be confused with the philosophical [[Immanence|immanent]].''

In mathematics, the '''immanant of a matrix'''  was defined by [[Dudley E. Littlewood]] and [[Archibald Read Richardson]] as a generalisation of the concepts of [[determinant]] and [[permanent]].

Let &lt;math&gt;\lambda=(\lambda_1,\lambda_2,\ldots)&lt;/math&gt; be a [[Partition_%28number_theory%29|partition]] of &lt;math&gt;n&lt;/math&gt; and let &lt;math&gt;\chi_\lambda&lt;/math&gt; be the corresponding irreducible [[Group representation|representation-theoretic]] [[Character_%28group_theory%29|character]] of the [[symmetric group]] &lt;math&gt;S_n&lt;/math&gt;. The ''immanant'' of an &lt;math&gt;n\times n&lt;/math&gt; [[Matrix_%28mathematics%29|matrix]] &lt;math&gt;A=(a_{ij})&lt;/math&gt; associated with the character &lt;math&gt;\chi_\lambda&lt;/math&gt; is defined as the expression

:&lt;math&gt;{\rm Imm}_\lambda(A)=\sum_{\sigma\in S_n}\chi_\lambda(\sigma)a_{1\sigma(1)}a_{2\sigma(2)}\cdots a_{n\sigma(n)}.&lt;/math&gt;

The determinant is a special case of the immanant, where &lt;math&gt;\chi_\lambda&lt;/math&gt; is the [[alternating character]] &lt;math&gt;\sgn&lt;/math&gt;, of ''S''&lt;sub&gt;''n''&lt;/sub&gt;, defined by the [[parity of a permutation]].

The permanent is the case where &lt;math&gt;\chi_\lambda&lt;/math&gt; is the [[trivial character]], which is identically equal to 1.

Littlewood and Richardson also studied its relation to [[Schur polynomial|Schur functions]] in the [[representation theory of the symmetric group]].

==References==

* {{cite journal | author=D.E. Littlewood | authorlink=Dudley E. Littlewood | coauthors=[[Archibald Read Richardson|A.R. Richardson]] | title=Group characters and algebras | journal=[[Philosophical Transactions of the Royal Society A]] | year=1934 | volume=233 | pages=99–124 | doi=10.1098/rsta.1934.0015 | issue=721–730 }}

* {{cite book | author=D.E. Littlewood | authorlink=Dudley E. Littlewood | title=The Theory of Group Characters and Matrix Representations of Groups | edition=2nd | year=1950 | publisher=Oxford Univ. Press (reprinted by AMS, 2006) | page=81 }}

==External links==
*[http://planetmath.org/encyclopedia/Immanent.html Immanent] at ''[[PlanetMath]]''

[[Category:Algebra]]
[[Category:Linear algebra]]
[[Category:Matrix theory]]
[[Category:Permutations]]</text>
      <sha1>m6633k840gx7vy5pf7j2gwk5usla8yz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Alternatization</title>
    <ns>0</ns>
    <id>8200947</id>
    <revision>
      <id>607118239</id>
      <parentid>596522013</parentid>
      <timestamp>2014-05-05T04:20:24Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>{{abstract-algebra-stub}}</comment>
      <text xml:space="preserve" bytes="1161">{{Unreferenced|date=November 2006}}
In [[mathematics]], the notion of '''alternatization''' or '''alternatisation''' is used to pass from any map to an alternating map.

Let &lt;math&gt;S&lt;/math&gt; be a set and &lt;math&gt;A&lt;/math&gt; an [[abelian group]]. Given a map &lt;math&gt;\alpha: S \times S \to A&lt;/math&gt;, &lt;math&gt;\alpha&lt;/math&gt; is termed an '''alternating map''' if &lt;math&gt;\alpha(s,s) = 0&lt;/math&gt; for all &lt;math&gt;s \in S&lt;/math&gt; and &lt;math&gt;\alpha(s,t) + \alpha(t,s) = 0&lt;/math&gt; for all &lt;math&gt;s,t \in S&lt;/math&gt;.

The alternatization of a general map &lt;math&gt;\alpha: S \times S \to A&lt;/math&gt; is the map &lt;math&gt;(x,y) \mapsto \alpha(x,y) - \alpha(y,x)&lt;/math&gt;.

The alternatization of an alternating map is simply its double, while the alternatization of a [[symmetric map]] is zero.

The alternatization of a [[bilinear map]] is bilinear. There may be non-bilinear maps whose alternatization is bilinear. Most notably, the alternatization of any [[Cocycle (algebraic topology)|cocycle]] is bilinear. This fact plays a crucial role in identifying the second cohomology group of a lattice with the group of alternating bilinear forms on a lattice.

[[Category:Algebra]]


{{abstract-algebra-stub}}</text>
      <sha1>6uw3oxotlybwus267f2f9compm1ephj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Power set</title>
    <ns>0</ns>
    <id>23799</id>
    <revision>
      <id>605102982</id>
      <parentid>599753485</parentid>
      <timestamp>2014-04-21T04:11:25Z</timestamp>
      <contributor>
        <ip>190.222.229.158</ip>
      </contributor>
      <comment>/* Properties */</comment>
      <text xml:space="preserve" bytes="13161">{{For|the search engine developer|Powerset (company)}}

[[Image:Hasse diagram of powerset of 3.svg|right|thumb|250px|The elements of the power set of the set {''x'', ''y'', ''z''}  [[order theory|ordered]] in respect to [[Inclusion (set theory)|inclusion]].]]

In [[mathematics]], the '''power set''' (or '''powerset''') of any [[Set (mathematics)|set]] ''S'', written &lt;math&gt;\mathcal{P}(S)&lt;/math&gt;, ''P''(''S''), ℙ(''S''), [[Weierstrass p|&amp;weierp;]](''S'') or [[Power set#Representing subsets as functions|2&lt;sup&gt;''S''&lt;/sup&gt;]], is the set of all [[subset]]s of ''S'', including the [[empty set]] and S itself. In [[axiomatic set theory]] (as developed, for example, in the [[ZFC]] axioms), the existence of the power set of any set is postulated by the [[axiom of power set]].&lt;ref&gt;Devlin (1979) p.50&lt;/ref&gt;

Any subset of &lt;math&gt;\mathcal{P}(S)&lt;/math&gt; is called a ''[[family of sets]]'' over ''S''.

==Example==
If ''S'' is the set {''x'', ''y'', ''z''}, then the subsets of  ''S'' are:

* {} (also denoted &lt;math&gt;\varnothing&lt;/math&gt;, the  [[empty set]])
* {''x''} 
* {''y''}
* {''z''}
* {''x'', ''y''}
* {''x'', ''z''}
* {''y'', ''z''}
* {''x'', ''y'', ''z''}
and hence the power set of ''S'' is {{}, {''x''}, {''y''}, {''z''}, {''x'', ''y''}, {''x'', ''z''}, {''y'', ''z''}, {''x'', ''y'', ''z''}}.&lt;ref&gt;Puntambekar (2007), {{Google books quote|id=IYnkhA92BzIC|pg=SA1-PA2|text=The power set is|p. 1-2}}&lt;/ref&gt;

==Properties==
If ''S'' is a finite set with |''S''| = ''n'' elements, then the number of subsets of ''S'' is &lt;math&gt;|\mathcal{P}(S)| = 2^n&lt;/math&gt;. This fact, which is the motivation for the notation 2&lt;sup&gt;''S''&lt;/sup&gt;, may be demonstrated simply as follows,
: We write any subset of ''S'' in the format &lt;math&gt;\{\omega_1, \omega_2, \ldots, \omega_n\}&lt;/math&gt; where &lt;math&gt;\omega_i , 1 \le i \le n&lt;/math&gt;, can take the value of &lt;math&gt;0 &lt;/math&gt; or &lt;math&gt; 1&lt;/math&gt;. If &lt;math&gt; \omega_i = 1&lt;/math&gt;, the &lt;math&gt; i &lt;/math&gt;-th element of ''S'' is in the subset; otherwise, the &lt;math&gt;i&lt;/math&gt;-th element is not in the subset. Clearly the number of distinct subsets that can be constructed this way is &lt;math&gt;2^n&lt;/math&gt;.

[[Cantor's diagonal argument]] shows that the power set of a set (whether infinite or not) always has strictly higher [[cardinality]] than the set itself (informally the power set must be larger than the original set). In particular, [[Cantor's theorem]] shows that the power set of a [[countable set|countably infinite]] set is [[uncountable|uncountably]] infinite. For example, the power set of the set of [[natural number]]s can be put in a [[bijection|one-to-one correspondence]] with the set of [[real number]]s (see  [[cardinality of the continuum]]).

The power set of a set ''S'', together with the operations of [[union (set theory)|union]], [[intersection (set theory)|intersection]] and [[complement (set theory)|complement]] can be viewed as the prototypical example of a [[Boolean algebra (structure)|Boolean algebra]]. In fact, one can show that any ''finite'' Boolean algebra is isomorphic to the Boolean algebra of the power set of a finite set. For ''infinite'' Boolean algebras this is no longer true, but every infinite Boolean algebra can be represented as a [[subalgebra]] of a power set Boolean algebra (see [[Stone's representation theorem]]).

The power set of a set ''S'' forms an [[Abelian group]] when considered with the operation of [[symmetric difference]] (with the empty set as the identity element and each set being its own inverse) and a [[commutative]] [[monoid]] when considered with the operation of intersection. It can hence be shown (by proving the [[Distributive property|distributive laws]]) that the power set considered together with both of these operations forms a [[Boolean ring]].

==Representing subsets as functions== &lt;!-- this section is referenced forme some other place in this article, do not change its title carelessly --&gt;
In set theory, [[Function (mathematics)#Function_spaces|''X''&lt;sup&gt;''Y''&lt;/sup&gt;]] is the set of all [[function (mathematics)|function]]s from ''Y'' to ''X''.  As &quot;2&quot; can be defined as {0,1} (see [[Natural_number#A_standard_construction|natural number]]), 2&lt;sup&gt;''S''&lt;/sup&gt; (i.e., {0,1}&lt;sup&gt;''S''&lt;/sup&gt;) is the set of all [[function (mathematics)|function]]s from ''S'' to {0,1}.  By identifying a function in 2&lt;sup&gt;''S''&lt;/sup&gt; with the corresponding [[preimage]] of 1, we see that there is a [[bijection]] between 2&lt;sup&gt;''S''&lt;/sup&gt; and &lt;math&gt;\mathcal{P}(S)&lt;/math&gt;, where each function is the [[indicator function|characteristic function]] of the subset in &lt;math&gt;\mathcal{P}(S)&lt;/math&gt; with which it is identified.  Hence 2&lt;sup&gt;''S''&lt;/sup&gt; and &lt;math&gt;\mathcal{P}(S)&lt;/math&gt; could be considered identical set-theoretically.  (Thus there are two distinct [[set notation#Metaphor in denoting sets|notational motivations]] for denoting the power set by 2&lt;sup&gt;''S''&lt;/sup&gt;:  the fact that this function-representation of subsets makes it a special case of the ''X''&lt;sup&gt;''Y''&lt;/sup&gt; notation and the property, [[power set#Properties|mentioned above]], that |2&lt;sup&gt;S&lt;/sup&gt;| = 2&lt;sup&gt;|S|&lt;/sup&gt;.)

This notion can be applied to the example [[Power set#Example|above]] in which &lt;math&gt;S = \{x, y, z\}&lt;/math&gt; to see the isomorphism with the binary numbers 
from 0 to 2&lt;sup&gt;n&lt;/sup&gt;−1 with n being the number of elements in the set. 
In ''S'', a ''1'' in the position corresponding to the location in the set indicates the presence of the 
element. So {''x'', ''y''} = 110.

For the whole power set of ''S'' we get:

* {&amp;nbsp;} = 000 (Binary) = 0 (Decimal)
* {''x''} = 100 = 4
* {''y''} = 010 = 2
* {''z''} = 001 = 1
* {''x'', ''y''} = 110 = 6
* {''x'', ''z''} = 101 = 5
* {''y'', ''z''} = 011 = 3
* {''x'', ''y'', ''z''} = 111 = 7

==Relation to binomial theorem==
The power set is closely related to the [[binomial theorem]].  The number of sets with &lt;math&gt;k&lt;/math&gt; elements in the power set of a set with &lt;math&gt;n&lt;/math&gt; elements will be a [[combination]] &lt;math&gt;C(n,k),&lt;/math&gt; also called a [[binomial coefficient]].

For example the power set of a set with three elements, has:

*&lt;math&gt;C(3, 0) = 1&lt;/math&gt; set with 0 elements
*&lt;math&gt;C(3, 1) = 3&lt;/math&gt; sets with 1 element
*&lt;math&gt;C(3, 2) = 3&lt;/math&gt; sets with 2 elements
*&lt;math&gt;C(3, 3) = 1&lt;/math&gt; set with 3 elements.

==Algorithms==
If &lt;math&gt;S \!&lt;/math&gt; is a [[finite set]], there is a [[recursive algorithm]] to calculate &lt;math&gt; \mathcal{P}(S) &lt;/math&gt;.

Define the operation &lt;math&gt; \mathcal{F}(e,T) =  \{ X \cup \{ e \} | X \in T \} &lt;/math&gt;

In English, return the set with the element &lt;math&gt;e \!&lt;/math&gt; added to each set &lt;math&gt;X \!&lt;/math&gt; in &lt;math&gt;T \!&lt;/math&gt;.

*If &lt;math&gt;S = \{\} \!&lt;/math&gt;,then &lt;math&gt; \mathcal{P}(S) = \{ \{ \} \} &lt;/math&gt; is returned.
*Otherwise:
:*Let &lt;math&gt;e \!&lt;/math&gt; be any single element of &lt;math&gt;S \!&lt;/math&gt;.
:*Let &lt;math&gt; T = S \setminus \{ e \} \!&lt;/math&gt;, where '&lt;math&gt;S \setminus \{ e \} \!&lt;/math&gt;' denotes the [[Complement (set theory)|relative complement]] of &lt;math&gt;\{ e \} \!&lt;/math&gt; in &lt;math&gt;S \!&lt;/math&gt;.
:*And the result: &lt;math&gt; \mathcal{P}(S) = \mathcal{P}(T) \cup \mathcal{F}(e,\mathcal{P}(T)) &lt;/math&gt; is returned.

In other words, the power set of the empty set is the set containing the empty set and the power set of any other set is all the subsets of the set containing some specific element and all the subsets of the set not containing that specific element.

There are other more efficient ways to calculate the power set. For example, use a list of the ''n'' elements of ''S'' to fix a mapping from the [[bit]] positions of ''n''-bit  numbers to those elements; then with a simple loop run through all the 2&lt;sup&gt;''n''&lt;/sup&gt; numbers representable with ''n'' bits, and for each contribute the subset of ''S'' corresponding to the bits that are set (to 1) in the number.  When ''n'' exceeds the [[Word (computer architecture)|word-length]] of the computer (typically 64 in modern [[CPU]]s) the representation is naturally extended by using an array of words instead of a single word.

==Subsets of limited cardinality==
The set of subsets of ''S'' of [[cardinality]] less than κ is denoted by &lt;math&gt;\mathcal{P}_{\kappa}(S)&lt;/math&gt; or &lt;math&gt;\mathcal{P}_{&lt;\kappa}(S) \,.&lt;/math&gt; Similarly, the set of non-empty subsets of ''S'' might be denoted by &lt;math&gt;\mathcal{P}_{\geq 1}(S) \,.&lt;/math&gt;

==Power object==
A set can be regarded as an algebra having no nontrivial operations or defining equations.  From this perspective the idea of the power set of ''X'' as the set of subsets of ''X'' generalizes naturally to the subalgebras of an [[algebraic structure]] or algebra.

Now the power set of a set, when ordered by inclusion, is always a complete atomic Boolean algebra, and every complete atomic Boolean algebra arises as the [[lattice (order)|lattice]] of all subsets of some set.  The generalization to arbitrary algebras is that the set of subalgebras of an algebra, again ordered by inclusion, is always an [[algebraic lattice]], and every algebraic lattice arises as the lattice of subalgebras of some algebra.  So in that regard subalgebras behave analogously to subsets.

However there are two important properties of subsets that do not carry over to subalgebras in general.  First, although the subsets of a set form a set (as well as a lattice), in some classes it may not be possible to organize the subalgebras of an algebra as itself an algebra in that class, although they can always be organized as a lattice.  Secondly, whereas the subsets of a set are in bijection with the functions from that set to the set {0,1} = 2, there is no guarantee that a class of algebras contains an algebra that can play the role of 2 in this way.

Certain classes of algebras enjoy both of these properties.  The first property is more common, the case of having both is relatively rare.  One class that does have both is that of [[multigraph]]s.  Given two multigraphs ''G'' and ''H'', a [[homomorphism]] ''h'': ''G'' → ''H'' consists of two functions, one mapping vertices to vertices and the other mapping edges to edges.  The set ''H''&lt;sup&gt;''G''&lt;/sup&gt; of homomorphisms from ''G'' to ''H'' can then be organized as the graph whose vertices and edges are respectively the vertex and edge functions appearing in that set.  Furthermore the subgraphs of a multigraph ''G'' are in [[bijection]] with the graph homomorphisms from ''G'' to the multigraph Ω definable as the [[complete graph|complete directed graph]] on two vertices (hence four edges, namely two self-loops and two more edges forming a cycle) augmented with a fifth edge, namely a second self-loop at one of the vertices.  We can therefore organize the subgraphs of ''G'' as the multigraph Ω&lt;sup&gt;''G''&lt;/sup&gt;, called the '''power object''' of ''G''.

What is special about a multigraph as an algebra is that its operations are unary.  A multigraph has two sorts of elements forming a set ''V'' of vertices and ''E'' of edges, and has two unary operations ''s'',''t'': ''E'' → ''V'' giving the source (start) and target (end) vertices of each edge.  An algebra all of whose operations are unary is called a [[presheaf]].  Every class of presheaves contains a presheaf Ω that plays the role for subalgebras that 2 plays for subsets.  Such a class is a special case of the more general notion of elementary [[topos]] as a [[category (mathematics)|category]] that is [[closed category|closed]] (and moreover [[cartesian closed category|cartesian closed]]) and has an object Ω, called a [[subobject classifier]].  Although the term &quot;power object&quot; is sometimes used synonymously with [[exponential object]] ''Y''&lt;sup&gt;''X''&lt;/sup&gt;, in topos theory ''Y'' is required to be Ω.

== Functors and quantifiers ==
In [[category theory]] and the theory of [[elementary topos|elementary topoi]], the [[universal quantifier]] can be understood as the [[right adjoint]] of a [[functor]] between power sets, the [[inverse image]] functor of a function between sets; likewise, the [[existential quantifier]] is the [[left adjoint]].&lt;ref&gt;Saunders Mac Lane, Ieke Moerdijk, (1992) ''Sheaves in Geometry and Logic'' Springer-Verlag. ISBN 0-387-97710-4 ''See page 58''&lt;/ref&gt;

==See also==
* [[Set theory]]
* [[Axiomatic set theory]]
* [[Family of sets]]

==Notes==
&lt;references/&gt;

==References==
* {{cite book | last=Devlin | first=Keith J. | authorlink=Keith Devlin | title=Fundamentals of contemporary set theory | series=Universitext | publisher=[[Springer-Verlag]] | year=1979 | isbn=0-387-90441-7 | zbl=0407.04003 }} 
* {{cite book | last=Halmos | first=Paul R. | authorlink=Paul Halmos | title=Naive set theory | series=The University Series in Undergraduate Mathematics | publisher=van Nostrand Company | year=1960 | zbl=0087.04403 }} 
* {{cite book |last=Puntambekar | first=A.A. | title=Theory Of Automata And Formal Languages | year=2007 | publisher=Technical Publications | isbn=978-81-8431-193-8 }}

==External links==
{{Wiktionary|power set}}
*{{mathworld|urlname=PowerSet|title=Power Set}}
*{{planetmath reference|id=136|title=Power set}}
* {{nlab|id=power+set|title=Power set}}
* {{nlab|id=power+object|title=Power object}}

{{logic}}
{{Set theory}}

[[Category:Abstract algebra]]
[[Category:Algebra]]
[[Category:Basic concepts in set theory]]</text>
      <sha1>of51zc4lwa1ylein64dv50cmj49ad3j</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Distributive homomorphism</title>
    <ns>0</ns>
    <id>9754647</id>
    <revision>
      <id>607145974</id>
      <parentid>525871684</parentid>
      <timestamp>2014-05-05T09:59:04Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <text xml:space="preserve" bytes="1939">{{multiple issues|
{{expert-subject|date=November 2011}}
{{technical|date=November 2011}}
}}

A [[Congruence relation|congruence]] θ of a [[Semilattice|join-semilattice]] ''S'' is ''monomial'', if the θ-[[equivalence class]] of any element of ''S'' has a largest element. We say that θ is ''distributive'', if it is a [[Join (mathematics)|join]], in the [[Compact element|congruence lattice]] Con ''S'' of ''S'', of monomial join-congruences of ''S''.

The following definition originates in Schmidt's 1968 work and was subsequently adjusted by Wehrung.

'''Definition (weakly distributive homomorphisms).''' A homomorphism 
''&amp;mu; : S → T'' between join-semilattices ''S'' and ''T'' is ''weakly distributive'', if for all ''a, b'' in ''S'' and all ''c'' in ''T'' such that ''&amp;mu;(c)&amp;le; a &amp;or; b'', there are elements ''x'' and ''y'' of ''S'' such that ''c&amp;le; x &amp;or; y'', ''&amp;mu;(x)&amp;le; a'', and ''&amp;mu;(y)&amp;le; b''.

'''Examples:'''

(1) For an [[Universal algebra|algebra]] ''B'' and a ''reduct'' ''A'' of ''B'' (that is, an algebra with same underlying set as ''B'' but whose set of operations is a subset of the one of ''B''), the canonical (∨, 0)-homomorphism from Con&lt;sub&gt;c&lt;/sub&gt; A to Con&lt;sub&gt;c&lt;/sub&gt; B is weakly distributive. Here, Con&lt;sub&gt;c&lt;/sub&gt; A denotes the (∨, 0)-semilattice of all [[Compact element|compact congruences]] of ''A''.

(2) For a [[Lattice (order)|convex sublattice]] ''K'' of a lattice ''L'', the canonical (∨, 0)-homomorphism from Con&lt;sub&gt;c&lt;/sub&gt; ''K'' to Con&lt;sub&gt;c&lt;/sub&gt; ''L'' is weakly distributive.

== References ==

E.T. Schmidt, ''Zur Charakterisierung der Kongruenzverb&amp;auml;nde der Verb&amp;auml;nde'', Mat. Casopis Sloven. Akad. Vied. '''18''' (1968), 3--20.

F. Wehrung, ''A uniform refinement property for congruence lattices'', Proc. Amer. Math. Soc. '''127''', no. 2 (1999), 363–370.

F. Wehrung, ''A solution to Dilworth's congruence lattice problem'', preprint 2006.

[[Category:Algebra]]</text>
      <sha1>8pdbn7whq0ohapnvfgx9zhhxaavt3df</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Generalizations of the derivative</title>
    <ns>0</ns>
    <id>2144007</id>
    <revision>
      <id>605305526</id>
      <parentid>567927783</parentid>
      <timestamp>2014-04-22T15:00:38Z</timestamp>
      <contributor>
        <username>Nick Number</username>
        <id>1526960</id>
      </contributor>
      <comment>added dn tag(s) after attempting to disambiguate links ([[WP:DPL|you can help]])</comment>
      <text xml:space="preserve" bytes="19864">{{about|the term as used in mathematics|other uses|derivative (disambiguation)}}
{{Calculus |Differential}}

In [[mathematics]], the [[derivative]] is a fundamental construction of [[differential calculus]] and admits many possible generalizations within the fields of [[mathematical analysis]], [[combinatorics]], [[algebra]], and [[geometry]].

== Derivatives in analysis ==
In real, complex, and functional analysis, derivatives are generalized to functions of several real or complex variables and functions between [[topological vector spaces]]. An important case is the [[functional derivative|variational derivative]] in the [[calculus of variations]]. Repeated application of differentiation leads to derivatives of higher order and differential operators. 

=== Multivariable calculus ===
{{main|Fréchet derivative}}
The '''derivative''' is often met for the first time as an operation on a single real function of a single real variable.  One of the simplest settings for generalizations is to vector valued functions of several variables (most often the domain forms a vector space as well).  This is the field of [[multivariable calculus]].

In one-variable calculus, we say that a function &lt;math&gt;f: \R \to \R&lt;/math&gt; is '''differentiable''' at a point ''x'' if the limit
:&lt;math&gt;\lim_{h \to 0}\frac{f(x+h) - f(x)}{h}&lt;/math&gt;
exists. Its value is then the derivative ƒ'(''x'').  A function is differentiable on an [[Interval (mathematics)|interval]] if it is differentiable at every point within the interval. Since the line &lt;math&gt;L(z) = f'(x)z - f'(x)x + f(x)&lt;/math&gt; is tangent to the original function at the point &lt;math&gt;(x, f(x))&lt;/math&gt;, the derivative can be seen as a way to find the ''best linear approximation'' of a function. If one ignores the constant term, setting &lt;math&gt;L(z) = f'(x)z&lt;/math&gt;, ''L''(''z'') becomes an actual [[linear operator]] on '''R''' considered as a vector space over itself.

This motivates the following generalization to functions mapping '''R'''&lt;sup&gt;''m''&lt;/sup&gt; to '''R'''&lt;sup&gt;''n''&lt;/sup&gt;: ƒ is differentiable at ''x'' if there exists a [[linear operator]] ''A''(''x'') (depending on ''x'') such that
:&lt;math&gt;\lim_{\|h\| \to 0}\frac{\|f(x+h) - f(x) - A(x)h\|}{\|h\|} = 0.&lt;/math&gt;
Although this definition is perhaps not as explicit as the above, if such an operator exists, then it is unique, and in the one-dimensional case coincides with the original definition. (In this case the derivative is represented by a 1-by-1 matrix consisting of the sole entry ''f'''(''x'').) Note that, in general, we concern ourselves mostly with functions being differentiable in some open [[neighbourhood (mathematics)|neighbourhood]]  of &lt;math&gt;x&lt;/math&gt;  rather than at individual points, as not doing so tends to lead to many [[Pathological (mathematics)|pathological]] [[counterexamples]].

An ''m'' by ''n'' [[matrix (mathematics)|matrix]], of the [[linear operator]] ''A''(''x'') is known as '''[[Jacobian matrix and determinant|Jacobian]]''' matrix '''J'''&lt;sub&gt;''x''&lt;/sub&gt;(ƒ) of the mapping ƒ at point ''x''. Each entry of this matrix represents a '''[[partial derivative]]''', specifying the rate of change of one range coordinate with respect to a change in a domain coordinate. Of course, the Jacobian
matrix of the composition ''g&lt;sub&gt;°&lt;/sub&gt;f'' is a product of corresponding Jacobian matrices:    
'''J'''&lt;sub&gt;''x''&lt;/sub&gt;(''g&lt;sub&gt;°&lt;/sub&gt;f'') ='''J'''&lt;sub&gt;ƒ(''x'')&lt;/sub&gt;(''g'')'''J'''&lt;sub&gt;''x''&lt;/sub&gt;(ƒ). This is a higher-dimensional statement of the [[chain rule]].

For real valued functions from '''R'''&lt;sup&gt;''n''&lt;/sup&gt; to '''R''' ([[scalar field]]s), the total derivative can be interpreted as a [[vector field]] called the '''[[gradient]]'''. An intuitive interpretation of the gradient is that it points &quot;up&quot;: in other words, it points in the direction of fastest increase of the function.  It can be used to calculate '''[[directional derivative]]s''' of [[Scalar (mathematics)|scalar]] functions or normal directions.

Several linear combinations of partial derivatives are especially useful in the context of differential equations defined by a vector valued function '''R'''&lt;sup&gt;''n''&lt;/sup&gt; to '''R'''&lt;sup&gt;''n''&lt;/sup&gt;. The '''[[divergence]]''' gives a measure of how much &quot;source&quot; or &quot;sink&quot; near a point there is.  It can be used to calculate [[flux]] by [[divergence theorem]]. The '''[[curl (mathematics)|curl]]''' measures how much &quot;[[rotation]]&quot; a vector field has near a point.

For [[vector-valued functions]] from '''R''' to '''R'''&lt;sup&gt;''n''&lt;/sup&gt; (i.e., [[parametric curve]]s), one can take the derivative of each component separately. The resulting derivative is another vector valued function. This is useful, for example, if the vector-valued function is the position vector of a particle through time, then the derivative is the velocity vector of the particle through time.

The '''[[convective derivative]]''' takes into account changes due to time dependence and motion through space along vector field.

=== Convex analysis ===

The [[subderivative]] and [[subgradient]] are generalizations of the derivative to [[convex function]]s.

=== Higher-order derivatives and differential operators ===
One can iterate the differentiation process, that is, apply derivatives more than once, obtaining derivatives of second and higher order. A more sophisticated idea is to combine several derivatives, possibly of different orders, in one algebraic expression, a [[differential operator]]. This is especially useful in considering ordinary [[linear differential equation]]s with constant coefficients. For example, if ''f''(''x'') is a twice differentiable function of one variable, the differential equation

: &lt;math&gt;f''+2f'-3f=4x-1\,&lt;/math&gt;

may be rewritten in the form 

: &lt;math&gt;L(f)=4x-1,\,&lt;/math&gt; &amp;ensp;&amp;ensp; where &amp;ensp;&amp;ensp; &lt;math&gt; L=\frac{d^2}{dx^2}+2\frac{d}{dx}-3&lt;/math&gt;

is a ''second order linear constant coefficient differential operator'' acting on functions of ''x''. The key idea here is that we consider a particular [[linear combination]] of zeroth, first and second order derivatives &quot;all at once&quot;. This allows us to think of the set of solutions of this differential equation as a &quot;generalized antiderivative&quot; of its right hand side 4''x''&amp;nbsp;&amp;minus;&amp;nbsp;1, by analogy with ordinary [[Integral|integration]], and formally write 

: &lt;math&gt;f(x)=L^{-1}(4x-1).\, &lt;/math&gt;

Higher derivatives can also be defined for functions of several variables, studied in [[multivariable calculus]]. In this case, instead of repeatedly applying the derivative, one repeatedly applies [[partial derivative]]s with respect to different variables. For example, the second order partial derivatives of a scalar function of ''n'' variables can be organized into an ''n'' by ''n'' matrix, the '''[[Hessian matrix]]'''.  One of the subtle points is that the higher derivatives are not intrinsically defined, and depend on the choice of the coordinates in a complicated fashion (in particular, the Hessian matrix of a function is not a [[tensor]]). Nevertheless, higher derivatives have important applications to analysis of [[maxima and minima|local extrema]] of a function at its [[critical point (mathematics)|critical points]]. For an advanced application of this analysis to topology of [[manifold]]s, see  [[Morse theory]].

As in the case of functions of one variable, we can combine first and higher order partial derivatives to arrive at a notion of a [[partial differential operator]]. Some of these operators are so important that they have their own names: 

*The [[Laplace operator]] or '''Laplacian''' on '''R'''&lt;sup&gt;3&lt;/sup&gt; is a second-order partial differential operator ''Δ'' given by the  [[divergence]] of the [[gradient]] of a scalar function of three variables, or explicitly as

:: &lt;math&gt; \Delta=\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}. &lt;/math&gt;
Analogous operators can be defined for functions of any number of variables.

*The [[d'Alembertian]] or '''wave operator''' is similar to the Laplacian, but acts on functions of four variables. Its definition uses the indefinite [[metric tensor]] of [[Minkowski space]], instead of the [[Euclidean space|Euclidean]] [[dot product]] of '''R'''&lt;sup&gt;''3''&lt;/sup&gt;:

:: &lt;math&gt; \square=\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}-\frac{1}{c^2}\frac{\partial^2}{\partial t^2}. &lt;/math&gt;

=== Analysis on fractals ===
Laplacians and differential equations can be defined on [[analysis on fractals|fractals]].

=== Fractional derivatives ===
In addition to ''n''-th derivatives for any natural number ''n'', there are various ways to define derivatives of fractional or negative orders, which are studied in '''[[fractional calculus]]'''.  The -1 order derivative corresponds to the integral, whence the term '''[[differintegral]]'''.

=== Complex analysis ===

In [[complex analysis]], the central objects of study are [[holomorphic function]]s, which are complex-valued functions on the [[complex numbers]] satisfying a [[Fréchet derivative|suitably extended definition of differentiability]].

The '''[[Schwarzian derivative]]''' describes how a complex function is approximated by a [[fractional-linear map]], in much the same way that a normal derivative describes how a function is approximated by a linear map.

=== Functional analysis ===

In [[functional analysis]], the '''[[functional derivative]]''' defines the derivative with respect to a function of a functional on a space of functions.  This is an extension of the directional derivative to an infinite [[dimension]]al vector space.  

The '''[[Fréchet derivative]]''' allows the extension of the directional derivative to a general [[Banach space]].  The '''[[Gâteaux derivative]]''' extends the concept to [[locally convex]] [[topological vector space]]s.  Fréchet differentiability is a strictly stronger condition than Gâteaux differentiability, even in finite dimensions.  Between the two extremes is the '''[[quasi-derivative]]'''.

In [[measure theory]], the '''[[Radon–Nikodym derivative]]''' generalizes the [[Jacobian matrix and determinant|Jacobian]], used for changing variables, to measures. It expresses one measure μ in terms of another measure ν (under certain conditions).

In the theory of [[abstract Wiener space]]s, the [[H-derivative|''H''-derivative]] defines a derivative in certain directions corresponding to the Cameron-Martin [[Hilbert space]].

The derivative also admits a generalization to the space of '''[[distribution (mathematics)|distributions]]''' on a space of functions using [[integration by parts]] against a suitably well-behaved subspace.

On a [[function space]], the [[linear operator]] which assigns to each function its derivative is an example of a '''[[differential operator]]'''.  General differential operators include higher order derivatives.  By means of the [[Fourier transform]], '''[[pseudo-differential operator]]s''' can be defined which allow for fractional calculus.

===Analogues of derivatives in fields of positive characteristic===
The [[Carlitz derivative]] is an operation similar to usual differentiation have been devised with the usual context of real or complex numbers changed to [[local fields]] of positive [[Characteristic_(algebra)|characteristic]] in the form of [[formal Laurent series]] with coefficients in some [[finite field]] '''F'''&lt;sub&gt;''q''&lt;/sub&gt; (it is known that any local field of positive characteristic is isomorphic to a Laurent series field).

Along with suitably defined analogs to the [[exponential function]], [[logarithms]] and others the derivative can be used to develop notions of smoothness, analycity, integration, Taylor series as well as a theory of differential equations.&lt;ref&gt;{{cite book |title=Analysis in Positive Characteristic |last=Kochubei |first= Anatoly N.|year=2009 |publisher= Cambridge University Press |location= New York |isbn= 978-0-521-50977-0}}&lt;/ref&gt;

==Difference operator, q-analogues and time scales==

* The '''[[q-derivative]]'''  of a function is defined by the formula

: &lt;math&gt; D_q f(x)=\frac{f(qx)-f(x)}{(q-1)x}.&lt;/math&gt;

For ''x'' nonzero, if ''f'' is a differentiable function of ''x'' then in the limit as ''q''&amp;ensp;→ 1 we obtain the ordinary derivative, thus the ''q''-derivative may be viewed as its [[q-deformation]]. A large body of results from ordinary differential calculus, such as [[binomial formula]] and [[Taylor expansion]], have natural ''q''-analogues that were discovered in the 19th century, but remained relatively obscure for a big part of the 20th century, outside of the theory of [[special functions]]. The progress of [[combinatorics]] and the discovery of [[quantum group]]s have changed the situation dramatically, and the popularity of ''q''-analogues is on the rise.

* The '''[[difference operator]]''' of [[difference equations]] is another discrete analog of the standard derivative.
:&lt;math&gt;\Delta f(x)=f(x+1)-f(x)\,&lt;/math&gt;

* The '''q-derivative''', the '''difference operator''' and the '''standard derivative''' can all be viewed as the same thing on different [[time scale calculus|time scales]].

== Derivatives in algebra ==
In algebra, generalizations of the derivative can be obtained by imposing the [[product rule|Leibniz rule of differentiation]] in an algebraic structure, such as a [[ring (mathematics)|ring]] or a [[Lie algebra]].

=== Derivations ===

A '''[[derivation (abstract algebra)|derivation]]''' is a linear map on a  ring or [[algebra over a field|algebra]] which satisfies the Leibniz law (the product rule).  Higher derivatives and [[algebraic differential equation|algebraic differential operators]] can also be defined. They are studied in a purely algebraic setting in [[differential Galois theory]] and the theory of [[D-module]]s, but also turn up in many other areas, where they often agree with less algebraic definitions of derivatives.

For example, the '''formal derivative''' of a [[polynomial]] over a commutative ring ''R'' is defined by
:&lt;math&gt;(a_dx^d + a_{d-1}x^{d-1} + \cdots+a_1x+a_0)' = da_dx^{d-1}+(d-1)a_{d-1}x^{d-2} + \cdots+a_1.&lt;/math&gt;
The mapping &lt;math&gt;f\mapsto f'&lt;/math&gt; is then a derivation on the [[polynomial ring]] ''R''[''X''].  This definition can be extended to [[rational function]]s as well.

The notion of derivation applies to noncommutative as well as commutative rings, and even to non-associative algebraic structures, such as Lie algebras.

Also see [[Pincherle derivative]].

=== Commutative algebra ===

In [[commutative algebra]], '''[[Kähler differential]]s''' are universal derivations of a [[commutative ring]] or [[module (algebra)|module]]. They can be used to define an analogue of exterior derivative 
from differential geometry that applies to arbitrary [[algebraic varieties]], instead of just smooth manifolds.

=== Number theory ===

In [[p-adic analysis]], the usual definition of derivative is not quite strong enough, and one requires [[strictly differentiable|strict differentiability]] instead.

Also see [[arithmetic derivative]] and [[Hasse derivative]].

=== Type theory ===
Many [[abstract data type]]s in mathematics and [[computer science]] can be described as the [[universal algebra|algebra]] generated by a transformation that maps structures based on the type back into the type. For example, the type T of [[binary tree]]s containing values of type A can be represented as the algebra generated by the transformation 1+A&amp;times;T&lt;sup&gt;2&lt;/sup&gt;→T. The &quot;1&quot; represents the construction of an empty tree, and the second term represents the construction of a tree from a value and two subtrees. The &quot;+&quot; indicates that a tree can be constructed either way.

The derivative of such a type is the type that describes the context of a particular substructure with respect to its next outer containing structure. Put another way, it is the type representing the &quot;difference&quot; between the two. In the tree example, the derivative is a type that describes the information needed, given a particular subtree, to construct its parent tree. This information is a tuple that contains a binary indicator of whether the child is on the left or right, the value at the parent, and the sibling subtree. This type can be represented as 2&amp;times;A&amp;times;T, which looks very much like the derivative of the transformation that generated the tree type.

This concept of a derivative of a type has practical applications, such as the [[zipper (data structure)|zipper]] technique used in [[functional programming language]]s.

== Derivatives in geometry ==
Main types of derivatives in geometry are Lie derivatives along a vector field, exterior differential, and covariant derivatives.

=== Differential topology ===

In [[differential topology]], a '''[[vector field]]''' may be defined as a derivation on the ring of [[smooth function]]s on a [[manifold]], and a '''[[tangent vector]]''' may be defined as a derivation at a point.  This allows the abstraction of the notion of a [[directional derivative]] of a scalar function to general manifolds.  For manifolds that are [[subset]]s of '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, this tangent vector will agree with the directional derivative defined above.

The '''[[pushforward (differential)|differential or pushforward]]''' of a map between manifolds is the induced map between tangent spaces of those maps.  It abstracts the [[Jacobian matrix]].

On the [[exterior algebra]] of [[differential forms]] over a [[smooth manifold]], the '''[[exterior derivative]]''' is the unique linear map which satisfies a [[graded]]{{Disambiguation needed|date=April 2014}} version of the Leibniz law and squares to zero.  It is a grade 1 derivation on the exterior algebra.

The '''[[Lie derivative]]''' is the rate of change of a vector or tensor field along the flow of another vector field.  On vector fields, it is an example of a [[Lie bracket]] (vector fields form the [[Lie algebra]] of the [[diffeomorphism group]] of the manifold).  It is a grade 0 derivation on the algebra.

Together with the '''[[interior product]]''' (a degree -1 derivation on the exterior algebra defined by contraction with a vector field), the exterior derivative and the Lie derivative form a [[Lie superalgebra]].

=== Differential geometry ===

In [[differential geometry]], the '''[[covariant derivative]]''' makes a choice for taking directional derivatives of vector fields along [[curve]]s.  This extends the directional derivative of scalar functions to sections of [[vector bundle]]s or [[principal bundle]]s.  In [[Riemannian geometry]], the existence of a metric chooses a unique preferred [[Torsion tensor|torsion]]-free covariant derivative, known as the [[Levi-Civita connection]].  See also [[gauge covariant derivative]] for a treatment oriented to physics.

The '''[[exterior covariant derivative]]''' extends the exterior derivative to vector valued forms.

== Other generalizations ==

It may be possible to combine two or more of the above different notions of extension or abstraction of the original derivative.  For example, in [[Finsler geometry]], one studies spaces which look [[locally]] like [[Banach space]]s.  Thus one might want a derivative with some of the features of a [[functional derivative]] and the [[covariant derivative]].

The study of [[stochastic processes]] requires a form of calculus known as the [[Malliavin calculus]]. One notion of derivative in this setting is the [[H-derivative|''H''-derivative]] of a function on an [[abstract Wiener space]].

== See also ==
*[[Arithmetic derivative]]
*[[Non-classical analysis]]

== Notes ==

{{reflist}}

[[Category:Differential calculus]]
[[Category:Generalizations of the derivative| ]]
[[Category:Real analysis]]
[[Category:Functional analysis]]
[[Category:Algebra]]
[[Category:Q-analogs]]
[[Category:Differential geometry]]</text>
      <sha1>4n4orp90guez0i7f86oiltijf87pf1v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Operator (mathematics)</title>
    <ns>0</ns>
    <id>22213</id>
    <revision>
      <id>592060469</id>
      <parentid>590127765</parentid>
      <timestamp>2014-01-23T19:07:16Z</timestamp>
      <contributor>
        <username>Airwoz</username>
        <id>19855403</id>
      </contributor>
      <text xml:space="preserve" bytes="10860">{{About|operators in [[mathematics]]|other uses|Operator (disambiguation)}}
{{cleanup-rewrite|date=November 2010}}

An '''operator''' is a [[Map (mathematics)|mapping]] from one [[vector space]] or [[Module (mathematics)|module]] to another. Operators are of critical importance to both [[linear algebra]] and [[functional analysis]], and they find application in many other fields of pure and applied mathematics. For example, in [[classical mechanics]], the [[derivative]] is used ubiquitously, and in [[quantum mechanics]], [[observable]]s are represented by [[hermitian operators]]. Important properties that various operators may exhibit include [[Linear operator|linearity]], [[Continuous operator|continuity]], and [[Bounded operator|boundedness]].

== Definitions ==
&lt;!-- I'm giving definition for vector spaces. Please generalize to modules, I'm weak in these

--&gt;Let ''U, V'' be two [[vector space]]s. Any mapping from ''U'' to ''V'' is called an '''operator'''. Let ''V'' be a vector space over the field ''K''. We can define the structure of a vector space on the set of all operators from ''U'' to ''V'' (''A'' and ''B'' are operators):
:&lt;math&gt;(A + B)\mathbf{x} := A\mathbf{x} + B\mathbf{x},&lt;/math&gt;
:&lt;math&gt;(\alpha A)\mathbf{x} := \alpha A \mathbf{x}&lt;/math&gt;
for all ''A, B: U → V'', for all '''x''' in ''U'' and for all ''α'' in ''K''.

Additionally, operators from any vector space to itself form a [[unital algebra|unital]] [[associative algebra]]:
:&lt;math&gt;(AB)\mathbf{x} := A(B\mathbf{x})&lt;/math&gt;
with the [[identity mapping]] (usually denoted ''E'', ''I'' or id) being the unit.

=== Bounded operators and operator norm ===
{{Main|bounded operator|operator norm|Banach algebra}}

Let ''U'' and ''V'' be two vector spaces over the same [[ordered field]] (for example, &lt;math&gt;\mathbf{R}&lt;/math&gt;), and they are equipped with [[norm (mathematics)|norm]]s. Then a linear operator from ''U'' to ''V'' is called '''bounded''' if there exists ''C &gt; 0'' such that
:&lt;math&gt;||A\mathbf{x}||_V \leq C||\mathbf{x}||_U&lt;/math&gt;
for all '''x''' in ''U''.

Bounded operators form a vector space. On this vector space we can introduce a norm that is compatible with the norms of ''U'' and ''V'':
:&lt;math&gt;||A|| = \inf\{C: ||A\mathbf{x}||_V \leq C||\mathbf{x}||_U\}&lt;/math&gt;.

In case of operators from ''U'' to itself it can be shown that
:&lt;math&gt;||AB|| \leq ||A||\cdot||B||&lt;/math&gt;.

Any unital [[normed algebra]] with this property is called a [[Banach algebra]]. It is possible to generalize [[spectral theory]] to such algebras. [[C*-algebra]]s, which are [[Banach algebras]] with some additional structure, play an important role in [[quantum mechanics]].

== Special cases ==
=== Functionals ===
{{Main|Functional (mathematics)}}
A functional is an operator that maps a vector space to its underlying [[Field (mathematics)|field]]. Important applications of functionals are the theories of [[generalized function]]s and [[calculus of variations]]. Both are of great importance to theoretical physics.

=== Linear operators ===
{{Main|Linear operator}}

The most common kind of operator encountered are ''linear operators''. Let ''U'' and ''V'' be vector spaces over a field ''K''. Operator ''A: U → V'' is called linear if
:&lt;math&gt;A(\alpha \mathbf{x} + \beta \mathbf{y}) = \alpha A \mathbf{x} + \beta A \mathbf{y}&lt;/math&gt;
for all '''x''', '''y''' in ''U'' and for all ''α, β'' in ''K''.

The importance of linear operators is partially because they are [[morphism]]s between vector spaces.

In finite-dimensional case linear operators can be represented by [[Matrix (mathematics)|matrices]] in the following way. Let &lt;math&gt;K&lt;/math&gt; be a field, and &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; be finite-dimensional vector spaces over &lt;math&gt;K&lt;/math&gt;. Let us select a basis &lt;math&gt;\mathbf{u}_1, \ldots, \mathbf{u}_n&lt;/math&gt; in &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;\mathbf{v}_1, \ldots, \mathbf{v}_m&lt;/math&gt; in &lt;math&gt;V&lt;/math&gt;. Then let &lt;math&gt;\mathbf{x} = x^i \mathbf{u}_i&lt;/math&gt; be an arbitrary vector in &lt;math&gt;U&lt;/math&gt; (assuming [[Einstein convention]]), and &lt;math&gt;A: U \to V&lt;/math&gt; be a linear operator. Then
: &lt;math&gt;A\mathbf{x} = x^i A\mathbf{u}_i = x^i (A\mathbf{u}_i)^j \mathbf{v}_j &lt;/math&gt;.
Then &lt;math&gt;a_i^j := (A\mathbf{u}_i)^j \in K&lt;/math&gt; is the matrix of the operator &lt;math&gt;A&lt;/math&gt; in fixed bases. &lt;math&gt;a_i^j&lt;/math&gt; does not depend on the choice of &lt;math&gt;x&lt;/math&gt;, and &lt;math&gt;A\mathbf{x} = \mathbf{y}&lt;/math&gt; iff &lt;math&gt;a_i^j x^i = y^j&lt;/math&gt;. Thus in fixed bases n-by-m matrices are in bijective correspondence to linear operators from &lt;math&gt;U&lt;/math&gt; to &lt;math&gt;V&lt;/math&gt;.

The important concepts directly related to operators between finite-dimensional vector spaces are the ones of [[Matrix rank|rank]], [[determinant]], [[inverse operator]], and [[eigenspace]].

Linear operators also play a great role in the infinite-dimensional case. The concepts of rank and determinant cannot be extended to infinite-dimensional matrices. This is why very different techniques are employed when studying linear operators (and operators in general) in the infinite-dimensional case. The study of linear operators in the infinite-dimensional case is known as [[functional analysis]] (so called because various classes of functions form interesting examples of infinite-dimensional vector spaces).

The space of [[sequence]]s of real numbers, or more generally sequences of vectors in any vector space, themselves form an infinite-dimensional vector space. The most important cases are sequences of real or complex numbers, and these spaces, together with linear subspaces, are known as [[sequence space]]s. Operators on these spaces are known as [[sequence transformation]]s.

Bounded linear operators over [[Banach space]] form a [[Banach algebra]] in respect to the standard operator norm. The theory of Banach algebras develops a very general concept of [[Spectrum (functional analysis)|spectra]] that elegantly generalizes the theory of eigenspaces.
&lt;!-- Please expand --&gt;

== Examples ==
=== Geometry ===
{{Main|general linear group|isometry}}

In [[geometry]], additional structures on [[vector space]]s are sometimes studied. Operators that map such vector spaces to themselves bijectively are very useful in these studies, they naturally form [[group (mathematics)|group]]s by composition.

For example, bijective operators preserving the structure of a vector space are precisely the [[invertible]] [[linear operator]]s. They form the [[general linear group]] under composition. They ''do not'' form a vector space under the addition of operators, e.g. both ''id'' and ''-id'' are invertible (bijective), but their sum, 0, is not.

Operators preserving the Euclidean metric on such a space form the [[isometry group]], and those that fix the origin form a subgroup known as the [[orthogonal group]]. Operators in the orthogonal group that also preserve the orientation of vector tuples form the [[special orthogonal group]], or the group of rotations.

=== Probability theory ===
{{Main|Probability theory}}

Operators are also involved in probability theory, such as [[expected value|expectation]], [[variance]], [[covariance]], [[factorial]]s, etc.
&lt;!-- if you know it and you know it please expand... --&gt;

=== Calculus ===
{{Main|differential operator|integral operator}}
From the point of view of [[functional analysis]], [[calculus]] is the study of two linear operators: the [[differential operator]] &lt;math&gt;\frac{\mathrm{d}}{\mathrm{d}t}&lt;/math&gt;, and the [[Volterra operator|indefinite integral operator]] &lt;math&gt;\int_0^t&lt;/math&gt;.

==== Fourier series and Fourier transform ====

{{Main|Fourier series|Fourier transform}}

The Fourier transform is useful in applied mathematics, particularly physics and signal processing. It is another integral operator; it is useful mainly because it converts a function on one (temporal) domain to a function on another (frequency) domain, in a way effectively [[invertible]]. Nothing significant is lost, because there is an inverse transform operator. In the simple case of [[periodic function]]s, this result is based on the theorem that any continuous periodic function can be represented as the sum of a series of [[sine wave]]s and cosine waves:
:&lt;math&gt;f(t) = {a_0 \over 2} + \sum_{n=1}^{\infty}{ a_n \cos ( \omega n t ) + b_n \sin ( \omega n t ) } &lt;/math&gt;
Coefficients ''(a&lt;sub&gt;0&lt;/sub&gt;, a&lt;sub&gt;1&lt;/sub&gt;, b&lt;sub&gt;1&lt;/sub&gt;, a&lt;sub&gt;2&lt;/sub&gt;, b&lt;sub&gt;2&lt;/sub&gt;, ...)'' are in fact an element of an infinite-dimensional vector space [[Sequence space|ℓ&lt;sup&gt;2&lt;/sup&gt;]], and thus Fourier series is a linear operator.

When dealing with general function '''R''' → '''C''', the transform takes on an [[integral]] form:

:&lt;math&gt;f(t) = {1 \over \sqrt{2 \pi}} \int_{- \infty}^{+ \infty}{g( \omega )e^{ i \omega t } \,d\omega }. &lt;/math&gt;

==== Laplace transform ====

{{Main|Laplace transform}}
The ''Laplace transform'' is another integral operator and is involved in simplifying the process of solving differential equations.

Given ''f'' = ''f''(''s''), it is defined by:
:&lt;math&gt;F(s) = (\mathcal{L}f)(s) =\int_0^\infty e^{-st} f(t)\,dt.&lt;/math&gt;

=== Fundamental operators on scalar and vector fields ===
{{Main|vector calculus|vector field|scalar field|gradient|divergence|curl (mathematics)|l6=curl}}

Three operators are key to [[vector calculus]]:
* Grad ([[gradient]]), (with operator symbol [[Del|&lt;math&gt;\nabla&lt;/math&gt;]]) assigns a vector at every point in a scalar field that points in the direction of greatest rate of change of that field and whose norm measures the absolute value of that greatest rate of change.
* Div ([[divergence]]), (with operator symbol [[Del#Divergence|&lt;math&gt;\nabla \cdot&lt;/math&gt;]]) is a vector operator that measures a vector field's divergence from or convergence towards a given point.
* [[Curl (mathematics)|Curl]], (with operator symbol [[Del#Curl|&lt;math&gt;\nabla \times&lt;/math&gt;]]) is a vector operator that measures a vector field's curling (winding around, rotating around) trend about a given point.

As an extension of vector calculus operators to physics, engineering and tensor spaces, Grad, Div and Curl operators also are often associatied with [[Tensor calculus]] as well as vector calculus. &lt;ref name=&quot;Vector and Tensor Operators&quot;&gt;{{cite book |isbn= 0-393-92516-1 |url= http://www.amazon.com/Div-Grad-Curl-All-That/dp/0393925161/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1388768941&amp;sr=1-1&amp;keywords=div+grad+curl |title=Div Grad Cural and All that |author=h.m. schey |location=New York|publisher=W W Norton|year=2005}}&lt;/ref&gt;

== See also ==
* [[Operation (mathematics)|Operation]]
* [[Function (mathematics)|Function]]
* [[List of mathematical operators]]
* [[Vector space]]
* [[Dual space]]
* [[Operator algebra]]
* [[Banach algebra]]
* [[List of operators]]
* [[Operator (physics)]]
* [[Operator (programming)]]

== References ==
{{reflist}}

[[Category:Algebra]]
[[Category:Functional analysis]]
[[Category:Mathematical notation]]</text>
      <sha1>h4xy99y3iwpeu11vi4rlgs3adj279x4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Differential equations of addition</title>
    <ns>0</ns>
    <id>10322441</id>
    <revision>
      <id>482744243</id>
      <parentid>305345056</parentid>
      <timestamp>2012-03-19T16:23:28Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* References */[[WP:CHECKWIKI]] error  fixes + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (8024)</comment>
      <text xml:space="preserve" bytes="2268">In [[cryptography]], '''differential equations of addition''' (DEA) are one of the most basic equations related to [[differential cryptanalysis]] that mix additions over two different groups (e.g. addition modulo 2&lt;sup&gt;32&lt;/sup&gt; and addition over GF(2)) and  where input and output differences are expressed as XORs.

== Examples of Differential Equations of Addition ==
'''Differential equations of addition''' (DEA) are of the following form:

&lt;math&gt;(x+y)\oplus((x\oplus a)+(y\oplus b))=c&lt;/math&gt;

where &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; are &lt;math&gt;n&lt;/math&gt;-bit '''unknown''' variables and &lt;math&gt;a&lt;/math&gt;, &lt;math&gt;b&lt;/math&gt; and &lt;math&gt;c&lt;/math&gt; are '''known''' variables. The symbols &lt;math&gt;+&lt;/math&gt; and &lt;math&gt;\oplus&lt;/math&gt; denote ''addition modulo'' &lt;math&gt;2^n&lt;/math&gt; and ''bitwise exclusive-or'' respectively. The above equation is denoted by &lt;math&gt;(a, b, c)&lt;/math&gt;.

Let a set &lt;math&gt;S=\{(a_i, b_i, c_i)|i&lt;/math&gt; is an integer less than &lt;math&gt;k\}&lt;/math&gt; denote a system of &lt;math&gt;k&lt;/math&gt; '''DEA''' where &lt;math&gt;k&lt;/math&gt; is a polynomial in &lt;math&gt;n&lt;/math&gt;. It has been proved that the satisfiability of an arbitrary set of DEA is in the '''[[P = NP problem|complexity class P]]''' when a brute force search requires an [[exponential time]].

== Usage of Differential Equations of Addition ==
Solution to an arbitrary set of DEA (either in batch and or in adaptive query model) was due to [[Souradyuti Paul]] and [[Bart Preneel]]. The solution techniques have been used to attack the stream cipher [[Phelix|Helix]].

== References ==
* [[Souradyuti Paul]] and [[Bart Preneel]], Solving Systems of Differential Equations of Addition, ACISP 2005.  [http://www.cosic.esat.kuleuven.be/publications/article-566.pdf Full version] ([[PDF]])
* [[Souradyuti Paul]] and [[Bart Preneel]], Near Optimal Algorithms for Solving Differential Equations of Addition With Batch Queries, [[Indocrypt]] 2005.  [http://www.cosic.esat.kuleuven.be/publications/article-587.pdf Full version] ([[PDF]])
* Helger Lipmaa, Johan Wallén, Philippe Dumas: On the Additive Differential Probability of Exclusive-Or. [[Fast Software Encryption|FSE]] 2004: 317-331.

{{Cryptography navbox | block}}

[[Category:Cryptographic attacks]]
[[Category:Theory of cryptography]]
[[Category:Ciphers]]
[[Category:Algebra]]</text>
      <sha1>f7u21idb7qj3towzzpp8m3tz4o0e3ra</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Conservation form</title>
    <ns>0</ns>
    <id>8719288</id>
    <revision>
      <id>499542530</id>
      <parentid>132519910</parentid>
      <timestamp>2012-06-27T02:53:48Z</timestamp>
      <contributor>
        <username>Eastlaw</username>
        <id>1046972</id>
      </contributor>
      <comment>added [[Category:Conservation equations]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="846">'''Conservation form''' refers to an arrangement of an [[equation]] or [[system of equations]], usually representing a physical system, that show that a property represented is conserved by making overall change equal to zero.

An example of a set of equations written in conservation form could be the [[Euler equations]] of fluid flow:

:&lt;math&gt;
{\partial\rho\over\partial t}+
\nabla\cdot(\rho\bold u)=0
&lt;/math&gt;
:&lt;math&gt;
{\partial\rho{\bold u}\over\partial t}+
\nabla\cdot(\rho \bold u)\bold u+\nabla p=0
&lt;/math&gt;
:&lt;math&gt;
{\partial E\over\partial t}+
\nabla\cdot(\bold u(E+p))=0&lt;/math&gt;

Each of these represents the [[conservation of mass]], [[conservation of momentum|momentum]] and [[conservation of energy|energy]], respectively.

{{Unreferenced|date=April 2007}}


{{mathapplied-stub}}

[[Category:Algebra]]
[[Category:Conservation equations]]</text>
      <sha1>7is2zms2y1tyu4jbm5uqebfmopff8ou</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cylindrical algebraic decomposition</title>
    <ns>0</ns>
    <id>12218154</id>
    <revision>
      <id>610527714</id>
      <parentid>603929805</parentid>
      <timestamp>2014-05-28T18:32:10Z</timestamp>
      <contributor>
        <ip>2001:4870:E08E:201:9CCE:FA69:BB23:BE24</ip>
      </contributor>
      <text xml:space="preserve" bytes="2743">In [[mathematics]], '''cylindrical algebraic decomposition''' is a notion and an [[algorithm]] to compute it, which are fundamental for [[computer algebra]] and [[real algebraic geometry]]. Given a set ''S'' of polynomials in '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, a '''cylindrical algebraic decomposition''', commonly abbreviated '''CAD''', is a decomposition of '''R'''&lt;sup&gt;''n''&lt;/sup&gt; into connected [[semialgebraic set]]s called ''cells'', on which each polynomial has constant sign, either +, − or 0. For being ''cylindrical'', this decomposition must satisfy the following condition: If 1&amp;nbsp;≤&amp;nbsp;''k''&amp;nbsp;&lt;&amp;nbsp;''n'' and π is the projection from '''R'''&lt;sup&gt;''n''&lt;/sup&gt; onto '''R'''&lt;sup&gt;''n''−''k''&lt;/sup&gt; consisting in removing the ''k'' last coordinates, then for every cells ''c'' and ''d'', one has either π(''c'')&amp;nbsp;=&amp;nbsp;π(''d'') or π(''c'')&amp;nbsp;∩&amp;nbsp;π(''d'')&amp;nbsp;=&amp;nbsp;∅. This implies that the images by π of the cells define a cylindrical decomposition of&amp;nbsp;'''R'''&lt;sup&gt;''n''−''k''&lt;/sup&gt;.

The notion has been introduced by [[Georges E. Collins]] in 1975, together with an [[algorithm]] for computing it.

Collins' algorithm has a [[computational complexity]] that is [[double exponential function|double exponential]] in ''n''. This is an upper bound, which is reached on most entries. There are also example for which the minimal number of cells is doubly exponential, showing that every general algorithm for cylindrical algebraic decomposition has a double exponential complexity.

'''CAD''' provides an effective version of [[Alfred Tarski]]'s real [[quantifier elimination]] and [[Tarski–Seidenberg theorem]], which is efficient enough for being implemented on a computer. It is one of the most important algorithms of computational [[real algebraic geometry]]. Searching to improve Collins algorithm, or to provide algorithms that have a better complexity for subproblems of general interest is an active field of research.
==Implementations==
* [[Mathematica]]: [https://reference.wolfram.com/mathematica/ref/CylindricalDecomposition.html CylindricalDecomposition]
==References==
*Basu, Saugata; Pollack, Richard; Roy, Marie-Françoise Algorithms in real algebraic geometry. Second edition. Algorithms and Computation in Mathematics, 10. Springer-Verlag, Berlin, 2006. x+662 pp. ISBN 978-3-540-33098-1; 3-540-33098-4
*Strzebonski, Adam. ''[http://mathworld.wolfram.com/CylindricalAlgebraicDecomposition.html Cylindrical Algebraic Decomposition]''  from [[MathWorld]].
*[http://planning.cs.uiuc.edu/node292.html Cylindrical Algebraic Decomposition] in ''Planning algorithms'' by Steven M. LaValle. Accessed 13 July 2007

[[Category:Algebra]]
[[Category:Real algebraic geometry]]


{{Algebra-stub}}</text>
      <sha1>hig653vx5yxv7e4pgstooti99akxruk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Reduct</title>
    <ns>0</ns>
    <id>12747972</id>
    <revision>
      <id>583634220</id>
      <parentid>553831972</parentid>
      <timestamp>2013-11-28T06:53:47Z</timestamp>
      <contributor>
        <username>John Baez</username>
        <id>233394</id>
      </contributor>
      <minor/>
      <comment>/* Definition */ fixed typo</comment>
      <text xml:space="preserve" bytes="2260">{{about|a relation on algebraic structures|reducts in abstract rewriting|Confluence (abstract rewriting)}}

In [[universal algebra]] and in [[model theory]], a '''reduct''' of an algebraic structure is obtained by omitting some of the operations and relations of that structure.  The converse of &quot;reduct&quot; is &quot;expansion.&quot;

==Definition==
Let ''A'' be an [[algebraic structure]] (in the sense of [[universal algebra]]) or equivalently a [[structure (mathematical logic)|structure]] in the sense of [[model theory]], organized as a set ''X'' together with an indexed [[family]] of [[Operation (mathematics)|operations]] and relations φ&lt;sub&gt;i&lt;/sub&gt; on that set, with [[index set]] ''I''.  Then the '''reduct''' of ''A'' defined by a subset ''J'' of ''I'' is the structure consisting of the set ''X'' and ''J''-indexed family of operations and relations whose ''j''-th operation or relation for ''j''∈''J'' is the ''j''-th operation or relation of ''A''.  That is, this reduct is the structure ''A'' with the omission of those operations and relations φ&lt;sub&gt;''i''&lt;/sub&gt; for which ''i'' is not in ''J''.

A structure ''A'' is an '''expansion''' of ''B'' just when ''B'' is a reduct of ''A''.  That is, reduct and expansion are mutual converses.

==Examples==
The [[monoid]] ('''Z''', +, 0) of [[integer]]s under [[addition]] is a reduct of the [[Group (mathematics)|group]] ('''Z''', +, &amp;minus;, 0) of integers under addition and negation, obtained by omitting negation. By contrast, the monoid ('''N''',+,0) of [[natural numbers]] under addition is not the reduct of any group.

Conversely the group ('''Z''', +, &amp;minus;, 0) is the expansion of the monoid ('''Z''', +, 0), expanding it with the operation of negation.

==References==
* {{ cite book | last=Burris | first=Stanley N. | coauthors=H. P. Sankappanavar | publisher = [[Springer Science+Business Media|Springer]] | title=A Course in Universal Algebra | year=1981 | isbn=3-540-90578-2 | url=http://www.thoralf.uwaterloo.ca/htdocs/ualg.html }}
* {{ cite book | last=Hodges | first=Wilfrid | publisher=[[Cambridge University Press]] | title=Model theory | year=1993 | isbn=0-521-30442-3 }}

[[Category:Algebra]]
[[Category:Mathematical relations]]
[[Category:Model theory]]
[[Category:Universal algebra]]</text>
      <sha1>lqz6p5kfo34g6forfo82nl1qhre16ws</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Multiplicative digital root</title>
    <ns>0</ns>
    <id>12735825</id>
    <revision>
      <id>600153058</id>
      <parentid>562548450</parentid>
      <timestamp>2014-03-18T13:18:04Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor/>
      <comment>l</comment>
      <text xml:space="preserve" bytes="950">The '''multiplicative digital root''' of a positive integer ''n'' is found by [[multiplication|multiplying]] the digits of ''n'' together, then repeating this operation until only a single digit remains. This single-digit number is called the multiplicative digital root of ''n''.&lt;ref&gt;{{Mathworld|title=Multiplicative Persistence|urlname=MultiplicativePersistence}}&lt;/ref&gt;

Multiplicative digital roots depend upon the [[radix|base]] in which ''n'' is written. If the term is used without qualification, it is assumed that ''n'' is written in base 10.

Multiplicative digital roots are the multiplicative equivalent of [[digital root]]s.

== Example ==
9876 would be reduced as 9876 -&gt; 9×8×7×6 = 3024 -&gt; 3×0×2×4 = 0. So the multiplicative digital root of 9876 is 0 and its [[multiplicative persistence]] (the number of steps required to reach a single digit) is 2.

== References ==
&lt;references/&gt;

[[Category:Algebra]]
[[Category:Number theory]]</text>
      <sha1>qtk110cqzr5pjefhqim6jni0qbbjfxf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Dixmier conjecture</title>
    <ns>0</ns>
    <id>12929412</id>
    <revision>
      <id>539971189</id>
      <parentid>508409464</parentid>
      <timestamp>2013-02-23T23:33:44Z</timestamp>
      <contributor>
        <username>RjwilmsiBot</username>
        <id>10996774</id>
      </contributor>
      <minor/>
      <comment>/* References */fixing page range dashes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="1185">In [[algebra]] the '''Dixmier conjecture''',  asked by {{harvtxt|Dixmier|1968|loc=problem 1}}, is the conjecture that any [[endomorphism]] of a [[Weyl algebra]] is an automorphism.

{{harvtxt|Tsuchimoto|2005}}, and independently {{harvtxt|Belov-Kanel|Kontsevich|2007}}, showed that the Dixmier conjecture is stably equivalent to the [[Jacobian conjecture]].

==References==
* {{Citation | last1=Dixmier | first1=Jacques | authorlink=Jacques Dixmier |title=Sur les algèbres de Weyl | url=http://www.numdam.org/item?id=BSMF_1968__96__209_0 | mr=0242897 | year=1968 | journal=Bulletin de la Société Mathématique de France   | volume=96 | pages=209–242}}
* {{Citation | last1=Tsuchimoto | first1=Yoshifumi | title=Endomorphisms of Weyl algebra and p-curvatures | year=2005 | journal=Osaka J. Math.| volume=42 | pages=435–452}}
* {{Citation | last1=Belov-Kanel | first1=Alexei | last2=Kontsevich | first2=Maxim | title=The Jacobian conjecture is stably equivalent to the Dixmier conjecture | arxiv=math/0512171 | mr=2337879 | year=2007 | journal=Moscow Mathematical Journal   | volume=7 | issue=2 | pages=209–218}}

[[Category:Algebra]]
[[Category:Conjectures]]


{{algebra-stub}}</text>
      <sha1>i7lkb392r3q661rh8a4migql6xvuup4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Boundedly generated group</title>
    <ns>0</ns>
    <id>14331851</id>
    <revision>
      <id>599177142</id>
      <parentid>597567089</parentid>
      <timestamp>2014-03-11T19:26:38Z</timestamp>
      <contributor>
        <username>Wavelength</username>
        <id>271168</id>
      </contributor>
      <comment>inserting 1 [[hyphen]]—[[wikt:finite-dimensional]]</comment>
      <text xml:space="preserve" bytes="12946">In [[mathematics]], a  [[group (mathematics)|group]] is called '''boundedly generated'''  if it can be expressed as a finite product of [[cyclic group|cyclic]] [[subgroup]]s. The property of bounded generation is also closely related with the [[congruence subgroup#Congruence subgroups and topological groups|congruence subgroup problem]] (see {{harvnb|Lubotzky|Segal|2003}}).

== Definitions ==

A group ''G'' is called ''boundedly generated'' if there exists a finite subset ''S'' of ''G'' and a positive integer ''m'' such that every element ''g'' of ''G'' can be represented as a product of at most ''m'' powers of the elements of ''S'':
 
: &lt;math&gt;g = s_1^{k_1} \cdots s_m^{k_m},&lt;/math&gt; where &lt;math&gt;s_i \in S&lt;/math&gt; and &lt;math&gt;k_i&lt;/math&gt; are integers.

The finite set ''S'' generates ''G'', so  a boundedly generated group is [[finitely generated group|finitely generated]].

An equivalent definition can be given in terms of cyclic subgroups. A group ''G'' is called ''boundedly generated'' if there is a finite family ''C''&lt;sub&gt;1&lt;/sub&gt;, …, ''C''&lt;sub&gt;''M''&lt;/sub&gt; of not necessarily distinct [[cyclic group|cyclic]] subgroups such that ''G'' = ''C''&lt;sub&gt;1&lt;/sub&gt;…''C''&lt;sub&gt;''M''&lt;/sub&gt; as a set.

== Properties ==

* Bounded generation is unaffected by passing to a subgroup of [[index of a subgroup|finite index]]: if ''H'' is a finite index subgroup of ''G'' then ''G'' is boundedly generated if and only if ''H'' is boundedly generated.
* Any [[quotient group]] of a boundedly generated group is also boundedly generated.
* A [[Finitely generated group|finitely generated]] [[periodic group]] must be ''finite'' if it is boundedly generated; equivalently, an ''infinite'' finitely generated periodic group is not boundedly generated.

A ''pseudocharacter'' on a discrete group ''G'' is defined to be a real-valued function ''f'' on a ''G'' such that 
: ''f''(''gh'') &amp;minus; ''f''(''g'') &amp;minus; ''f''(''h'') is uniformly bounded and ''f''(''g''&lt;sup&gt;''n''&lt;/sup&gt;) = ''n''·''f''(''g'').

* The vector space of pseudocharacters of a boundedly generated group ''G'' is finite-dimensional.

== Examples ==
* If ''n'' ≥ 3, the group ''SL''&lt;sub&gt;''n''&lt;/sub&gt;('''Z''') is boundedly generated by its ''elementary subgroups,'' formed by matrices differing from the identity matrix only in one off-diagonal entry. In 1984, Carter and Keller gave an elementary proof of this result, motivated by a question in [[algebraic K-theory]].
* A [[free group]] on at least two generators is not boundedly generated (see below).
* The group ''SL''&lt;sub&gt;''2''&lt;/sub&gt;('''Z''') is not boundedly generated, since it contains a free subgroup with two generators of index 12.
* A [[Gromov-hyperbolic group]] is boundedly generated if and only if it is ''virtually cyclic'' (or ''elementary''), i.e. contains a cyclic subgroup of finite index.

== Free groups are not boundedly generated ==
Several authors have stated in the mathematical literature that it is obvious that finitely generated free groups are not boundedly generated. This section contains various obvious and less obvious ways of proving this. Some of the methods, which touch on bounded cohomology, are important because they are geometric rather than algebraic, so can be applied to a wider class of groups, for example [[Gromov-hyperbolic group]]s.

Since for any ''n'' ≥ 2, the [[free group]] on 2 generators ''F''&lt;sub&gt;2&lt;/sub&gt;  contains the free group on ''n'' generators ''F''&lt;sub&gt;''n''&lt;/sub&gt; as a subgroup of finite index (in fact ''n'' – 1), once one non-cyclic free group on finitely many generators is known to be not boundedly generated, this will be true for all of them. Similarly, since ''SL''&lt;sub&gt;2&lt;/sub&gt;('''Z''') contains  ''F''&lt;sub&gt;2&lt;/sub&gt;  as a subgroup of index 12, it is enough to consider ''SL''&lt;sub&gt;2&lt;/sub&gt;('''Z'''). In other words, to show that no ''F''&lt;sub&gt;''n''&lt;/sub&gt; with ''n'' ≥ 2 has bounded generation, it is sufficient to prove this for one of them or even just for ''SL''&lt;sub&gt;2&lt;/sub&gt;('''Z''') .

===Burnside couterexamples===
Since bounded generation is preserved under taking homomorphic images, if a single [[finitely generated group]] with at least two generators is known to be not boundedly generated, this will be true for the free group on the same number of generators, and hence for all free groups. To show that no (non-cyclic) free group has bounded generation, it is therefore enough to produce one example of a finitely generated group which is not boundedly generated, and any finitely generated infinite [[periodic group]] will work. The existence of such groups constitutes [[Golod–Shafarevich theorem|Golod and Shafarevich]]'s negative solution of the [[Burnside problem|generalized Burnside problem]] in 1964; later, other explicit examples of infinite finitely generated periodic groups were constructed by Aleshin, Olshanskii, and Grigorchuk, using [[automata theory|automata]]. Consequently, free groups of rank at least two are not boundedly generated.

===Symmetric groups===
The [[symmetric group]] ''S''&lt;sub&gt;''n''&lt;/sub&gt; can be generated by two elements, a 2-cycle and an ''n''-cycle, so that it is a quotient group of  ''F''&lt;sub&gt;2&lt;/sub&gt;. On the other hand, it is easy to show that the maximal order ''M''(''n'') of an element in ''S''&lt;sub&gt;''n''&lt;/sub&gt; satisfies

: log ''M''(''n'') ≤ n/e

([[Edmund Landau]] proved the more precise asymptotic estimate log ''M''(''n'') ~ (''n'' log ''n'')&lt;sup&gt;1/2&lt;/sup&gt;).  In fact if the cycles in a [[cycle decomposition (group theory)|cycle decomposition]] of a [[permutation]] have length ''N''&lt;sub&gt;1&lt;/sub&gt;, ..., ''N''&lt;sub&gt;''k''&lt;/sub&gt; with ''N''&lt;sub&gt;1&lt;/sub&gt; + ··· + ''N''&lt;sub&gt;''k''&lt;/sub&gt; = ''n'', then the order of the permutation divides the product ''N''&lt;sub&gt;1&lt;/sub&gt; ···''N''&lt;sub&gt;''k''&lt;/sub&gt;, which in turn is bounded by (''n''/''k'')&lt;sup&gt;''k''&lt;/sup&gt;, using the [[inequality of arithmetic and geometric means]]. On the other hand, (''n''/''x'')&lt;sup&gt;''x''&lt;/sup&gt; is maximized when ''x''=''e''. If ''F''&lt;sub&gt;2&lt;/sub&gt; could be written as a product of ''m'' cyclic subgroups, then necessarily ''n''! would have to be less than or equal to ''M''(''n'')&lt;sup&gt;''m''&lt;/sup&gt; for all ''n'', contradicting [[Stirling's approximation|Stirling's asymptotic formula]].

===Hyperbolic geometry===
There is also a simple geometric proof that that ''G'' = ''SL''&lt;sub&gt;2&lt;/sub&gt;('''Z''') is not boundedly generated. It acts by [[Möbius transformation]]s on the [[upper half-plane]] '''H''', with the [[Poincaré metric]]. Any [[compact support|compactly supported]] [[1-form]] α on a [[fundamental domain]] of ''G'' extends uniquely to a ''G''-invariant 1-form on '''H'''. If ''z'' is in '''H''' and γ is the [[geodesic]] from ''z'' to ''g''(''z''), the function defined by
 
:&lt;math&gt;  F(g)\equiv F_{\alpha,z}(g)=\int_{\gamma}\, \alpha&lt;/math&gt;

satisfies the first condition for a pseudocharacter since by the [[Stokes theorem]]
 
:&lt;math&gt;  F(gh) - F(g)-F(h) = \int_{\Delta}\, d\alpha,&lt;/math&gt;

where Δ is the geodesic triangle with vertices ''z'', ''g''(''z'') and ''h''&lt;sup&gt;−1&lt;/sup&gt;(''z''), and geodesics triangles have area bounded by π. The homogenized function

:&lt;math&gt;f_\alpha(g) = \lim_{n\rightarrow \infty} F_{\alpha,z}(g^n)/n&lt;/math&gt;

defines a pseudocharacter, depending only on α. As is well known from the theory of [[dynamical system]]s, any orbit (''g''&lt;sup&gt;''k''&lt;/sup&gt;(''z'')) of a [[Möbius transformation#Hyperbolic transforms|hyperbolic element]] ''g'' has limit set consisting of two fixed points on the extended real axis; it follows that the geodesic segment from ''z'' to ''g''(''z'') cuts through only finitely many translates of the fundamental domain. It is therefore easy to choose α so that ''f''&lt;sub&gt;α&lt;/sub&gt; equals one on a given hyperbolic element and vanishes on a finite set of other hyperbolic elements with distinct fixed points. Since ''G'' therefore has an infinite-dimensional space of pseudocharacters, it cannot be boundedly generated.

Dynamical properties of hyperbolic elements can similarly be used to prove that any non-elementary [[Gromov-hyperbolic group]] is not boundedly generated.

===Brooks pseudocharacters===
Robert Brooks gave a combinatorial scheme to produce pseudocharacters of any free group ''F''&lt;sub&gt;''n''&lt;/sub&gt;; this scheme was later shown to yield
an infinite-dimensional family of pseudocharacters (see {{harvnb|Grigorchuk|1994}}). [[David Epstein|Epstein]] and Fujiwara later extended these results to all non-elementary Gromov-hyperbolic groups.

===Gromov boundary===
This simple [[folklore]] proof uses dynamical properties of the action of hyperbolic elements on the [[Gromov boundary]] of a [[Gromov-hyperbolic group]]. For the special case of the free group ''F''&lt;sub&gt;''n''&lt;/sub&gt;, the boundary (or space of ends) can be identified with the space ''X'' of [[semi-infinite]] [[word (group theory)#Reduced words|reduced words]]

:''g''&lt;sub&gt;1&lt;/sub&gt; ''g''&lt;sub&gt;2&lt;/sub&gt; ···

in the generators and their inverses. It gives a natural compactification of the [[tree (graph theory)|tree]], given by the [[Cayley graph]] with respect to the generators. A sequence of semi-infinite words converges to another such word provided that the initial segments agree after a certain stage, so that ''X'' is compact (and [[metrizable]]). The free group acts by left multiplication on the semi-infinite words. Moreover any element ''g'' in ''F''&lt;sub&gt;''n''&lt;/sub&gt; has exactly two fixed points ''g''&lt;sup&gt;±∞&lt;/sup&gt;, namely the reduced infinite words given by the limits of ''g''&lt;sup&gt;''n''&lt;/sup&gt; as ''n'' tends to ±∞. Furthermore ''g''&lt;sup&gt;''n''&lt;/sup&gt;·''w'' tends to ''g''&lt;sup&gt;±∞&lt;/sup&gt; as ''n'' tends to ±∞ for any semi-infinite word ''w''; and more generally if ''w''&lt;sub&gt;''n''&lt;/sub&gt; tends to ''w''≠ ''g''&lt;sup&gt; ±∞&lt;/sup&gt;, then ''g''&lt;sup&gt;''n''&lt;/sup&gt;·''w''&lt;sub&gt;''n''&lt;/sub&gt; tends to ''g''&lt;sup&gt;+∞&lt;/sup&gt; as ''n'' tends to ∞.

If ''F''&lt;sub&gt;''n''&lt;/sub&gt; were boundedly generated, it could be written as a product of cyclic groups ''C''&lt;sub&gt;''i''&lt;/sub&gt;
generated by elements ''h''&lt;sub&gt;''i''&lt;/sub&gt;. Let ''X''&lt;sub&gt;0&lt;/sub&gt; be the countable subset given by the finitely many ''F''&lt;sub&gt;''n''&lt;/sub&gt;-orbits
of the fixed points ''h''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt; ±∞&lt;/sup&gt;, the fixed points of the ''h''&lt;sub&gt;''i''&lt;/sub&gt; and all their conjugates. Since ''X'' is uncountable, there
is an element of ''g'' with fixed points outside ''X''&lt;sub&gt;0&lt;/sub&gt; and a point ''w'' outside ''X''&lt;sub&gt;0&lt;/sub&gt; different from these fixed points. Then for
some subsequence (''g''&lt;sub&gt;m&lt;/sub&gt;) of (''g''&lt;sup&gt;n&lt;/sup&gt;)

:''g''&lt;sub&gt;''m''&lt;/sub&gt; = ''h''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''n''(''m'',1)&lt;/sup&gt; ··· ''h''&lt;sub&gt;''k''&lt;/sub&gt;&lt;sup&gt;''n''(''m'',''k'')&lt;/sup&gt;, with each ''n''(''m'',''i'') constant or strictly monotone.

On the one hand, by successive use of the rules for computing limits of the form ''h''&lt;sup&gt;''n''&lt;/sup&gt;·''w''&lt;sub&gt;''n''&lt;/sub&gt;, the limit of the right hand side applied to ''x'' is necessarily a fixed point of one of the conjugates of the ''h''&lt;sub&gt;''i''&lt;/sub&gt;'s. On the other hand, this limit also must be ''g''&lt;sup&gt;+∞&lt;/sup&gt;, which is not one of these points, a contradiction.

== References ==
*{{cite journal
|author = Carter, David and Keller, Gordon|title= Elementary expressions for unimodular matrices|journal= Comm. Alg.|volume=12|year=1984|pages=379–389|doi= 10.1080/00927878408823008
|ref = harv
|issue = 4}}

*{{cite journal
|author= Epstein, David and Fujiwara, Koji| title = The second bounded cohomology of word-hyperbolic groups| journal=Topology|volume=36|year=1997|pages=1275–1289| doi = 10.1016/S0040-9383(96)00046-8
|ref= harv
|issue= 6}}

*{{cite journal | author= Ghys, Etienne and Barge, Jean| title= Surfaces et cohomologie bornée| journal=Inventiones Mathematicae| year=1988|volume=92|
pages=509–526| doi= 10.1007/BF01393745 | ref= harv | issue= 3}}

*{{cite journal|
author=Grigorchuk, R.I.|  title=On Burnside's problem on periodic groups|journal= Functional Anal. Appl.|volume= 14|year =1980|pages=41–43|
ref=harv}}

*{{cite journal
|author= Grigorchuk, R.I.|title=Some results in bounded cohomology |journal =London Mathematical Society Lecture Note Series|volume=224|year =1994|pages=111–163
|ref= harv
|isbn= 0-521-46595-8}}

*{{cite book
|author = Landau, Edmund |title =Handbuch der Lehrer von der Verteilung der Primzahlen, Vol. I|publisher =Chelsea|year= 1974| isbn =  0-8284-0096-2}} (see pages 222-229, also available on the [[arXiv|Cornell archive]])

*{{Cite journal
|last = [[Alexander Lubotzky|Lubotzky]]|first=Alexander|last2=Segal|first2= Dan | title = Subgroup growth | series= Progress in Mathematics| publisher=Birkhäuser|year=2003
|ref = harv
|postscript = &lt;!--None--&gt;
|isbn = 3-7643-6989-2}}.

*{{cite journal | author = Polterovich, Leonid and Rudnick, Zeev| title= Stable mixing for cat maps and quasi-morphisms of the modular group| year=2004| journal = Erg. Th. &amp; Dynam. Syst.| volume=24|pages=609–619| doi= 10.1017/S0143385703000531 | ref = harv | issue = 2}}

[[Category:Algebra]]
[[Category:Geometric group theory]]</text>
      <sha1>h2e1wuy4i3q0imhmjst6x3y4nkpfncx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Polynomial Diophantine equation</title>
    <ns>0</ns>
    <id>14626628</id>
    <revision>
      <id>491955984</id>
      <parentid>485601968</parentid>
      <timestamp>2012-05-11T06:45:42Z</timestamp>
      <contributor>
        <username>Helpful Pixie Bot</username>
        <id>14216826</id>
      </contributor>
      <minor/>
      <comment>ISBNs (Build KG)</comment>
      <text xml:space="preserve" bytes="1881">
In [[mathematics]], a '''polynomial Diophantine equation''' is an [[indeterminate equation|indeterminate]] [[polynomial]] [[equation]] for which one seeks solutions restricted to be [[polynomial]]s in the indeterminate. A [[Diophantine equation]], in general, is one where the solutions are restricted to some algebraic system, typically integers.  (In another usage )  ''Diophantine'' refers to the [[Hellenistic civilization|Hellenistic]] mathematician of the 3rd century, [[Diophantus]] of [[Alexandria]], who made initial studies of integer Diophantine equations.

An important type of polynomial Diophantine equations takes the form:

:&lt;math&gt;sa+tb=c\ &lt;/math&gt;

where ''a'', ''b'', and ''c'' are known polynomials, and we wish to solve for ''s'' and&amp;nbsp;''t''.

A simple example (and a solution) is:

:&lt;math&gt;s(x^2+1)+t(x^3+1)=2x\ &lt;/math&gt;
:&lt;math&gt;s=-x^3-x^2+x\ &lt;/math&gt;
:&lt;math&gt;t=x^2+x.\ &lt;/math&gt;

A necessary and sufficient condition for a polynomial Diophantine equation to have a solution is for ''c'' to be a multiple of the [[greatest common divisor|GCD]] of ''a'' and&amp;nbsp;''b''.  In the example above, the GCD of ''a'' and ''b'' was&amp;nbsp;1, so solutions would exist for any value of c.

Solutions to polynomial Diophantine equations are not unique.  Any multiple of &lt;math&gt;ab&lt;/math&gt; (say &lt;math&gt;rab&lt;/math&gt;) can be used to transform &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; into another solution &lt;math&gt;s'=s+rb&lt;/math&gt; &lt;math&gt;t'=t-ra&lt;/math&gt;:
:&lt;math&gt;(s+rb)a+(t-ra)b=c.\ &lt;/math&gt;

Some polynomial Diophantine equations can be solved using the [[extended Euclidean algorithm]], which works as well with polynomials as it does with integers.

==References==
*{{cite book
  | last = Bronstein
  | first = Manuel
  | title = Symbolic Integration I
  | publisher = [[Springer Science+Business Media|Springer]]
  | date = 2005
  | pages = 12–14
  | isbn = 3-540-21493-3 }}

[[Category:Algebra]]</text>
      <sha1>jwanhymvyhz2dsge4yn609k5sesfb3w</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Field arithmetic</title>
    <ns>0</ns>
    <id>14721989</id>
    <revision>
      <id>513928388</id>
      <parentid>513928265</parentid>
      <timestamp>2012-09-21T21:03:46Z</timestamp>
      <contributor>
        <username>Deltahedron</username>
        <id>16759019</id>
      </contributor>
      <minor/>
      <comment>/* References */ expand bibliodata</comment>
      <text xml:space="preserve" bytes="4783">In [[mathematics]], '''field arithmetic''' is a subject that studies the interrelations between arithmetic properties of a {{ql|field_(mathematics)|field}} and its [[absolute Galois group]].
It is an interdisciplinary subject as it uses tools from [[algebraic number theory]], [[arithmetic geometry]], [[algebraic geometry]], [[model theory]], the theory of [[finite groups]] and of [[profinite groups]].

==Fields with finite absolute Galois groups==
Let ''K'' be a field and let ''G'' = Gal(''K'') be its absolute Galois group. If ''K'' is [[algebraically closed]], then ''G'' = 1. If ''K'' = '''R''' is the real numbers, then 

:&lt;math&gt;G=Gal(\mathbf{C}/\mathbf{R})=\mathbf{Z}/2 \mathbf{Z}.&lt;/math&gt; 

Here '''C''' is the field of complex numbers and '''Z''' is the ring of integer numbers. 
A [[Artin–Schreier theorem|theorem of Artin and Schreier]] asserts that (essentially) these are all the possibilities for finite absolute Galois groups.

'''Artin–Schreier theorem.''' Let ''K'' be a field whose absolute Galois group ''G'' is finite. Then either ''K'' is separably closed and ''G'' is trivial or ''K'' is [[real closed]] and ''G'' = '''Z'''/2'''Z'''.

==Fields that are defined by their absolute Galois groups==
Some profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is 

:&lt;math&gt;\hat{\mathbf{Z}}=\lim_{\longleftarrow}\mathbf{Z}/n \mathbf{Z}.\,&lt;/math&gt; 

This group is isomorphic to the absolute Galois group of an arbitrary [[finite field]]. Also the absolute Galois group of the field of [[formal Laurent series]] '''C'''((''t'')) over the complex numbers is isomorphic to that group.

To get another example, we bring below two non-isomorphic fields whose absolute Galois groups are free (that is [[free profinite group]]). 

* Let ''C'' be an [[algebraically closed]] field and ''x'' a variable.  Then Gal(''C''(''x'')) is free of rank equal to the cardinality of ''C''. (This result is due to [[Adrien Douady]] for 0 characteristic and has its origins in [[Riemann's existence theorem]]. For a field of arbitrary characteristic it is due to [[David Harbater]] and [[Florian Pop]], and was also proved later by [[Dan Haran]] and [[Moshe Jarden]].)

* The absolute Galois group Gal('''Q''') (where '''Q''' are the rational numbers) is compact, and hence equipped with a normalized [[Haar measure]]. For a Galois automorphism ''s'' (that is an element in Gal('''Q''')) let ''N&lt;sub&gt;s&lt;/sub&gt;'' be the maximal Galois extension of '' '''Q''' '' that ''s'' fixes. Then with probability 1 the absolute Galois group Gal(''N''&lt;sub&gt;''s''&lt;/sub&gt;) is free of countable rank. (This result is due to [[Moshe Jarden]].)

In contrast to the above examples, if the fields in question are finitely generated over '''''Q''''', [[Florian Pop]] proves that an isomorphism of the absolute Galois groups yields an isomorphism of the fields:

'''Theorem.''' Let ''K'', ''L'' be finitely generated fields over '''''Q''''' and let ''a'':&amp;nbsp;Gal(''K'')&amp;nbsp;→&amp;nbsp;Gal(''L'') be an isomorphism. Then there exists a unique isomorphism of the algebraic closures, ''b'': ''K''&lt;sub&gt;alg&lt;/sub&gt;&amp;nbsp;→&amp;nbsp;''L''&lt;sub&gt;alg&lt;/sub&gt;, that induces  ''a''.

This generalizes an earlier work of [[Jürgen Neukirch]] and [[Koji Uchida]] on number fields.

==Pseudo algebraically closed fields==
{{Main|Pseudo algebraically closed field}}

A [[pseudo algebraically closed field]] (in short PAC) ''K'' is a field satisfying the following geometric property. Each [[absolutely irreducible]] algebraic variety ''V'' defined over ''K'' has a ''K''-[[rational point]].

Over PAC fields there is a firm link between arithmetic properties of the field and group theoretic properties of its absolute Galois group. A nice theorem in this spirit connects [[Hilbertian field]]s with ω-free fields (''K'' is ω-free if any [[embedding problem]] for ''K'' is properly solvable). 

'''Theorem.''' Let ''K'' be a PAC field. Then ''K'' is Hilbertian if and only if ''K'' is ω-free.

[[Peter Roquette]] proved the right-to-left direction of this theorem and conjectured the opposite direction. [[Michael Fried (mathematician)|Michael Fried]] and [[Helmut Völklein]] applied algebraic topology and complex analysis to establish Roquette's conjecture in characteristic zero. Later Pop 
proved the Theorem for arbitrary characteristic by developing &quot;[[rigid patching]]&quot;.

==References==
{{reflist}}
* {{cite book | last1=Fried | first1=Michael D. | last2=Jarden | first2=Moshe | title=Field arithmetic | edition=2nd revised and enlarged | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge | volume=11 | publisher=[[Springer-Verlag]] | year=2004 | isbn=3-540-22811-X | zbl=1055.12003 }}
*{{Neukirch et al. CNF}}

[[Category:Algebra]]
[[Category:Galois theory]]</text>
      <sha1>i7p59pqwmqqwf00m72t9brojjq53cj6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Identity element</title>
    <ns>0</ns>
    <id>14962</id>
    <revision>
      <id>610321987</id>
      <parentid>599717982</parentid>
      <timestamp>2014-05-27T09:05:59Z</timestamp>
      <contributor>
        <username>Eric Kvaalen</username>
        <id>122969</id>
      </contributor>
      <comment>A semigroup can have left-identities without any right-identity.</comment>
      <text xml:space="preserve" bytes="5915">{{single source|date=July 2013}}
In [[mathematics]], an '''identity element''' (or '''neutral element''') is a special type of element of a [[Set (mathematics)|set]] with respect to a [[binary operation]] on that set. It leaves other elements unchanged when combined with them. This is used for [[group (mathematics)|group]]s and related concepts.

The term ''identity element'' is often shortened to ''identity'' (as will be done in this article) when there is no possibility of confusion.

Let {{math|(''S'', ∗)}} be a set&amp;nbsp;{{mvar|S}} with a binary operation&amp;nbsp;∗ on it (known as a [[magma (algebra)|magma]]). Then an element&amp;nbsp;{{mvar|e}} of&amp;nbsp;{{mvar|S}} is called a '''[[left and right (algebra)|left]] identity''' if {{math|1=''e'' ∗ ''a'' = ''a''}} for all&amp;nbsp;{{mvar|a}} in&amp;nbsp;{{mvar|S}}, and a '''[[left and right (algebra)|right]] identity''' if {{math|1=''a'' ∗ ''e'' = ''a''}} for all&amp;nbsp;{{mvar|a}} in&amp;nbsp;{{mvar|S}}. If {{mvar|e}} is both a left identity and a right identity, then it is called a '''two-sided identity''', or simply an '''identity'''.

An identity with respect to addition is called an [[additive identity]] (often denoted as&amp;nbsp;0) and an identity with respect to multiplication is called a [[multiplicative identity]] (often denoted as&amp;nbsp;1).  The distinction is used most often for sets that support both binary operations, such as [[ring (mathematics)|ring]]s. The multiplicative identity is often called the '''unit''' in the latter context, where, though, a [[unit (ring theory)|unit]] is often used in a broader sense, to mean an element with a multiplicative inverse.

==Examples==
{| class=&quot;wikitable&quot; style=&quot;margin: 1em auto 1em auto&quot;
!set!!operation!!identity
|-
|[[real number]]s||+ ([[addition]])||[[0 (number)|0]]
|-
|real numbers||· ([[multiplication]])||[[1 (number)|1]]
|-
|[[non-negative number]]s
| rowspan=2 |{{math|''a''&lt;sup&gt;''b''&lt;/sup&gt;}} ([[exponentiation]])
| rowspan=2 |1 (right identity only)
|-
|[[integer]]s (to [[projective line|extended]] [[rational number|rationals]])
|-
|[[positive integer]]s||[[least common multiple]]||1
|-
|[[non-negative integer]]s||[[greatest common divisor]]||0 (under most definitions of GCD)
|-
&lt;!-- ||'''R'''&lt;sup&gt;''n''&lt;/sup&gt;  ||· (multiplication)|[[1 (number)|1]] --&gt;
|{{mvar|m}}-by-{{mvar|n}} [[matrix (mathematics)|matrices]]|| + (addition)
|[[zero matrix]]
|-
|{{mvar|n}}-by-{{mvar|n}} square matrices|| [[matrix multiplication]]
|''I''&lt;sub&gt;''n''&lt;/sub&gt; ([[identity matrix]])
|-
|{{mvar|m}}-by-{{mvar|n}} matrices|| &lt;math&gt;\circ&lt;/math&gt; ([[Hadamard product (matrices)|Hadamard product]])
| {{math|''J''&lt;sub&gt;''m'', ''n''&lt;/sub&gt;}} ([[Matrix of ones]])
|-
|all [[function (mathematics)|functions]] from a set&amp;nbsp;{{mvar|M}} to itself||∘ ([[function composition]])||[[identity function]]
|-
|all [[distribution (mathematics)|distributions]] on a [[group (mathematics)|group]]&amp;nbsp;{{mvar|G}}&lt;!-- a crap refurbished --&gt;||∗ ([[convolution]])||δ ([[Dirac delta]])
|-
|[[extended real number]]s || [[minimum]]/infimum || +∞
|-
|extended real numbers || [[maximum]]/supremum || −∞
|-
|subsets of a [[Set (mathematics)|set]]&amp;nbsp;{{mvar|M}} || ∩ ([[set intersection|intersection]]) || {{mvar|M}}
|-
|sets || ∪ ([[set union|union]]) || ∅ ([[empty set]])
|-
|[[string (computer science)|strings]], [[tuple|lists]]|| [[concatenation]] || [[empty string]], empty list
|-
|a [[boolean algebra (structure)|boolean algebra]] || ∧ ([[logical and]]) || ⊤ (truth)
|-
|a boolean algebra || ∨ ([[logical or]]) || ⊥ (falsity)
|-
|a boolean algebra || ⊕ ([[Exclusive or]]) || ⊥ (falsity)
|-
|[[knot (mathematics)|knots]] || [[knot sum]] || [[unknot]]
|-
|[[compact surfaces]] || # ([[connected sum]]) || [[sphere|''S''&lt;sup&gt;2&lt;/sup&gt;]]
|-
|only two elements {{math|{''e'', ''f''} }}
|∗ defined by&lt;br&gt; {{math|1=''e'' ∗ ''e'' = ''f'' ∗ ''e'' = ''e''}} and &lt;br&gt; {{math|1=''f'' ∗ ''f'' = ''e'' ∗ ''f'' = ''f''}}
|both {{mvar|e}} and {{mvar|f}} are left identities,&lt;br&gt; but there is no right identity&lt;br&gt; and no two-sided identity
|}

==Properties==
As the last example (a [[semigroup]]) shows, it is possible for {{math|(''S'', ∗)}} to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if {{mvar|l}} is a left identity and {{mvar|r}} is a right identity then {{math|1=''l'' = ''l'' ∗ ''r'' = ''r''}}. In particular, there can never be more than one two-sided identity.  If there were two, {{mvar|e}} and {{mvar|f}}, then {{math|''e'' ∗ ''f''}} would have to be equal to both {{mvar|e}} and {{mvar|f}}.

It is also quite possible for {{math|(''S'', ∗)}} to have ''no'' identity element. A common example of this is the [[cross product]] of [[Euclidean vector|vectors]]. The absence of an identity element is related to the fact that the [[Direction (geometry)|direction]] of any nonzero cross product is always [[orthogonal]] to any element multiplied&amp;nbsp;– so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive [[semigroup]] of [[Positive number|positive]] [[natural number]]s.

==See also==
*[[Absorbing element]]
*[[Inverse element]]
*[[Additive inverse]]
*[[Monoid]]
*[[Unital (disambiguation)]]
*[[Quasigroup]]
*[[Pseudo-ring #Properties weaker than having an identity|Pseudo-ring]]

==References==
* M. Kilp, U. Knauer, A.V. Mikhalev, ''Monoids, Acts and Categories with Applications to Wreath Products and Graphs'', De Gruyter Expositions in Mathematics vol.&amp;nbsp;29, Walter de Gruyter, 2000, ISBN 3-11-015248-7, p.&amp;nbsp;14–15

[[Category:Abstract algebra]]
[[Category:Algebra]]
[[Category:Binary operations|*Identity element]]
[[Category:One]]

__NOTOC__</text>
      <sha1>984cmpqhuhvidwumqtk008m5gl7zzry</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Symbolic method</title>
    <ns>0</ns>
    <id>565742</id>
    <revision>
      <id>498340059</id>
      <parentid>492847154</parentid>
      <timestamp>2012-06-19T13:53:19Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */Journal cites, added 1 DOI, using [[Project:AWB|AWB]] (8079)</comment>
      <text xml:space="preserve" bytes="8158">In [[mathematics]], the '''symbolic method''' in [[invariant theory]] is an [[algorithm]] developed by {{harvs|txt|authorlink=Arthur Cayley|last=Cayley|year=1846}}, {{harvs|txt|authorlink=Siegfried Heinrich Aronhold|first=Siegfried Heinrich |last=Aronhold|year=1858}},  {{harvs|txt|authorlink=Alfred  Clebsch|last=Clebsch|first=Alfred|year=1861}}, and {{harvs|txt|authorlink=Paul Gordan|first=Paul |last=Gordan|year=1887}} in the 19th century for computing  [[invariant (mathematics)|invariant]]s of [[algebraic form]]s. It is based on treating the form as if it were a power of a degree one form, which corresponds to embedding a symmetric power of a vector space into the symmetric elements of a tensor product of copies of it.

==Symbolic notation==

The symbolic method uses a compact but rather confusing and mysterious notation for invariants, depending on the introduction of new symbols ''a'', ''b'', ''c'', ...  (from which the symbolic method gets its name) with apparently contradictory properties.

===Example: the discriminant of a binary quadratic form===
These symbols can be explained by the following example from {{harv|Gordan|1887|loc=volume 2, pages 1-3}}. Suppose that 
:&lt;math&gt;\displaystyle  f(x) = A_0x_1^2+2A_1x_1x_2+A_2x_2^2&lt;/math&gt;
is a binary quadratic form with an invariant given by the discriminant
:&lt;math&gt;\displaystyle \Delta=A_0A_2-A_1^2.&lt;/math&gt;
The symbolic representation of the discriminant is 
:&lt;math&gt;\displaystyle 2\Delta=(ab)^2&lt;/math&gt;
where ''a'' and ''b'' are the symbols. The meaning of  the expression (''ab'')&lt;sup&gt;2&lt;/sup&gt; is as follows. First of all, (''ab'') is a shorthand form for the determinant of a matrix whose rows are ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt; and ''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;, so 
:&lt;math&gt;\displaystyle (ab)=a_1b_2-a_2b_1.&lt;/math&gt;
Squaring this we get
:&lt;math&gt;\displaystyle (ab)^2=a_1^2b_2^2-2a_1a_2b_1b_2+a_2^2b_1^2.&lt;/math&gt;
Next we pretend that
:&lt;math&gt;\displaystyle f(x)=(a_1x_1+a_2x_2)^2=(b_1x_1+b_2x_2)^2&lt;/math&gt;
so that
:&lt;math&gt;\displaystyle  A_i=a_1^{2-i}a_2^{i}= b_1^{2-i}b_2^{i}&lt;/math&gt;
and we ignore the fact that this does not seem to make sense if ''f'' is not a power of a linear form. 
Substituting these values gives
:&lt;math&gt;\displaystyle (ab)^2= A_2A_0-2A_1A_1+A_0A_2 = 2\Delta.&lt;/math&gt;

===Higher degrees===
More generally if 
:&lt;math&gt;\displaystyle  f(x) = A_0x_1^n+\binom{n}{1}A_1x_1^{n-1}x_2+\cdots+A_nx_2^n&lt;/math&gt;
is a binary form of higher degree, then one introduces new variables ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;,  ''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;, ''c''&lt;sub&gt;1&lt;/sub&gt;, ''c''&lt;sub&gt;2&lt;/sub&gt;, with the properties
:&lt;math&gt;f(x)=(a_1x_1+a_2x_2)^n=(b_1x_1+b_2x_2)^n=(c_1x_1+c_2x_2)^n=\cdots.&lt;/math&gt;

What this means is that the following two vector spaces are naturally isomorphic:
*The  vector space of homogeneous polynomials in ''A''&lt;sub&gt;0&lt;/sub&gt;,...''A''&lt;sub&gt;''n''&lt;/sub&gt; of degree ''m''
*The vector space of  polynomials in 2''m'' variables ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;,  ''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;, ''c''&lt;sub&gt;1&lt;/sub&gt;, ''c''&lt;sub&gt;2&lt;/sub&gt;, ... that have degree ''n'' in each of the ''m'' pairs of variables (''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;),  (''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;), (''c''&lt;sub&gt;1&lt;/sub&gt;, ''c''&lt;sub&gt;2&lt;/sub&gt;), ... and are symmetric under permutations of the ''m'' symbols ''a'', ''b'', ....,
The isomorphism is given by mapping ''a''{{su|p=''n''&amp;minus;''j''|b=1}}''a''{{su|p=''j''|b=2}}, ''b''{{su|p=''n''&amp;minus;''j''|b=1}}''b''{{su|p=''j''|b=2}}, .... to ''A''&lt;sub&gt;''j''&lt;/sub&gt;. This mapping does not preserve products of polynomials.

===More variables===
The extension to a form ''f'' in more than two variables ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,''x''&lt;sub&gt;3&lt;/sub&gt;,... is similar: one introduces symbols ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;,''a''&lt;sub&gt;3&lt;/sub&gt; and so on  with the properties
:&lt;math&gt;f(x)=(a_1x_1+a_2x_2+a_3x_3+\cdots)^n=(b_1x_1+b_2x_2+b_3x_3+\cdots)^n=(c_1x_1+c_2x_2+c_3x_3+\cdots)^n=\cdots.&lt;/math&gt;

==Symmetric products==

The rather mysterious formalism of the symbolic method corresponds to embedding a symmetric product S&lt;sup&gt;''n''&lt;/sup&gt;(''V'') of a vector space ''V'' into a tensor product of ''n'' copies of ''V'', as the elements preserved by the action of the symmetric group. In fact this is done twice, because the invariants of degree ''n'' of a quantic of degree ''m'' are the invariant elements of S&lt;sup&gt;''n''&lt;/sup&gt;S&lt;sup&gt;''m''&lt;/sup&gt;(''V''), which gets embedded into a tensor product of ''mn'' copies of ''V'', as the elements invariant under a wreath product of the two symmetric groups. The brackets of the symbolic method are really invariant linear forms on this tensor product, which give invariants of S&lt;sup&gt;''n''&lt;/sup&gt;S&lt;sup&gt;''m''&lt;/sup&gt;(''V'') by restriction.

==See also==

*[[Umbral calculus]]

==References==

*{{Citation | last1=Aronhold | first1=Siegfried Heinrich | title=Theorie der homogenen Functionen dritten Grades von drei Veränderlichen. | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN00215028X | language=German | year=1858 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=55 | pages=97–191}}
*{{Citation | last1=Cayley | first1=Arthur | author1-link=Arthur Cayley | title=On linear transformations | url=http://resolver.sub.uni-goettingen.de/purl?PPN600493962_0001 | year=1846 | journal=Cambridge and Dublin Mathematical Journal  | pages=104–122}}
*{{Citation | last1=Clebsch | first1=A. | title=Ueber symbolische Darstellung algebraischer Formen | url=http://resolver.sub.uni-goettingen.de/purl?PPN243919689_0059 | language=German  | year=1861 | journal=Journal für Reine und Angewandte Mathematik | issn=0075-4102 | volume=59 | pages=1–62}}
*{{citation|first=Jean|last=Dieudonné|authorlink=Jean Dieudonné|first2=James B.|last2=Carrell|title=Invariant theory, old and new|url=http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6W9F-4D7JKM7-1&amp;_user=1495569&amp;_coverDate=02%2F28%2F1970&amp;_rdoc=1&amp;_fmt=high&amp;_orig=search&amp;_origin=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_acct=C000053194&amp;_version=1&amp;_urlVersion=0&amp;_userid=1495569&amp;md5=66afdc20312efc67ffb295764befef82&amp;searchtype=a|
journal=Advances in Mathematics|volume =4|year= 1970|pages=1–80|doi=10.1016/0001-8708(70)90015-0}}, pages 32–37, &quot;Invariants of ''n''-ary forms: the symbolic method. Reprinted as {{citation|first=Jean|last=Dieudonné|authorlink=Jean Dieudonné|first2=James B.|last2=Carrell|title=Invariant theory, old and new|publisher=Academic Press|year=1971|isbn=0-12-215540-8}} 
*{{Citation | last1=Dolgachev | first1=Igor | title=Lectures on invariant theory | publisher=[[Cambridge University Press]] | series=London Mathematical Society Lecture Note Series | isbn=978-0-521-52548-0 | doi=10.1017/CBO9780511615436 | mr=2004511 | year=2003 | volume=296}}
*{{Citation | last1=Gordan | first1=Paul | editor1-last=Kerschensteiner | editor1-first=Georg | title=Vorlesungen über Invariantentheorie | year=1887 | url=http://books.google.com/books?isbn=978-0-8284-0328-3 | publisher=Chelsea Publishing Co. | location=New York | edition=2nd | isbn=978-0-8284-0328-3 | mr=917266 }}
*{{citation|title=The Algebra of invariants|first=John Hilton|last= Grace|authorlink=John Hilton Grace|first2= Alfred|last2= Young|authorlink2=Alfred Young|year=1903|publisher=Cambridge University Press}}
*{{Citation | last1=Hilbert | first1=David | author1-link=David Hilbert | url=http://books.google.com/books?isbn=0521449030|title=Theory of algebraic invariants | origyear=1897 | publisher=[[Cambridge University Press]] | isbn=978-0-521-44457-6 | mr=1266168 | year=1993}}
*{{citation|editor-first=Sebastian S.|editor-last=Koh|title=Invariant Theory|series=Lecture Notes in Mathematics|volume=1278|year=1987|isbn=3-540-18360-4}}
*{{Citation | last1=Kung | first1=Joseph P. S. | last2=Rota | first2=Gian-Carlo | author2-link=Gian-Carlo Rota | title=The invariant theory of binary forms | url=http://www.ams.org/journals/bull/1984-10-01/S0273-0979-1984-15188-7 | doi=10.1090/S0273-0979-1984-15188-7 | mr=722856 | year=1984 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=10 | issue=1 | pages=27–85}}

[[Category:Algebra]]
[[Category:Invariant theory]]</text>
      <sha1>k7et5hamxd1alurjdknspdhhjxwutr8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Freshman's dream</title>
    <ns>0</ns>
    <id>17373539</id>
    <revision>
      <id>584664701</id>
      <parentid>584658441</parentid>
      <timestamp>2013-12-05T09:18:25Z</timestamp>
      <contributor>
        <ip>89.239.225.81</ip>
      </contributor>
      <comment>Undid revision 584658441 by [[Special:Contributions/212.201.44.246|212.201.44.246]] ([[User talk:212.201.44.246|talk]]) Vandalism</comment>
      <text xml:space="preserve" bytes="6293">[[File:Freshman's Dream.svg|right|thumbnail|An illustration of the Freshman's dream in two dimensions. Each side of the square is X+Y in length. The area of the square is the sum of the area of the yellow region (=X&lt;sup&gt;2&lt;/sup&gt;), the area of the green region (=Y&lt;sup&gt;2&lt;/sup&gt;), and the area of the two white regions (=2×X×Y).]]

The '''freshman's dream''' is a name sometimes given to the error (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sup&gt;''n''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''y''&lt;sup&gt;''n''&lt;/sup&gt;, where ''n'' is a real number (usually a positive integer greater than 1). Beginning students commonly make this error in computing the [[exponentiation|power]] of a sum of real numbers.&lt;ref&gt;Julio R. Bastida, ''Field Extensions and Galois Theory'', Addison-Wesley Publishing Company, 1984, p.8.&lt;/ref&gt;&lt;ref&gt;Fraleigh, John B., ''A First Course in Abstract Algebra'', Addison-Wesley Publishing Company, 1993, p.453, ISBN 0-201-53467-3.&lt;/ref&gt; When ''n'' = 2, it is easy to see why this is incorrect: (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;2&lt;/sup&gt; can be correctly computed  as ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;2''xy''&amp;nbsp;+&amp;nbsp;''y''&lt;sup&gt;2&lt;/sup&gt; using [[distributivity]] (or commonly known as the [[FOIL method]]). For larger positive integer values of ''n'', the correct result is given by the [[binomial theorem]].

The name &quot;freshman's dream&quot; also sometimes refers to the theorem that says that for a [[prime number]] ''p'', if ''x'' and ''y'' are members of a [[commutative ring]] of [[characteristic_(algebra)|characteristic]] ''p'', then 
(''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''p''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sup&gt;''p''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''y''&lt;sup&gt;''p''&lt;/sup&gt;. In this case, the &quot;mistake&quot; actually gives the correct result, due to ''p'' dividing all the [[binomial coefficient]]s save the first and the last.

==Examples==
*&lt;math&gt;(1+4)^2 = 5^2 = 25&lt;/math&gt;, but &lt;math&gt;1^2+4^2 = 17&lt;/math&gt;. 
*&lt;math&gt;\sqrt{x^2+y^2}&lt;/math&gt; does not generally equal &lt;math&gt;\sqrt{x^2}+\sqrt{y^2}=|x|+|y|&lt;/math&gt;. For example, &lt;math&gt;\sqrt{9+16}=\sqrt{25}=5&lt;/math&gt;, which does not equal 3+4=7. In this example, the error is being committed with the exponent ''n''&amp;nbsp;=&amp;nbsp;{{frac|1|2}}.

==Prime characteristic==

When ''p'' is a prime number and ''x'' and ''y'' are members of a [[commutative ring]] of [[characteristic_(algebra)|characteristic]] ''p'', then (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''p''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sup&gt;''p''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''y''&lt;sup&gt;''p''&lt;/sup&gt;. This can be seen by examining the prime factors of the binomial coefficients: the ''n''th binomial coefficient is

:&lt;math&gt;\binom{p}{n} = \frac{p!}{n!(p-n)!}.&lt;/math&gt;

The [[numerator]] is ''p'' [[factorial]], which is divisible by ''p''. However, when {{nowrap|0 &lt; ''n'' &lt; ''p''}}, neither ''n''! nor {{nowrap|(''p'' &amp;minus; ''n'')!}} is divisible by ''p'' since all the terms are less than ''p'' and ''p'' is prime. Since a binomial coefficient is always an integer, the ''n''th binomial coefficient is divisible by ''p'' and hence equal to 0 in the ring. We are left with the zeroth and ''p''th coefficients, which both equal 1, yielding the desired equation.

Thus in characteristic ''p'' the freshman's dream is a valid identity. This result demonstrates that exponentiation by ''p'' produces an [[endomorphism]], known as the [[Frobenius endomorphism]] of the ring. 

The demand that the characteristic ''p'' be a prime number is central to the truth of the freshman's dream. In fact, a related theorem states that a number ''n'' is prime [[if and only if]] {{nowrap|(''x''+1)&lt;sup&gt;''n''&lt;/sup&gt; ≡ ''x&lt;sup&gt;n&lt;/sup&gt;'' + 1 (mod ''n'')}} in the [[polynomial ring]] &lt;math&gt;\mathbb{Z}_n[x]&lt;/math&gt;. This theorem is a direct consequence of [[Fermat's Little Theorem]] and it is a key fact in modern primality testing.&lt;ref name=Granville&gt;A. Granville, ''[http://www.ams.org/bull/2005-42-01/S0273-0979-04-01037-7/S0273-0979-04-01037-7.pdf It Is Easy To Determine Whether A Given Integer Is Prime]'', Bull. of the AMS, Volume 42, Number 1 (Sep. 2004), Pages 3–38.&lt;/ref&gt;

==History and alternate names==
The history of the term &quot;freshman's dream&quot; is somewhat unclear. In a 1940 article on [[modular field]]s, [[Saunders Mac Lane]] quotes [[Stephen Cole Kleene|Stephen Kleene]]'s remark that a knowledge of (''a'' + ''b'')&lt;sup&gt;2&lt;/sup&gt;= ''a''&lt;sup&gt;2&lt;/sup&gt; + ''b''&lt;sup&gt;2&lt;/sup&gt; in a [[field (mathematics)|field]] of characteristic 2 would corrupt freshman students of [[abstract algebra|algebra]]. This may be the first connection between &quot;freshman&quot; and binomial expansion in fields of positive characteristic.&lt;ref&gt;Colin R. Fletcher, Review of '''Selected papers on algebra, edited by [[Susan Montgomery]], Elizabeth W. Ralston and others. Pp xv, 537. 1977. SBN 0 88385 203 9 (Mathematical Association of America)''', ''The Mathematical Gazette'', Vol. 62, No. 421 (Oct., 1978), The Mathematical Association. p. 221.&lt;/ref&gt; Since then, authors of undergraduate algebra texts took note of the common error. The first actual attestation of the phrase &quot;freshman's dream&quot; seems to be in [[Thomas W. Hungerford|Hungerford's]] undergraduate algebra textbook (1974), where he quotes McBrien.&lt;ref&gt;Thomas W. Hungerford, ''Algebra,'' Springer, 1974, p. 121; also in ''Abstract Algebra: An Introduction'', 2nd edition. Brooks Cole, July 12, 1996, p. 366.&lt;/ref&gt; Alternative terms include &quot;'''freshman exponentiation'''&quot;, used in Fraleigh (1998).&lt;ref&gt;John B. Fraleigh, ''A First Course In Abstract Algebra'', 6th edition, Addison-Wesley, 1998. pp. 262 and 438.&lt;/ref&gt; The term &quot;freshman's dream&quot; itself, in non-mathematical contexts, is recorded since the 19th century.&lt;ref&gt;[http://www.google.com/search?tbo=p&amp;tbm=bks&amp;q=%22freshman%27s+dream%22&amp;tbs=,cdr:1,cd_min:Jan%201_2%201800,cd_max:Dec%2031_2%201900&amp;num=10 Google books 1800–1900 search for &quot;freshman's dream&quot;]: [http://books.google.com/books?id=3XNHAAAAYAAJ&amp;pg=PA176&amp;dq=%22freshman%27s+dream%22 Bentley's miscellany, Volume 26, p. 176], 1849&lt;/ref&gt;

Since the expansion of (''x'' + ''y'')&lt;sup&gt;''n''&lt;/sup&gt; is correctly given by the [[binomial theorem]], the freshman's dream is also known as the &quot;'''Child's Binomial Theorem'''&quot; &lt;ref name=Granville/&gt; or &quot;'''Schoolboy Binomial Theorem'''&quot;.

==See also==
*[[Primality test]]
*[[Sophomore's dream]]

==References==
&lt;references/&gt;

[[Category:Algebra]]
[[Category:Mathematics education]]</text>
      <sha1>aw7hfz5l4wb7wrk7umog7venatemsiy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Algebraic signal processing</title>
    <ns>0</ns>
    <id>17960231</id>
    <revision>
      <id>448839152</id>
      <parentid>317020297</parentid>
      <timestamp>2011-09-06T23:57:23Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <minor/>
      <comment>Various citation cleanup., added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="832">{{Orphan|date=September 2011}}

In the [[Abstract algebra|algebraic]] theory of linear [[signal processing]], a set of [[Filter (signal processing)|filter]]s is treated as an [[Algebra (ring theory)|algebra]] and a set of [[Signal (electrical engineering)|signal]]s is treated as a [[Module (mathematics)|module]] and the [[z-transform]] is generalized to [[linear map]]s.

==References==
* {{cite arxiv | last1=Püschel | first1=Markus | last2=Moura | first2=Jose M. F. | title=Algebraic Signal Processing Theory | date=2006 | eprint=cs/0612077}}.

==External links==
*[http://www.ece.cmu.edu/~smart/research.html Smart Project: Algebraic Theory of Signal Processing] at the Department of Electrical and Computer Engineering at Carnegie Mellon University.

[[Category:Algebra]]
[[Category:Signal processing]]


{{electronics-stub}}</text>
      <sha1>q4ndtfvqc7y2363nh8zbewoqu5xak2g</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Median algebra</title>
    <ns>0</ns>
    <id>18416734</id>
    <revision>
      <id>545369342</id>
      <parentid>483156258</parentid>
      <timestamp>2013-03-19T06:26:51Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6806108]]</comment>
      <text xml:space="preserve" bytes="3498">In [[mathematics]], a '''median algebra''' is a set with a [[ternary operation]] &lt;math&gt;\langle x,y,z \rangle&lt;/math&gt; satisfying a set of axioms which generalise the notion of median or [[majority function]], as a [[Boolean function]].

The axioms are 
#   &lt;math&gt;\langle x,y,y \rangle = y&lt;/math&gt;
#   &lt;math&gt;\langle x,y,z \rangle = \langle z,x,y \rangle&lt;/math&gt;  
#   &lt;math&gt;\langle x,y,z \rangle = \langle x,z,y \rangle&lt;/math&gt;    
#   &lt;math&gt;\langle \langle x,w,y\rangle ,w,z \rangle = \langle x,w, \langle y,w,z \rangle\rangle&lt;/math&gt;

The second and third axioms imply [[commutativity]]: it is possible (but not easy) to show that in the presence of the other three, axiom (3) is redundant.  The fourth axiom implies associativity.
There are other possible axiom systems: for example the two
*  &lt;math&gt;\langle x,y,y \rangle = y&lt;/math&gt;
*  &lt;math&gt;\langle u,v, \langle u,w,x \rangle\rangle = \langle u,x, \langle w,u,v \rangle\rangle&lt;/math&gt;
also suffice.

In a [[Boolean algebra (introduction)|Boolean algebra]], or more generally a [[distributive lattice]], the median function &lt;math&gt;\langle x,y,z \rangle = (x \vee y) \wedge (y \vee z) \wedge (z \vee x)&lt;/math&gt; satisfies these axioms, so that every Boolean algebra and every distributive lattice forms a median algebra.

Birkhoff and Kiss showed that a median algebra with elements 0 and 1 satisfying &amp;lt; 0,x,1 &amp;gt; = x is a [[distributive lattice]].

==Relation to median graphs==
A [[median graph]] is an [[undirected graph]] in which for every three vertices ''x'', ''y'', and ''z'' there is a unique vertex &amp;lt; x,y,z &amp;gt; that belongs to [[shortest path]]s between any two of ''x'', ''y'', and ''z''. If this is the case, then the operation &amp;lt; x,y,z &amp;gt; defines a median algebra having the vertices of the graph as its elements.

Conversely, in any median algebra, one may define an ''interval'' [''x'', ''z''] to be the set of elements ''y'' such that &amp;lt; x,y,z &amp;gt; = ''y''. One may define a graph from a median algebra by creating a vertex for each algebra element and an edge for each pair (''x'', ''z'') such that the interval [''x'', ''z''] contains no other elements. If the algebra has the property that every interval is finite, then this graph is a median graph, and it accurately represents the algebra in that the median operation defined by shortest paths on the graph coincides with the algebra's original median operation.

==References==
* {{cite journal | last=Birkhoff | first=Garrett | authorlink=Garrett Birkhoff | last2=Kiss | title=A ternary operation in distributive lattices | journal=[[Bulletin of the American Mathematical Society|Bull. Amer. Math. Soc.]] | volume=53 | date=1947 | issue=8 | pages=749–752 | doi=10.1090/S0002-9904-1947-08864-9 | first2=S.A. }}
* {{cite journal | last=Isbell | first=John R. | authorlink = John R. Isbell | title=Median algebra | journal=[[Transactions of the American Mathematical Society|Trans. Amer. Math. Soc.]] | volume=260 | issue=2 | date=August 1980 | pages=319–362 | doi=10.2307/1998007 | jstor=1998007 | publisher=American Mathematical Society }}
* {{ cite book | last=Knuth | first=Donald E. | authorlink=Donald Knuth | title=Introduction to combinatorial algorithms and Boolean functions | series=[[The Art of Computer Programming]] | volume=4.0 | date=2008 | isbn=0-321-53496-4 | pages=64–74 | publisher=Addison-Wesley | location=Upper Saddle River, NJ }}

==External links==
* [http://www.cs.unm.edu/~veroff/MEDIAN_ALGEBRA/ Median Algebra Proof]

[[Category:Algebra]]</text>
      <sha1>a57wfubljduks1kbu7fyen8xak9nnx4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ping-pong lemma</title>
    <ns>0</ns>
    <id>18786361</id>
    <revision>
      <id>607158668</id>
      <parentid>604418350</parentid>
      <timestamp>2014-05-05T11:56:43Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <text xml:space="preserve" bytes="16602">In [[mathematics]], the '''ping-pong lemma''', or '''table-tennis lemma''', is any of several mathematical statements that ensure that several elements in a group [[group action|acting]] on a set freely [[Generating set of a group|generates]] a [[free group|free]] [[subgroup]] of that group.

==History==

The ping-pong argument goes back to late 19th century and is commonly attributed&lt;ref name=&quot;DH&quot;/&gt; to [[Felix Klein]] who used it to study subgroups of [[Kleinian group]]s, that is, of discrete groups of isometries of the [[hyperbolic 3-space]] or, equivalently [[Möbius transformation]]s of the [[Riemann sphere]]. The ping-pong lemma was a key tool used by [[Jacques Tits]] in his 1972 paper&lt;ref name=&quot;T&quot;&gt;J. Tits. [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WH2-4D7K6RV-19X&amp;_user=10&amp;_coverDate=02%2F29%2F1972&amp;_rdoc=3&amp;_fmt=high&amp;_orig=browse&amp;_srch=doc-info(%23toc%236838%231972%23999799997%23518342%23FLP%23display%23Volume)&amp;_cdi=6838&amp;_sort=d&amp;_docanchor=&amp;_ct=12&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=bb8d98f320404accc3525ca235cf821e ''Free subgroups in linear groups.''] [[Journal of Algebra]], vol. 20 (1972), pp. 250–270&lt;/ref&gt; containing the proof of a famous result now known as the [[Tits alternative]]. The result states that a [[finitely generated group|finitely generated]] [[linear group]] is either [[virtually]] [[solvable group|solvable]] or contains a [[free group|free]] [[subgroup]] of rank two. The ping-pong lemma and its variations are widely used in [[geometric topology]] and [[geometric group theory]].

Modern versions of the ping-pong lemma can be found in many books such as Lyndon&amp;Schupp,&lt;ref name=&quot;LS&quot;&gt;[[Roger Lyndon|Roger C. Lyndon]] and Paul E. Schupp. ''Combinatorial Group Theory.'' Springer-Verlag, New York, 2001. &quot;Classics in Mathematics&quot; series, reprint of the 1977 edition. ISBN 978-3-540-41158-1; Ch II, Section 12, pp. 167&amp;ndash;169&lt;/ref&gt; de la Harpe,&lt;ref name=&quot;DH&quot;&gt;Pierre de la Harpe. [http://books.google.com/books?id=cRT01C5ADroC&amp;pg=PA25&amp;dq=ping+pong+lemma+group+theory&amp;sig=_1EZ9oSfAdljZFH1g7uvFiHuI-w#PPA25,M1 ''Topics in geometric group theory.''] Chicago Lectures in Mathematics. University of Chicago Press, Chicago. ISBN 0-226-31719-6; Ch. II.B &quot;The table-Tennis Lemma (Klein's criterion) and examples of free products&quot;; pp. 25&amp;ndash;41.&lt;/ref&gt; Bridson&amp;Haefliger&lt;ref name=&quot;BH&quot;&gt;Martin R. Bridson, and André Haefliger. [http://books.google.com/books?id=3DjaqB08AwAC&amp;printsec=frontcover&amp;dq=Martin+R.+Bridson,+and+Andr%C3%A9+Haefliger.+%22Metric+spaces+of+non-positive+curvature%22 ''Metric spaces of non-positive curvature.''] Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], 319. Springer-Verlag, Berlin, 1999. ISBN 3-540-64324-9; Ch.III.&amp;Gamma;, pp. 467&amp;ndash;468&lt;/ref&gt; and others.

==Formal statements==

===Ping-pong lemma for several subgroups===

This version of the ping-pong lemma ensures that several [[subgroup]]s of a group acting on a set generate a [[free product]]. The following statement appears in,&lt;ref&gt;Andrij Olijnyk and Vitaly Suchchansky. [http://www.worldscinet.com/cgi-bin/details.cgi?id=pii:S0218196704001931&amp;type=html Representations of free products by infinite unitriangular matrices over finite fields.] International Journal of Algebra and Computation. Vol. 14 (2004), no. 5&amp;ndash;6, pp. 741&amp;ndash;749; Lemma 2.1&lt;/ref&gt; and the proof is from.&lt;ref name=&quot;DH&quot;/&gt;

Let ''G'' be a group acting on a set ''X'' and let ''H''&lt;sub&gt;1&lt;/sub&gt;, ''H''&lt;sub&gt;2&lt;/sub&gt;,...., ''H''&lt;sub&gt;''k''&lt;/sub&gt; be nontrivial subgroups of ''G'' where ''k''≥2, such that at least one of these subgroups has [[order (group theory)|order]] greater than 2.
Suppose there exist disjoint nonempty subsets ''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;,....,''X''&lt;sub&gt;''k''&lt;/sub&gt; of ''X'' such that the following holds:

*For any ''i''≠''s'' and for any ''h''∈''H''&lt;sub&gt;''i''&lt;/sub&gt;, ''h''≠1  we have ''h''(''X''&lt;sub&gt;''s''&lt;/sub&gt;)⊆''X''&lt;sub&gt;''i''&lt;/sub&gt;.

Then
:&lt;math&gt;\langle H_1,\dots, H_k\rangle=H_1\ast\dots \ast H_k.&lt;/math&gt;

====Proof====
By the definition of free product, it suffices to check that a given reduced word is nontrivial. Let ''w'' be such a word, and let

:&lt;math&gt; w= \prod_{i=1}^m w_{\alpha_i,\beta_i}. &lt;/math&gt;

Where ''w''&lt;sub&gt;''s'',''β''&lt;sub&gt;''k''&lt;/sub&gt;&lt;/sub&gt;∈ ''H''&lt;sub&gt;''s''&lt;/sub&gt; for all such ''β''&lt;sub&gt;''k''&lt;/sub&gt;, and since ''w'' is fully reduced ''α''&lt;sub&gt;''i''&lt;/sub&gt;≠ α&lt;sub&gt;''i''+1 &lt;/sub&gt; for any ''i''. We then let ''w'' act on an element of one of the sets ''X''&lt;sub&gt;''i''&lt;/sub&gt;. As we assume for at least one subgroup ''H''&lt;sub&gt;i&lt;/sub&gt; has order at least 3, without loss we may assume that ''H''&lt;sub&gt;1&lt;/sub&gt; is at least 3. We first make the assumption that α&lt;sub&gt;1&lt;/sub&gt; and α&lt;sub&gt;m&lt;/sub&gt; are both 1. From here we consider ''w'' acting on ''X''&lt;sub&gt;2&lt;/sub&gt;. We get the following chain of containments and note that since the ''X''&lt;sub&gt;i&lt;/sub&gt; are disjoint that ''w'' acts nontrivially and is thus not the identity element.

:&lt;math&gt; w(X_2) \subseteq \prod_{i=1}^{m-1} w_{\alpha_i,\beta_i}(X_1) \subseteq \prod_{i=1}^{m-2} w_{\alpha_i,\beta_i}(X_{\alpha_{m-1}})\subseteq \dots \subseteq w_{1,\beta_1}w_{\alpha_2,\beta_2}(X_{\alpha_3}) \subseteq &lt;/math&gt; &lt;math&gt; \subseteq w_{1,\beta_1}(X_{\alpha_2})\subseteq X_1 &lt;/math&gt;

To finish the proof we must consider the three cases: 
*If &lt;math&gt; \alpha_1 = 1;\alpha_m\neq 1 &lt;/math&gt;, then let &lt;math&gt; h\in H_1\setminus \{w_{1,\beta_1}^{-1},1\} &lt;/math&gt;
*If &lt;math&gt;\alpha_1\neq 1;\alpha_m=1&lt;/math&gt;, then let &lt;math&gt; h\in H_1\setminus \{w_{1,\beta_m},1\} &lt;/math&gt;
*And if &lt;math&gt; \alpha_1\neq 1;\alpha_m\neq 1&lt;/math&gt;, then let &lt;math&gt; h\in H_1\setminus \{1\} &lt;/math&gt;
In each case, ''hwh''&lt;sup&gt;−1&lt;/sup&gt; is a reduced word with ''α''&lt;sub&gt;1&lt;/sub&gt;'  and ''α''&lt;sub&gt;''m'' '&lt;/sub&gt;' both 1, and thus is nontrivial. Finally, ''hwh''&lt;sup&gt;−1&lt;/sup&gt; is not 1, and so neither is ''w''. This proves the claim.

===The Ping-pong lemma for cyclic subgroups===

Let ''G'' be a group [[group action|acting]] on a set ''X''. Let ''a''&lt;sub&gt;1&lt;/sub&gt;,...,''a''&lt;sub&gt;''k''&lt;/sub&gt; be elements of ''G'', where ''k'' ≥ 2. Suppose there exist disjoint nonempty subsets

:''X''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;+&lt;/sup&gt;,...,''X''&lt;sub&gt;''k''&lt;/sub&gt;&lt;sup&gt;+&lt;/sup&gt; and ''X''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;&amp;ndash;&lt;/sup&gt;,...,''X''&lt;sub&gt;''k''&lt;/sub&gt;&lt;sup&gt;&amp;ndash;&lt;/sup&gt;

of ''X'' with the following properties:

*''a''&lt;sub&gt;''i''&lt;/sub&gt;(''X''&amp;nbsp;&amp;minus;&amp;nbsp;''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;&amp;ndash;&lt;/sup&gt;) ⊆ ''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;+&lt;/sup&gt; for ''i'' = 1, ..., ''k'';

*''a''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;&amp;minus;1&lt;/sup&gt;(''X''&amp;nbsp;&amp;minus;&amp;nbsp;''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;+&lt;/sup&gt;) ⊆ ''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;&amp;ndash;&lt;/sup&gt; for ''i'' = 1, ..., ''k''.

Then the subgroup ''H'' = &lt;''a''&lt;sub&gt;1&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''k''&lt;/sub&gt;&gt; ≤ ''G'' [[Generating set of a group|generated]] by ''a''&lt;sub&gt;1&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''k''&lt;/sub&gt; is [[free group|free]] with free basis {''a''&lt;sub&gt;1&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''k''&lt;/sub&gt;}.

====Proof====
This statement follows as a corollary of the version for general subgroups if we let ''X''&lt;sub&gt;''i''&lt;/sub&gt;= ''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;+&lt;/sup&gt;∪''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt; and let ''H''&lt;sub&gt;''i''&lt;/sub&gt; = ⟨''a''&lt;sub&gt;''i''&lt;/sub&gt;⟩.

==Examples==

===Special linear group example===
One can use the ping-pong lemma to prove&lt;ref name=&quot;DH&quot;/&gt; that the subgroup ''H'' = &lt;''A'',''B''&gt;≤SL(2,'''Z'''), generated by  the matrices

:&lt;math&gt;\scriptstyle A=\begin{pmatrix}1 &amp; 2\\ 0 &amp;1 \end{pmatrix} &lt;/math&gt; and &lt;math&gt;\scriptstyle B=\begin{pmatrix}1 &amp; 0\\ 2 &amp;1 \end{pmatrix} &lt;/math&gt;

is [[free group|free]] of rank two.

====Proof====
Indeed, let ''H''&lt;sub&gt;1&lt;/sub&gt; = &lt;''A''&gt; and ''H''&lt;sub&gt;2&lt;/sub&gt; = &lt;''B''&gt; be [[cyclic group|cyclic]] [[subgroup]]s of SL(2,'''Z''') generated by ''A'' and ''B'' accordingly.   It is not hard to check that A and B are elements of infinite order in SL(2,'''Z''') and that

:&lt;math&gt;H_1=\{A^n|n\in \mathbb Z\}=\left\{\begin{pmatrix}1 &amp; 2n\\ 0 &amp; 1 \end{pmatrix} : n\in\mathbb Z\right\}&lt;/math&gt;

and

:&lt;math&gt;H_2=\{B^n|n\in \mathbb Z\}=\left\{\begin{pmatrix}1 &amp; 0\\ 2n &amp; 1 \end{pmatrix} : n\in\mathbb Z\right\}.&lt;/math&gt;

Consider the standard action of SL(2,'''Z''') on '''R'''&lt;sup&gt;2&lt;/sup&gt; by [[linear transformation]]s. Put

:&lt;math&gt;X_1=\left\{\begin{pmatrix}x \\ y \end{pmatrix}\in \mathbb R^2 : |x|&gt;|y|\right\}&lt;/math&gt;

and

:&lt;math&gt;X_2=\left\{\begin{pmatrix}x \\ y \end{pmatrix}\in \mathbb R^2 : |x|&lt;|y|\right\}.&lt;/math&gt;

It is not hard to check, using the above explicitly descriptions of ''H''&lt;sub&gt;1&lt;/sub&gt; and ''H''&lt;sub&gt;2&lt;/sub&gt; that for every nontrivial ''g''&amp;nbsp;∈&amp;nbsp;''H''&lt;sub&gt;1&lt;/sub&gt; we have ''g''(''X''&lt;sub&gt;2&lt;/sub&gt;)&amp;nbsp;⊆&amp;nbsp;''X''&lt;sub&gt;1&lt;/sub&gt; and that for every nontrivial ''g''&amp;nbsp;∈&amp;nbsp;''H''&lt;sub&gt;2&lt;/sub&gt; we have ''g''(''X''&lt;sub&gt;1&lt;/sub&gt;)&amp;nbsp;⊆&amp;nbsp;''X''&lt;sub&gt;2&lt;/sub&gt;. Using the alternative form of the ping-pong lemma, for two subgroups, given above, we conclude that ''H''&amp;nbsp;=&amp;nbsp;''H''&lt;sub&gt;1&lt;/sub&gt;∗''H''&lt;sub&gt;2&lt;/sub&gt;. Since the groups ''H''&lt;sub&gt;1&lt;/sub&gt; and ''H''&lt;sub&gt;2&lt;/sub&gt; are [[infinite cyclic group|infinite cyclic]], it follows that ''H'' is a [[free group]] of rank two.

===Word-hyperbolic group example===

Let ''G'' be a [[word-hyperbolic group]] which is [[torsion-free group|torsion-free]], that is, with no nontrivial elements of finite [[Order (group theory)|order]]. Let ''g'',&amp;nbsp;''h''&amp;nbsp;∈&amp;nbsp;''G'' be two non-commuting elements, that is such that ''gh''&amp;nbsp;≠&amp;nbsp;''hg''. Then there exists ''M''≥1 such that for any integers ''n''&amp;nbsp;≥&amp;nbsp;''M'', ''m''&amp;nbsp;≥&amp;nbsp;''M'' the subgroup H&amp;nbsp;=&amp;nbsp;&lt;''g''&lt;sup&gt;''n''&lt;/sup&gt;, ''h''&lt;sup&gt;''m''&lt;/sup&gt;&gt;&amp;nbsp;≤&amp;nbsp;''G'' is [[free group|free]] of rank two.

====Sketch of the proof&lt;ref name=&quot;Gromov&quot;&gt;M. Gromov. ''Hyperbolic groups.'' Essays in group theory, pp. 75&amp;ndash;263,
Mathematical Sciiences Research Institute Publications, 8, Springer, New York, 1987;  ISBN 0-387-96618-8; Ch. 8.2, pp. 211&amp;ndash;219.&lt;/ref&gt;====
The group ''G'' [[Group action|acts]] on its ''hyperbolic boundary'' ∂''G'' by [[homeomorphism]]s. It is known that if ''a''&amp;nbsp;∈&amp;nbsp;''G'' is a nontrivial element then ''a'' has exactly two distinct fixed points, ''a''&lt;sup&gt;∞&lt;/sup&gt; and ''a''&lt;sup&gt;&amp;minus;∞&lt;/sup&gt; in ∂''G'' and that ''a''&lt;sup&gt;∞&lt;/sup&gt; is an [[attracting fixed point]] while ''a''&lt;sup&gt;&amp;minus;∞&lt;/sup&gt; is a [[Fixed point (mathematics)|repelling fixed point]].

Since ''g'' and ''h'' do not commute, the basic facts about [[word-hyperbolic group]]s imply that ''g''&lt;sup&gt;∞&lt;/sup&gt;, ''g''&lt;sup&gt;&amp;minus;∞&lt;/sup&gt;, ''h''&lt;sup&gt;∞&lt;/sup&gt; and ''h''&lt;sup&gt;&amp;minus;∞&lt;/sup&gt; are four distinct points in   ∂''G''. Take disjoint [[Neighbourhood (mathematics)|neighborhoods]] ''U''&lt;sub&gt;+&lt;/sub&gt;, ''U''&lt;sub&gt;&amp;ndash;&lt;/sub&gt;, ''V''&lt;sub&gt;+&lt;/sub&gt; and ''V''&lt;sub&gt;&amp;ndash;&lt;/sub&gt; of ''g''&lt;sup&gt;∞&lt;/sup&gt;, ''g''&lt;sup&gt;&amp;minus;∞&lt;/sup&gt;, ''h''&lt;sup&gt;∞&lt;/sup&gt; and ''h''&lt;sup&gt;&amp;minus;∞&lt;/sup&gt; in ∂''G'' respectively.
Then the attracting/repelling properties of the fixed points of ''g'' and ''h'' imply that there exists ''M''&amp;nbsp;≥&amp;nbsp;1 such that for any integers ''n''&amp;nbsp;≥&amp;nbsp;''M'', ''m''&amp;nbsp;≥&amp;nbsp;''M'' we have:
*''g''&lt;sup&gt;''n''&lt;/sup&gt;(∂''G'' &amp;ndash; ''U''&lt;sub&gt;&amp;ndash;&lt;/sub&gt;) ⊆ ''U''&lt;sub&gt;+&lt;/sub&gt;
*''g''&lt;sup&gt;&amp;minus;''n''&lt;/sup&gt;(∂''G'' &amp;ndash; ''U''&lt;sub&gt;+&lt;/sub&gt;) ⊆ ''U''&lt;sub&gt;&amp;ndash;&lt;/sub&gt;
*''h''&lt;sup&gt;''m''&lt;/sup&gt;(∂''G'' &amp;ndash; ''V''&lt;sub&gt;&amp;ndash;&lt;/sub&gt;) ⊆ ''V''&lt;sub&gt;+&lt;/sub&gt;
*''h''&lt;sup&gt;&amp;minus;''m''&lt;/sup&gt;(∂''G'' &amp;ndash; ''V''&lt;sub&gt;+&lt;/sub&gt;) ⊆ ''V''&lt;sub&gt;&amp;ndash;&lt;/sub&gt;

The ping-pong lemma now implies that ''H''&amp;nbsp;=&amp;nbsp;&lt;''g''&lt;sup&gt;''n''&lt;/sup&gt;, ''h''&lt;sup&gt;''m''&lt;/sup&gt;&gt;&amp;nbsp;≤&amp;nbsp;''G'' is [[free group|free]] of rank two.

==Applications of the ping-pong lemma==

*The ping-pong lemma is used in [[Kleinian group]]s to study their so-called [[Schottky group|Schottky subgroups]].  In the Kleinian groups context the ping-pong lemma can be used to show that a particular group of isometries of the [[hyperbolic 3-space]] is not just [[free group|free]] but also [[properly discontinuous]] and [[geometrically finite group|geometrically finite]].
*Similar Schottky-type arguments are widely used in [[geometric group theory]], particularly for subgroups of [[word-hyperbolic group]]s&lt;ref name=&quot;Gromov&quot;/&gt; and for automorphism groups of trees.&lt;ref&gt;[[Alexander Lubotzky]]. [http://www.springerlink.com/content/g374700j2401nl64/ ''Lattices in rank one Lie groups over local fields.'']  [[Geometric and Functional Analysis]], vol. 1 (1991),  no. 4, pp. 406&amp;ndash;431&lt;/ref&gt;
*Ping-pong lemma is also used for studying Schottky-type subgroups of [[mapping class group]]s of [[Riemann surface]]s, where the set on which the mapping class group acts is the Thurston boundary of the [[Teichmüller space]].&lt;ref&gt;Richard P. Kent, and Christopher J. Leininger. ''Subgroups of mapping class groups from the geometrical viewpoint.'' In the tradition of Ahlfors-Bers. IV, pp. 119&amp;ndash;141,
Contemporary Mathematics series, 432, [[American Mathematical Society]], Providence, RI, 2007; ISBN 978-0-8218-4227-0; 0-8218-4227-7&lt;/ref&gt; A similar argument is also utilized in the study of subgroups of the [[outer automorphism group]] of a [[free group]].&lt;ref&gt;[[Mladen Bestvina|M. Bestvina]], M. Feighn, and M. Handel. [http://www.springerlink.com/content/50hq64n0l6gpuukk/ ''Laminations, trees, and irreducible automorphisms of free groups.'']  [[Geometric and Functional Analysis]], vol. 7  (1997),  no. 2, pp. 215&amp;ndash;244.&lt;/ref&gt;
*One of the most famous applications of the ping-pong lemma is in the proof of [[Jacques Tits]] of the so-called [[Tits alternative]] for [[linear group]]s.&lt;ref name=&quot;T&quot;/&gt; (see also &lt;ref&gt;Pierre de la Harpe. ''Free groups in linear groups.'' L'Enseignement Mathématique (2), vol. 29 (1983), no. 1-2, pp. 129&amp;ndash;144&lt;/ref&gt; for an overview of Tits' proof and an explanation of the ideas involved, including the use of the ping-pong lemma).
*There are generalizations of the ping-pong lemma that produce not just [[free product]]s but also [[free product with amalgamation|amalgamated free products]] and [[HNN extension]]s.&lt;ref name=&quot;LS&quot;/&gt; These generalizations are used, in particular, in the proof of Maskit's Combination Theorem for [[Kleinian group]]s.&lt;ref&gt;Bernard Maskit.
''Kleinian groups.'' Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], 287. Springer-Verlag, Berlin, 1988. ISBN 3-540-17746-9;  Ch. VII.C and Ch. VII.E pp.149&amp;ndash;156 and pp. 160&amp;ndash;167&lt;/ref&gt;
*There are also versions of the ping-pong lemma which guarantee that several elements in a group generate a [[free semigroup]]. Such versions are available both in the general context of a [[group action]] on a set,&lt;ref name=&quot;DH1&quot;&gt;Pierre de la Harpe. [http://books.google.com/books?id=cRT01C5ADroC&amp;pg=PA188&amp;vq=semi-group&amp;dq=ping+pong+lemma+group+theory&amp;source=gbs_search_s&amp;sig=ACfU3U2oMEeKTE_pB7Gt_MqNjOaUNZL8yw ''Topics in geometric group theory.''] Chicago Lectures in Mathematics. University of Chicago Press, Chicago. ISBN 0-226-31719-6; Ch. II.B &quot;The table-Tennis Lemma (Klein's criterion) and examples of free products&quot;; pp. 187&amp;ndash;188.&lt;/ref&gt;  and for specific types of actions, e.g. in the context of [[linear group]]s,&lt;ref&gt;Alex Eskin, Shahar Mozes and Hee Oh. [http://www.springerlink.com/content/3ybuud1bpkkkcxn0/ On uniform exponential growth for linear groups.] [[Inventiones Mathematicae]]. vol. 60 (2005), no. 1, pp.1432&amp;ndash;1297; Lemma 2.2&lt;/ref&gt; groups [[Bass-Serre theory|acting on trees]]&lt;ref&gt;Roger C. Alperin and Guennadi A. Noskov. [http://books.google.com/books?id=w7LO6AkB8Y8C&amp;pg=PA2&amp;lpg=PA2&amp;dq=%22ping-pong+lemma%22+semigroup&amp;source=web&amp;ots=aBPNu6adQ2&amp;sig=7mZjESpp-6Bkekw68RCPEDYJSTM&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=4&amp;ct=result#PPA2,M1 Uniform growth, actions on trees and GL&lt;sub&gt;2&lt;/sub&gt;.] Computational and Statistical Group Theory:AMS Special Session Geometric Group Theory, April 21–22, 2001, Las Vegas, Nevada, AMS Special Session Computational Group Theory, April 28–29, 2001, Hoboken, New Jersey. (Robert H. Gilman, Vladimir Shpilrain, Alexei G. Myasnikov, editors). [[American Mathematical Society]], 2002. ISBN 978-0-8218-3158-8; page 2, Lemma 3.1&lt;/ref&gt; and others.&lt;ref&gt;Yves de Cornulier and Romain Tessera. [http://msp.warwick.ac.uk/gt/2008/12-01/p011.xhtml Quasi-isometrically embedded free sub-semigroups.] [[Geometry &amp; Topology]], vol. 12 (2008), pp. 461&amp;ndash;473; Lemma 2.1&lt;/ref&gt;

==References==
{{reflist}}

==See also==
*[[Free group]]
*[[Free product]]
*[[Kleinian group]]
*[[Tits alternative]]
*[[Word-hyperbolic group]]
*[[Schottky group]]

[[Category:Algebra]]
[[Category:Group theory]]
[[Category:Discrete groups]]
[[Category:Lie groups]]
[[Category:Combinatorics on words]]</text>
      <sha1>94ftyf7uluk1d4emo7ybqeqb3njyq6f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Continuant (mathematics)</title>
    <ns>0</ns>
    <id>18881681</id>
    <revision>
      <id>555349526</id>
      <parentid>545398369</parentid>
      <timestamp>2013-05-16T11:04:53Z</timestamp>
      <contributor>
        <ip>82.197.31.180</ip>
      </contributor>
      <comment>/* Applications */</comment>
      <text xml:space="preserve" bytes="2830">In [[algebra]], the '''continuant''' is a [[multivariate polynomial]] representing the [[determinant]] of a [[tridiagonal matrix]] and having applications in [[generalized continued fraction]]s.

==Definition==
The ''n''-th ''continuant'', ''K''(''n''), of a sequence '''a''' = ''a''&lt;sub&gt;1&lt;/sub&gt;,...,''a''&lt;sub&gt;''n''&lt;/sub&gt;,... is defined recursively by

:&lt;math&gt; K(0) = 1 ; \, &lt;/math&gt; 
:&lt;math&gt; K(1) = a_1 ; \, &lt;/math&gt;
:&lt;math&gt; K(n) = a_n K(n-1) + K(n-2) . \, &lt;/math&gt;

It may also be obtained by taking the sum of all possible products of ''a''&lt;sub&gt;1&lt;/sub&gt;,...,''a''&lt;sub&gt;''n''&lt;/sub&gt; in which any pairs of consecutive terms are deleted.

An extended definition takes the continuant with respect to three sequences '''a''', '''b''' and '''c''', so that ''K''(''n'') is a polynomial of ''a''&lt;sub&gt;1&lt;/sub&gt;,...,''a''&lt;sub&gt;''n''&lt;/sub&gt;, ''b''&lt;sub&gt;1&lt;/sub&gt;,...,''b''&lt;sub&gt;''n''&amp;minus;1&lt;/sub&gt; and ''c''&lt;sub&gt;1&lt;/sub&gt;,...,''c''&lt;sub&gt;''n''&amp;minus;1&lt;/sub&gt;.  In this case the [[recurrence relation]] becomes

:&lt;math&gt; K(0) = 1 ; \, &lt;/math&gt; 
:&lt;math&gt; K(1) = a_1 ; \, &lt;/math&gt;
:&lt;math&gt; K(n) = a_n K(n-1) - b_{n-1}c_{n-1} K(n-2) . \, &lt;/math&gt;

Since ''b''&lt;sub&gt;''r''&lt;/sub&gt; and ''c''&lt;sub&gt;''r''&lt;/sub&gt; enter into ''K'' only as a product ''b''&lt;sub&gt;''r''&lt;/sub&gt;''c''&lt;sub&gt;''r''&lt;/sub&gt; there is no loss of generality in assuming that the ''b''&lt;sub&gt;''r''&lt;/sub&gt; are all equal to 1.

==Applications==
The simple continuant gives the value of a [[continued fraction]] of the form &lt;math&gt;[a_0;a_1,a_2,\ldots]&lt;/math&gt;.  The ''n''-th convergent is

:&lt;math&gt; \frac{K(n+1,(a_0,\ldots,a_n))}{K(n,(a_1,\ldots,a_n))} . &lt;/math&gt;

The extended continuant is precisely the determinant of the tridiagonal matrix

:&lt;math&gt; \begin{pmatrix}
a_1 &amp; b_1 &amp;  0  &amp; \ldots &amp; 0 &amp; 0 \\
c_1 &amp; a_2 &amp; b_2 &amp; \ldots &amp; 0 &amp; 0 \\
 0  &amp; c_2 &amp; a_3 &amp; \ldots &amp; 0 &amp; 0 \\
 \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
 0 &amp; 0 &amp; 0 &amp; \ldots &amp; a_{n-1} &amp; b_{n-1} \\
 0 &amp; 0 &amp; 0 &amp; \ldots &amp; c_{n-1} &amp; a_n
\end{pmatrix} . &lt;/math&gt;

==References==
* {{cite book | author=Thomas Muir | authorlink=Thomas Muir (mathematician) | title=A treatise on the theory of determinants | date=1960 | publisher=[[Dover Publications]] | pages=516&amp;ndash;525 }}
* {{cite book | title=The Markoff and Lagrange Spectra | first1=Thomas W. | last1=Cusick | first2=Mary E. | last2=Flahive | publisher=[[American Mathematical Society]] | year=1989 | isbn=0-8218-1531-8 | pages=89 | zbl=0685.10023 | series=Mathematical Surveys and Monographs | volume=30 | location=Providence, RI }}
* {{cite book | title=Algebra, an Elementary Text-book for the Higher Classes of Secondary Schools and for Colleges: Pt. 1 | author=George Chrystal | authorlink=George Chrystal | publisher=American Mathematical Society | year=1999 | isbn=0-8218-1649-7 | pages=500 }}

[[Category:Algebra]]
[[Category:Matrices]]
[[Category:Polynomials]]


{{Linear-algebra-stub}}</text>
      <sha1>efxgxplq7gw2i0i4cw9dhwjoxcq7iz7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cyclotomic polynomial</title>
    <ns>0</ns>
    <id>171992</id>
    <revision>
      <id>611675751</id>
      <parentid>608429672</parentid>
      <timestamp>2014-06-05T13:06:16Z</timestamp>
      <contributor>
        <ip>2602:306:BD3F:F00:6D10:FBD:3220:6FFD</ip>
      </contributor>
      <comment>/* Prime Cyclotomic numbers */</comment>
      <text xml:space="preserve" bytes="18030">In [[algebra]], the''' ''n''th cyclotomic polynomial''', for any positive [[integer]] ''n'', is the unique [[irreducible polynomial]] with integer coefficients, which is a [[divisor]] of &lt;math&gt;x^n-1&lt;/math&gt; and is not a divisor of &lt;math&gt;x^k-1&lt;/math&gt; for any {{nowrap|''k'' &lt; ''n''.}} Its [[root of a function|roots]] are the ''n''th [[Root of unity#Definition|primitive roots of unity]] 
&lt;math&gt;
e^{2i\pi\frac{k}{n}}
&lt;/math&gt;, where ''k'' runs over the integers lower than ''n'' and [[coprime integers|coprime]] to ''n''. In other words, the''' ''n''th cyclotomic polynomial''' is equal to
:&lt;math&gt;
\Phi_n(x) =
\prod_\stackrel{1\le k\le n}{\gcd(k,n)=1}
\left(x-e^{2i\pi\frac{k}{n}}\right)
&lt;/math&gt;

It may also be defined as the [[monic polynomial]] with integer coefficients, which is the [[minimal polynomial (field theory)|minimal polynomial]] over the [[Field (mathematics)|field]] of the [[rational number]]s of any [[Root of unity#Definition|primitive ''n''th-root of unity]] (&lt;math&gt; e^{2i\pi/n} &lt;/math&gt; is such a primitive root).

==Examples==

If ''n'' is a [[prime number]] then 
:&lt;math&gt;~\Phi_n(x) = 1+x+x^2+\cdots+x^{n-1}=\sum_{i=0}^{n-1} x^i.&lt;/math&gt;
If ''n''=2''p'' where ''p'' is an odd [[prime number]] then
:&lt;math&gt;~\Phi_{2p}(x) = 1-x+x^2-\cdots+x^{p-1}=\sum_{i=0}^{p-1} (-x)^i.&lt;/math&gt;
For  ''n'' up to 10 we have:

:&lt;math&gt;~\Phi_0(x) = 1&lt;/math&gt;

:&lt;math&gt;~\Phi_1(x) = x-1&lt;/math&gt;

:&lt;math&gt;~\Phi_2(x) = x+1&lt;/math&gt;

:&lt;math&gt;~\Phi_3(x) = x^2 + x + 1&lt;/math&gt;

:&lt;math&gt;~\Phi_4(x) = x^2 + 1&lt;/math&gt;

:&lt;math&gt;~\Phi_5(x) = x^4 + x^3 + x^2 + x +1&lt;/math&gt;

:&lt;math&gt;~\Phi_6(x) = x^2 - x + 1&lt;/math&gt;

:&lt;math&gt;~\Phi_7(x) = x^6 + x^5 + x^4 + x^3 + x^2 + x + 1&lt;/math&gt;

:&lt;math&gt;~\Phi_8(x) = x^4 + 1&lt;/math&gt;

:&lt;math&gt;~\Phi_9(x) = x^6 + x^3 + 1&lt;/math&gt;

:&lt;math&gt;~\Phi_{10}(x) = x^4 - x^3 + x^2 - x + 1&lt;/math&gt;

For ''n'' up to 30, the cyclotomic polynomials not covered by above formulas are:

:&lt;math&gt;~\Phi_{12}(x) = x^4 - x^2 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{15}(x) = x^8 - x^7 + x^5 - x^4 + x^3 - x + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{16}(x) = x^8 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{18}(x) = x^6 - x^3 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{20}(x) = x^8 - x^6 + x^4 - x^2 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{21}(x) = x^{12} - x^{11} + x^9 - x^8 + x^6 - x^4 + x^3 - x + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{24}(x) = x^8 - x^4 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{25}(x) = x^{20} + x^{15} + x^{10} + x^5 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{27}(x) = x^{18} + x^9 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{28}(x) = x^{16} - x^{14} + x^{12} - x^{10} + x^8 - x^6 + x^4 - x^2 + 1&lt;/math&gt;
:&lt;math&gt;~\Phi_{30}(x) = x^8 + x^7 - x^5 - x^4 - x^3 + x + 1&lt;/math&gt;
The case of 105 is interesting because it is the first integer that is the product of three distinct odd prime numbers and the 105th cyclotomic polynomial is the first one that has a coefficient of magnitude greater than 1:
:&lt;math&gt;\begin{align}
\Phi_{105}(x) = &amp; \; x^{48} + x^{47} + x^{46} - x^{43} - x^{42} - 2 x^{41} - x^{40} - x^{39} + x^{36} + x^{35} + x^{34} \\
&amp; {} + x^{33} + x^{32} + x^{31} - x^{28} - x^{26} - x^{24} - x^{22} - x^{20} + x^{17} + x^{16} + x^{15} \\
&amp; {} + x^{14} + x^{13} + x^{12} - x^9 - x^8 - 2 x^7 - x^6 - x^5 + x^2 + x + 1
\end{align}&lt;/math&gt;

==Properties==

===Fundamental tools===

The cyclotomic polynomials are monic polynomials with integer coefficients that are [[irreducible polynomial|irreducible]] over the field of the rational numbers. Except for ''n'' equal to 1 or 2, they are [[palindromic polynomial]]s of even degree.

The degree of &lt;math&gt;\Phi_n&lt;/math&gt;, or in other words the number of ''n''th primitive roots of unity, is &lt;math&gt;\varphi (n)&lt;/math&gt;, where &lt;math&gt;\varphi&lt;/math&gt; is [[Euler's totient function]].

The fact that &lt;math&gt;\Phi_n&lt;/math&gt; is an irreducible polynomial of degree &lt;math&gt;\varphi (n)&lt;/math&gt; in the ring &lt;math&gt;\mathbb{Z}[x]&lt;/math&gt; is a nontrivial result due to [[Carl Friedrich Gauss|Gauss]].&lt;ref&gt;{{Lang Algebra}}&lt;/ref&gt; Depending on the chosen definition, it is either the value of the degree or the irreducibility which is a nontrivial result. The case of [[Prime number|prime]] ''n'' is easier to prove than the general case, thanks to [[Eisenstein's criterion#Examples|Eisenstein's criterion]].

A fundamental relation involving cyclotomic polynomials is 
:&lt;math&gt;\prod_{d\mid n}\Phi_d(x) = x^n - 1&lt;/math&gt;
which means that each ''n''-th root of unity is a primitive ''d''-th root of unity for a unique ''d'' dividing ''n''.

The [[Möbius inversion formula#Multiplicative notation|Möbius inversion formula]] allows the expression of &lt;math&gt;\Phi_n(x)&lt;/math&gt; as an explicit rational fraction:
:&lt;math&gt;\Phi_n(x)=\prod_{d\mid n}(x^d-1)^{\mu(n/d)} &lt;/math&gt;

The cyclotomic polynomial &lt;math&gt;\Phi_{n}(x)&lt;/math&gt; may be computed by (exactly) dividing &lt;math&gt;x^n-1&lt;/math&gt; by the cyclotomic polynomials of the proper divisors of ''n'' previously computed recursively by the same method:

:&lt;math&gt;\Phi_n(x)=\frac{x^{n}-1}{\prod_{\stackrel{d|n}{{}_{d&lt;n}}}\Phi_{d}(x)}&lt;/math&gt;

(Recall that &lt;math&gt;\Phi_{1}(x)=x-1&lt;/math&gt;).

This formula allows to compute &lt;math&gt;\Phi_n(x)&lt;/math&gt; on a computer for any ''n'', as soon as [[integer factorization]] and [[Euclidean division of polynomials|division of polynomials]] are available. Many [[computer algebra systems]] have a built in function to compute the cyclotomic polynomials. For example in [[Maple (software)|Maple]], &lt;math&gt;\Phi_n(x)&lt;/math&gt; may be computed by typing &quot;numtheory[cyclotomic](n,x);&quot;.

===Easy cases for the computation===
As noted above, if ''n'' is a [[prime number]] then
:&lt;math&gt;\Phi_n(x) = 1+x+x^2+\cdots+x^{n-1}=\sum_{i=0}^{n-1}x^i.&lt;/math&gt;

If ''n'' is an odd integer greater than one, then 
:&lt;math&gt;\Phi_{2n}(x) = \Phi_n(-x)&lt;/math&gt;.

If ''n'' is an even integer, then 
:&lt;math&gt;\Phi_{2n}(x) = \Phi_n(x^2)&lt;/math&gt;.

In particular, if ''n''=2''p'' is twice an odd prime then (as noted above)
:&lt;math&gt;\Phi_n(x) = 1-x+x^2-\cdots+x^{p-1}=\sum_{i=0}^{p-1}(-x)^i.&lt;/math&gt;

If ''n''=''p&lt;sup&gt;m&lt;/sup&gt;'' is a [[prime power]] (where ''p'' is prime), then
:&lt;math&gt;\Phi_n(x) = \Phi_p(x^{p^{m-1}}) =\sum_{i=0}^{p-1}x^{ip^{m-1}}.&lt;/math&gt;

More generally, if ''n''=''q&lt;sup&gt;m&lt;/sup&gt;r'' with ''m''&gt;1 then 
:&lt;math&gt;\Phi_n(x) = \Phi_{qr}(x^{q^{m-1}}).&lt;/math&gt;{{citation needed|date=March 2014}}

This formula may be iterated to get a simple expression of any cyclotomic polynomial &lt;math&gt;\Phi_n(x)&lt;/math&gt; in term of a cyclotomic polynomial of [[square-free number|square free]] index: If ''q'' is the product of the prime divisors of ''n'' (its [[Radical of an integer|radical]]), then
:&lt;math&gt;\Phi_n(x) = \Phi_q(x^{n/q}).&lt;/math&gt;

This allows to give formulas for the ''n''th cyclotomic polynomial when ''n'' has at most one odd prime factor: If ''p'' is an odd prime number, and ''h'' and ''k'' are positive integers, then: 
:&lt;math&gt;\Phi_{2^h}(x) = x^{2^{h-1}}+1&lt;/math&gt;
:&lt;math&gt;\Phi_{p^k}(x) = \sum_{i=0}^{p-1}x^{ip^{k-1}}&lt;/math&gt;
:&lt;math&gt;\Phi_{2^hp^k}(x) = \sum_{i=0}^{p-1}(-1)^ix^{i2^{h-1}p^{k-1}}&lt;/math&gt;

For the other values of ''n'', the computation of the ''n''th cyclotomic polynomial is similarly reduced to that of  &lt;math&gt;\Phi_q(x),&lt;/math&gt; where ''q'' is the product of the distinct odd prime divisors of ''n''. To deal with this case, one has that, for ''p'' relatively prime to ''n'',&lt;ref name=&quot;WolframCyclotomic&quot;&gt;{{cite web | url=http://mathworld.wolfram.com/CyclotomicPolynomial.html | title=Cyclotomic Polynomial | accessdate=12 March 2014 | author=Weisstein, Eric W.}}&lt;/ref&gt;
:&lt;math&gt;\Phi_{np}(x)=\Phi_{n}(x^p)/\Phi_n(x)\,.&lt;/math&gt;

===Integers appearing as coefficients===

The problem of bounding the magnitude of the coefficients of the cyclotomic polynomials has been the object of a number of research papers.
 
If ''n'' has at most two distinct odd prime factors, then Migotti showed that the coefficients of &lt;math&gt;\Phi_n&lt;/math&gt; are all in the set {1, &amp;minus;1, 0}.&lt;ref&gt;{{cite book |title=Algebra: A Graduate Course |first=Martin |last=Isaacs |page=310 |isbn=978-0-8218-4799-2 |publisher=AMS Bookstore |year=2009}}&lt;/ref&gt;

The first cyclotomic polynomial for a product of 3 different odd prime factors is &lt;math&gt;\Phi_{105}(x);&lt;/math&gt; it has a coefficient &amp;minus;2 (see its expression [[#Examples|above]]). The converse isn't true: &lt;math&gt;\Phi_{231}(x)&lt;/math&gt; = &lt;math&gt;\Phi_{3\times 7\times 11}(x)&lt;/math&gt; only has coefficients in {1, &amp;minus;1, 0}.

If ''n'' is a product of more odd different prime factors, the coefficients may increase to very high values. E.g., &lt;math&gt;\Phi_{15015}(x)&lt;/math&gt; = &lt;math&gt;\Phi_{3\times 5\times 7\times 11\times 13}(x)&lt;/math&gt; has coefficients running from &amp;minus;22 to 22, &lt;math&gt;\Phi_{255255}(x)&lt;/math&gt; = &lt;math&gt;\Phi_{3\times 5\times 7\times 11\times 13\times 17}(x)&lt;/math&gt;, the smallest ''n'' with 6 different odd primes, has coefficients up to ±532.

Let ''A''(''n'') denote the maximum absolute value of the coefficients of Φ&lt;sub&gt;''n''&lt;/sub&gt;.  It is known that for any positive ''k'', the number of ''n'' up to ''x'' with ''A''(''n'') &gt; ''n''&lt;sup&gt;''k''&lt;/sup&gt; is at least ''c''(''k'')⋅''x'' for a positive ''c''(''k'') depending on ''k'' and ''x'' sufficiently large.  In the opposite direction, for any function ψ(''n'') tending to infinity with ''n'' we have ''A''(''n'') bounded above by ''n''&lt;sup&gt;ψ(''n'')&lt;/sup&gt; for almost all ''n''.&lt;ref name=Mei2008&gt;Meier (2008)&lt;/ref&gt;

===Gauss's formula===

Let ''n'' be odd, square-free, and greater than 3. Then&lt;ref&gt;Gauss, DA, Articles 356-357&lt;/ref&gt;&lt;ref&gt;Riesel, pp. 315-316, p. 436&lt;/ref&gt;

:&lt;math&gt;
4\Phi_n(z) = A_n^2(z) - (-1)^{\frac{n-1}{2}}nz^2B_n^2(z)
&lt;/math&gt;

where  both ''A''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') and ''B''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') have integer coefficients, ''A''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') has degree ''&amp;phi;''(''n'')/2, and ''B''&lt;sub&gt;''n''&lt;/sub&gt;(z) has degree ''&amp;phi;''(''n'')/2 − 2. Furthermore, ''A''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') is palindromic when its degree is even; if its degree is odd it is antipalindromic. Similarly, ''B''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') is palindromic unless ''n'' is composite and ≡ 3 (mod 4), in which case it is antipalindromic.

The first few cases are

:&lt;math&gt;
\begin{align}
4\Phi_5(z)
&amp;=4(z^4+z^3+z^2+z+1)\\ 
&amp;= (2z^2+z+2)^2 - 5z^2
\end{align}
&lt;/math&gt;

:&lt;math&gt;
\begin{align}
4\Phi_7(z)
&amp;=4(z^6+z^5+z^4+z^3+z^2+z+1)\\ 
&amp;= (2z^3+z^2-z-2)^2+7z^2(z+1)^2
\end{align} 
&lt;/math&gt;

:&lt;math&gt;
\begin{align}
4\Phi_{11}(z)
&amp;=4(z^{10}+z^9+z^8+z^7+z^6+z^5+z^4+z^3+z^2+z+1)\\ 
&amp;= (2z^5+z^4-2z^3+2z^2-z-2)^2+11z^2(z^3+1)^2
\end{align} 
&lt;/math&gt;

===Lucas's formula===

Let ''n'' be odd, square-free and greater than 3. Then&lt;ref&gt;
Riesel, pp. 309-315, p. 443
&lt;/ref&gt;

:&lt;math&gt;
\Phi_n(z) = U_n^2(z) - (-1)^{\frac{n-1}{2}}nzV_n^2(z)
&lt;/math&gt;

where  both ''U''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') and ''V''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') have integer coefficients, ''U''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') has degree ''&amp;phi;''(''n'')/2, and ''V''&lt;sub&gt;''n''&lt;/sub&gt;(z) has degree ''&amp;phi;''(''n'')/2 − 1. This can also be written

:&lt;math&gt;
\Phi_n((-1)^{\frac{n-1}{2}}z) = C_n^2(z) - nzD_n^2(z).
&lt;/math&gt;

If ''n'' is even, square-free and greater than 2 (this forces ''n'' to be ≡ 2 (mod 4)),

:&lt;math&gt;
\Phi_{n/2}(-z^2) = C_n^2(z) - nzD_n^2(z)
&lt;/math&gt;

where  both ''C''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') and ''D''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') have integer coefficients, ''C''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') has degree ''&amp;phi;''(''n''), and ''D''&lt;sub&gt;''n''&lt;/sub&gt;(z) has degree ''&amp;phi;''(''n'') − 1. ''C''&lt;sub&gt;''n''&lt;/sub&gt;(''z'') and ''D''&lt;sub&gt;''n''&lt;/sub&gt;(z) are both palindromic.

The first few cases are:

:&lt;math&gt;
\begin{align}
\Phi_3(-z) 
&amp;=z^2-z+1 \\
&amp;= (z+1)^2 - 3z
\end{align}
&lt;/math&gt;

:&lt;math&gt;
\begin{align}
\Phi_5(z) 
&amp;=z^4+z^3+z^2+z+1 \\
&amp;= (z^2+3z+1)^2 - 5z(z+1)^2
\end{align}
&lt;/math&gt;

:&lt;math&gt;
\begin{align}
\Phi_3(-z^2) 
&amp;=z^4-z^2+1 \\
&amp;= (z^2+3z+1)^2 - 6z(z+1)^2
\end{align}
&lt;/math&gt;

==Prime Cyclotomic numbers==
{{unreferenced section|date=April 2014}}
If &lt;math&gt;\Phi_n(b)&lt;/math&gt; is a prime, then the prime is a [[base]]-b [[unique prime]]. For example, if n=39, b=10, than &lt;math&gt;\Phi_n(b)&lt;/math&gt; = 900900900900990990990991 is a prime, so it is a [[Base 10|base-10]] unique prime, and if n=30, b=2, than &lt;math&gt;\Phi_n(b)&lt;/math&gt; = 331 is also a prime, so it is a [[Base 2|base-2]] unique prime.

The list is about the smallest natural number b&gt;1 that &lt;math&gt;\Phi_n(b)&lt;/math&gt; is a prime.
{{OEIS|id=A085398}}

{| class=&quot;wikitable&quot;
|n
|1
|2
|3
|4
|5
|6
|7
|8
|9
|10
|11
|12
|13
|14
|15
|16
|17
|18
|19
|20
|-
|min b
|3
|2
|2
|2
|2
|2
|2
|2
|2
|2
|5
|2
|2
|2
|2
|2
|2
|6
|2
|4
|-
|n
|21
|22
|23
|24
|25
|26
|27
|28
|29
|30
|31
|32
|33
|34
|35
|36
|37
|38
|39
|40
|-
|min b
|3
|2
|10
|2
|22
|2
|2
|4
|6
|2
|2
|2
|2
|2
|14
|3
|61
|2
|10
|2
|-
|n
|41
|42
|43
|44
|45
|46
|47
|48
|49
|50
|51
|52
|53
|54
|55
|56
|57
|58
|59
|60
|-
|min b
|14
|2
|15
|25
|11
|2
|5
|5
|2
|6
|30
|11
|24
|7
|7
|2
|5
|7
|19
|3
|-
|n
|61
|62
|63
|64
|65
|66
|67
|68
|69
|70
|71
|72
|73
|74
|75
|76
|77
|78
|79
|80
|-
|min b
|2
|2
|3
|30
|2
|9
|46
|85
|2
|3
|3
|3
|11
|16
|59
|7
|2
|2
|22
|2
|-
|n
|81
|82
|83
|84
|85
|86
|87
|88
|89
|90
|91
|92
|93
|94
|95
|96
|97
|98
|99
|100
|-
|min b
|21
|61
|41
|7
|2
|2
|8
|5
|2
|2
|11
|4
|2
|6
|44
|4
|12
|2
|63
|20
|-
|n
|101
|102
|103
|104
|105
|106
|107
|108
|109
|110
|111
|112
|113
|114
|115
|116
|117
|118
|119
|120
|-
|min b
|22
|13
|3
|4
|7
|10
|2
|3
|12
|5
|12
|40
|86
|14
|268
|5
|24
|6
|148
|2
|-
|n
|121
|122
|123
|124
|125
|126
|127
|128
|129
|130
|131
|132
|133
|134
|135
|136
|137
|138
|139
|140
|-
|min b
|43
|2
|12
|6
|127
|2
|2
|102
|2
|3
|7
|3
|2
|5
|33
|56
|13
|8
|11
|4
|-
|n
|141
|142
|143
|144
|145
|146
|147
|148
|149
|150
|151
|152
|153
|154
|155
|156
|157
|158
|159
|160
|-
|min b
|5
|46
|3
|6
|2
|18
|13
|4
|5
|2
|29
|9
|14
|3
|62
|4
|56
|2
|189
|20
|}

In fact, if p is a prime, than &lt;math&gt;\Phi_p(b)&lt;/math&gt; is (b^p-1)/(b-1) and a base-b [[repunit]] number, (111111...111111)&lt;sub&gt;b&lt;/sub&gt;, so it is a list of the smallest b&gt;1 which &lt;math&gt;\Phi_p(b)&lt;/math&gt; is a prime.

{{OEIS|id=A066180}}

The list is about the first 160 primes.

{|class=&quot;wikitable&quot;
|p
|2
|3
|5
|7
|11
|13
|17
|19
|23
|29
|31
|37
|41
|43
|47
|53
|59
|61
|67
|71
|-
|min b
|2
|2
|2
|2
|5
|2
|2
|2
|10
|6
|2
|61
|14
|15
|5
|24
|19
|2
|46
|3
|-
|p
|73
|79
|83
|89
|97
|101
|103
|107
|109
|113
|127
|131
|137
|139
|149
|151
|157
|163
|167
|173
|-
|min b
|11
|22
|41
|2
|12
|22
|3
|2
|12
|86
|2
|7
|13
|11
|5
|29
|56
|30
|44
|60
|-
|p
|179
|181
|191
|193
|197
|199
|211
|223
|227
|229
|233
|239
|241
|251
|257
|263
|269
|271
|277
|281
|-
|min b
|304
|5
|74
|118
|33
|156
|46
|183
|72
|606
|602
|223
|115
|37
|52
|104
|41
|6
|338
|217
|-
|p
|283
|293
|307
|311
|313
|317
|331
|337
|347
|349
|353
|359
|367
|373
|379
|383
|389
|397
|401
|409
|-
|min b
|13
|136
|220
|162
|35
|10
|218
|19
|26
|39
|12
|22
|67
|120
|195
|48
|54
|463
|38
|41
|-
|p
|419
|421
|431
|433
|439
|443
|449
|457
|461
|463
|467
|479
|487
|491
|499
|503
|509
|521
|523
|541
|-
|min b
|17
|808
|404
|46
|76
|793
|38
|28
|215
|37
|236
|59
|15
|514
|260
|498
|6
|2
|95
|3
|-
|p
|547
|557
|563
|569
|571
|577
|587
|593
|599
|601
|607
|613
|617
|619
|631
|641
|643
|647
|653
|659
|-
|min b
|473
|417
|123
|30
|89
|88
|236
|76
|124
|2061
|2
|192
|187
|5
|39
|1267
|190
|321
|24
|79
|-
|p
|661
|673
|677
|683
|691
|701
|709
|719
|727
|733
|739
|743
|751
|757
|761
|769
|773
|787
|797
|809
|-
|min b
|24
|102
|101
|500
|110
|12
|114
|283
|1004
|566
|75
|398
|40
|62
|70
|61
|1276
|368
|477
|818
|-
|p
|811
|821
|823
|827
|829
|839
|853
|857
|859
|863
|877
|881
|883
|887
|907
|911
|919
|929
|937
|941
|-
|min b
|342
|217
|168
|119
|202
|55
|430
|22
|438
|1539
|865
|275
|13
|340
|11
|178
|908
|5
|828
|240
|}

==Applications==

Using &lt;math&gt;\Phi_n&lt;/math&gt;, one can give an elementary proof for the infinitude of [[prime]]s [[Congruence relation|congruent]] to 1 modulo ''n'',&lt;ref&gt;S. Shirali. ''Number Theory''. Orient Blackswan, 2004. p. 67. ISBN 81-7371-454-1&lt;/ref&gt; which is a special case of [[Dirichlet's theorem on arithmetic progressions]].

==See also==
* [[Cyclotomic field]]
* [[Aurifeuillean factorization]]

==Notes==

{{Reflist}}

==References==

The ''[[Disquisitiones Arithmeticae]]'' has been translated from Latin into English and German. The German edition includes all of his papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes.
*{{citation
  | last1 = Gauss  | first1 = Carl Friedrich
  | last2 = Clarke | first2 = Arthur A. (translator into English)  
  | title = Disquisitiones Arithemeticae (Second, corrected edition)
  | publisher = [[Springer Science+Business Media|Springer]]
  | location = New York
  | year = 1986
  | isbn = 0387962549}}

*{{citation
  | last1 = Gauss  | first1 = Carl Friedrich
  | last2 = Maser | first2 = H. (translator into German)  
  | title = Untersuchungen uber hohere Arithmetik (Disquisitiones Arithemeticae &amp; other papers on number theory) (Second edition)
  | publisher = Chelsea
  | location = New York
  | year = 1965
  | isbn = 0-8284-0191-8}}

*{{citation
  | last1 = Lemmermeyer  | first1 = Franz
  | title = Reciprocity Laws: from Euler to Eisenstein
  | publisher = [[Springer Science+Business Media|Springer]]
  | location = Berlin
  | year = 2000
  | isbn = 978-3-642-08628-1
  | doi= 10.1007/978-3-662-12893-0}}
* {{citation | last=Maier | first=Helmut | chapter=Anatomy of integers and cyclotomic polynomials | editor1-last=De Koninck | editor1-first=Jean-Marie | editor2-last=Granville | editor2-first=Andrew | editor2-link=Andrew Granville | editor3-last=Luca | editor3-first=Florian | title=Anatomy of integers. Based on the CRM workshop, Montreal, Canada, March 13--17, 2006 | location=Providence, RI | publisher=[[American Mathematical Society]] | series=CRM Proceedings and Lecture Notes | volume=46 | pages=89–95 | year=2008 | isbn=978-0-8218-4406-9 | zbl=1186.11010 }}
*{{citation
  | last1 = Riesel  | first1 = Hans
  | title = Prime Numbers and Computer Methods for Factorization (second edition)
  | publisher = Birkhäuser
  | location = Boston
  | year = 1994
  | isbn = 0-8176-3743-5}}

==External links==
* {{springer|title=Cyclotomic polynomials|id=p/c027580}}
*{{SloanesRef |sequencenumber=A013594|name=Smallest order of cyclotomic polynomial containing n or −n as a coefficient}}

[[Category:Number theory]]
[[Category:Algebra]]</text>
      <sha1>h6z3bmydvne9afgsa6ovjpe4vlavbgo</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Content (algebra)</title>
    <ns>0</ns>
    <id>11934455</id>
    <revision>
      <id>607418644</id>
      <parentid>558097177</parentid>
      <timestamp>2014-05-07T01:37:46Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* References */Task 3: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="948">In [[algebra]], the '''content''' of a [[polynomial]] is the [[highest common factor]] of its coefficients.

A polynomial is ''[[Primitive polynomial (ring theory)|primitive]]'' if it has content unity.

[[Gauss's lemma (polynomial)|Gauss's lemma for polynomials]] may be expressed as stating that for polynomials over a [[unique factorization domain]], the content of the product of two polynomials is the product of their contents.

==See also==
*[[Rational root theorem]]

==References==
* {{cite book | author=B. Hartley | authorlink=Brian Hartley |author2=T.O. Hawkes | title=Rings, modules and linear algebra | publisher=Chapman and Hall | year=1970 | isbn=0-412-09810-5 }}
* Page 181 of {{Lang Algebra|edition=3}}
* {{cite book | author=David Sharpe | title=Rings and factorization | publisher=[[Cambridge University Press]] | year=1987 | isbn=0-521-33718-6 | pages=68–69 }}

[[Category:Algebra]]
[[Category:Polynomials]]

{{algebra-stub}}</text>
      <sha1>59ksebmajw1e51to7gi7qhu7owvbk8d</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Filtration (mathematics)</title>
    <ns>0</ns>
    <id>726014</id>
    <revision>
      <id>577990750</id>
      <parentid>545940012</parentid>
      <timestamp>2013-10-20T16:11:09Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>General Fixes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="10703">In [[mathematics]], a '''filtration''' is an [[indexed set]] ''S&lt;sub&gt;i&lt;/sub&gt;'' of [[subobject]]s of a given [[algebraic structure]] ''S'', with the index ''i'' running over some [[index set]] ''I'' that is a [[totally ordered set]], subject to the condition that if ''i'' ≤ ''j'' in ''I'' then ''S&lt;sub&gt;i&lt;/sub&gt;'' ⊆ ''S&lt;sub&gt;j&lt;/sub&gt;''. The concept [[Dual (category theory)|dual]] to a filtration is called a ''cofiltration''.

Sometimes, as in a [[filtered algebra]], there is instead the requirement that the &lt;math&gt;S_i&lt;/math&gt; be [[Subalgebra#Subalgebras in universal algebra|subalgebras]] with respect to certain operations (say, vector addition), but with respect to other operations (say, multiplication), they instead satisfy &lt;math&gt;S_i \cdot S_j \subset S_{i+j}&lt;/math&gt;, where here the index set is the natural numbers; this is by analogy with a [[graded algebra]].

Sometimes, filtrations are supposed to satisfy the additional requirement that the union of the &lt;math&gt;S_i&lt;/math&gt; be the whole &lt;math&gt;S&lt;/math&gt;, or (in more general cases, when the notion of union does not make sense) that the canonical homomorphism from the direct limit of the &lt;math&gt;S_i&lt;/math&gt; to &lt;math&gt;S&lt;/math&gt; is an isomorphism. Whether this requirement is assumed or not usually depends on the author of the text and is often explicitly stated. We are ''not'' going to impose this requirement in this article.

There is also the notion of a '''descending filtration''', which is required to satisfy &lt;math&gt;S_i \supseteq S_j&lt;/math&gt; in lieu of &lt;math&gt;S_i \subseteq S_j&lt;/math&gt; (and, occasionally, &lt;math&gt;\bigcap_{i\in I} S_i=0&lt;/math&gt; instead of &lt;math&gt;\bigcup_{i\in I} S_i=S&lt;/math&gt;). Again, it depends on the context how exactly the word &quot;filtration&quot; is to be understood. Descending filtrations are not to be confused with cofiltrations (which consist of [[quotient]] objects rather than subobjects).

Filtrations are widely used in [[abstract algebra]], [[homological algebra]] (where they are related in an important way to [[spectral sequence]]s), and in [[measure theory]] and [[probability theory]] for nested sequences of [[sigma algebra|σ-algebras]]. In [[functional analysis]] and [[numerical analysis]], other terminology is usually used, such as [[scale of spaces]] or [[nested spaces]].

==Examples==

===Algebra===
{{See also|Filtered algebra}}

====Groups====
{{See also|Length function}}

In algebra, filtrations are ordinarily indexed by '''N''', the set of natural numbers. A ''filtration'' of a group ''G'', is then a nested sequence ''G''&lt;sub&gt;''n''&lt;/sub&gt; of [[normal subgroup]]s of ''G'' (that is, for any ''n'' we have ''G''&lt;sub&gt;''n''+1&lt;/sub&gt; ⊆ ''G''&lt;sub&gt;''n''&lt;/sub&gt;). Note that this use of the word &quot;filtration&quot; corresponds to our &quot;descending filtration&quot;.

Given a group ''G'' and a filtration ''G''&lt;sub&gt;''n''&lt;/sub&gt;, there is a natural way to define a topology on ''G'', said to be ''associated'' to the filtration. A basis for this topology is the set of all translates of subgroups appearing in the filtration, that is, a subset of ''G'' is defined to be open if it is a union of sets of the form ''aG''&lt;sub&gt;''n''&lt;/sub&gt;, where ''a''∈''G'' and ''n'' is a natural number.

The topology associated to a filtration on a group ''G'' makes ''G'' into a [[topological group]].

The topology associated to a filtration ''G''&lt;sub&gt;''n''&lt;/sub&gt; on a group ''G'' is [[Hausdorff space|Hausdorff]] if and only if ∩''G''&lt;sub&gt;''n''&lt;/sub&gt; = {1}.

If two filtrations ''G''&lt;sub&gt;''n''&lt;/sub&gt; and ''G&amp;prime;''&lt;sub&gt;''n''&lt;/sub&gt; are defined on a group ''G'', then the identity map from ''G'' to ''G'', where the first copy of ''G'' is given the ''G''&lt;sub&gt;''n''&lt;/sub&gt;-topology and the second the ''G&amp;prime;''&lt;sub&gt;''n''&lt;/sub&gt;-topology, is continuous if and only if for any ''n'' there is an ''m'' such that ''G''&lt;sub&gt;''m''&lt;/sub&gt; ⊆''G&amp;prime;''&lt;sub&gt;''n''&lt;/sub&gt;, that is, if and only if the identity map is continuous at 1. In particular, the two filtrations define the same topology if and only if for any subgroup appearing in one there is a smaller or equal one appearing in the other.

====Rings and modules: descending filtrations====

Given a ring ''R'' and an ''R''-module ''M'', a ''descending filtration'' of ''M'' is a decreasing sequence of submodules ''M''&lt;sub&gt;''n''&lt;/sub&gt;. This is therefore a special case of the notion for groups, with the additional condition that the subgroups be submodules. The associated topology is defined as for groups.

An important special case is known as the ''I''-adic topology (or ''J''-adic, etc.). Let ''R'' be a commutative ring, and ''I'' an ideal of ''R''.

Given an ''R''-module ''M'', the sequence ''I&lt;sup&gt;n&lt;/sup&gt;M'' of submodules of ''M'' forms a filtration of ''M''. The ''I-adic topology'' on ''M'' is then the topology associated to this filtration. If ''M'' is just the ring ''R'' itself, we have defined the ''I-adic topology'' on ''R''.

When ''R'' is given the ''I''-adic topology, ''R'' becomes a [[topological ring]]. If an ''R''-module ''M'' is then given the ''I''-adic topology, it becomes a [[topological module|topological ''R''-module]], relative to the topology given on ''R''.

====Rings and modules: ascending filtrations====

Given a ring ''R'' and an ''R''-module ''M'', an ''ascending filtration'' of ''M'' is an increasing sequence of submodules ''M''&lt;sub&gt;''n''&lt;/sub&gt;. In particular, if ''R'' is a field, then an ascending filtration of the ''R''-vector space ''M'' is an increasing sequence of vector subspaces of ''M''. [[Flag (linear algebra)|Flags]] are one important class of such filtrations.

====Sets====
A maximal filtration of a set is equivalent to an ordering (a [[permutation]]) of the set. For instance, the filtration &lt;math&gt;\{0\} \subset \{0,1\} \subset \{0,1,2\}&lt;/math&gt; corresponds to the ordering &lt;math&gt;(0,1,2)&lt;/math&gt;. From the point of view of the [[field with one element]], an ordering on a set corresponds to a maximal [[Flag (linear algebra)|flag]] (a filtration on a vector space), considering a set to be a vector space over the field with one element.

===Measure theory===
In [[measure theory]], in particular in [[martingale theory]] and the theory of [[stochastic process]]es, a filtration is an increasing [[sequence (mathematics)|sequence]] of [[sigma algebra|''&amp;sigma;''-algebras]] on a [[measurable space]]. That is, given a measurable space &lt;math&gt;(\Omega, \mathcal{F})&lt;/math&gt;, a filtration is a sequence of ''σ''-algebras &lt;math&gt;\{ \mathcal{F}_{t} \}_{t \geq 0}&lt;/math&gt; with &lt;math&gt;\mathcal{F}_{t} \subseteq \mathcal{F}&lt;/math&gt; for each ''t'' and

:&lt;math&gt;t_{1} \leq t_{2} \implies \mathcal{F}_{t_{1}} \subseteq \mathcal{F}_{t_{2}}.&lt;/math&gt;

The exact range of the &quot;times&quot; ''t'' will usually depend on context: the set of values for ''t'' might be [[discrete set|discrete]] or continuous, [[bounded set|bounded]] or unbounded. For example,

:&lt;math&gt;t \in \{ 0, 1, \dots, N \}, \mathbb{N}_{0}, [0, T] \mbox{ or } [0, + \infty).&lt;/math&gt;

Similarly, a '''filtered probability space''' (also known as a '''stochastic basis''') &lt;math&gt;\left(\Omega, \mathcal{F}, \left\{\mathcal{F}_{t}\right\}_{t\geq 0}, \mathbb{P}\right)&lt;/math&gt;, is a [[probability space]] equipped with the filtration &lt;math&gt;\left\{\mathcal{F}_t\right\}_{t\geq 0}&lt;/math&gt; of its σ-algebra &lt;math&gt;\mathcal{F}&lt;/math&gt;. A filtered probability space is said to satisfy the ''usual conditions'' if it is [[complete measure|complete]] (i.e. &lt;math&gt;\mathcal{F}_0&lt;/math&gt; contains all &lt;math&gt;\mathbb{P}&lt;/math&gt;-[[null set]]s) and [[right-continuous]] (i.e. &lt;math&gt;\mathcal{F}_t = \mathcal{F}_{t+} := \bigcap_{s &gt; t} \mathcal{F}_s&lt;/math&gt; for all times &lt;math&gt;t&lt;/math&gt;).&lt;ref&gt;{{cite web|title=Stochastic Processes: A very simple introduction|author=Péter Medvegyev|date=January 2009|url=http://medvegyev.uni-corvinus.hu/St1.pdf|format=pdf|accessdate=June 25, 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite book|title=Probabilities and Potential|author=Claude Dellacherie|publisher=Elsevier|year=1979|isbn=9780720407013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Filtrations and Adapted Processes|author=George Lowther|url=http://almostsure.wordpress.com/2009/11/08/filtrations-and-adapted-processes/|date=November 8, 2009|accessdate=June 25, 2012}}&lt;/ref&gt;

It is also useful (in the case of an unbounded index set) to define &lt;math&gt;\mathcal{F}_{\infty}&lt;/math&gt; as the ''σ''-algebra generated by the infinite union of the &lt;math&gt;\mathcal{F}_{t}&lt;/math&gt;'s, which is contained in &lt;math&gt;\mathcal{F}&lt;/math&gt;:

:&lt;math&gt;\mathcal{F}_{\infty} = \sigma\left(\bigcup_{t \geq 0} \mathcal{F}_{t}\right) \subseteq \mathcal{F}.&lt;/math&gt;

A ''σ''-algebra defines the set of events that can be measured, which in a [[probability]] context is equivalent to events that can be discriminated, or &quot;questions that can be answered at time ''t''&quot;. Therefore a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of [[information]]. A typical example is in [[mathematical finance]], where a filtration represents the information available up to and including each time ''t'', and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.

====Relation to stopping times====
Let &lt;math&gt;\left(\Omega, \mathcal{F}, \left\{\mathcal{F}_{t}\right\}_{t\geq 0}, \mathbb{P}\right)&lt;/math&gt; be a filtered probability space. A random variable &lt;math&gt;\tau : \Omega \rightarrow [0, \infty]&lt;/math&gt; is said to be a [[stopping time]] with respect to filtration &lt;math&gt;\left\{\mathcal{F}_{t}\right\}_{t\geq 0}&lt;/math&gt;, provided the event &lt;math&gt;\{\tau \leq t\} \in \mathcal{F}_t&lt;/math&gt; for all &lt;math&gt;t\geq 0&lt;/math&gt;. We may also define the ''stopping time'' &lt;math&gt;\sigma&lt;/math&gt;-algebra,
:&lt;math&gt;\mathcal{F}_{\tau} := \left\{A\in\mathcal{F}:A\cap\{\tau \leq t\}\in\mathcal{F}_t, \ \forall t\geq 0\right\} &lt;/math&gt;
In other words, &lt;math&gt;\mathcal{F}_{\tau}\subseteq\mathcal{F}&lt;/math&gt; encodes information up to the ''random'' time &lt;math&gt;\tau&lt;/math&gt;.

It can be shown that &lt;math&gt;\tau&lt;/math&gt; is &lt;math&gt;\mathcal{F}_{\tau}&lt;/math&gt;-measurable. Furthermore, if &lt;math&gt;\tau_ 1&lt;/math&gt; and &lt;math&gt;\tau_ 2&lt;/math&gt; are [[stopping time]]s on &lt;math&gt;\left(\Omega, \mathcal{F}, \left\{\mathcal{F}_{t}\right\}_{t\geq 0}, \mathbb{P}\right)&lt;/math&gt;, and &lt;math&gt;\tau_1 \leq \tau_2&lt;/math&gt; [[almost surely]], then &lt;math&gt;\mathcal{F}_{\tau_1} \subseteq \mathcal{F}_{\tau_2}&lt;/math&gt;.

==See also==
*[[Natural filtration]]

==References==
{{Reflist}}
* {{cite book | author=Øksendal, Bernt K. | authorlink=Bernt Øksendal | title=Stochastic Differential Equations: An Introduction with Applications | publisher=Springer| location=Berlin | year=2003 | isbn=3-540-04758-1}}

[[Category:Algebra]]
[[Category:Measure theory]]
[[Category:Stochastic processes]]</text>
      <sha1>mnhxtv2o8kyj97q4p7tj7j4qxbfu7iv</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cycle (mathematics)</title>
    <ns>0</ns>
    <id>2539394</id>
    <revision>
      <id>603556075</id>
      <parentid>600820063</parentid>
      <timestamp>2014-04-10T05:46:22Z</timestamp>
      <contributor>
        <username>Wcherowi</username>
        <id>13428914</id>
      </contributor>
      <comment>Clean-up, added references</comment>
      <text xml:space="preserve" bytes="8179">{{About|group theory|cycles in homological algebra|Chain complex#Fundamental terminology|cycles in graph theory|Cycle (graph theory)}}
In [[mathematics]], and in particular in [[group theory]], a '''cycle''' is a [[permutation]] of the elements of some set ''X'' which maps the elements of some subset ''S'' of ''X'' to each other in a cyclic fashion, while fixing (i.e., mapping to themselves) all other elements of ''X''. For example, the permutation of {1, 2, 3, 4} that sends 1 to 3, 2 to 4, 3 to 2 and 4 to 1 is a cycle, while the permutation that sends 1 to 3, 2 to 4, 3 to 1 and 4 to 2 is not (it separately permutes the pairs {1, 3} and {2, 4}).  The set ''S'' is called the [[orbit (group theory)|orbit]] of the cycle.

== Definition ==
A permutation of a set ''X'', which is a [[bijection|bijective function]] &lt;math&gt;\sigma:X\to X&lt;/math&gt;, is called a cycle if the action on ''X'' of the subgroup generated by &lt;math&gt;\sigma&lt;/math&gt; has at most one orbit with more than a single element.&lt;ref&gt;{{harvnb|Fraleigh|1993|loc=p. 103}}&lt;/ref&gt; This notion is most commonly used when ''X'' is a finite set; then of course the largest orbit, ''S'', is also finite. Let &lt;math&gt;s_0&lt;/math&gt; be any element of ''S'', and put &lt;math&gt;s_i=\sigma^i(s_0) \,&lt;/math&gt; for any &lt;math&gt;i\in\mathbf{Z}&lt;/math&gt;. If ''S'' is finite, there is a minimal number &lt;math&gt;k \geq 1&lt;/math&gt; for which &lt;math&gt;s_k=s_0&lt;/math&gt;. Then &lt;math&gt;S=\{ s_0, s_1, \ldots, s_{k-1}\}&lt;/math&gt;, and &lt;math&gt;\sigma&lt;/math&gt; is the permutation defined by

:&lt;math&gt;\sigma(s_i) = s_{i+1} \quad\mbox{for }0\leq i&lt;k&lt;/math&gt;

and &lt;math&gt;\sigma(x)=x&lt;/math&gt; for any element of &lt;math&gt;X\setminus S&lt;/math&gt;. The elements not fixed by &lt;math&gt;\sigma&lt;/math&gt; can be pictured as

:&lt;math&gt;s_0\mapsto s_1\mapsto s_2\mapsto\cdots\mapsto s_{k-1}\mapsto s_k=s_0&lt;/math&gt;.

A cycle can be written using the compact [[cycle notation]] &lt;math&gt;\sigma = (s_0~s_1~\dots~s_{k-1})&lt;/math&gt; (there are no commas between elements in this notation, to avoid confusion with a ''k''-[[tuple]]). The '''length''' of a cycle, is the number of elements of its largest orbit. A cycle of length ''k'' is also called a ''' ''k''-cycle'''. 

The orbit of a 1-cycle is called a ''fixed point'' of the permutation, but as a permutation every 1-cycle is the [[identity permutation]].&lt;ref&gt;{{harvnb|Rotman|2006|loc=p. 108}}&lt;/ref&gt; When cycle notation is used, the 1-cycles are often suppressed when no confusion will result.&lt;ref&gt;{{harvnb|Sagan|1991|loc=p. 2}}&lt;/ref&gt;

== Basic properties ==

One of the basic results on [[symmetric group]]s says that any permutation can be expressed as the product of [[Disjoint sets|disjoint]] cycles (more precisely: cycles with disjoint orbits); such cycles commute with each other, and the expression of the permutation is unique up to the order of the cycles (but note that the cycle notation is not unique: each ''k''-cycle can itself be written in ''k'' different ways, depending on the choice of &lt;math&gt;s_0&lt;/math&gt; in its orbit).&lt;ref&gt;To be technically correct a factorization should contain one 1-cycle for each fixed point of the permutation. See {{harvtxt|Rotman|2006|loc=pp. 113-114}}.&lt;/ref&gt; The [[multiset]] of lengths of the cycles in this expression (the [[cycle type]]) is therefore uniquely determined by the permutation, and both the signature and the [[conjugacy class]] of  the permutation in the symmetric group are determined by it.&lt;ref&gt;{{harvnb|Rotman|2006|loc=p. 117, 121}}&lt;/ref&gt;

The number of ''k''-cycles in the symmetric group ''S''&lt;sub&gt;''n''&lt;/sub&gt; is given, for &lt;math&gt;1\leq k\leq n&lt;/math&gt;, by the following equivalent formulas
:&lt;math&gt;\binom nk(k-1)!=\frac{n(n-1)\cdots(n-k+1)}{k}=\frac{n!}{(n-k)!k}&lt;/math&gt;

A ''k''-cycle has [[signature of a permutation|signature]] (−1)&lt;sup&gt;''k''&amp;nbsp;−&amp;nbsp;1&lt;/sup&gt;.

== Transpositions ==
[[File:Array of permutations; transpositions.svg|thumb|400px|Array of transpositions]]
A cycle with only two elements is called a '''transposition'''.  For example, the permutation of {1, 2, 3, 4} that sends 1 to 1, 2 to 4, 3 to 3 and 4 to 2 is a transposition (specifically, the transposition that swaps 2 and 4).

=== Properties ===
Any permutation can be expressed as the [[function composition|composition]] (product) of transpositions—formally, they are [[Generating set of a group|generators]] for the [[group (mathematics)|group]].&lt;ref&gt;{{harvnb|Rotman|2006|loc=p. 118, Prop. 2.35}}&lt;/ref&gt; In fact, if one takes &lt;math&gt;a=1&lt;/math&gt;, &lt;math&gt;b=2&lt;/math&gt;, ..., &lt;math&gt;e=5&lt;/math&gt;, then any permutation can be expressed as a product of '''{{visible anchor|adjacent transpositions}}''', meaning the transpositions &lt;math&gt;(k~~k+1),&lt;/math&gt; in this case &lt;math&gt;(1~2)&lt;/math&gt;, &lt;math&gt;(2~3)&lt;/math&gt;, &lt;math&gt;(3~4)&lt;/math&gt;, and &lt;math&gt;(4~5).&lt;/math&gt; This follows because an arbitrary transposition can be expressed as the product of adjacent transpositions. Concretely, one can express the transposition &lt;math&gt;(k~~l)&lt;/math&gt; where &lt;math&gt;k &lt; l&lt;/math&gt; by moving ''k'' to ''l'' one step at a time, then moving ''l'' back to where ''k'' was, which interchanges these two and makes no other changes:

:&lt;math&gt;(k~~l) = (k~~k+1)\cdot(k+1~~k+2)\cdots(l-1~~l)\cdot(l-2~~l-1)\cdots(k~~k+1).&lt;/math&gt;

The decomposition of a permutation into a product of transpositions is obtained for example by writing the permutation as a product of disjoint cycles, and then splitting iteratively each of the cycles of length 3 and longer into a product of a transposition and a cycle of length one less:

:&lt;math&gt;(a,b,c,d,\ldots,y,z) = (a,b)\cdot (b,c,d,\ldots y,z)&lt;/math&gt; 

This means the initial request is to move &lt;math&gt;a&lt;/math&gt; to &lt;math&gt;b&lt;/math&gt;, &lt;math&gt;b&lt;/math&gt; to &lt;math&gt;c&lt;/math&gt;, &lt;math&gt;y&lt;/math&gt; to &lt;math&gt;z&lt;/math&gt; and finally &lt;math&gt;z&lt;/math&gt; to &lt;math&gt;a&lt;/math&gt;. Instead one may roll the elements keeping &lt;math&gt;a&lt;/math&gt; where it is by executing the right factor first (as usual in operator notation, and following the convention in the article on [[Permutation#Product and inverse|Permutations]]). This has moved &lt;math&gt;z&lt;/math&gt; to the position of &lt;math&gt;b&lt;/math&gt;, so after the first permutation, the elements &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are not yet at their final positions. The transposition &lt;math&gt;(a,b)&lt;/math&gt;, executed thereafter, then addresses &lt;math&gt;z&lt;/math&gt; by the index of &lt;math&gt;b&lt;/math&gt; to swap what initially were &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt;.

In fact, the [[symmetric group]] is a [[Coxeter group]], meaning that it is generated by elements of order 2 (the adjacent transpositions), and all relations are of a certain form.

One of the main results on symmetric groups states that either all of the decompositions of a given permutation into transpositions have an even number of transpositions, or they all have an odd number of transpositions.&lt;ref&gt;{{harvnb|Rotman|2006|loc=p. 122}}&lt;/ref&gt; This  permits the [[parity of a permutation]] to be a [[well-defined]] concept.

== See also ==
* [[Cycle detection]] – the algorithmic problem of finding a cycle in a sequence of iterated function values.
* [[Cycle sort]] – a sorting algorithm that is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result
* [[Cycles and fixed points]]
* [[Dihedral group]]
* [[Fifteen puzzle]]

==Notes==
{{reflist|3}}

== References ==
* Anderson, Marlow and Feil, Todd (2005), ''A First Course in Abstract Algebra'', Chapman &amp; Hall/CRC; 2nd edition. ISBN 1-58488-515-7.
*{{citation|first=John|last=Fraleigh|authorlink=|title=A first course in abstract algebra|edition=5th|year=1993|publisher=Addison Wesley|isbn=978-0-201-53467-2}}
* {{citation|first=Joseph J.|last=Rotman|title=A First Course in Abstract Algebra with Applications|edition=3rd|year=2006|publisher=Prentice-Hall|isbn=978-0-13-186267-8}}
* {{citation|first=Bruce E.|last=Sagan|title=The Symmetric Group / Representations, Combinatorial Algorithms &amp; Symmetric Functions|year=1991|publisher=Wadsworth &amp; Brooks/Cole|isbn=978-0-534-15540-7}}

== External links ==
* [http://www.cut-the-knot.org/Curriculum/Combinatorics/PermByTrans.shtml Permutations as a Product of Transpositions]

{{PlanetMath attribution|id=2262|title=cycle}}

{{DEFAULTSORT:Cycle (Mathematics)}}
[[Category:Algebra]]
[[Category:Permutations]]</text>
      <sha1>9ap8k3u7casvmok28tzpk84p6o5trr8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Invertible module</title>
    <ns>0</ns>
    <id>22352470</id>
    <revision>
      <id>507902845</id>
      <parentid>346133440</parentid>
      <timestamp>2012-08-17T22:31:28Z</timestamp>
      <contributor>
        <ip>198.102.153.1</ip>
      </contributor>
      <text xml:space="preserve" bytes="1271">In [[mathematics]], particularly [[commutative algebra]], an '''invertible module''' is intuitively a [[module (mathematics)|module]] that has an [[inverse element|inverse]] with respect to the [[tensor product]]. Invertible modules form the foundation for the definition of [[invertible sheaf|invertible sheaves]] in [[algebraic geometry]].

Formally, a [[finitely generated module]] ''M'' over a ring ''R'' is said to be invertible if it is locally a [[free module]] of [[rank of a free module|rank]] 1. In other words &lt;math&gt; M_P\cong R_P &lt;/math&gt; for all [[prime element|primes]] ''P'' of ''R''. Now, if ''M'' is an invertible ''R''-module, then its [[Duality (mathematics)#Dual objects|dual]] {{nowrap|''M''&lt;sup&gt;*&lt;/sup&gt; {{=}} Hom(''M'',''R'')}} is its inverse with respect to the tensor product, i.e. &lt;math&gt;M\otimes _R M^*\cong R&lt;/math&gt;.

The theory of invertible modules is closely related to the theory of [[codimension]] one [[algebraic variety|varieties]] including the theory of [[divisor (algebraic geometry)|divisor]]s.

==See also==
* [[Picard group]]

==References==
* [[David Eisenbud|Eisenbud, David]], ''Commutative Algebra with a View Toward Algebraic Geometry'', Springer, ISBN 978-0-387-94269-8

[[Category:Mathematical structures]]
[[Category:Algebra]]</text>
      <sha1>ky75ox3jpv0yy2i01uc5l5nliv03kmq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>De Morgan algebra</title>
    <ns>0</ns>
    <id>8495580</id>
    <revision>
      <id>596925857</id>
      <parentid>491307733</parentid>
      <timestamp>2014-02-24T15:41:15Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>Fixing [[Wikipedia:Disambiguation pages with links|links to disambiguation pages]] using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="1751">In [[mathematics]], a '''De Morgan algebra''' (named after [[Augustus De Morgan]], a British mathematician and logician) is a structure ''A'' =&amp;nbsp;(A,&amp;nbsp;∨,&amp;nbsp;∧,&amp;nbsp;0,&amp;nbsp;1,&amp;nbsp;¬) such that:

* (''A'',&amp;nbsp;∨,&amp;nbsp;∧,&amp;nbsp;0,&amp;nbsp;1) is a [[Bounded set|bounded]] [[distributive lattice]], and
* ¬ is a De Morgan involution:  ¬(''x''&amp;nbsp;∧&amp;nbsp;''y'') = ¬''x''&amp;nbsp;∨&amp;nbsp;¬''y'' and ¬¬''x'' = ''x''. (i.e. an  [[Involution (mathematics)|involution]] that additionally satisfies [[De Morgan's laws]])

In a De Morgan algebra:

* ¬''x''&amp;nbsp;∨&amp;nbsp;''x'' = 1 ([[law of the excluded middle]]), and
* ¬''x''&amp;nbsp;∧&amp;nbsp;''x'' = 0 ([[law of noncontradiction]])

do not always hold (when they do, the algebra becomes a [[Boolean algebra (structure)|Boolean algebra]]).

Remark: It follows that ¬( x∨y)  = ¬x∧¬y,  ¬1 = 0 and ¬0 = 1 (e.g. ¬1 = ¬1∨0 = ¬1∨¬¬0 = ¬(1∧¬0) = ¬¬0 = 0). Thus ¬ is a dual [[automorphism]].

De Morgan algebras are important for the study of the mathematical aspects of [[fuzzy logic]]. 

The standard fuzzy algebra ''F'' =&amp;nbsp;([0,&amp;nbsp;&amp;nbsp;1],&amp;nbsp;max(''x'',&amp;nbsp;''y''),&amp;nbsp;min(''x'',&amp;nbsp;''y''),&amp;nbsp;0,&amp;nbsp;1,&amp;nbsp;1&amp;nbsp;&amp;minus;&amp;nbsp;''x'') is an example of a De Morgan algebra where the laws of excluded middle and noncontradiction do not hold.

==References==

* &quot;Injective de Morgan and Kleene Algebras&quot;, Roberto Cignoli, ''Proceedings of the American Mathematical Society'', Vol. 47, No. 2 (Feb., 1975), pp. 269&amp;ndash;278
* {{cite book|author1=Thomas Scott Blyth|author2=J. C. Varlet|title=Ockham algebras|year=1994|publisher=Oxford University Press|isbn=978-0-19-859938-8}}

[[Category:Algebra]]
[[Category:Lattice theory]]
[[Category:Algebraic logic]]</text>
      <sha1>6xxeh9e9zrc8j0khonyzkixvr1m5pyb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Spherical design</title>
    <ns>0</ns>
    <id>9730285</id>
    <revision>
      <id>610016260</id>
      <parentid>608584476</parentid>
      <timestamp>2014-05-25T01:42:57Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>minor fixes, mostly [[Wikipedia:Disambiguation pages with links|disambig links]] using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="3839">A '''spherical design''', part of [[combinatorial design]] theory in [[mathematics]], is a finite set of ''N'' points on the ''d''-dimensional unit [[hypersphere]] ''S&lt;sup&gt;d&lt;/sup&gt;'' such that the average value of any polynomial ''f'' of degree ''t'' or less on the set equals the average value of ''f'' on the whole sphere (that is, the integral of ''f'' over ''S&lt;sup&gt;d&lt;/sup&gt;'' divided by the area or [[Measure (mathematics)|measure]] of ''S&lt;sup&gt;d&lt;/sup&gt;'').  Such a set is often called a '''spherical''' ''t''-'''design''' to indicate the value of ''t'', which is a fundamental parameter.

Spherical designs can be of value in [[approximation theory]], in [[statistics]] for [[experimental design]] (being usable to construct [[rotatable design]]s), in [[combinatorics]], and in [[geometry]].  The main problem is to find examples, given ''d'' and ''t'', that are not too large.  However, such examples may be hard to come by.
Spherical t-designs have also recently been appropriated in [[quantum mechanics]] in the form of [[quantum t-designs]] with various applications to [[quantum information theory]], [[quantum computing]] and [[POVM]]s.

The concept of a spherical design is due to Delsarte, Goethals, and Seidel (1977). The existence and structure of spherical designs with ''d'' = 1 (that is, in a circle) was studied in depth by Hong (1982).

== arbitrary-dimensional spherical design ==

Shortly thereafter, Seymour and Zaslavsky (1984) proved that such designs exist of all sufficiently large sizes; that is, there is a number ''N''(''d'',''t'') such that for every ''N'' &amp;ge; ''N''(''d'',''t'') there exists a spherical ''t''-design of ''N'' points in dimension ''d''.  However, their proof gave no idea of how big ''N''(''d'',''t'') is.  Good estimates for that were found later on.  Besides these &quot;large&quot; sizes, there are many sporadic small spherical designs; many of them are related to finite [[group action]]s on the sphere and are of great interest in themselves.

Recently, Bondarenko, Radchenko, and Viazovska obtained the optimal asymptotic upper bound
&lt;math&gt; N(d,t)&lt;C_d t^d&lt;/math&gt; for all positive integers ''d'' and ''t''.

== 2-dimensional spherical design ==
{{main | geodesic grid }}

Spherical designs with ''d'' = 2 (that is, on the surface of a sphere) ...

One application of spherical designs is for whole-sphere data collection.
Spherical t-designs meet the &quot;accurately approximate integrals by sums&quot; criteria for &quot;good&quot; pixelizations of the sphere.&lt;ref&gt;
[http://www.iop.org/EJ/article/1538-4357/470/2/L81/5407.text.html &quot;An Icosahedron-based Method for Pixelizing the Celestial Sphere&quot;]
by Max Tegmark 1996
&lt;/ref&gt;

==See also==
* [[Thomson problem]]

==External Links==
* Spherical t-designs for different values of  ''N'' and ''t'' can be found precomputed at [http://neilsloane.com/sphdesigns/ Neil Sloane's website].

==References==
* Bondarenko, A., Radchenko D., and Viazovska M. (2013), &quot;On optimal asymptotic bounds for spherical designs&quot;, ''Annals of Mathematics'', vol. 178, pp.&amp;nbsp;443–452.
* Delsarte, P., Goethals, J.M., and Seidel, J.J. (1977), &quot;Spherical codes and designs.&quot; ''Geometriae Dedicata'' vol. 6, pp.&amp;nbsp;363–388.
* Hong, Yiming (1982), &quot;On spherical ''t''-designs in ''R''&lt;sup&gt;2&lt;/sup&gt;.&quot;  ''European Journal of Combinatorics'', vol. 3, pp.&amp;nbsp;255–258.
* Seidel, J.J. (1991), ''Geometry and Combinatorics:  Selected Works of J.J. Seidel''.  [[Derek Corneil|D.G. Corneil]] and R. Mathon, eds.  Boston: Academic Press.  Reprints Delsarte et al. (1977).
* Seymour, P.D., and Zaslavsky, Thomas (1984), &quot;Averaging sets: A generalization of mean values and spherical designs.&quot;  ''Advances in Mathematics'', vol. 52, pp.&amp;nbsp;213–240.  Much more general than spheres.
{{reflist}}

{{Experimental design}}
{{Statistics}}

[[Category:Algebra]]
[[Category:Design of experiments]]</text>
      <sha1>6fz90d3eyygobzcl1fb8wkh1v2yyua8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Algebraic solution</title>
    <ns>0</ns>
    <id>1601833</id>
    <revision>
      <id>567997596</id>
      <parentid>567952832</parentid>
      <timestamp>2013-08-10T21:37:50Z</timestamp>
      <contributor>
        <username>Duoduoduo</username>
        <id>10585217</id>
      </contributor>
      <comment>synonym that links here</comment>
      <text xml:space="preserve" bytes="1987">An '''algebraic solution''' or '''solution in radicals''' is a [[closed form expression]], and more specifically a closed-form [[algebraic expression]], that is the solution of an [[algebraic equation]] in terms of the coefficients, relying only on [[addition]], [[subtraction]], [[multiplication]], [[Division (mathematics)|division]], raising to integer powers, and the extraction of roots (square roots, cube roots, etc.). 

The most well-known example is the solution

:&lt;math&gt;
x=\frac{-b \pm \sqrt {b^2-4ac\  }}{2a},&lt;/math&gt;

introduced in secondary school, of the [[quadratic equation]]

:&lt;math&gt;ax^2 + bx + c =0\,&lt;/math&gt;

(where ''a'' ≠ 0).

There exist more complicated algebraic solutions for the general [[cubic equation]]&lt;ref&gt;Nickalls, R. W. D., &quot;A new approach to solving the cubic: Cardano's solution revealed,&quot; ''Mathematical Gazette'' 77, November 1993, 354-359.&lt;/ref&gt; and [[quartic equation]].&lt;ref&gt;Carpenter, William, &quot;On the solution of the real quartic,&quot; ''Mathematics Magazine'' 39, 1966, 28-30.&lt;/ref&gt;  The [[Abel-Ruffini theorem]]&lt;ref&gt;Jacobson, Nathan (2009), Basic Algebra 1 (2nd ed.), Dover, ISBN 978-0-486-47189-1&lt;/ref&gt;{{rp|211}} states that the general [[quintic equation]] lacks an algebraic solution, and this directly implies that the general polynomial equation of degree ''n'', for ''n'' ≥ 5, cannot be solved algebraically. However, under certain conditions algebraic solutions can be obtained; for example, the equation &lt;math&gt;x^{10} = a&lt;/math&gt; can be solved as &lt;math&gt;x=a^{1/10}.&lt;/math&gt;

Algebraic solutions form a subset of [[closed-form expression]]s, because the latter permit [[transcendental functions]] (non-algebraic functions) such as the exponential function, the logarithmic function, and the trigonometric functions and their inverses.

==See also==

*[[sextic equation#Solvable sextics|Solvable sextics]]
*[[septic equation#Solvable septics|Solvable septics]]

==References==
{{reflist}}

{{DEFAULTSORT:Algebraic Solution}}
[[Category:Algebra]]</text>
      <sha1>g4hfyfyv6vc4hfncac068mzdzbpkssf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Prosolvable group</title>
    <ns>0</ns>
    <id>23941767</id>
    <revision>
      <id>581840240</id>
      <parentid>543334713</parentid>
      <timestamp>2013-11-15T23:43:57Z</timestamp>
      <contributor>
        <username>Mark viking</username>
        <id>17698045</id>
      </contributor>
      <comment>Added wl</comment>
      <text xml:space="preserve" bytes="1848">In [[mathematics]], more precisely in [[algebra]], a '''prosolvable group''' (less common: '''prosoluble group''') is a [[group (mathematics)|group]] that is [[isomorphic]] to the [[inverse limit]] of an [[inverse system]] of [[solvable group]]s. Equivalently, a group is called '''prosolvable''', if, viewed as a [[topological group]], every [[open neighborhood]] of the identity contains a [[normal subgroup]] whose corresponding [[quotient group]] is a [[solvable group]].

== Examples ==
* Let ''p'' be a [[prime]], and denote the [[Field (mathematics)|field]] of [[p-adic numbers]], as usually, by &lt;math&gt;\mathbf{Q}_p&lt;/math&gt;. Then the [[Galois group]] &lt;math&gt;\text{Gal}(\overline{\mathbf{Q}}_p/\mathbf{Q}_p)&lt;/math&gt;, where &lt;math&gt;\overline{\mathbf{Q}}_p&lt;/math&gt; denotes the [[algebraic closure]] of &lt;math&gt;\mathbf{Q}_p&lt;/math&gt;, is prosolvable. This follows from the fact that, for any finite [[Galois extension]] &lt;math&gt;L&lt;/math&gt; of &lt;math&gt;\mathbf{Q}_p&lt;/math&gt;, the [[Galois group]] &lt;math&gt;\text{Gal}(L/\mathbf{Q}_p)&lt;/math&gt; can be written as [[semidirect product]] &lt;math&gt;\text{Gal}(L/\mathbf{Q}_p)=(R \rtimes Q) \rtimes P&lt;/math&gt;, with &lt;math&gt;P&lt;/math&gt; cyclic of order &lt;math&gt;f&lt;/math&gt; for some &lt;math&gt;f\in\mathbf{N}&lt;/math&gt;, &lt;math&gt;Q&lt;/math&gt; cyclic of order dividing &lt;math&gt;p^f-1&lt;/math&gt;, and &lt;math&gt;R&lt;/math&gt; of &lt;math&gt;p&lt;/math&gt;-power order. Therefore, &lt;math&gt;\text{Gal}(L/\mathbf{Q}_p)&lt;/math&gt; is [[solvable group|solvable]].&lt;ref&gt;{{citation|last=Boston|first=Nigel|title=The Proof of Fermat's Last Theorem|year=2003|publisher=University of Wisconsin Press|location=Madison, Wisconsin, USA}}&lt;/ref&gt;

== See also ==
* [[Solvable group]]
* [[Galois theory]]
* [[p-adic integers]]

==References==
{{reflist}}

[[Category:Mathematical structures]]
[[Category:Algebra]]
[[Category:Number theory]]
[[Category:Topology]]
[[Category:Group theory]]
[[Category:Topological groups]]</text>
      <sha1>c7kdnsg9rgk04moz2keti9d5ac8b4av</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Laws of Form</title>
    <ns>0</ns>
    <id>660651</id>
    <revision>
      <id>606826120</id>
      <parentid>593614676</parentid>
      <timestamp>2014-05-02T22:03:45Z</timestamp>
      <contributor>
        <username>Wcherowi</username>
        <id>13428914</id>
      </contributor>
      <comment>/* Proof theory */ fix DAB</comment>
      <text xml:space="preserve" bytes="54908">{{Abbreviations|article|date=August 2009}}
{{italic title}}
'''''Laws of Form''''' (hereinafter '''''LoF''''') is a book by [[G. Spencer-Brown]], published in 1969, that straddles the boundary between [[mathematics]] and [[philosophy]]. ''LoF'' describes three distinct [[logical system]]s:
* The ''primary arithmetic'' (described in Chapter 4 of ''LoF''), whose models include [[Two-element Boolean algebra#Some basic identities|Boolean arithmetic]];
* &lt;!-- Please see the discussion page. --&gt; The ''primary [[algebra]]'' (Chapter 6 of ''LoF''), whose [[interpretation (logic)|models]] include the [[two-element Boolean algebra]] (hereinafter abbreviated '''2'''), [[Boolean logic]], and the classical [[propositional calculus]];
* ''Equations of the second degree'' (Chapter 11), whose [[interpretation (logic)|interpretations]] include [[finite automata]] and [[Alonzo Church]]'s Restricted Recursive Arithmetic (RRA).

'''Boundary algebra''' is [http://www.canterbury.ac.nz/spark/Researcher.aspx?researcherid=84894 Dr Philip Meguire]'s (2011) term&lt;ref&gt;Meguire, P. (2011) Boundary Algebra: A Simpler Approach to Basic Logic and Boolean Algebra. Saarbrücken: VDM Publishing Ltd. 168pp&lt;/ref&gt; for the union of the primary algebra (hereinafter abbreviated ''pa'') and the primary arithmetic. &quot;Laws of Form&quot; sometimes loosely refers to the ''pa'' as well as to ''LoF''.

==The book==
''LoF'' emerged from work in electronic engineering its author did around 1960, and from subsequent lectures on [[mathematical logic]] he gave under the auspices of the [[University of London]]'s extension program. ''LoF'' has appeared in several editions, the most recent being a 1997 German translation, and has never gone out of print.

The mathematics fills only about 55pp and is rather elementary. But ''LoF'''s mystical and declamatory prose, and its love of [[paradox]], make it a challenging read for all. Spencer-Brown was influenced by [[Wittgenstein]] and [[R. D. Laing]]. ''LoF'' also echoes a number of themes from the writings of [[Charles Sanders Peirce]], [[Bertrand Russell]], and [[Alfred North Whitehead]].

The entire book is written in an operational way, giving instructions to the reader instead of telling him what ''is''. In accordance with G. Spencer-Brown's interest in paradoxes, the only sentence that makes a statement that something is, is the statement, which says no such statements are used in this book.&lt;ref&gt;Felix Lau: &quot;Die Form der Paradoxie&quot;, 2005 Carl-Auer Verlag, ISBN 9783896703521 {{Please check ISBN|reason=Invalid length.}}&lt;/ref&gt; Except for this one sentence the book can be seen as an example of [[E-Prime]].

==Reception==
Ostensibly a work of formal mathematics and philosophy, ''LoF'' became something of a [[cult classic]], praised in the ''[[Whole Earth Catalog]]''. Those who agree point to ''LoF'' as embodying an enigmatic &quot;mathematics of [[consciousness]],&quot; its algebraic symbolism capturing an (perhaps even ''the'') implicit root of [[cognition]]:  the ability to ''distinguish''. ''LoF'' argues that the ''pa'' (primary algebra) reveals striking connections among [[logic]], [[Boolean algebra (logic)|Boolean algebra]], and arithmetic, and the [[philosophy of language]] and [[Philosophy of mind|mind]].

[http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ndjfl/1093888028?abstract= Banaschewski (1977)] argues that the ''pa'' is nothing but new notation for Boolean algebra. Indeed, the [[two-element Boolean algebra]] '''2''' can be seen as the intended interpretation of the ''pa''. Yet the notation of the ''pa'':
* Fully exploits the [[Duality (mathematics)|duality]] characterizing not just [[Boolean algebra (structure)|Boolean algebra]]s but all [[Lattice (order)|lattice]]s;
*Highlights how syntactically distinct statements in logic and '''2''' can have identical [[semantics]];
* Dramatically simplifies Boolean algebra calculations, and proofs in [[sentential logic|sentential]] and [[syllogism|syllogistic]] [[logic]].
Moreover, the syntax of the ''pa'' can be extended to formal systems other than '''2''' and sentential logic, resulting in ''boundary mathematics'' (see Related Work below).

''LoF'' has influenced, among others, [[Heinz von Foerster]], [[Louis Kauffman]], [[Niklas Luhmann]], [[Humberto Maturana]], [[Francisco Varela]] and [[William Bricken]]. Some of these authors have modified the primary algebra in a variety of interesting ways.

''LoF'' claimed that certain well-known mathematical conjectures of very long standing, such as the [[Four Color Theorem]], [[Fermat's Last Theorem]], and the [[Goldbach conjecture]], are provable using extensions of the ''pa''. Spencer-Brown eventually circulated a purported proof of the Four Color Theorem, but it met with skepticism.&lt;ref&gt;For a sympathetic evaluation, see [http://www.arxiv.org/math.CO/0112266 Kauffman (2001)].&lt;/ref&gt; (The Four Color Theorem and Fermat's Last Theorem were proved in 1976 and 1995, respectively, using methods owing nothing to ''LoF''.)

==The form (Chapter 1)==
The symbol:

:[[Image:Laws of Form - cross.gif]]

also called the '''''mark''''' or '''''cross''''', is the essential feature of the Laws of Form. In Spencer-Brown's inimitable and enigmatic fashion, the Mark symbolizes the root of [[cognition]], i.e., the [[dualism|dualistic]] Mark indicates the capability of differentiating a &quot;this&quot; from &quot;everything else ''but'' this.&quot;

In ''LoF'', a Cross denotes the drawing of a &quot;distinction&quot;, and can be thought of as signifying the following, all at once:
* The act of drawing a boundary around something, thus separating it from everything else;
* That which becomes distinct from everything by drawing the boundary;
* Crossing from one side of the boundary to the other.

All three ways imply an ''action'' on the part of the cognitive entity (e.g., person) making the distinction.  As ''LoF'' puts it:
&lt;blockquote&gt;
&quot;The first command:
* Draw a distinction
can well be expressed in such ways as:
* Let there be a distinction,
* Find a distinction,
* See a distinction,
* Describe a distinction,
* Define a distinction,
Or:
* Let a distinction be drawn.&quot; (''LoF'', Notes to chapter 2)
&lt;/blockquote&gt;

The counterpoint to the Marked state is the Unmarked state, which is simply nothing, the void, represented by a blank space. It is simply the absence of a Cross. No distinction has been made and nothing has been crossed. The Marked state and the void are the two primitive values of the Laws of Form.

The Cross can be seen as denoting the distinction between two states, one &quot;considered as a symbol&quot; and another not so considered. From this fact arises a curious resonance with some theories of [[consciousness]] and [[language]]. Paradoxically, the Form is at once Observer and Observed, and is also the creative act of making an observation. ''LoF'' (excluding back matter) closes with the words:

&quot;...the first distinction, the Mark and the observer are not only interchangeable, but, in the form, identical.&quot;

[[Charles Sanders Peirce|C. S. Peirce]] came to a related insight in the 1890s; see [[#Related work|Related Work]].

==The primary arithmetic (Chapter 4)==
The [[syntax]] of the '''primary arithmetic''' ('''PA''') goes as follows. There are just two [[atomic formula|atomic expressions]]:
* The empty Cross [[Image:Laws of Form - cross.gif]]  ;
*All or part of the blank page (the &quot;void&quot;).
There are two inductive rules:
* A Cross [[Image:Laws of Form - cross.gif]]  may be written over any expression;
* Any two expressions may be [[concatenation|concatenated]].
The [[semantics]] of the primary arithmetic are perhaps nothing more than the sole explicit [[definition]] in ''LoF'':  ''Distinction is perfect continence''.

Let the ''unmarked state'' be a synonym for the void. Let an empty Cross denote the ''marked state.'' To cross is to move from one of the unmarked or marked states to the other. We can now state the &quot;arithmetical&quot; [[axiom]]s '''A1''' and '''A2''', which ground the primary arithmetic (and hence all of the Laws of Form):

'''A1. The law of Calling'''. Calling twice from a state is indistinguishable from calling once. To make a distinction twice has the same effect as making it once. For example, saying &quot;Let there be light&quot; and then saying &quot;Let there be light&quot; again, is the same as saying it once.  Formally:

::[[Image:Laws of Form - cross.gif]] [[Image:Laws of Form - cross.gif]] &lt;math&gt;\ =&lt;/math&gt;[[Image:Laws of Form - cross.gif]]

'''A2. The law of Crossing.''' After crossing from the unmarked to the marked state, crossing again (&quot;recrossing&quot;) starting from the marked state returns one to the unmarked state. Hence recrossing annuls crossing. Formally:

::[[Image:Laws of Form - double cross.gif]] &lt;math&gt;\ =&lt;/math&gt;

In both '''A1''' and '''A2''', the expression to the right of '=' has fewer symbols than the expression to the left of '='. This suggests that every primary arithmetic expression can, by repeated application of '''A1''' and '''A2''', be ''simplified'' to one of two states: the marked or the unmarked state. This is indeed the case, and the result is the expression's ''simplification''. The two fundamental metatheorems of the primary arithmetic state that:
* Every finite expression has a unique simplification. ('''T3''' in ''LoF'');
* Starting from an initial marked or unmarked state, &quot;complicating&quot; an expression by a finite number of repeated application of '''A1''' and '''A2''' cannot yield an expression whose simplification differs from the initial state. ('''T4''' in ''LoF'').
Thus the [[relation (mathematics)|relation]] of ''[[logical equivalence]]'' [[partition of a set|partitions]] all primary arithmetic expressions into two [[equivalence class]]es: those that simplify to the Cross, and those that simplify to the void.

'''A1''' and '''A2''' have loose analogs in the properties of series and parallel electrical circuits, and in other ways of diagramming processes, including flowcharting. '''A1''' corresponds to a parallel connection and '''A2''' to a series connection, with the understanding that making a distinction corresponds to changing how two points in a circuit are connected, and not simply to adding wiring.

The primary arithmetic is analogous to the following formal languages from [[mathematics]] and [[computer science]]:
* A [[Dyck language]] of order 1 with a null alphabet;
* The simplest [[context-free language]] in the [[Chomsky hierarchy]];
* A [[rewrite system]] that is [[strongly normalizing]] and [[confluence (abstract rewriting)|confluent]].

The phrase ''calculus of indications'' in ''LoF'' is a synonym for &quot;primary arithmetic&quot;.

===The notion of canon===
A concept peculiar to ''LoF'' is that of ''canon''. While ''LoF'' does not define canon, the following two excerpts from the Notes to chpt. 2 are apt:

&lt;blockquote&gt;&quot;The more important structures of command are sometimes called ''canons''. They are the ways in which the guiding injunctions appear to group themselves in constellations, and are thus by no means independent of each other. A canon bears the distinction of being outside (i.e., describing) the system under construction, but a command to construct (e.g., 'draw a distinction'), even though it may be of central importance, is not a canon. A canon is an order, or set of orders, to permit or allow, but not to construct or create.&quot;&lt;/blockquote&gt;

&lt;blockquote&gt;&quot;...the primary form of mathematical communication is not description but injunction... Music is a similar art form, the composer does not even attempt to describe the set of sounds he has in mind, much less the set of feelings occasioned through them, but writes down a set of commands which, if they are obeyed by the performer, can result in a reproduction, to the listener, of the composer's original experience.&quot;&lt;/blockquote&gt;

These excerpts relate to the distinction in [[metalogic]] between the [[object language]], the formal language of the logical system under discussion, and the [[metalanguage]], a language (often a natural language) distinct from the object language, employed to exposit and discuss the object language. The first quote seems to assert that the ''canons'' are part of the metalanguage. The second quote seems to assert that statements in the object language are essentially commands addressed to the reader by the author. Neither assertion holds in standard metalogic.

==The primary algebra (Chapter 6)==

===Syntax===
Given any valid primary arithmetic expression, insert into one or more locations any number of Latin letters bearing optional numerical subscripts; the result is a ''pa'' [[formula]]. Letters so employed in [[mathematics]] and [[logic]] are called [[Variable (mathematics)|variables]]. A ''pa'' variable indicates a location where one can write the primitive value [[Image:Laws of Form - cross.gif]] or its complement [[Image:Laws of Form - double cross.gif]]. Multiple instances of the same variable denote multiple locations of the same primitive value.

===Rules governing logical equivalence===
The sign '=' may link two logically equivalent expressions; the result is an [[equation]]. By &quot;logically equivalent&quot; is meant that the two expressions have the same simplification. [[Logical equivalence]] is an [[equivalence relation]] over the set of ''pa'' formulas, governed by the rules '''R1''' and '''R2'''. Let ''C'' and ''D'' be formulae each containing at least one instance of the subformula ''A'':
*'''R1''', ''Substitution of equals''. Replace ''one or more'' instances of ''A'' in ''C'' by ''B'', resulting in ''E''. If ''A''=''B'', then ''C''=''E''.
*'''R2''', ''Uniform replacement''. Replace ''all'' instances of ''A'' in ''C'' and ''D'' with ''B''. ''C'' becomes ''E'' and ''D'' becomes ''F''. If ''C''=''D'', then ''E''=''F''. Note that ''A''=''B'' is not required.
'''R2''' is employed very frequently in ''pa'' demonstrations (see below), almost always silently. These rules are routinely invoked in [[logic]] and most of mathematics, nearly always unconsciously.

The ''pa'' consists of [[equations]], i.e., pairs of formulae linked by an infix '='. '''R1''' and '''R2''' enable transforming one equation into another. Hence the ''pa'' is an ''equational''  formal system, like the many [[algebraic structures]], including [[Boolean algebra (structure)|Boolean algebra]], that are [[variety (universal algebra)|varieties]]. Equational logic was common before ''Principia Mathematica'' (e.g., Peirce,&lt;sup&gt;1,2,3&lt;/sup&gt; Johnson 1892), and has present-day advocates (Gries and Schneider 1993).

Conventional [[mathematical logic]] consists of [[Tautology (logic)|tautological]] formulae, signalled by a prefixed [[Turnstile (symbol)|turnstile]]. To denote that the ''pa'' formula ''A'' is a [[Tautology (logic)|tautology]], simply write &quot;''A'' =[[Image:Laws of Form - cross.gif]] &quot;. If one replaces '=' in '''R1''' and '''R2''' with the [[biconditional]], the resulting rules hold in conventional logic. However, conventional logic relies mainly on the rule [[modus ponens]]; thus conventional logic is ''ponential''. The equational-ponential dichotomy distills much of what distinguishes mathematical logic from the rest of mathematics.

===Initials===
An ''initial'' is a ''pa'' equation verifiable by a [[decision procedure]] and as such is ''not'' an [[axiom]]. ''LoF'' lays down the initials:

*&lt;math&gt; \ J1: ((A)A) = . &lt;/math&gt;
The absence of anything to the right of the &quot;=&quot; above, is deliberate.

*&lt;math&gt; \ J2: ((A)(B))C = ((AC)(BC)).&lt;/math&gt;

'''J2''' is the familiar [[distributive law]] of [[sentential logic]] and [[Boolean algebra (structure)|Boolean algebra]].

Another set of initials, friendlier to calculations, is:

*&lt;math&gt; \ J0: (())A = A.&lt;/math&gt;

*&lt;math&gt; \ J1a: (A)A = ()&lt;/math&gt;

*&lt;math&gt; \ C2: A(AB)=A(B).&lt;/math&gt;

It is thanks to '''C2''' that the ''pa'' is a [[lattice (order)|lattice]]. By virtue of '''J1a''', it is a [[complemented lattice]] whose upper bound is (). By '''J0''', (()) is the corresponding lower bound and [[identity element]]. '''J0''' is also an algebraic version of '''A2''' and makes clear the sense in which (()) aliases with the blank page.

T13 in ''LoF'' generalizes '''C2''' as follows. Any ''pa'' (or sentential logic) formula ''B'' can be viewed as an [[tree data structure|ordered tree]] with ''branches''. Then:

'''T13''': A [[formula|subformula]] ''A'' can be copied at will into any depth of ''B'' greater than that of ''A'', as long as ''A'' and its copy are in the same branch of ''B''. Also, given multiple instances of ''A'' in the same branch of ''B'', all instances but the shallowest are redundant.

While a proof of T13 would require [[mathematical induction|induction]], the intuition underlying it should be clear.

'''C2''' or its equivalent is named:
*&quot;Generation&quot; in ''LoF'';
*&quot;Exclusion&quot; in Johnson (1892);
*&quot;Pervasion&quot; in the work of William Bricken;
*&quot;Mimesis&quot; in the entry [[logical nand]].
Perhaps the first instance of an axiom or rule with the power of '''C2''' was the &quot;Rule of (De)Iteration,&quot; combining T13 and ''AA=A'', of [[Charles Sanders Peirce|C. S. Peirce]]'s [[existential graph]]s.

''LoF'' asserts that concatenation can be read as [[commutativity|commuting]] and [[associativity|associating]] by default and hence need not be explicitly assumed or demonstrated. (Peirce made a similar assertion about his [[existential graph]]s.) Let a period be a temporary notation to establish grouping. That concatenation commutes and associates may then be demonstrated from the:
* Initial ''AC.D''=''CD.A'' and the consequence ''AA''=''A'' (Byrne 1946). This result holds for all [[lattice (order)|lattices]], because ''AA''=''A'' is an easy consequence of the [[absorption law]], which holds for all lattices;
* Initials ''AC.D''=''AD.C'' and '''J0'''. Since '''J0''' holds only for lattices with a lower bound, this method holds only for [[bounded lattice]]s (which include the ''pa'' and '''2'''). Commutativity is trivial; just set ''A''=(()). Associativity: ''AC.D'' = ''CA.D'' = ''CD.A'' = ''A.CD''.
Having demonstrated associativity, the period can be discarded.

The initials in Meguire (2011) are ''AC.D''=''CD.A'', called '''B1'''; '''B2''', J0 above; '''B3''', J1a above; and '''B4''', C2. By design, these initials are very similar to the axioms for an [[abelian group]], '''G1-G3''' below.

===Proof theory===
The ''pa'' contains three kinds of proved assertions:
* ''Consequence'' is a ''pa'' equation verified by a ''demonstration''. A demonstration consists of a sequence of ''steps'', each step justified by an initial or a previously demonstrated consequence.
* ''[[Theorem]]'' is a statement in the [[metalanguage]] verified by a ''[[Mathematical proof|proof]]'', i.e., an argument, formulated in the metalanguage, that is accepted by trained mathematicians and logicians.
* ''Initial'', defined above. Demonstrations and proofs invoke an initial as if it were an axiom.

The distinction between consequence and [[theorem]] holds for all formal systems, including mathematics and logic, but is usually not made explicit. A demonstration or [[decision procedure]] can be carried out and verified by computer. The [[Mathematical proof|proof]] of a [[theorem]] cannot be.

Let ''A'' and ''B'' be ''pa'' [[formula]]s. A demonstration of ''A''=''B'' may proceed in either of two ways:
* Modify ''A'' in steps until ''B'' is obtained, or vice versa;
* Simplify both (''A'')''B'' and (''B'')''A'' to [[Image:Laws of Form - cross.gif]]. This is known as a &quot;calculation&quot;.
Once ''A''=''B'' has been demonstrated, ''A''=''B'' can be invoked to justify steps in subsequent demonstrations. ''pa'' demonstrations and calculations often require no more than '''J1a''', '''J2''', '''C2''', and the consequences ()''A''=() ('''C3''' in ''LoF''), ((''A''))=''A'' ('''C1'''), and ''AA''=''A'' ('''C5''').

The consequence (((''A'')''B'')''C'') = (''AC'')((''B'')''C''), '''C7''' in ''LoF'', enables an [[algorithm]], sketched in ''LoF''s proof of T14, that transforms an arbitrary ''pa'' formula to an equivalent formula whose depth does not exceed two. The result is a ''normal form'', the ''pa'' analog of the [[conjunctive normal form]]. ''LoF'' (T14-15) proves the ''pa'' analog of the well-known [[Boolean algebra (logic)|Boolean algebra]] theorem that every formula has a normal form.

Let ''A'' be a [[formula|subformula]] of some [[formula]] ''B''. When paired with '''C3''', '''J1a''' can be viewed as the closure condition for calculations: ''B'' is a [[Tautology (logic)|tautology]] [[if and only if]] ''A'' and (''A'') both appear in depth 0 of ''B''. A related condition appears in some versions of [[natural deduction]]. A demonstration by calculation is often little more than:
* Invoking T13 repeatedly to eliminate redundant subformulae;
* Erasing any subformulae having the form ((''A'')''A'').
The last step of a calculation always invokes '''J1a'''.

''LoF'' includes elegant new proofs of the following standard [[metatheory]]:
* ''[[Completeness (logic)|Completeness]]'': all ''pa'' consequences are demonstrable from the initials (T17).
* ''[[axiom|Independence]]'': '''J1''' cannot be demonstrated from '''J2''' and vice versa (T18).
That [[sentential logic]] is complete is taught in every first university course in [[mathematical logic]]. But university courses in Boolean algebra seldom mention the completeness of '''2'''.

===Interpretations===
If the Marked and Unmarked states are read as the [[two-element Boolean algebra|Boolean]] values 1 and 0 (or '''True''' and '''False'''), the ''pa'' [[interpretation (logic)|interprets]] '''[[two-element Boolean algebra|2]]''' (or [[sentential logic]]). ''LoF'' shows how the ''pa'' can interpret the [[syllogism]]. Each of these [[interpretation (logic)|interpretations]] is discussed in a subsection below. Extending the ''pa'' so that it could [[interpretation (logic)|interpret]] standard [[first-order logic]] has yet to be done, but [[Charles Sanders Peirce|Peirce]]'s ''beta'' [[existential graph]]s suggest that this extension is feasible.

====Two-element Boolean algebra 2====
The ''pa'' is an elegant minimalist notation for the [[two-element Boolean algebra]] '''2'''. Let:
* One of Boolean [[meet (mathematics)|meet]] (×) or [[join (mathematics)|join]] (+) interpret [[concatenation]];
* The [[Complement (order theory)|complement]] of ''A'' interpret [[Image:Laws of Form - not a.gif]]
* 0 (1) interpret the empty Mark if meet (join) interprets [[concatenation]].
If meet (join) interprets ''AC'', then join (meet) interprets ((''A'')(''C'')). Hence the ''pa'' and '''2''' are isomorphic but for one detail: ''pa'' complementation can be nullary, in which case it denotes a primitive value. Modulo this detail, '''2''' is a [[model theory|model]] of the primary algebra. The primary arithmetic suggests the following arithmetic axiomatization of '''2''': 1+1=1+0=0+1=1=~0, and 0+0=0=~1.

The [[Set (mathematics)|set]] &lt;math&gt;\ B=\{&lt;/math&gt;[[Image:Laws of Form - cross.gif]] &lt;math&gt;,&lt;/math&gt; [[Image:Laws of Form - double cross.gif]]&lt;math&gt;\ \}&lt;/math&gt; is the [[Boolean domain]] or ''carrier''. In the language of [[universal algebra]], the ''pa'' is the [[algebraic structure]] &lt;math&gt;\lang B,--,(-),() \rang&lt;/math&gt; of type &lt;math&gt;\lang 2,1,0 \rang&lt;/math&gt;. The [[functional completeness|expressive adequacy]] of the [[Sheffer stroke]] points to the ''pa'' also being a &lt;math&gt;\lang B,(--),()\rang&lt;/math&gt; algebra of type &lt;math&gt;\lang 2,0 \rang&lt;/math&gt;. In both cases, the identities are J1a, J0, C2, and ''ACD=CDA''. Since the ''pa'' and '''2''' are [[isomorphic]], '''2''' can be seen as a &lt;math&gt;\lang B,+,\lnot,1 \rang&lt;/math&gt; algebra of type &lt;math&gt;\lang 2,1,0 \rang&lt;/math&gt;. This description of '''2''' is simpler than the conventional one, namely an &lt;math&gt;\lang B,+,\times,\lnot,1,0 \rang&lt;/math&gt; algebra of type &lt;math&gt;\lang 2,2,1,0,0 \rang&lt;/math&gt;.

====Sentential logic====
Let the blank page denote '''True''' or '''False''', and let a Cross be read as '''Not'''. Then the primary arithmetic has the following sentential reading:

:::&amp;nbsp;= &amp;nbsp; '''False'''

::[[Image:Laws of Form - cross.gif]] &amp;nbsp;= &amp;nbsp;'''True''' &amp;nbsp;= &amp;nbsp;'''not False'''

::[[Image:Laws of Form - double cross.gif]] &amp;nbsp;= &amp;nbsp;'''Not True''' &amp;nbsp;= &amp;nbsp;'''False'''

The ''pa'' interprets sentential logic as follows. A letter represents any given sentential expression. Thus:

::[[Image:Laws of Form - not a.gif]] interprets '''Not A'''

::[[Image:Laws of Form - a or b.gif]] interprets '''A Or B'''

::[[Image:Laws of Form - if a then b.gif]] interprets '''Not A Or B'''  or   '''If A Then B'''.

::[[Image:Laws of Form - a and b.gif]] interprets '''Not (Not A Or Not B)'''
:::::or '''Not (If A Then Not B)'''
:::::or '''A And B'''.

&lt;math&gt; \ (((A)B)(A(B))), ((A)(B))(AB) \ &lt;/math&gt;  both interpret '''A [[if and only if]] B''' or '''A is [[logical equivalence|equivalent]] to B'''.

Thus any expression in [[sentential logic]] has a ''pa'' translation. Equivalently, the ''pa'' [[interpretation (logic)|interprets]] sentential logic. Given an assignment of every variable to the Marked or Unmarked states, this ''pa'' translation reduces to a PA expression, which can be simplified. Repeating this exercise for all possible assignments of the two primitive values to each variable, reveals whether the original expression is [[Tautology (logic)|tautological]] or [[Satisfiability|satisfiable]]. This is an example of a [[decision procedure]], one more or less in the spirit of conventional truth tables. Given some ''pa'' formula containing ''N'' variables, this decision procedure requires simplifying 2&lt;sup&gt;''N''&lt;/sup&gt; PA formulae. For a less tedious decision procedure more in the spirit of [[Willard Van Orman Quine|Quine]]'s &quot;truth value analysis,&quot; see Meguire (2003).

Schwartz (1981) proved that the ''pa'' is equivalent -- [[syntax|syntactically]], [[semantics|semantically]], and [[proof theory|proof theoretically]]—with the [[Propositional calculus|classical propositional calculus]]. Likewise, it can be shown that the ''pa'' is syntactically equivalent with expressions built up in the usual way from the classical [[truth value]]s '''true''' and '''false''', the [[logical connective]]s NOT, OR, and AND, and parentheses.

Interpreting the Unmarked State as '''False''' is wholly arbitrary; that state can equally well be read as '''True'''. All that is required is that the interpretation of [[concatenation]] change from OR to AND. IF A THEN B now translates as (''A''(''B'')) instead of (''A'')''B''. More generally, the ''pa'' is &quot;self-[[Duality (mathematics)|dual]],&quot; meaning that any ''pa'' formula has two [[sentential logic|sentential]] or [[two-element Boolean algebra|Boolean]] readings, each the [[Duality (mathematics)|dual]] of the other. Another consequence of self-duality is the irrelevance of [[De Morgan's laws]]; those laws are built into the syntax of the ''pa'' from the outset.

The true nature of the distinction between the ''pa'' on the one hand, and '''2''' and sentential logic on the other, now emerges. In the latter formalisms, [[Logical complement|complementation]]/[[negation]] operating on &quot;nothing&quot; is not well-formed. But an empty Cross is a well-formed ''pa'' expression, denoting the Marked state, a primitive value. Hence a nonempty Cross is an [[Operator (mathematics)|operator]], while an empty Cross is an [[operand]] because it denotes a primitive value. Thus the ''pa'' reveals that the heretofore distinct mathematical concepts of operator and operand are in fact merely different facets of a single fundamental action, the making of a distinction.

====Syllogisms====
Appendix 2 of ''LoF'' shows how to translate traditional [[syllogism]]s and [[polysyllogism|sorites]] into the ''pa''. A valid syllogism is simply one whose ''pa'' translation simplifies to an empty Cross. Let ''A''* denote a ''literal'', i.e., either ''A'' or (''A''), indifferently. Then all syllogisms that do not require that one or more terms be assumed nonempty are one of 24 possible permutations of a generalization of [[syllogism|Barbara]] whose ''pa'' equivalent is (''A''*''B'')((''B'')''C''*)''A''*''C''*. These 24 possible permutations include the 19 syllogistic forms deemed valid in [[Aristotelian logic|Aristotelian]] and [[medieval logic]]. This ''pa'' translation of syllogistic logic also suggests that the ''pa'' can [[interpretation (logic)|interpret]] [[monadic logic|monadic]] and [[term logic]], and that the ''pa'' has affinities to the [[Boolean term schema]]ta of Quine (1982: Part II).

===An example of calculation===
The following calculation of [[Gottfried Wilhelm Leibniz|Leibniz]]'s nontrivial ''Praeclarum Theorema'' exemplifies the demonstrative power of the ''pa''. Let C1 be ((''A''))=''A'', and let OI mean that variables and subformulae have been reordered in a way that commutativity and associativity permit. Because the only [[commutative]] [[Logical connective|connective]] appearing in the ''Theorema'' is [[Logical conjunction|conjunction]], it is simpler to translate the ''Theorema'' into the ''pa'' using the dual interpretation. The objective then becomes one of simplifying that translation to (()).

*  [(''P''→''R'')∧(''Q''→''S'')]→[(''P''∧''Q'')→(''R''∧''S'')].  ''Praeclarum Theorema''.
*    ((''P''(''R''))(''Q''(''S''))((''PQ''(''RS'')))).  ''pa'' translation.
*= (('''P'''(''R''))''P''('''Q'''(''S''))''Q''(''RS'')).  OI; C1.
*= ((''R''))((''S''))''PQ''(''RS''). Invoke C2 2x to eliminate the bold letters in the previous expression; OI.
*= (''RSPQ''(''RS'')).  C1,2x.
*= ((''RSPQ'')''RSPQ'').  C2; OI.
*= (()).  J1.&lt;math&gt;\square&lt;/math&gt;

''Remarks'':
* C1 (C2) is repeatedly invoked in a fairly mechanical way to eliminate nested parentheses (variable instances). This is the essence of the calculation method;
* A single invocation of J1 (or, in other contexts, J1a) terminates the calculation. This too is typical;
* Experienced users of the ''pa'' are free to invoke OI silently. OI aside, the demonstration requires a mere 7 steps.

===A technical aside===
Given some standard notions from [[mathematical logic]] and some suggestions in Bostock (1997: 83, fn 11, 12), {} and  may be interpreted as the classical bivalent [[truth value]]s. Let the [[Extension (predicate logic)|extension]] of an ''n''-place [[atomic formula]] be the set of ordered ''n''-tuples of [[individual]]s that satisfy it (i.e., for which it comes out true). Let a [[sentential variable]] be a 0-place atomic formula, whose extension is a classical truth value, by definition. An ordered 2-tuple is an [[ordered pair]], whose standard ([[Kazimierz Kuratowski|Kuratowski]]'s definition) [[set theory|set theoretic]] definition is &lt;a,b&gt; = &lt;nowiki&gt;{{a},{{a,b}}&lt;/nowiki&gt;, where ''a'',''b'' are individuals. Ordered ''n''-tuples for any ''n''&gt;2 may be obtained from ordered pairs by a well-known [[recursion|recursive]] construction. [[Dana Scott]] has remarked that the extension of a sentential variable can also be seen as the empty ordered pair (ordered 0-tuple), {{},{}} =  because {''a'',''a''}={''a''} for all ''a''. Hence has the interpretation '''True'''. Reading {} as '''False''' follows naturally.

===Relation to magmas===
The ''pa'' embodies a point noted by [[Edward Vermilye Huntington|Huntington]] in 1933: [[Boolean algebra (logic)|Boolean algebra]] requires, in addition to one [[unary operation]], one, and not two, [[binary operation]]s. Hence the seldom-noted fact that Boolean algebras are [[magma (algebra)|magmas]]. (Magmas were called [[groupoid]]s until the latter term was appropriated by [[category theory]].) To see this, note that the ''pa'' is a [[commutative]]:
*[[Semigroup]] because ''pa'' juxtaposition [[Commutative property|commute]]s and [[associative property|associates]];
*[[Monoid]] with [[identity element]] (()), by virtue of '''J0'''.

[[group (mathematics)|Groups]] also require a [[unary operation]], called [[Inverse (mathematics)|inverse]], the group counterpart of [[Boolean algebra (logic)|Boolean complementation]]. Let (''a'') denote the inverse of ''a''. Let () denote the group [[identity element]]. Then groups and the ''pa'' have the same [[signature (logic)|signatures]], namely they are both 〈--,(-),()〉 algebras of type 〈2,1,0〉. Hence the ''pa'' is a [[list of algebraic structures|boundary algebra]]. The axioms for an [[abelian group]], in boundary notation, are:
* '''G1'''. ''abc'' = ''acb'' (assuming association from the left);
* '''G2'''. ()''a'' = ''a'';
* '''G3'''. (''a'')''a'' = ().
From '''G1''' and '''G2''', the commutativity and associativity of concatenation may be derived, as above. Note that '''G3''' and '''J1a''' are identical. '''G2''' and '''J0''' would be identical if (())=() replaced '''A2'''. This is the defining arithmetical identity of group theory, in boundary notation.

The ''pa'' differs from an [[abelian group]] in two ways:
*From '''A2''', it follows that (()) ≠ (). If the ''pa'' were a [[group (mathematics)|group]], (())=() would hold, and one of (''a'')''a''=(()) or ''a''()=''a'' would have to be a ''pa'' consequence. Note that () and (()) are mutual ''pa'' complements, as group theory requires, so that ((())) = () is true of both group theory and the ''pa'';
*'''C2''' most clearly demarcates the ''pa'' from other magmas, because '''C2''' enables demonstrating the [[absorption law]] that defines [[lattice (order)|lattices]], and the [[distributive law]] central to [[Boolean algebra (structure)|Boolean algebra]].
Both '''A2''' and '''C2''' follow from ''B'' 's being an [[ordered set]].

==Equations of the second degree (Chapter 11)==
Chapter 11 of ''LoF'' introduces ''equations of the second degree'', composed of [[recursion|recursive]] formulae that can be seen as having &quot;infinite&quot; depth. Some recursive formulae simplify to the marked or unmarked state. Others &quot;oscillate&quot; indefinitely between the two states depending on whether a given depth is even or odd. Specifically, certain recursive formulae can be interpreted as oscillating between '''true''' and '''false''' over successive intervals of time, in which case a formula is deemed to have an &quot;imaginary&quot; truth value. Thus the flow of time may be introduced into the ''pa''.

Turney (1986) shows how these recursive formulae can be interpreted via [[Alonzo Church]]'s Restricted Recursive Arithmetic (RRA). Church introduced RRA in 1955 as an axiomatic formalization of [[finite automata]]. Turney (1986) presents a general method for translating equations of the second degree into Church's RRA, illustrating his method using the formulae '''E1''', '''E2''', and '''E4''' in chapter 11 of ''LoF''. This translation into RRA sheds light on the names Spencer-Brown gave to '''E1''' and '''E4''', namely &quot;memory&quot; and &quot;counter&quot;. RRA thus formalizes and clarifies ''LoF'' 's notion of an imaginary truth value.

==Resonances in religion, philosophy, and science==
{{Original research|date=March 2011}}
The mathematical and logical content of ''LoF'' is wholly consistent with a [[secular humanist|secular]] point of view. Nevertheless, ''LoF'''s &quot;first distinction&quot;, and the Notes to its chapter 12, bring to mind the following landmarks in religious belief, and in philosophical and scientific reasoning, presented in rough historical order:

* [[Historical Vedic religion|Vedic]], [[Hindu]] and [[Buddhist]]:  Related ideas can be noted in the ancient [[Historical Vedic religion|Vedic]] [[Upanishads]], which form the [[Monism|monastic]] foundations of [[Hinduism]] and later [[Buddhism]]. As stated in the ''Aitareya Upanishad'' (&quot;The Microcosm of Man&quot;), the Supreme [[Atman (Buddhism)|Atman]] manifests itself as the objective [[Universe]] from one side, and as the subjective individual from the other side. In this process, things which are ''effects'' of God's creation become ''causes'' of our perceptions, by a reversal of the process. In the ''Svetasvatara Upanishad'', the core concept of Vedicism and Monism is &quot;Thou art That.&quot;
* [[Taoism]], ([[Chinese traditional religion|Chinese Traditional Religion]]):  &quot;...The Tao that can be told is not the eternal Tao; The name that can be named is not the eternal name. The nameless is the beginning of heaven and earth...&quot; ([[Tao Te Ching]]).
* [[Zoroastrianism]]:  &quot;This I ask Thee, tell me truly, Ahura. What artist made light and darkness?&quot; ([[Gathas]] 44.5)
* [[Judaism]] (from the [[Tanakh]], called [[Old Testament]] by Christians): &quot;In the beginning when God created the heavens and the earth, the earth was a formless ''void''... Then God said, 'Let there be light'; and there was light. ...God ''separated'' the light from the darkness. God called the light Day, and the darkness he called Night.
::&quot;...And God said, 'Let there be a dome in the midst of the waters, and let it ''separate'' the waters from the waters.' So God made the dome and ''separated'' the waters that were under the dome from the waters that were above the dome.
::&quot;...And God said, 'Let the waters under the sky be gathered together into one place, and let the dry land appear.' ...God called the dry land Earth, and the waters that were gathered together he called Seas.
::&quot;...And God said, 'Let there be lights in the dome of the sky to ''separate'' the day from the night...' God made the two great lights... to ''separate'' the light from the darkness.&quot; (Genesis 1:1-18; Revised Standard Version, emphasis added).
::&quot;And the whole earth was of ''one'' language, and of ''one'' speech.&quot; (Genesis 11:1; emphasis added).
::&quot;I am; that is who I am.&quot;  (Exodus 3:14)
* [[Confucianism]]: [[Confucius]] claimed that he sought &quot;a unity all pervading&quot; (''Analects'' XV.3) and that there was &quot;one single thread binding my way together.&quot; (''Ana''. IV.15). The ''Analects'' also contain the following remarkable passage on how the social, moral, and aesthetic orders are grounded in right language, grounded in turn in the ability to &quot;rectify names,&quot; i.e., to make correct distinctions: &quot;Zilu said, 'What would be master's priority?&quot; The master replied, &quot;Rectifying names! ...If names are not rectified then language will not flow. If language does not flow, then affairs cannot be completed. If affairs are not completed, ritual and music will not flourish. If ritual and music do not flourish, punishments and penalties will miss their mark. When punishments and penalties miss their mark, people lack the wherewithal to control hand and foot.&quot; (''Ana''. XIII.3)
* [[Heraclitus]]: Pre-socratic philosopher, credited with forming the idea of [[logos]].  &quot;He who hears not me but the ''logos'' will say: ''All is one''.&quot;  Further:  &quot;I am as I am not.&quot;
* [[Parmenides]]: Argued that the every-day perception of reality of the physical world is mistaken, and that the reality of the world is 'One Being': an unchanging, ungenerated, indestructible whole.
* [[Plato]]: ''Logos'' is also a fundamental technical term in the Platonic worldview.
* [[Christianity]]: &quot;In the Beginning was the Word, and the Word was with God, and the Word was God.&quot; (John 1:1). &quot;Word&quot; translates [[logos]] in the [[koine]] original. &quot;If you do not believe ''that I am'', you will die in your sins.&quot; (John 8:24).  &quot;The Father and I are ''one''.&quot; (John 10:30). &quot;That they all may be ''one''; as thou, Father, art in me, and I in thee, that they may also be ''one'' in us: that the world may believe that thou has sent me.&quot; (John 17:21). (emphases added)
* [[Object relations theory]], [[psychodynamics]]: The primary separation experienced by infants between self and other objects, distinguishing of reality from phantasy.
* [[Islamic philosophy]] distinguishes essence (''Dhat'') from attribute (''Sifat''), which are neither identical nor separate.
* [[Gottfried Wilhelm Leibniz|Leibniz]]: &quot;All creatures derive from God and from nothingness. Their self-being is of God, their nonbeing is of nothing. Numbers too show this in a wonderful way, and the essences of things are like numbers. No creature can be without nonbeing; otherwise it would be God... The only self-knowledge is to distinguish well between our self-being and our nonbeing... Within our selfbeing there lies an infinity, a footprint or reflection of the omniscience and omnipresence of God.&quot;&lt;ref&gt;&quot;On the True ''Theologia Mystica''&quot; in Loemker, Leroy, ed. and trans., 1969. ''Leibniz: Philosophical Papers and Letters''. Reidel: 368.&lt;/ref&gt;
* [[Josiah Royce]]: &quot;Without negation, there is no inference. Without inference, there is no order, in the strictly logical sense of the word. The fundamentally significant position of the idea of negation in determining and controlling our idea of the orderliness of both the natural and the spiritual order, becomes, in the light of all these considerations, as momentous as it is, in our ordinary popular views of this subject, neglected. ...From this point of view, negation appears as one of the most significant. ideas that lie at the base of all the exact sciences. By virtue of the idea of negation we are able to define processes of inference-processes which, in their abstract form, the purely mathematical sciences illustrate, and which, in their natural expression, the laws of the physical world, as known to our inductive science, exemplify.&quot;
:&quot;When logically analyzed, order turns out to be something that would be inconceivable and incomprehensible to us unless we had the idea which is expressed by the term 'negation'. Thus it is that negation, which is always also something intensely positive, not only aids us in giving order to life, and in finding order in the world, but logically determines the very essence of order.&quot; &lt;ref&gt;&quot;Order&quot; in Hasting, J., ed., 1917. ''[[Encyclopedia of Religion and Ethics]]''. Scribner's: 540. Reprinted in Robinson, D. S., ed., 1951, ''Royce's Logical Essays''. Dubuque IA: Wm. C. Brown: 230-31.&lt;/ref&gt;

Returning to the Bible, the injunction &quot;Let there be light&quot; conveys:
* &quot;... and there was light&quot;&amp;nbsp;— the light itself;
* &quot;... called the light Day&quot;&amp;nbsp;— the manifestation of the light;
* &quot;... morning and evening&quot;&amp;nbsp;— the boundaries of the light.

A Cross denotes a distinction made, and the absence of a Cross means that no distinction has been made. In the Biblical example, light is distinct from the void&amp;nbsp;– the absence of light. The Cross and the Void are, of course, the two primitive values of the Laws of Form.

==Related work==
[[Gottfried Leibniz]], in memoranda not published before the late 19th and early 20th centuries, invented [[Boolean algebra (logic)|Boolean logic]]. His notation was isomorphic to that of ''LoF'': concatenation read as [[Logical conjunction|conjunction]], and &quot;non-(''X'')&quot; read as the [[Logical complement|complement]] of ''X''. Leibniz's pioneering role in [[algebraic logic]] was foreshadowed by [[Clarence Irving Lewis|Lewis]] (1918) and [[Nicholas Rescher|Rescher]] (1954). But a full appreciation of Leibniz's accomplishments had to await the work of Wolfgang Lenzen, published in the 1980s and reviewed in [http://www.philosophie.uni-osnabrueck.de/Publikationen%20Lenzen/Lenzen%20Leibniz%20Logic.pdf Lenzen (2004).]

[[Charles Sanders Peirce]] (1839–1914) anticipated the ''pa'' in three veins of work:
#Two papers he wrote in 1886 proposed a logical algebra employing but one symbol, the ''streamer'', nearly identical to the Cross of ''LoF''.  The semantics of the streamer are identical to those of the Cross, except that Peirce never wrote a streamer with nothing under it. An excerpt from one of these papers was published in 1976,&lt;ref&gt;&quot;Qualitative Logic&quot;, MS 736 (c. 1886) in Eisele, Carolyn, ed. 1976. ''[[Charles Sanders Peirce bibliography#NEM|The New Elements of Mathematics by Charles S. Peirce]]. Vol. 4, Mathematical Philosophy''. (The Hague) Mouton: 101-15.1&lt;/ref&gt; but they were not published in full until 1993.&lt;ref&gt;&quot;Qualitative Logic&quot;, MS 582 (1886) in Kloesel, Christian et al., eds., 1993. ''[[Charles Sanders Peirce bibliography#W|Writings of Charles S. Peirce: A Chronological Edition]], Vol. 5, 1884-1886''. [[Indiana University Press]]: 323-71. &quot;The Logic of Relatives: Qualitative and Quantitative&quot;, MS 584 (1886) in Kloesel, Christian et al., eds., 1993. ''Writings of Charles S. Peirce: A Chronological Edition, Vol. 5, 1884-1886''. Indiana University Press: 372-78.&lt;/ref&gt;
#In a 1902 encyclopedia article,&lt;ref&gt;Reprinted in Peirce, C.S. (1933) ''[[Charles Sanders Peirce bibliography#CP|Collected Papers of Charles Sanders Peirce]], Vol. 4'', [[Charles Hartshorne]] and [[Paul Weiss (philosopher)|Paul Weiss]], eds. [[Harvard University Press]]. Paragraphs 378-383&lt;/ref&gt; Peirce notated Boolean algebra and sentential logic in the manner of this entry, except that he employed two styles of brackets, toggling between '(', ')' and '[', ']' with each increment in formula depth.
#The [[syntax]] of his alpha [[existential graph]]s is merely [[concatenation]], read as [[Logical conjunction|conjunction]], and enclosure by ovals, read as [[negation]].&lt;ref&gt;The existential graphs are described at length in Peirce, C.S. (1933) ''Collected Papers, Vol. 4'', [[Charles Hartshorne]] and [[Paul Weiss (philosopher)|Paul Weiss]], eds. Harvard University Press. Paragraphs 347-529.&lt;/ref&gt; If ''pa'' concatenation is read as [[Logical conjunction|conjunction]], then these graphs are [[isomorphic]] to the ''pa'' [http://www2.math.uic.edu/~kauffman/CHK.pdf (Kauffman 2001).]
Ironically, ''LoF'' cites vol. 4 of Peirce's ''Collected Papers,'' the source for the formalisms in (2) and (3) above.
(1)-(3) were virtually unknown at the time when (1960s) and in the place where (UK) ''LoF'' was written. Peirce's [[semiotics]], about which ''LoF'' is silent, may yet shed light on the philosophical aspects of ''LoF''.

[http://www2.math.uic.edu/~kauffman/CHK.pdf Kauffman  (2001)] discusses another notation similar to that of ''LoF'', that of a 1917 article by [[Jean Nicod]], who was a disciple of [[Bertrand Russell]]'s.

The above formalisms are, like the ''pa'', all instances of ''boundary mathematics'', i.e., mathematics whose syntax is limited to letters and brackets (enclosing devices). A minimalist syntax of this nature is a &quot;boundary notation.&quot; Boundary notation is free of [[infix]], [[Polish notation|prefix]], or [[Reverse Polish notation|postfix]] operator symbols. The very well known curly braces ('{', '}') of set theory can be seen as a boundary notation.

The work of Leibniz, Peirce, and Nicod is innocent of metatheory, as they wrote before [[Emil Post]]'s landmark 1920 paper (which ''LoF'' cites), proving that [[sentential logic]] is complete, and before [[David Hilbert|Hilbert]] and [[Jan Łukasiewicz|Lukasiewicz]] showed how to prove [[axiom independence]] using [[model theory|model]]s.

[[William Craig (logician)|Craig]] (1979) argued that the world, and how humans perceive and interact with that world, has a rich Boolean structure. Craig was an orthodox logician and an authority on [[algebraic logic]].

Second-generation [[cognitive science]] emerged in the 1970s, after ''LoF'' was written. On cognitive science and its relevance to Boolean algebra, logic, and [[set theory]], see Lakoff (1987) (see index entries under &quot;Image schema examples: container&quot;) and Lakoff and Núñez (2001). Neither book cites ''LoF''.

The biologists and cognitive scientists [[Humberto Maturana]] and his student [[Francisco Varela]] both discuss ''LoF'' in their writings, which identify &quot;distinction&quot; as the fundamental cognitive act. The Berkeley psychologist and cognitive scientist [[Eleanor Rosch]] has written extensively on the closely related notion of categorization.

The [http://multiforms.netfirms.com Multiple Form Logic], by G.A. Stathis, &quot;generalises [the primary algebra] into Multiple Truth Values&quot; so as to be &quot;more consistent with Experience.&quot; Multiple Form Logic, which is ''not'' a boundary formalism, employs two primitive [[binary operation]]s: [[concatenation (mathematics)|concatenation]], read as Boolean [[logical disjunction|OR]], and [[infix]] &quot;#&quot;, read as [[XOR]]. The primitive values are 0 and 1, and the corresponding arithmetic is 11=1 and 1#1=0. The axioms are 1A=1, A#X#X = A, and A(X#(AB)) = A(X#B).

Other formal systems with possible affinities to the primary algebra include:
*[[Mereology]] which typically has a [[lattice (order)|lattice]] structure very similar to that of Boolean algebra. For a few authors, mereology is simply a [[model theory|model]] of [[Boolean algebra (structure)|Boolean algebra]] and hence of the primary algebra as well.
*[[Mereotopology]], which is inherently richer than Boolean algebra;
*The system of  Whitehead (1934), whose fundamental primitive is &quot;indication.&quot;

The primary arithmetic and algebra are a minimalist formalism for [[sentential logic]] and Boolean algebra. Other minimalist formalisms having the power of [[set theory]] include:
* The [[lambda calculus]];
* [[Combinatory logic]] with two ('''S''' and '''K''') or even one ('''X''') primitive combinators;
* [[Mathematical logic]] done with merely three primitive notions: one connective, [[Sheffer stroke|NAND]] (whose ''pa'' translation is (''AB'') or—dually -- (''A'')(''B'') ), universal [[quantification]], and one [[binary relation|binary]] [[atomic formula]], denoting [[Set (mathematics)|set]] membership. This is the system of [[Willard Quine|Quine]] (1951).
* The ''beta'' [[existential graph]]s, with a single [[binary predicate]] denoting set membership. This has yet to be explored. The ''alpha'' graphs mentioned above are a special case of the ''beta'' graphs.

==See also==
* [[:simple:Boolean algebra|Boolean algebra]] (Simple English Wikipedia)
*[[Boolean algebra (introduction)]]
*[[Boolean algebra (logic)]]
*[[Boolean algebra (structure)]]
*[[Boolean algebras canonically defined]]
*[[Boolean logic]]
*[[Entitative graph]]
*[[Existential graph]]
*[[List of Boolean algebra topics]]
*[[Propositional calculus]]
*[[Two-element Boolean algebra]]

==Notes==
{{Reflist}}

==References==
*Editions of ''Laws of Form'':
**1969. London: Allen &amp; Unwin, hardcover.
**1972. Crown Publishers, hardcover: ISBN 0-517-52776-6
**1973. Bantam Books, paperback. ISBN 0-553-07782-1
**1979. E.P. Dutton, paperback. ISBN 0-525-47544-3
**1994. Portland OR: Cognizer Company, paperback. ISBN 0-9639899-0-1
**1997 German translation, titled ''Gesetze der Form''. Lübeck: Bohmeier Verlag. ISBN 3-89094-321-7

*Bostock, David, 1997. ''Intermediate Logic''. Oxford Univ. Press.
*Byrne, Lee, 1946, &quot;Two Formulations of Boolean Algebra,&quot; ''Bulletin of the American Mathematical Society'': 268-71.
*{{Cite journal | doi = 10.2307/3131383 | last1 = Craig | first1 = William | year = 1979 | title = Boolean Logic and the Everyday Physical World | jstor = 3131383| journal = Proceedings and Addresses of the American Philosophical Association | volume = 52 | issue = 6| pages = 751–78 }}
* [[David Gries]], and Schneider, F B, 1993. ''A Logical Approach to Discrete Math''. Springer-Verlag.
*[[William Ernest Johnson]], 1892, &quot;The Logical Calculus,&quot; ''Mind'' 1 (n.s.): 3-30.
* [http://www.math.uic.edu/~kauffman/ Louis H. Kauffman], 2001, &quot;[http://www2.math.uic.edu/~kauffman/CHK.pdf The Mathematics of C.S. Peirce]&quot;, ''Cybernetics and Human Knowing'' 8: 79-110.
* ------, 2006, &quot;[http://www.arxiv.org/math.CO/0112266 Reformulating the Map Color Theorem.]&quot;
* ------, 2006a. &quot;[http://www.math.uic.edu/~kauffman/Laws.pdf Laws of Form - An Exploration in Mathematics and Foundations.]&quot; Book draft (hence big).
* Lenzen, Wolfgang, 2004, &quot;[http://www.philosophie.uni-osnabrueck.de/Publikationen%20Lenzen/Lenzen%20Leibniz%20Logic.pdf Leibniz's Logic]&quot; in Gabbay, D., and Woods, J., eds., ''The Rise of Modern Logic: From Leibniz to Frege (Handbook of the History of Logic&amp;nbsp;– Vol. 3)''. Amsterdam: Elsevier, 1-83.
*[[George Lakoff|Lakoff, George]], 1987. ''Women, Fire, and Dangerous Things''. University of Chicago Press.
*-------- and [[Rafael E. Núñez]], 2001. ''[[Where Mathematics Comes From]]: How the Embodied Mind Brings Mathematics into Being''. Basic Books.
* Meguire, P. G., 2003, &quot;Discovering Boundary Algebra: A Simplified Notation for Boolean Algebra and the Truth Functors,&quot; ''International Journal of General Systems'' 32: 25-87.
*--------, 2011. ''Boundary Algebra: A Simpler Approach to Basic Logic and Boolean Algebra''. VDM Publishing Ltd. IBSN 978-3639367492. The source for much of this entry, including the notation which encloses in parentheses what ''LoF'' places under a cross. Steers clear of the more speculative aspects of ''LoF''.
*[[Willard Quine]], 1951. ''Mathematical Logic'', 2nd ed. Harvard University Press.
*--------, 1982. ''Methods of Logic'', 4th ed. Harvard University Press.
* {{Cite journal | last1 = Rescher | first1 = Nicholas | authorlink = Nicholas Rescher | year = 1954 | title = Leibniz's Interpretation of His Logical Calculi | url = | journal = Journal of Symbolic Logic | volume = 18 | issue = | pages = 1–13 }}
* {{Cite journal | doi = 10.1080/03081078108934802 | last1 = Schwartz | first1 = Daniel G. | year = 1981 | title = [[Isomorphism]]s of [[G. Spencer-Brown]]'s ''Laws of Form'' and [[Francisco Varela|F. Varela's]] Calculus for Self-Reference | url = | journal = International Journal of General Systems | volume = 6 | issue = 4| pages = 239–55 }}
* {{Cite journal | doi = 10.1080/03081078608934939 | last1 = Turney | first1 = P. D. | year = 1986 | title = ''Laws of Form'' and Finite Automata | url = | journal = International Journal of General Systems | volume = 12 | issue = 4| pages = 307–18 }}
*[[A. N. Whitehead]], 1934, &quot;Indication, classes, number, validation,&quot; ''Mind'' 43 (n.s.): 281-97, 543. The corrigenda on p.&amp;nbsp;543 are numerous and important, and later reprints of this article do not incorporate them.

==External links==
* ''Laws of Form'' [http://www.lawsofform.org/ web site], by Richard Shoup.
* [http://www.lawsofform.org/aum/session1.html Spencer-Brown's talks at Esalen, 1973.] Self-referential forms are introduced in the section entitled &quot;Degree of Equations and the Theory of Types.&quot;
* [http://www.math.uic.edu/~kauffman/ Louis H. Kauffman,] &quot;[http://www.math.uic.edu/~kauffman/Arithmetic.htm Box Algebra, Boundary Mathematics, Logic, and Laws of Form.]&quot;
* Kissel, Matthias, &quot;[http://web.archive.org/web/20070310071916/http://de.geocities.com/matthias_kissel/gdf/LoF.html  A nonsystematic but easy to understand introduction to ''Laws of Form''.]&quot;
* The [http://multiforms.netfirms.com Multiple Form Logic], by G.A. Stathis, owes much to the primary algebra.
* The [http://groups.yahoo.com/group/lawsofform Laws of Form Forum], where the primary algebra and related formalisms have been discussed since 2002.

* A meeting  [http://www.omath.org.il/112431/4CT with G.S.B] by Moshe Klein
{{Use dmy dates|date=September 2010}}

{{DEFAULTSORT:Laws Of Form}}
[[Category:Algebra]]
[[Category:Boolean algebra]]
[[Category:Logic books]]
[[Category:Logical calculi]]
[[Category:Mathematical logic]]</text>
      <sha1>ezqbw4225z0dr69p60yc9entwixx7sc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Permanent</title>
    <ns>0</ns>
    <id>215889</id>
    <revision>
      <id>606189309</id>
      <parentid>606189124</parentid>
      <timestamp>2014-04-28T15:15:06Z</timestamp>
      <contributor>
        <username>Boriaj</username>
        <id>14665977</id>
      </contributor>
      <comment>/* See also */ Reverted a previous change: eliminated link to MMT since it was included somewhere else</comment>
      <text xml:space="preserve" bytes="22971">{{other uses}}
The '''permanent''' of a [[square matrix]] in [[linear algebra]] is a function of the matrix similar to the [[determinant]]. The permanent, as well as the determinant, is a polynomial in the entries of the matrix. Both permanent and determinant are special cases of a more general function of a matrix called the [[immanant]].

== Definition ==
The permanent of an ''n''-by-''n'' matrix ''A'' = (''a''&lt;sub&gt;''i,j''&lt;/sub&gt;) is defined as

: &lt;math&gt; \operatorname{perm}(A)=\sum_{\sigma\in S_n}\prod_{i=1}^n a_{i,\sigma(i)}.&lt;/math&gt;

The sum here extends over all elements σ of the [[symmetric group]] ''S''&lt;sub&gt;''n''&lt;/sub&gt;; i.e. over all [[permutation]]s of the numbers 1, 2, ..., ''n''.

For example,

:&lt;math&gt;\operatorname{perm}\begin{pmatrix}a&amp;b \\ c&amp;d\end{pmatrix}=ad+bc,&lt;/math&gt;

and

:&lt;math&gt;\operatorname{perm}\begin{pmatrix}a&amp;b&amp;c \\ d&amp;e&amp;f \\ g&amp;h&amp;i \end{pmatrix}=aei + bfg + cdh + ceg + bdi + afh.&lt;/math&gt;

The definition of the permanent of ''A'' differs from that of the [[determinant]] of ''A'' in that the [[signature (permutation)|signatures]] of the permutations are not taken into account. 

The permanent of a matrix A is denoted per ''A'', perm ''A'', or Per ''A'', sometimes with parentheses around the argument. In his monograph, {{harvtxt|Minc|1984}} uses Per(''A'') for the permanent of rectangular matrices, and uses per(''A'') when ''A'' is a square matrix. {{harvtxt|Muir|1882}} uses the notation &lt;math&gt;\overset{+}{|}\quad \overset{+}{|}&lt;/math&gt;.

The word, ''permanent'', originated with Cauchy in 1812 as “fonctions symétriques permanentes” for a related type of function,&lt;ref&gt;{{Citation| last=Cauchy | first=A. L.| title=Mémoire sur les fonctions qui ne peuvent obtenir que deux valeurs égales et de signes contraires par suite des transpositions opérées entre les variables qu’elles renferment. |url=http://gallica.bnf.fr/ark:/12148/bpt6k90193x/f97 |journal=Journal de l'École Polytechnique |volume=10 |pages=91–169 |year=1815}}&lt;/ref&gt; and was used by {{harvtxt|Muir|1882}} in the modern, more specific, sense.&lt;ref&gt;{{harvnb|van Lint|Wilson|2001|loc=p. 108}}&lt;/ref&gt;

== Properties and applications ==
If one views the permanent as a map that takes ''n'' vectors as arguments, then it is a [[multilinear map]] and it is symmetric (meaning that any order of the vectors results in the same permanent). Furthermore, given a square matrix &lt;math&gt;A = (a_{ij})&lt;/math&gt; of order ''n'', we have:&lt;ref&gt;{{harvnb|Ryser|1963|loc=pp. 25 &amp;ndash; 26}}&lt;/ref&gt;
* perm(''A'') is invariant under arbitrary permutations of the rows and/or columns of ''A''. This property may be written symbolically as perm(''A'') = perm(''PAQ'') for any appropriately sized [[permutation matrix|permutation matrices]] ''P'' and ''Q'',
* multiplying any single row or column of ''A'' by a [[Scalar (mathematics)|scalar]] ''s'' changes perm(''A'') to ''s''⋅perm(''A''),
* perm(''A'') is invariant under [[Transposition (mathematics)|transposition]], that is, perm(''A'') = perm(''A''&lt;sup&gt;⊤&lt;/sup&gt;).

If &lt;math&gt;A = (a_{ij})&lt;/math&gt; and &lt;math&gt;B=(b_{ij})&lt;/math&gt; are square matrices of order ''n'' then,&lt;ref&gt;{{harvnb|Percus|1971|loc=p. 2}}&lt;/ref&gt;
:&lt;math&gt;\operatorname{perm}(A + B) = \sum_{s,t} \operatorname{perm} (a_{ij})_{i \in s, j \in t} \operatorname{perm} (b_{ij})_{i \in \bar{s}, j \in \bar{t}},&lt;/math&gt;
where ''s'' and ''t'' are subsets of the same size of {1,2,...,''n''} and &lt;math&gt;\bar{s}, \bar{t}&lt;/math&gt; are their respective complements in that set.

On the other hand, the basic multiplicative property of determinants is not valid for permanents.&lt;ref&gt;{{harvnb|Ryser|1963|loc=p. 26}}&lt;/ref&gt; A simple example shows that this is so.
:&lt;math&gt; 4 = \operatorname{perm} \left ( \begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \right )\operatorname{perm} \left ( \begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \right ) \neq \operatorname{perm}\left ( \left ( \begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \right ) \left ( \begin{matrix} 1 &amp; 1 \\ 1 &amp; 1 \end{matrix} \right ) \right ) = \operatorname{perm} \left ( \begin{matrix} 2 &amp; 2 \\ 2 &amp; 2 \end{matrix} \right )= 8.&lt;/math&gt;

A formula similar to [[Expansion by minors|Laplace's]] for the development of a determinant along a row, column or diagonal is also valid for the permanent;&lt;ref&gt;{{harvnb|Percus|1971|loc=p. 12}}&lt;/ref&gt; all signs have to be ignored for the permanent. For example, expanding along the first column,
:&lt;math&gt;\operatorname{perm} \left ( \begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1\\2 &amp; 1 &amp; 0 &amp; 0\\3 &amp; 0 &amp; 1 &amp; 0\\4 &amp; 0 &amp; 0 &amp; 1 \end{matrix} \right ) = 1 \cdot \operatorname{perm} \left(\begin{matrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{matrix}\right) + 2\cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\0&amp;1&amp;0\\0&amp;0&amp;1\end{matrix}\right) +3\cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\1&amp;0&amp;0\\0&amp;0&amp;1\end{matrix}\right) + 4 \cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\1&amp;0&amp;0\\0&amp;1&amp;0\end{matrix}\right)= 1(1) + 2(1) + 3(1) + 4(1) = 10, &lt;/math&gt; 
while expanding along the last row gives,
:&lt;math&gt;\operatorname{perm} \left ( \begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1\\2 &amp; 1 &amp; 0 &amp; 0\\3 &amp; 0 &amp; 1 &amp; 0\\4 &amp; 0 &amp; 0 &amp; 1 \end{matrix} \right ) = 4 \cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\1&amp;0&amp;0\\0&amp;1&amp;0\end{matrix}\right) + 0\cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\2&amp;0&amp;0\\3&amp;1&amp;0\end{matrix}\right) +0\cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\2&amp;1&amp;0\\3&amp;0&amp;0\end{matrix}\right) + 1 \cdot \operatorname{perm} \left(\begin{matrix}1&amp;1&amp;1\\2&amp;1&amp;0\\3&amp;0&amp;1\end{matrix}\right)= 4(1) + 0 + 0 + 1(6) = 10. &lt;/math&gt; 
 
Unlike the determinant, the permanent has no easy geometrical interpretation; it is mainly used in [[combinatorics]] and in treating boson [[Green's functions]] in [[quantum field theory]]. However, it has two [[graph-theoretic]] interpretations: as the sum of weights of [[Vertex cycle cover|cycle cover]]s of a [[directed graph]], and as the sum of weights of perfect matchings in a [[bipartite graph]].

===Cycle covers===
Any square matrix &lt;math&gt;A = (a_{ij})&lt;/math&gt; can be viewed as the [[adjacency matrix]] of a weighted directed graph, with &lt;math&gt;a_{ij}&lt;/math&gt; representing the weight of the arc from vertex ''i'' to vertex ''j''. 
A [[Vertex cycle cover|cycle cover]] of a weighted directed graph is a collection of vertex-disjoint [[directed cycle]]s in the digraph that covers all vertices in the graph. Thus, each vertex ''i'' in the digraph has a unique &quot;successor&quot; &lt;math&gt;\sigma(i)&lt;/math&gt; in the cycle cover, and &lt;math&gt;\sigma&lt;/math&gt; is a [[permutation]] on &lt;math&gt;\{1,2,\dots,n\}&lt;/math&gt; where ''n'' is the number of vertices in the digraph. Conversely, any permutation &lt;math&gt;\sigma&lt;/math&gt; on &lt;math&gt;\{1,2,\dots,n\}&lt;/math&gt; corresponds to a cycle cover in which there is an arc from vertex ''i'' to vertex &lt;math&gt;\sigma(i)&lt;/math&gt; for each ''i''.
 
If the weight of a cycle-cover is defined to be the product of the weights of the arcs in each cycle, then
: &lt;math&gt; \operatorname{Weight}(\sigma) = \prod_{i=1}^n a_{i,\sigma(i)}.&lt;/math&gt;
The permanent of an &lt;math&gt;n \times n&lt;/math&gt; matrix ''A'' is defined as
:&lt;math&gt; \operatorname{perm}(A)=\sum_\sigma \prod_{i=1}^{n} a_{i,\sigma(i)}&lt;/math&gt;
where &lt;math&gt;\sigma&lt;/math&gt; is a permutation over &lt;math&gt;\{1,2,\dots,n\}&lt;/math&gt;. Thus the permanent of ''A'' is equal to the sum of the weights of all cycle-covers of the digraph.

===Perfect matchings===
A square matrix &lt;math&gt;A = (a_{ij})&lt;/math&gt; can also be viewed as the [[adjacency matrix]] of a [[bipartite graph]] which has [[vertex (graph theory)|vertices]] &lt;math&gt;x_1, x_2, \dots, x_n&lt;/math&gt; on one side and &lt;math&gt;y_1, y_2, \dots, y_n&lt;/math&gt; on the other side, with &lt;math&gt;a_{ij}&lt;/math&gt; representing the weight of the edge from vertex &lt;math&gt;x_i&lt;/math&gt; to vertex &lt;math&gt;y_j&lt;/math&gt;. If the weight of a [[perfect matching]] &lt;math&gt;\sigma&lt;/math&gt; that matches &lt;math&gt;x_i&lt;/math&gt; to &lt;math&gt;y_{\sigma(i)}&lt;/math&gt; is defined to be the product of the weights of the edges in the matching, then
:&lt;math&gt; \operatorname{Weight}(\sigma) = \prod_{i=1}^n a_{i,\sigma(i)}.&lt;/math&gt;
Thus the permanent of ''A'' is equal to the sum of the weights of all perfect matchings of the graph.

== Permanents of (0,1) matrices ==
The permanents of matrices that only have 0 and 1 as entries are often the answers to certain counting questions involving the structures that the matrices represent. This is particularly true of [[Adjacency matrix|adjacency matrices]] in graph theory and [[Incidence matrix|incidence matrices]] of [[Block design#Symmetric BIBDs|symmetric block designs]].

In an unweighted, directed, simple graph (a ''digraph''), if we set each &lt;math&gt;a_{ij}&lt;/math&gt; to be 1 if there is an edge from vertex ''i'' to vertex ''j'', then each nonzero cycle cover has weight 1, and the adjacency matrix has 0-1 entries. Thus the permanent of a (0,1)-matrix is equal to the ''number'' of [[vertex cycle cover]]s of an unweighted directed graph.

For an unweighted bipartite graph, if we set ''a''&lt;sub&gt;''i,j''&lt;/sub&gt; = 1 if there is an [[graph theory|edge]] between the vertices &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;y_j&lt;/math&gt; and ''a''&lt;sub&gt;''i,j''&lt;/sub&gt; = 0 otherwise, then each [[perfect matching]] has weight 1. Thus the number of perfect matchings in ''G'' is equal to the permanent of matrix ''A''.&lt;ref name=&quot;Kozen&quot;&gt;Dexter Kozen. [http://books.google.com/books?id=L_AMnf9UF9QC&amp;pg=PA141&amp;dq=%22permanent+of+a+matrix%22+valiant&amp;as_brr=3&amp;ei=h8BKScClJYOUMtTP6LEO#PPA142,M1 ''The Design and Analysis of Algorithms.''] [[Springer-Verlag]], New York, 1991.
ISBN 978-0-387-97687-7;  pp. 141&amp;ndash;142&lt;/ref&gt;

Let Ω(''n'',''k'') be the class of all (0,1)-matrices of order ''n'' with each row and column sum equal to ''k''. Every matrix ''A'' in this class has perm(''A'') &gt; 0.&lt;ref name=&quot;Ryser 1963 loc=p. 124&quot;&gt;{{harvnb|Ryser|1963|loc=p. 124}}&lt;/ref&gt; The incidence matrices of [[projective plane]]s are in the class Ω(''n''&lt;sup&gt;2&lt;/sup&gt; + ''n'' + 1, ''n'' + 1) for ''n'' an integer &gt; 1. The permanents corresponding to the smallest projective planes have been calculated. For ''n'' = 2, 3, and 4 the values are 24, 3852 and 18,534,400 respectively.&lt;ref name=&quot;Ryser 1963 loc=p. 124&quot;/&gt; Let ''Z'' be the incidence matrix of the projective plane with ''n'' = 2, the [[Fano plane]]. Remarkably, perm(''Z'') = 24 = |det (''Z'')|, the absolute value of the determinant of ''Z''. This is a consequence of ''Z'' being a [[circulant matrix]] and the theorem:&lt;ref&gt;{{harvnb|Ryser|1963|loc=p. 125}}&lt;/ref&gt;

:If ''A'' is a circulant matrix in the class Ω(''n'',''k'') then if ''k''&amp;nbsp;&gt;&amp;nbsp;3, perm(''A'')&amp;nbsp;&gt;&amp;nbsp;|det (''A'')| and if ''k''&amp;nbsp;=&amp;nbsp;3, perm(''A'')&amp;nbsp;=&amp;nbsp;|det (''A'')|. Furthermore, when ''k''&amp;nbsp;=&amp;nbsp;3, by permuting rows and columns, ''A'' can be put into the form of a direct sum of ''e'' copies of the matrix ''Z'' and consequently, ''n''&amp;nbsp;=&amp;nbsp;7''e'' and perm(''A'')&amp;nbsp;=&amp;nbsp;24&lt;sup&gt;e&lt;/sup&gt;.

Permanents can also be used to calculate the number of [[permutation]]s with restricted (prohibited) positions. For the standard ''n''-set, {1,2,...,''n''}, let &lt;math&gt;A = (a_{ij})&lt;/math&gt; be the (0,1)-matrix where ''a''&lt;sub&gt;''ij''&lt;/sub&gt; = 1 if ''i''&amp;nbsp;→&amp;nbsp;''j'' is allowed in a permutation and ''a''&lt;sub&gt;''ij''&lt;/sub&gt; = 0 otherwise. Then perm(''A'') counts the number of permutations of the ''n''-set which satisfy all the restrictions.&lt;ref&gt;{{harvnb|Percus|1971|loc=p. 12}}&lt;/ref&gt; Two well known special cases of this are the solution of the [[derangement]] problem (the number of permutations with no fixed points) given by:

::&lt;math&gt;\operatorname{perm}(J - I) = \operatorname{perm}\left (\begin{matrix} 0 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\ 1 &amp; 0 &amp; 1 &amp; \dots &amp; 1 \\ 1 &amp; 1 &amp; 0 &amp; \dots &amp; 1 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; 1 &amp; 1 &amp; \dots &amp; 0 \end{matrix} \right) = n! \sum_{i=0}^n \frac{(-1)^i}{i!},&lt;/math&gt;
where ''J'' is the all 1's matrix and ''I'' is the identity matrix, each of order ''n'', and the solution to the [[ménage problem]] given by:

::&lt;math&gt;\operatorname{perm}(J - I - I') = \operatorname{perm}\left (\begin{matrix} 0 &amp; 0 &amp; 1 &amp; \dots &amp; 1 \\ 1 &amp; 0 &amp; 0 &amp; \dots &amp; 1 \\ 1 &amp; 1 &amp; 0 &amp; \dots &amp; 1 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 1 &amp; 1 &amp; \dots &amp; 0 \end{matrix} \right) = 2 \cdot n! \sum_{k=0}^n (-1)^k \frac{2n}{2n-k} {2n-k\choose k} (n-k)!,&lt;/math&gt;
where ''I''' is the (0,1)-matrix whose only non-zero entries are on the first superdiagonal.

The following result was conjectured by H. Minc in 1967&lt;ref&gt;{{citation|first=H.|last=Minc|title=An inequality for permanents of (0,1) matrices|journal=Journal of Combinatorial Theory|year=1967|volume=2|pages=321-326}}&lt;/ref&gt; and proved by L. M. Brégman in 1973.&lt;ref&gt;{{harvnb|van Lint|Wilson|2001|loc=p. 101}}&lt;/ref&gt;

''Theorem'': Let ''A'' be an ''n'' × ''n'' (0,1)-matrix with ''r''&lt;sub&gt;''i''&lt;/sub&gt; ones in row ''i'', 1 ≤ ''i'' ≤ ''n''. Then
::&lt;math&gt;\operatorname{perm} A \leq \prod_{i=1}^n (r_i)!^{1/r_i}.&lt;/math&gt;

== Van der Waerden's conjecture ==
In 1926 [[Bartel Leendert van der Waerden|Van der Waerden]] conjectured that the minimum permanent among all {{nowrap|''n'' &amp;times; ''n''}} [[doubly stochastic matrix|doubly stochastic matrices]] is &lt;math&gt;n!/n^n&lt;/math&gt;, achieved by the matrix for which all entries are equal to &lt;math&gt;1/n&lt;/math&gt;.&lt;ref&gt;{{citation
 | last = van der Waerden | first = B. L. | author-link = Bartel Leendert van der Waerden
 | journal = Jber. Deutsch. Math.-Verein.
 | page = 117
 | title = Aufgabe 45
 | volume = 35
 | year = 1926}}.&lt;/ref&gt; Proofs of this conjecture were published in 1980 by B. Gyires&lt;ref&gt;{{citation
 | last = Gyires | first = B.
 | issue = 3-4
 | journal = Publicationes Mathematicae Institutum Mathematicum Universitatis Debreceniensis
 | mr = 604006
 | pages = 291–304
 | title = The common source of several inequalities concerning doubly stochastic matrices
 | volume = 27
 | year = 1980}}.&lt;/ref&gt; and in 1981 by G. P. Egorychev&lt;ref&gt;{{citation
 | last = Egoryčev | first = G. P.
 | language = Russian
 | location = Krasnoyarsk
 | mr = 602332
 | page = 12
 | publisher = Akad. Nauk SSSR Sibirsk. Otdel. Inst. Fiz.
 | title = Reshenie problemy van-der-Vardena dlya permanentov
 | year = 1980}}. {{citation
 | last = Egorychev | first = G. P.
 | issue = 6
 | journal = Akademiya Nauk SSSR
 | language = Russian
 | mr = 638007
 | pages = 65–71, 225
 | title = Proof of the van der Waerden conjecture for permanents
 | volume = 22
 | year = 1981}}. {{citation
 | last = Egorychev | first = G. P.
 | doi = 10.1016/0001-8708(81)90044-X
 | issue = 3
 | journal = Advances in Mathematics
 | mr = 642395
 | pages = 299–305
 | title = The solution of van der Waerden's problem for permanents
 | volume = 42
 | year = 1981}}.&lt;/ref&gt; and D. I. Falikman;&lt;ref&gt;{{citation
 | last = Falikman | first = D. I.
 | issue = 6
 | journal = Akademiya Nauk Soyuza SSR
 | language = Russian
 | mr = 625097
 | pages = 931–938, 957
 | title = Proof of the van der Waerden conjecture on the permanent of a doubly stochastic matrix
 | volume = 29
 | year = 1981}}.&lt;/ref&gt; Egorychev's proof is an application of the [[Alexandrov&amp;ndash;Fenchel inequality]].&lt;ref name=CMC487&gt;Brualdi (2006) p.487&lt;/ref&gt; For this work, Egorychev and Falikman won the [[Fulkerson Prize]] in 1982.&lt;ref&gt;[http://www.mathopt.org/?nav=fulkerson Fulkerson Prize], Mathematical Optimization Society, retrieved 2012-08-19.&lt;/ref&gt;

== Computation ==
{{main|Computing the permanent|Permanent is sharp-P-complete}}
The naïve approach, using the definition, of computing permanents is computationally infeasible even for relatively small matrices. One of the fastest known algorithms is due to [[H. J. Ryser]] ({{harvtxt|Ryser|1963|loc=p. 27}}). Ryser’s method is based on an [[inclusion-exclusion principle|inclusion–exclusion]] formula that can be given&lt;ref&gt;{{harvtxt|van Lint|Wilson|2001}} [http://books.google.com/books?id=5l5ps2JkyT0C&amp;pg=PA108&amp;dq=permanent+ryser&amp;lr=#PPA99,M1 p. 99]&lt;/ref&gt; as follows: Let &lt;math&gt;A_k&lt;/math&gt; be obtained from ''A'' by deleting ''k'' columns, let &lt;math&gt;P(A_k)&lt;/math&gt; be the product of the row-sums of &lt;math&gt;A_k&lt;/math&gt;, and  let &lt;math&gt;\Sigma_k&lt;/math&gt; be the sum of the values of &lt;math&gt;P(A_k)&lt;/math&gt; over all possible &lt;math&gt;A_k&lt;/math&gt;. Then  
:&lt;math&gt; \operatorname{perm}(A)=\sum_{k=0}^{n-1} (-1)^{k}\Sigma_k.&lt;/math&gt;

It may be rewritten in terms of the matrix entries as follows:
: &lt;math&gt;\operatorname{perm} (A) = (-1)^n \sum_{S\subseteq\{1,\dots,n\}} (-1)^{|S|} \prod_{i=1}^n \sum_{j\in S} a_{ij}.&lt;/math&gt;

The permanent is believed to be more difficult to compute than the determinant. While the determinant can be computed in [[polynomial time]] by [[Gaussian elimination]], Gaussian elimination cannot be used to compute the permanent. Moreover, computing the permanent of a (0,1)-matrix is [[sharp-P-complete|#P-complete]]. Thus, if the permanent can be computed in [[polynomial time]] by any method, then '''[[FP (complexity)|FP]]&amp;nbsp;=&amp;nbsp;[[sharp-P|#P]]''', which is an even stronger statement than [[P = NP problem|P&amp;nbsp;=&amp;nbsp;NP]]. When the entries of ''A'' are nonnegative, however, the permanent can be computed [[approximation algorithm|approximately]] in [[randomized algorithm|probabilistic]] polynomial time, up to an error of ε''M'', where ''M'' is the value of the permanent and ε &gt; 0 is arbitrary.&lt;ref&gt;{{Citation|last1= Jerrum | first1= M.|author1-link= Mark Jerrum |last2=Sinclair | first2= A.|author2-link=  Alistair Sinclair|last3=Vigoda | first3= E.|title=A polynomial-time approximation algorithm for the permanent of a matrix with nonnegative entries |journal=[[Journal of the ACM]] |year=2004 |volume= 51 |pages= 671–697 | doi=10.1145/1008731.1008738}}&lt;/ref&gt;

==MacMahon's Master Theorem==
{{main|MacMahon Master theorem}}
Another way to view permanents is via multivariate [[generating function]]s. Let &lt;math&gt;A = (a_{ij})&lt;/math&gt; be a square matrix of order ''n''. Consider the multivariate generating function:
:&lt;math&gt;F(x_1,x_2,\dots,x_n) = \prod_{i=1}^n \left ( \sum_{j=1}^n a_{ij} x_j \right ) = \left ( \sum_{j=1}^n a_{1j} x_j \right ) \left ( \sum_{j=1}^n a_{2j} x_j \right ) \cdots \left ( \sum_{j=1}^n a_{nj} x_j \right ).&lt;/math&gt;
The coefficient of  &lt;math&gt;x_1 x_2 \dots x_n&lt;/math&gt; in &lt;math&gt;F(x_1,x_2,\dots,x_n)&lt;/math&gt; is perm(''A'').&lt;ref&gt;{{harvnb|Percus|1971|loc=p. 14}}&lt;/ref&gt;

As a generalization, for any sequence of ''n'' non-negative integers, &lt;math&gt;s_1,s_2,\dots,s_n&lt;/math&gt; define:
:&lt;math&gt;\operatorname{perm}^{(s_1,s_2,\dots,s_n)}(A) := \text{ coefficient of }x_1^{s_1} x_2^{s_2} \cdots x_n^{s_n} \text{ in }\left ( \sum_{j=1}^n a_{1j} x_j \right )^{s_1} \left ( \sum_{j=1}^n a_{2j} x_j \right )^{s_2} \cdots \left ( \sum_{j=1}^n a_{nj} x_j \right )^{s_n}.&lt;/math&gt;

'''MacMahon's Master Theorem''' relating permanents and determinants is:&lt;ref&gt;{{harvnb|Percus|1971|loc=p. 17}}&lt;/ref&gt;
:&lt;math&gt;\operatorname{perm}^{(s_1,s_2,\dots,s_n)}(A) = \text{ coefficient of }x_1^{s_1} x_2^{s_2} \cdots x_n^{s_n} \text{ in } \frac{1}{\operatorname{Det}(I - XA)},&lt;/math&gt;
where ''I'' is the order ''n'' identity matrix and ''X'' is the diagonal matrix with diagonal &lt;math&gt;[x_1,x_2,\dots,x_n].&lt;/math&gt;

==Permanents of rectangular matrices==
The permanent function can be generalized to apply to non-square matrices. Indeed, several authors make this the definition of a permanent and consider the restriction to square matrices a special case.&lt;ref&gt;In particular, {{harvtxt|Minc|1984}} and {{harvtxt|Ryser|1963}} do this.&lt;/ref&gt; Specifically, for an ''m''&amp;nbsp;×&amp;nbsp;''n'' matrix &lt;math&gt;A = (a_{ij})&lt;/math&gt; with ''m''&amp;nbsp;≤&amp;nbsp;''n'', define
:&lt;math&gt;\operatorname{perm} (A) = \sum_{\sigma \in \operatorname{P}(n,m)} a_{1 \sigma(1)} a_{2 \sigma(2)} \ldots a_{m \sigma(m)}&lt;/math&gt;
where P(''n'',''m'') is the set of all ''m''-permutations of the ''n''-set {1,2,...,n}.&lt;ref&gt;{{harvnb|Ryser|1963|loc=p. 25}}&lt;/ref&gt;

Ryser's computational result for permanents also generalizes. If ''A'' is an ''m''&amp;nbsp;×&amp;nbsp;''n'' matrix with ''m''&amp;nbsp;≤&amp;nbsp;''n'', let &lt;math&gt;A_k&lt;/math&gt; be obtained from ''A'' by deleting ''k'' columns, let &lt;math&gt;P(A_k)&lt;/math&gt; be the product of the row-sums of &lt;math&gt;A_k&lt;/math&gt;, and  let &lt;math&gt;\sigma_k&lt;/math&gt; be the sum of the values of &lt;math&gt;P(A_k)&lt;/math&gt; over all possible &lt;math&gt;A_k&lt;/math&gt;. Then  
:&lt;math&gt; \operatorname{perm}(A)=\sum_{k=0}^{m-1} (-1)^{k}\binom{n-m+k}{k}\sigma_{n-m+k}.&lt;/math&gt;&lt;ref&gt;{{harvnb|Ryser|1963|loc=p. 26}}&lt;/ref&gt;

===Systems of distinct representatives===
The generalization of the definition of a permanent to non-square matrices allows the concept to be used in a more natural way in some applications. For instance:

Let ''S''&lt;sub&gt;1&lt;/sub&gt;, ''S''&lt;sub&gt;2&lt;/sub&gt;, ..., ''S''&lt;sub&gt;''m''&lt;/sub&gt; be subsets (not necessarily distinct) of an ''n''-set with ''m''&amp;nbsp;≤&amp;nbsp;''n''. The [[incidence matrix]] of this collection of subsets is an ''m''&amp;nbsp;×&amp;nbsp;''n'' (0,1)-matrix ''A''. The number of [[transversal (combinatorics)|systems of distinct representatives]] (SDR's) of this collection is perm(''A'').&lt;ref&gt;{{harvnb|Ryser|1963|loc=p. 54}}&lt;/ref&gt;

==See also==
*[[Determinant]]
*[[Bapat–Beg theorem]], an application of permanent in [[order statistics]]

==Notes==
{{reflist|3}}

==References==

*{{cite book | zbl=1106.05001 | last=Brualdi | first=Richard A. | authorlink=Richard A. Brualdi | title=Combinatorial matrix classes | series=Encyclopedia of Mathematics and Its Applications | volume=108 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86565-4 }}
*{{cite book | last=Minc | first=Henryk | title= Permanents | others=With a foreword by Marvin Marcus | series=Encyclopedia of Mathematics and its Applications | volume=6| publisher=Addison–Wesley | year= 1978 | issn=0953-4806 | oclc=3980645 | zbl=0401.15005 | location=Reading, MA }}
*{{Cite book| last=Muir | first=Thomas |coauthors=William H. Metzler.| year=1960 | origyear=1882| title = A Treatise on the Theory of Determinants |location=New York| publisher=Dover | oclc=535903}}
*{{citation|first=J.K.|last=Percus|title=Combinatorial Methods|series=Applied Mathematical Sciences #4|publisher=Springer-Verlag|place=New York|year=1971|isbn=0-387-90027-6}}
*{{citation|first=Herbert John|last=Ryser|authorlink=H. J. Ryser|title=Combinatorial Mathematics|series=The Carus Mathematical Monographs #14|year=1963|publisher=The Mathematical Association of America}}
*{{citation|last1=van Lint|first1=J.H. |last2=Wilson|first2=R.M. |title=A Course in Combinatorics|publisher=Cambridge University Press|year= 2001|isbn=0521422604}}

==Further reading==
* {{citation|first=Marshall|last=Hall, Jr.| authorlink=Marshall Hall (mathematician)|title=Combinatorial Theory|edition=2nd|year=1986|publisher=John Wiley &amp; Sons|place=New York|isbn=0-471-09138-3|pages=56&amp;ndash;72}} Contains a proof of the Van der Waerden conjecture.
* {{citation|first1=M.|last1=Marcus|first2=H.|last2=Minc|title=Permanents|journal=The American Mathematical Monthly|volume=72|year=1965|pages=577&amp;ndash;591}}

==External links==
*[http://planetmath.org/encyclopedia/Permanent.html Permanent at PlanetMath]
*{{planetmath reference|id= 6935|title=Van der Waerden's permanent conjecture}}

[[Category:Algebra]]
[[Category:Linear algebra]]
[[Category:Matrix theory]]
[[Category:Permutations]]</text>
      <sha1>t36xjociicjps1iuzlr9gqfms0s3sy5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Congruence relation</title>
    <ns>0</ns>
    <id>48167</id>
    <revision>
      <id>607143073</id>
      <parentid>601362609</parentid>
      <timestamp>2014-05-05T09:32:58Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <text xml:space="preserve" bytes="9390">{{For|the term as used in elementary geometry|congruence (geometry)}}

In [[abstract algebra]], a '''congruence relation''' (or simply '''congruence''') is an [[equivalence relation]] on an [[algebraic structure]] (such as a [[group (mathematics)|group]], [[ring (mathematics)|ring]], or [[vector space]]) that is compatible with the structure.  Every congruence relation has a corresponding [[quotient]] structure, whose elements are the [[equivalence class]]es (or '''congruence classes''') for the relation.

==Basic example==

The prototypical example of a congruence relation is [[Modular arithmetic#Congruence relation|congruence modulo &lt;math&gt;n&lt;/math&gt;]] on the set of [[integer]]s.  For a given [[positive integer]] &lt;math&gt;n&lt;/math&gt;, two integers &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are called '''congruent modulo &lt;math&gt;n&lt;/math&gt;''', written
: &lt;math&gt;a \equiv b \pmod{n}&lt;/math&gt;
if &lt;math&gt;a - b&lt;/math&gt; is [[divisible]] by &lt;math&gt;n&lt;/math&gt; (or equivalently if &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; have the same [[remainder]] when divided by &lt;math&gt;n&lt;/math&gt;).

for example, &lt;math&gt;37&lt;/math&gt; and &lt;math&gt;57&lt;/math&gt; are congruent modulo &lt;math&gt;10&lt;/math&gt;,

: &lt;math&gt;37 \equiv 57 \pmod{10}&lt;/math&gt;

since &lt;math&gt;37 - 57 = -20&lt;/math&gt; is a multiple of 10, or equivalently since both &lt;math&gt;37&lt;/math&gt; and &lt;math&gt;57&lt;/math&gt; have a remainder of &lt;math&gt;7&lt;/math&gt; when divided by &lt;math&gt;10&lt;/math&gt;.

Congruence modulo &lt;math&gt;n&lt;/math&gt; (for a fixed &lt;math&gt;n&lt;/math&gt;) is compatible with both [[addition]] and [[multiplication]] on the integers.  That is,

if

: &lt;math&gt;a_1 \equiv a_2 \pmod{n}&lt;/math&gt; and &lt;math&gt;b_1 \equiv b_2 \pmod{n}&lt;/math&gt;

then

: &lt;math&gt;a_1 + b_1 \equiv a_2 + b_2 \pmod{n}&lt;/math&gt;  and  &lt;math&gt;a_1 b_1 \equiv a_2b_2 \pmod{n}&lt;/math&gt;

The corresponding addition and multiplication of equivalence classes is known as [[modular arithmetic]].  From the point of view of abstract algebra, congruence modulo &lt;math&gt;n&lt;/math&gt; is a congruence relation on the [[ring (mathematics)|ring]] of integers, and arithmetic modulo &lt;math&gt;n&lt;/math&gt; occurs on the corresponding [[quotient ring]].

==Definition==
The definition of a congruence depends on the type of [[algebraic structure]] under consideration.  Particular definitions of congruence can be made for [[group (mathematics)|groups]], [[ring (mathematics)|rings]], [[vector space]]s, [[module (mathematics)|modules]], [[semigroup]]s, [[lattice (order)|lattices]], and so forth.  The common theme is that a congruence is an [[equivalence relation]] on an algebraic object that is compatible with the algebraic structure, in the sense that the [[Operation (mathematics)|operations]] are [[well-defined]] on the [[equivalence classes]].

For example, a group is an algebraic object consisting of a [[set (mathematics)|set]] together with a single [[binary operation]], satisfying certain axioms.  If &lt;math&gt;G&lt;/math&gt; is a group with operation &amp;lowast;, a '''congruence relation''' on ''G'' is an equivalence relation &amp;equiv; on the elements of ''G'' satisfying
:''g''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''g''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;nbsp;and&amp;nbsp;&amp;nbsp;''h''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''h''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;rArr;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''g''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;lowast;&amp;nbsp;''h''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''g''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;lowast;&amp;nbsp;''h''&lt;sub&gt;2&lt;/sub&gt;
for all ''g''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''g''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''h''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''h''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;isin;&amp;nbsp;''G''.  For a congruence on a group, the equivalence class containing the [[identity element]] is always a [[normal subgroup]], and the other equivalence classes are the [[coset]]s of this subgroup.  Together, these equivalence classes are the elements of a [[quotient group]].

When an algebraic structure includes more than one operation, congruence relations are required to be compatible with each operation.  For example, a ring possesses both addition and multiplication, and a congruence relation on a ring must satisfy
:''r''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''s''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''r''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''s''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;and&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''r''&lt;sub&gt;1&lt;/sub&gt;''s''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''r''&lt;sub&gt;2&lt;/sub&gt;''s''&lt;sub&gt;2&lt;/sub&gt;
whenever ''r''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''r''&lt;sub&gt;2&lt;/sub&gt; and ''s''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''s''&lt;sub&gt;2&lt;/sub&gt;.  For a congruence on a ring, the equivalence class containing 0 is always a two-sided [[Ideal (ring theory)|ideal]], and the two operations on the set of equivalence classes define the corresponding [[quotient ring]].

The general notion of a congruence relation can be given a formal definition in the context of [[universal algebra]], a field which studies ideas common to all [[algebraic structures]].  In this setting, a congruence relation is an equivalence relation &amp;equiv; on an algebraic structure that satisfies
:''&amp;mu;''(''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt;)&amp;nbsp;&amp;equiv;&amp;nbsp;''&amp;mu;''(''a''&lt;sub&gt;1&lt;/sub&gt;&amp;prime;, ''a''&lt;sub&gt;2&lt;/sub&gt;&amp;prime;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;prime;)
for every ''n''-ary operation ''&amp;mu;'', and all elements ''a''&lt;sub&gt;1&lt;/sub&gt;,...,''a''&lt;sub&gt;''n''&lt;/sub&gt;,''a''&lt;sub&gt;1&lt;/sub&gt;&amp;prime;,...,''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;prime; satisfying ''a''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''a''&lt;sub&gt;''i''&lt;/sub&gt;&amp;prime; for each ''i''.

==Relation with homomorphisms==
If &amp;fnof;:&amp;nbsp;''A''&amp;nbsp;&amp;rarr;&amp;nbsp;''B'' is a [[homomorphism]] between two algebraic structures (such as [[group homomorphism|homomorphism of groups]], or a [[linear map]] between [[vector space]]s), then the relation &amp;equiv; defined by
:''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&amp;equiv;&amp;nbsp;''a''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;if and only if&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;fnof;(''a''&lt;sub&gt;1&lt;/sub&gt;)&amp;nbsp;=&amp;nbsp;&amp;fnof;(''a''&lt;sub&gt;2&lt;/sub&gt;)
is a congruence relation.  By the [[first isomorphism theorem]], the [[image (mathematics)|image]] of ''A'' under &amp;fnof; is a substructure of ''B'' [[Isomorphism|isomorphic]] to the quotient of ''A'' by this congruence.

==Congruences of groups, and normal subgroups and ideals==
In the particular case of [[group (mathematics)|groups]], congruence relations can be described in elementary terms as follows:
If ''G'' is a group (with [[identity element]] ''e'' and operation *) and ~ is a [[binary relation]] on ''G'', then ~ is a congruence whenever:
#[[Given any]] element ''a'' of ''G'', ''a'' ~ ''a'' ('''[[Reflexive relation|reflexivity]]''');
#Given any elements ''a'' and ''b'' of ''G'', [[material conditional|if]] ''a'' ~ ''b'', then ''b'' ~ ''a'' ('''[[Symmetric relation|symmetry]]''');
#Given any elements ''a'', ''b'', and ''c'' of ''G'', if ''a'' ~ ''b'' [[logical conjunction|and]] ''b'' ~ ''c'', then ''a'' ~ ''c'' ('''[[Transitive relation|transitivity]]''');
#Given any elements ''a'', ''a' '', ''b'', and ''b' '' of ''G'', if ''a'' ~ ''a' '' and ''b'' ~ ''b' '', then ''a'' * ''b'' ~ ''a' '' * ''b' '';
#Given any elements ''a'' and ''a' '' of ''G'', if ''a'' ~ ''a' '', then ''a''&lt;sup&gt;&amp;minus;1&lt;/sup&gt; ~ ''a' ''&lt;sup&gt;&amp;minus;1&lt;/sup&gt; (this can actually be proven from the other four, so is strictly redundant).

Conditions 1, 2, and 3 say that ~ is an [[equivalence relation]].

A congruence ~ is determined entirely by the set {''a'' ∈ ''G'' : ''a'' ~ ''e''} of those elements of ''G'' that are congruent to the identity element, and this set is a [[normal subgroup]].
Specifically, ''a'' ~ ''b'' if and only if ''b''&lt;sup&gt;&amp;minus;1&lt;/sup&gt; * ''a'' ~ ''e''.
So instead of talking about congruences on groups, people usually speak in terms of normal subgroups of them; in fact, every congruence corresponds uniquely to some normal subgroup of ''G''.

=== Ideals of rings and the general case ===

A similar trick allows one to speak of kernels in [[ring (mathematics)|ring theory]] as [[ideal (ring theory)|ideals]] instead of congruence relations, and in [[module (mathematics)|module theory]] as [[submodule]]s instead of congruence relations.

The most general situation where this trick is possible is with [[Omega-group]]s (in the general sense allowing operators with multiple arity). But this cannot be done with, for example, [[monoid]]s, so the study of congruence relations plays a more central role in monoid theory.

==Universal algebra==

The idea is generalized in [[universal algebra]]:
A congruence relation on an algebra ''A'' is a [[subset]] of the [[direct product]] ''A'' &amp;times; ''A'' that is both an [[equivalence relation]] on ''A'' and a [[subalgebra]] of ''A'' &amp;times; ''A''.

The [[kernel (universal algebra)|kernel]] of a [[homomorphism]] is always a congruence. Indeed, every congruence arises as a kernel.
For a given congruence ~ on ''A'', the set ''A''/~ of [[equivalence class]]es can be given the structure of an algebra in a natural fashion, the [[quotient algebra]].
The function that maps every element of ''A'' to its equivalence class is a homomorphism, and the kernel of this homomorphism is ~.

The [[lattice (order)|lattice]] '''Con'''(''A'') of all congruence relations on an algebra ''A'' is [[algebraic lattice|algebraic]].

==See also==
*[[Table of congruences]]
*[[Linear congruence theorem]]
*[[Congruence lattice problem]]

==References==
* Horn and Johnson, ''Matrix Analysis,'' Cambridge University Press, 1985. ISBN 0-521-38632-2. (Section 4.5 discusses congruency of matrices.)

{{DEFAULTSORT:Congruence Relation}}
[[Category:Modular arithmetic]]
[[Category:Algebra]]
[[Category:Mathematical relations]]</text>
      <sha1>3vkw9i86aubt3q33fz0kbympr4f9dnw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Landau's algorithm</title>
    <ns>0</ns>
    <id>24610350</id>
    <revision>
      <id>606937135</id>
      <parentid>605342142</parentid>
      <timestamp>2014-05-03T19:51:42Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* Notes and references */Added 1 dois to journal cites using [[Project:AWB|AWB]] (10090)</comment>
      <text xml:space="preserve" bytes="1046">In mathematics, '''Landau's algorithm''', named after its creator [[Susan Landau]], is an [[algorithm]] for deciding which [[nested radical]]s can be denested.&lt;ref&gt;{{citation
|first=Susan|last=Landau|authorlink=Susan Landau
|title=Simplification of nested radicals
|journal=[[SIAM Journal on Computing]]
|volume=21
|number=1
|year=1992
|pages=85–110
|doi=10.1137/0221009}} ([http://www.computer.org/csdl/proceedings/focs/1989/1982/00/063496.pdf link to a conference version that can be viewed by anyone])&lt;/ref&gt;

== Notes and references ==

{{reflist}}

* {{cite web | first = Susan | last = Landau |authorlink=Susan Landau | title = A note on 'Zippel Denesting' | id = {{citeseerx|10.1.1.35.5512}} }}
* {{cite journal | first = Susan | last = Landau |authorlink=Susan Landau | title = How to Tangle with a Nested Radical | journal = [[Mathematical Intelligencer]] | volume = 16 | pages = 49–55 | year = 1994 | doi=10.1007/bf03024284}}

[[Category:Algebra]]
[[Category:Algebraic number theory]]
[[Category:Computer algebra]]


{{algebra-stub}}</text>
      <sha1>20hobboq235irw9j672w2fj9udhxxlu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Kernel (algebra)</title>
    <ns>0</ns>
    <id>45240</id>
    <revision>
      <id>606078677</id>
      <parentid>587454239</parentid>
      <timestamp>2014-04-27T19:50:48Z</timestamp>
      <contributor>
        <username>BiT</username>
        <id>488890</id>
      </contributor>
      <comment>/* Monoid homomorphisms */</comment>
      <text xml:space="preserve" bytes="15122">In the various branches of [[mathematics]] that fall under the heading of [[abstract algebra]], the '''kernel''' of a [[homomorphism]] measures the degree to which the homomorphism fails to be [[injective function|injective]].&lt;ref&gt;{{cite book | last1=Dummit | first1=David S. | last2=Foote | first2=Richard M. | title=Abstract Algebra | publisher=[[John Wiley &amp; Sons]] | year=2004 | edition=3rd | isbn=0-471-43334-9}}&lt;/ref&gt;&lt;ref&gt;{{cite book | last=Lang | first=Serge | authorlink=Serge Lang | title=Algebra | publisher=[[Springer Science+Business Media|Springer]] | series=[[Graduate Texts in Mathematics]] | year=2002 | isbn=0-387-95385-X}}&lt;/ref&gt; An important special case is the [[kernel (linear algebra)|kernel of a linear map]]. The [[kernel (matrix)|kernel of a matrix]], also called the ''null space'', is the kernel of the linear map defined by the matrix.

The definition of kernel takes various forms in various contexts. But in all of them, the kernel of a homomorphism is trivial (in a sense relevant to that context) if and only if the homomorphism is [[injective function|injective]]. The [[fundamental theorem on homomorphisms]] (or [[first isomorphism theorem]]) is a theorem, again taking various forms, that applies to the [[quotient algebra]] defined by the kernel.

In this article, we first survey kernels for some important types of [[algebraic structure]]s; then we give general definitions from [[universal algebra]] for generic algebraic structures.

==Survey of examples==
=== Linear maps ===
{{Main|Kernel (linear algebra)}}

Let ''V'' and ''W'' be [[vector space]]s (or more generally [[module (mathematics)|modules]]) and let ''T'' be a [[linear map]] from ''V'' to ''W''. If '''0'''&lt;sub&gt;''W''&lt;/sub&gt; is the [[zero vector]] of ''W'', then the kernel of ''T'' is the [[preimage]] of the [[zero space|zero subspace]] {'''0'''&lt;sub&gt;''W''&lt;/sub&gt;}; that is, the [[subset]] of ''V'' consisting of all those elements of ''V'' that are mapped by ''T'' to the element '''0'''&lt;sub&gt;''W''&lt;/sub&gt;. The kernel is usually denoted as &quot;ker ''T''&quot;, or some variation thereof:

:&lt;math&gt; \operatorname{ker} T := \{\mathbf{v} \in V : T\mathbf{v} = \mathbf{0}_{W}\}\text{.} &lt;/math&gt;

Since a linear map preserves zero vectors, the zero vector '''0'''&lt;sub&gt;''V''&lt;/sub&gt; of ''V'' must belong to the kernel. The transformation ''T'' is injective if and only if its kernel is reduced to the zero subspace.

The kernel ker ''T'' is always a [[linear subspace]] of ''V''. Thus, it makes sense to speak of the [[quotient space (linear algebra)|quotient space]] ''V''/(ker ''T''). The first isomorphism theorem for vector spaces states that this quotient space is [[natural isomorphism|naturally isomorphic]] to the [[image (function)|image]] of ''T'' (which is a subspace of ''W''). As a consequence, the [[dimension (linear algebra)|dimension]] of ''V'' equals the dimension of the kernel plus the dimension of the image.

If ''V'' and ''W'' are [[finite-dimensional vector space|finite-dimensional]] and [[basis (linear algebra)|bases]] have been chosen, then ''T'' can be described by a [[matrix (mathematics)|matrix]] ''M'', and the kernel can be computed by solving the homogeneous [[system of linear equations]] ''M'''''v''' = '''0'''. In this case, the kernel of ''T'' may be identified to the [[kernel (matrix)|kernel of the matrix]] ''M'', also called &quot;null space&quot; of ''M''. The dimension of the null space, called the nullity of ''M'', is given by the number of columns of ''M'' minus the [[rank (matrix theory)|rank]] of ''M'', as a consequence of the [[rank-nullity theorem]].

Solving [[homogeneous differential equation]]s often amounts to computing the kernel of certain [[differential operator]]s.
For instance, in order to find all twice-[[differentiable function]]s ''f'' from the [[real line]] to itself such that
: ''x'' ''f''′′(''x'') + 3''f''′(''x'') = ''f''(''x''),
let ''V'' be the space of all twice differentiable functions, let ''W'' be the space of all functions, and define a linear operator ''T'' from ''V'' to ''W'' by
: (''Tf'')(''x'') = ''x'' ''f''′′(''x'') + 3''f''′(''x'') − ''f''(''x'')
for ''f'' in ''V'' and ''x'' an arbitrary [[real number]].
Then all solutions to the differential equation are in ker ''T''.

One can define kernels for [[homomorphism]]s between [[module (mathematics)|module]]s over a [[ring (mathematics)|ring]] in an analogous manner. This includes kernels for homomorphisms between [[abelian group]]s as a special case. This example captures the essence of kernels in general [[abelian categories]]; see [[Kernel (category theory)]].

===Group homomorphisms===
Let ''G'' and ''H'' be [[group (mathematics)|group]]s and let ''f'' be a [[group homomorphism]] from ''G'' to ''H''.
If ''e''&lt;sub&gt;''H''&lt;/sub&gt; is the [[identity element]] of ''H'', then the ''kernel'' of ''f'' is the preimage of the singleton set {''e''&lt;sub&gt;''H''&lt;/sub&gt;}; that is, the subset of ''G'' consisting of all those elements of ''G'' that are mapped by ''f'' to the element ''e''&lt;sub&gt;''H''&lt;/sub&gt;.
The kernel is usually denoted &quot;ker ''f''&quot; (or a variation).
In symbols:
: &lt;math&gt; \operatorname{ker} f := \{g \in G : f(g) = e_{H}\}\mbox{.}&lt;/math&gt;

Since a group homomorphism preserves identity elements, the identity element ''e''&lt;sub&gt;''G''&lt;/sub&gt; of ''G'' must belong to the kernel.
The homomorphism ''f'' is injective if and only if its kernel is only the singleton set {''e''&lt;sub&gt;''G''&lt;/sub&gt;}.

It turns out that ker ''f'' is not only a [[subgroup]] of ''G'' but in fact a [[normal subgroup]].
Thus, it makes sense to speak of the [[quotient group]] ''G''/(ker ''f'').
The first isomorphism theorem for groups states that this quotient group is [[natural isomorphism|naturally isomorphic]] to the [[image (function)|image]] of ''f'' (which is a subgroup of ''H'').

In the special case of [[abelian group]]s, this works in exactly the same way as in the previous section.

===Ring homomorphisms===
Let ''R'' and ''S'' be [[ring (mathematics)|ring]]s (assumed [[unital algebra|unital]]) and let ''f'' be a [[ring homomorphism]] from ''R'' to ''S''.
If 0&lt;sub&gt;''S''&lt;/sub&gt; is the [[zero element]] of ''S'', then the ''kernel'' of ''f'' is its kernel as linear map over the integers, or, equivalently, as additive groups. It is the preimage of the [[zero ideal]] {0&lt;sub&gt;''S''&lt;/sub&gt;}, which is, the subset of ''R'' consisting of all those elements of ''R'' that are mapped by ''f'' to the element 0&lt;sub&gt;''S''&lt;/sub&gt;.
The kernel is usually denoted &quot;ker ''f''&quot; (or a variation).
In symbols:
: &lt;math&gt; \operatorname{ker} f := \{r \in R : f(r) = 0_{S}\}\mbox{.} \! &lt;/math&gt;

Since a ring homomorphism preserves zero elements, the zero element 0&lt;sub&gt;''R''&lt;/sub&gt; of ''R'' must belong to the kernel.
The homomorphism ''f'' is injective if and only if its kernel is only the singleton set {0&lt;sub&gt;''R''&lt;/sub&gt;}.

It turns out that, although ker ''f'' is generally not a [[subring]] of ''R'' since it may not contain  the multiplicative identity if ''S'' is not the [[null ring]] (although the kernel is a subring for nonunital rings). Nevertheless it is a two-sided [[ideal (ring theory)|ideal]] of ''R''.
Thus, it makes sense to speak of the [[quotient ring]] ''R''/(ker ''f'').
The first isomorphism theorem for rings states that this quotient ring is [[natural isomorphism|naturally isomorphic]] to the [[image (function)|image]] of ''f'' (which is a subring of ''S''). (note that rings need not be unital for the kernel definition).

To some extent, this can be thought of as a special case of the situation for modules, since these are all [[bimodule]]s over a ring ''R'':
* ''R'' itself;
* any two-sided ideal of ''R'' (such as ker ''f'');
* any quotient ring of ''R'' (such as ''R''/(ker ''f'')); and
* the [[codomain]] of any ring homomorphism whose domain is ''R'' (such as ''S'', the codomain of ''f'').
However, the isomorphism theorem gives a stronger result, because ring isomorphisms preserve multiplication while module isomorphisms (even between rings) in general do not.

This example captures the essence of kernels in general [[Mal'cev algebra]]s.

===Monoid homomorphisms===
Let ''M'' and ''N'' be [[monoid (algebra)|monoid]]s and let ''f'' be a [[monoid homomorphism]] from ''M'' to ''N''.
Then the ''kernel'' of ''f'' is the subset of the [[direct product]] ''M'' &amp;times; ''M'' consisting of all those [[ordered pair]]s of elements of ''M'' whose components are both mapped by ''f'' to the same element in ''N''.
The kernel is usually denoted &quot;ker ''f''&quot; (or a variation).
In symbols:
: &lt;math&gt; \operatorname{ker} f := \{(m,m') \in M \times M : f(m) = f(m')\}\mbox{.} \! &lt;/math&gt;

Since ''f'' is a [[function (mathematics)|function]], the elements of the form (''m'',''m'') must belong to the kernel.
The homomorphism ''f'' is injective if and only if its kernel is only the [[Equality (mathematics)|diagonal set]] {(m,m) : ''m'' in ''M''}.

It turns out that ker ''f'' is an [[equivalence relation]] on ''M'', and in fact a [[congruence relation]].
Thus, it makes sense to speak of the [[quotient monoid]] ''M''/(ker ''f'').
The first isomorphism theorem for monoids states that this quotient monoid is [[natural isomorphism|naturally isomorphic]] to the [[image (function)|image]] of ''f'' (which is a [[submonoid]] of ''N''),(for the congruence relation).

This is very different in flavour from the above examples.
In particular, the preimage of the identity element of ''N'' is ''not'' enough to determine the kernel of ''f''.
This is because monoids are not [[Malcev algebra]]s.

==Universal algebra==
All the above cases may be unified and generalized in [[universal algebra]].

===General case===
Let ''A'' and ''B'' be [[algebraic structure]]s of a given type and let ''f'' be a [[homomorphism]] of that type from ''A'' to ''B''.
Then the ''kernel'' of ''f'' is the subset of the [[direct product]] ''A'' &amp;times; ''A'' consisting of all those [[ordered pair]]s of elements of ''A'' whose components are both mapped by ''f'' to the same element in ''B''.
The kernel is usually denoted &quot;ker ''f''&quot; (or a variation).
In symbols:
: &lt;math&gt; \operatorname{ker} f := \{(a,a') \in A \times A : f(a) = f(a')\}\mbox{.} \! &lt;/math&gt;

Since ''f'' is a [[function (mathematics)|function]], the elements of the form (''a'',''a'') must belong to the kernel.

The homomorphism ''f'' is injective if and only if its kernel is only the diagonal set {(a,a) : ''a'' in ''A''}.

It turns out that ker ''f'' is an [[equivalence relation]] on ''A'', and in fact a [[congruence relation]].
Thus, it makes sense to speak of the [[quotient algebra]] ''A''/(ker ''f'').
The first isomorphism theorem in general universal algebra states that this quotient algebra is [[natural isomorphism|naturally isomorphic]] to the [[image (function)|image]] of ''f'' (which is a [[subalgebra]] of ''B'').

Note that the definition of kernel here (as in the monoid example) doesn't depend on the algebraic structure; it is a purely [[Set (mathematics)|set]]-theoretic concept.
For more on this general concept, outside of abstract algebra, see [[kernel of a function]].

===Mal'cev algebras===
{{Main|Malcev algebra}}
In the case of Mal'cev algebras, this construction can be simplified. Every Mal'cev algebra has a special [[neutral element]] (the [[null vector]] in the case of [[vector space]]s, the [[identity element]] in the case of [[commutative group]]s, and the [[zero element]] in the case of [[ring (mathematics)|ring]]s or [[module (mathematics)|module]]s). The characteristic feature of a Mal'cev algebra is that we can recover the entire equivalence relation ker ''f'' from the [[equivalence class]] of the neutral element.

To be specific, let ''A'' and ''B'' be Mal'cev algebraic structures of a given type and let ''f'' be a homomorphism of that type from ''A'' to ''B''. If ''e''&lt;sub&gt;''B''&lt;/sub&gt; is the neutral element of ''B'', then the ''kernel'' of ''f'' is the [[preimage]] of the [[singleton set]] {''e''&lt;sub&gt;''B''&lt;/sub&gt;}; that is, the [[subset]] of ''A'' consisting of all those elements of ''A'' that are mapped by ''f'' to the element ''e''&lt;sub&gt;''B''&lt;/sub&gt;.
The kernel is usually denoted &quot;ker ''f''&quot; (or a variation). In symbols:
: &lt;math&gt; \mathop{\mathrm{ker}} f := \{a \in A : f(a) = e_{B}\}\mbox{.} \! &lt;/math&gt;

Since a Mal'cev algebra homomorphism preserves neutral elements, the identity element ''e''&lt;sub&gt;''A''&lt;/sub&gt; of ''A'' must belong to the kernel. The homomorphism ''f'' is injective if and only if its kernel is only the singleton set {''e''&lt;sub&gt;''A''&lt;/sub&gt;}.

The notion of [[ideal (ring theory)|ideal]] generalises to any Mal'cev algebra (as [[linear subspace]] in the case of vector spaces, [[normal subgroup]] in the case of groups, two-sided ideals in the case of rings, and [[submodule]] in the case of [[module (algebra)|module]]s). 
It turns out that ker ''f'' is not [[subalgebra]] of ''A'', but it is an ideal.
Then it makes sense to speak of the [[quotient algebra]] ''G''/(ker ''f'').
The first isomorphism theorem for Mal'cev algebras states that this quotient algebra is naturally isomorphic to the image of ''f'' (which is a subalgebra of ''B'').

The connection between this and the congruence relation is for more general types of algebras is as follows.
First, the kernel-as-an-ideal is the equivalence class of the neutral element ''e''&lt;sub&gt;''A''&lt;/sub&gt; under the kernel-as-a-congruence. For the converse direction, we need the notion of [[quotient]] in the Mal'cev algebra (which is [[division (mathematics)|division]] on either side for groups and [[subtraction]] for vector spaces, modules, and rings).
Using this, elements ''a'' and ''b'' of ''A'' are equivalent under the kernel-as-a-congruence if and only if their quotient ''a''/''b'' is an element of the kernel-as-an-ideal.

==Algebras with nonalgebraic structure==
Sometimes algebras are equipped with a nonalgebraic structure in addition to their algebraic operations.
For example, one may consider [[topological group]]s or [[topological vector space]]s, with are equipped with a [[topology (structure)|topology]].
In this case, we would expect the homomorphism ''f'' to preserve this additional structure; in the topological examples, we would want ''f'' to be a [[continuous map]].
The process may run into a snag with the quotient algebras, which may not be well-behaved.
In the topological examples, we can avoid problems by requiring that topological algebraic structures be [[Hausdorff space|Hausdorff]] (as is usually done); then the kernel (however it is constructed) will be a [[closed set]] and the [[quotient space (topology)|quotient space]] will work fine (and also be Hausdorff).

==Kernels in category theory==
The notion of ''kernel'' in [[category theory]] is a generalisation of the kernels of abelian algebras; see [[Kernel (category theory)]].
The categorical generalisation of the kernel as a congruence relation is the ''[[kernel pair]]''.
(There is also the notion of [[difference kernel]], or binary [[equalizer (mathematics)|equaliser]].)

==References==
{{Reflist}}

{{DEFAULTSORT:Kernel (Algebra)}}
[[Category:Algebra]]
[[Category:Isomorphism theorems]]
[[Category:Linear algebra]]</text>
      <sha1>5e263360eszj7v6kjtkpq92a9vp32f4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Tertiary ideal</title>
    <ns>0</ns>
    <id>25162434</id>
    <revision>
      <id>594056721</id>
      <parentid>558104932</parentid>
      <timestamp>2014-02-05T15:31:34Z</timestamp>
      <contributor>
        <username>Yokesaway</username>
        <id>20367525</id>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="2088">In [[mathematics]], a '''tertiary ideal''' is an (two-sided) [[Ideal (ring theory)|ideal]] in a (perhaps noncommutative) [[Ring (mathematics)|ring]] that cannot be expressed as a nontrivial intersection of a right [[fractional ideal]] with another ideal. Tertiary ideals generalize [[primary ideal]]s to the case of [[noncommutative ring]]s. Although [[primary decomposition]]s do not exist in general for ideals in noncommutative rings, tertiary decompositions do, at least if the ring is [[Noetherian ring|Noetherian]].

Every primary ideal is tertiary. Tertiary ideals and primary ideals coincide for commutative rings. To any (two-sided) ideal, a tertiary ideal can be associated called the tertiary radical, defined as

:&lt;math&gt;t(I) = \{r \in R \mbox{ }|\mbox{ } \forall s \notin I, \mbox{ }\exists x \in (s)\mbox{ } x \notin I \text{ and } (x)(r) \subset I \}. \, &lt;/math&gt;

Then ''t''(''I'') always contains ''I''.

If ''R'' is a (not necessarily commutative) Noetherian ring and ''I'' a right ideal in ''R'', then ''I'' has a unique irredundant decomposition into tertiary ideals

:&lt;math&gt;I = T_1 \cap \dots \cap T_n&lt;/math&gt;.

== See also ==
* [[Primary ideal]]
* [[Lasker - Noether theorem]]

== References ==
* {{Citation | last=Riley | first=J.A. | title=Axiomatic primary and tertiary decomposition theory | journal=Trans. Amer. Math. Soc. | year=1962 | volume=105 | pages=117–201}}
* [http://www.encyclopediaofmath.org/index.php/Tertiary_ideal Tertiary ideal], Encyclopedia of Mathematics, Springer Online Reference Works.
* {{Citation | last=Behrens | first=Ernst-August | title=Ring Theory | publisher=Verlag	Academic Press | year=1972 | url=http://books.google.ch/books?id=ZKGq4IQHhHUC&amp;lpg=PP1&amp;pg=PP1#v=onepage&amp;q=&amp;f=false}}
* {{Citation | last=Kurata | first=Yoshiki | title=On an additive ideal theory in a non-associative ring | journal=Mathematische Zeitschrift | volume=88 | issue=2 | year=1965 | doi=10.1007/BF01112095 | pages=129–135 | url=http://www.springerlink.com/content/h772w68514700345/}}

{{algebra-stub}}

{{DEFAULTSORT:Tertiary ideals}}
[[Category:Algebra]]</text>
      <sha1>ljh8j7u5b421dxx8go2t2r9co26lgcu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Binomial</title>
    <ns>0</ns>
    <id>542399</id>
    <revision>
      <id>605430119</id>
      <parentid>605429895</parentid>
      <timestamp>2014-04-23T09:41:13Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>clean up using [[Project:AWB|AWB]] (10080)</comment>
      <text xml:space="preserve" bytes="3492">{{Other uses}}

In [[algebra]], a '''binomial''' is a [[polynomial]] which is the sum of two terms, which are [[monomial]]s.&lt;ref&gt;{{Cite web
  | last = Weisstein
  | first = Eric
  | authorlink = Eric Weisstein
  | coauthors = 
  | title = Binomial
  | work = 
  | publisher = Wolfram MathWorld
  | date = 
  | url = http://mathworld.wolfram.com/Binomial.html
  | doi = 
  | accessdate = 29 March 2011}}&lt;/ref&gt; It is the simplest kind of polynomial after the [[monomial]]s.

==Definition==
A binomial is a polynomial, which is the sum of two [[monomial]]s. A binomial in a single indeterminate (also known as a [[univariate]] binomial) can be written in the form
:&lt;math&gt;a x^n - bx^m \,,&lt;/math&gt;
where &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are [[number]]s, and &lt;math&gt;n&lt;/math&gt; and &lt;math&gt; m&lt;/math&gt; are [[nonnegative integer]]s and &lt;math&gt;x&lt;/math&gt; is a symbol which is called an [[indeterminate (variable)|indeterminate]] or, for historical reasons, a [[variable (mathematics)|variable]]. In some contexts, the exponents &lt;math&gt;m&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt; may be negative, in which case the monomial is a [[Laurent polynomial|Laurent binomial]].

More generally, a binomial may be written&lt;ref name=Sturmfels62&gt;{{Cite journal
  | last = Sturmfels
  | first = Bernd
  | authorlink = Bernd Sturmfels
  | journal = CBMS Regional Conference Series in Mathematics
  | title = Solving Systems of Polynomial Equations
  | publisher = Conference Board of the Mathematical Sciences
  | issue = 97
  | page = 62
  | year = 2002
  | url = http://books.google.com/books?id=N9c8bWxkz9gC
  | accessdate = 21 March 2014}}&lt;/ref&gt; as:
:&lt;math&gt;a x_1^{n_1}\dotsb x_i^{n_i} - b x_1^{m_1}\dotsb x_i^{m_i}&lt;/math&gt;

Some examples of binomials are:
:&lt;math&gt;3x - 2x^2&lt;/math&gt;
:&lt;math&gt;xy + yx^2&lt;/math&gt;
:&lt;math&gt;x^2 + y^2&lt;/math&gt;

==Operations on simple binomials==
*The binomial &lt;math&gt; x^2 - y^2 &lt;/math&gt; can be factored as the product of two other binomials.
::&lt;math&gt; x^2 - y^2 = (x + y)(x - y). &lt;/math&gt;
:This is a special case of the more general formula: &lt;math&gt; x^{n+1} - y^{n+1} = (x - y)\sum_{k=0}^{n} x^{k}\,y^{n-k}&lt;/math&gt;.
:This can also be extended to &lt;math&gt; x^2 + y^2 = x^2 - (iy)^2 = (x - iy)(x + iy) &lt;/math&gt; when working over the complex numbers

*The product of a pair of linear binomials &lt;math&gt;(ax+b)&lt;/math&gt; and &lt;math&gt;(cx+d)&lt;/math&gt; is a [[trinomial]]:
::&lt;math&gt; (ax+b)(cx+d) = acx^2+(ad+bc)x+bd.&lt;/math&gt;

*A binomial raised to the ''n&lt;sup&gt;th&lt;/sup&gt;'' [[Exponentiation|power]], represented as
::&lt;math&gt; (x + y)^n &lt;/math&gt;
:can be expanded by means of the [[binomial theorem]] or, equivalently, using [[Pascal's triangle]]. For example, the square &lt;math&gt;(x+y)^2&lt;/math&gt; of the binomial &lt;math&gt;x+y&lt;/math&gt; is equal to the sum of the squares of the two terms and twice the product of the terms, that is &lt;math&gt;x^2+2xy+y^2&lt;/math&gt;.

*An application of above formula for the square of a binomial is the &quot;(m,n)-formula&quot; for generating [[Pythagorean triple]]s: for ''m &lt; n'', let &lt;math&gt;a=n^2-m^2&lt;/math&gt;, &lt;math&gt;b=2mn&lt;/math&gt;, &lt;math&gt;c=n^2+m^2&lt;/math&gt;, then &lt;math&gt;a^2+b^2=c^2&lt;/math&gt;.

==See also==
*[[Binomial theorem]]
*[[Completing the square]]
*[[Binomial distribution]]
*[[Binomial coefficient]]
*[[Binomial-QMF]] (Daubechies Wavelet Filters)
*The [[list of factorial and binomial topics]] contains a large number of related links.
*[[Binomial series]]

== Notes ==
{{reflist}}

==References==
* L. Bostock, and S. Chandler (1978). Pure Mathematics 1. ISBN 0-85950-092-6. pp.&amp;nbsp;36
* {{springer|title=Binomial|id=p/b016400}}

[[Category:Algebra]]</text>
      <sha1>rhavugiq51jubaa5gwtejf4gfhdz11k</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Algebra</title>
    <ns>0</ns>
    <id>18716923</id>
    <revision>
      <id>610783471</id>
      <parentid>609917285</parentid>
      <timestamp>2014-05-30T12:33:29Z</timestamp>
      <contributor>
        <username>Sardanaphalus</username>
        <id>427947</id>
      </contributor>
      <comment>/* External links */ template names/states</comment>
      <text xml:space="preserve" bytes="39503">{{redirect|Algebraist|the novel by Iain M. Banks|The Algebraist}}
{{for|beginner's introduction to algebra|Wikibooks: Algebra}}
{{pp-move-indef}}
{{pp-semi-indef}}
[[File:Quadratic_formula.svg|thumb|The [[quadratic formula]] expresses the solution of the degree two equation &lt;math&gt;ax^2 + bx +c=0&lt;/math&gt; in terms of its coefficients &lt;math&gt;a, b, c&lt;/math&gt;.]]

'''Algebra''' (from [[Arabic language|Arabic]] ''al-jebr'' meaning &quot;reunion of broken parts&quot;&lt;ref&gt;{{cite web|title=algebra|work=[[Online Etymology Dictionary]]|url=http://www.etymonline.com/index.php?term=algebra&amp;allowed_in_frame=0}}&lt;/ref&gt;) is one of the broad parts of [[mathematics]], together with [[number theory]], [[geometry]] and [[mathematical analysis|analysis]]. As such, it includes everything from elementary equation solving to the study of abstractions such as [[group (mathematics)|groups]], [[ring (mathematics)|rings]], and [[field (mathematics)|fields]]. The more basic parts of algebra are called [[elementary algebra]], the more abstract parts are called [[abstract algebra]] or modern algebra. Elementary algebra is essential for any study of mathematics, science, or engineering, as well as such applications as medicine and economics. Abstract algebra is a major area in advanced mathematics, studied primarily by professional mathematicians. Much early work in algebra, as the origin of its name suggests, was done in the [[Near East]], by such mathematicians as [[Omar Khayyam]] (1050-1123).

Elementary algebra differs from [[arithmetic]] in the use of abstractions, such as using letters to stand for numbers that are either unknown or allowed to take on many values.&lt;ref name=citeboyer /&gt; For example, in &lt;math&gt;x + 2 = 5&lt;/math&gt; the letter &lt;math&gt;x&lt;/math&gt; is unknown, but the law of inverses can be used to discover its value: &lt;math&gt;x=3&lt;/math&gt;. In [[Mass–energy equivalence|&lt;math&gt;E=mc^2&lt;/math&gt;]], the letters &lt;math&gt;E&lt;/math&gt; and &lt;math&gt;m&lt;/math&gt; are variables, and the letter &lt;math&gt;c&lt;/math&gt; is a [[Constant (mathematics)|constant]]. Algebra gives methods for solving equations and expressing formulas that are much easier (for those who know how to use them) than the older method of writing everything out in words.

The word ''algebra'' is also used in certain specialized ways. A special kind of mathematical object in abstract algebra is called an &quot;algebra&quot;, and the word is used, for example, in the phrases [[linear algebra]] and [[algebraic topology]] (see [[#How to distinguish between different meanings of &quot;algebra&quot;|below]]).

A mathematician who does research in algebra is called an '''algebraist'''.

== How to distinguish between different meanings of &quot;algebra&quot; ==
For historical reasons, the word &quot;algebra&quot; has several related meanings in mathematics, as a single word or with qualifiers. Such a situation, where a single word has many meanings in the same area of mathematics, may be confusing. However the distinction is easier if one recalls that the name of a scientific area is usually singular and without an article and the name of a specific structure requires an article or the plural. Thus we have:

* As a single word without article, &quot;algebra&quot; names a broad part of mathematics (see below).
* As a single word with article or in plural, &quot;algebra&quot; denotes a specific mathematical structure. See [[algebra (ring theory)]] and [[algebra over a field]].
* With a qualifier, there is the same distinction:
** Without article, it means a part of algebra, such as [[linear algebra]], [[elementary algebra]] (the symbol-manipulation rules taught in elementary courses of mathematics as part of [[primary education|primary]] and [[secondary education]]), or [[abstract algebra]] (the study of the algebraic structures for themselves).
** With an article, it means an instance of some abstract structure, like a [[Lie algebra]] or an [[associative algebra]].
** Frequently both meanings exist for the same qualifier, as in the sentence: ''[[Commutative algebra]] is the study of [[commutative ring]]s, which are [[algebra (ring theory)|commutative algebra]]s over the integers''.

==Algebra as a branch of mathematics==

Algebra began with computations similar to those of [[arithmetic]], with letters standing for numbers.&lt;ref name=citeboyer /&gt; This allowed proofs of properties that are true no matter which numbers are involved. For example, in the [[quadratic equation]]
:&lt;math&gt;ax^2+bx+c=0,&lt;/math&gt;
&lt;math&gt;a, b, c&lt;/math&gt; can be any numbers whatsoever (except that &lt;math&gt;a&lt;/math&gt; cannot be &lt;math&gt;0&lt;/math&gt;), and the [[quadratic formula]] can be used to quickly and easily find the value of the unknown quantity &lt;math&gt;x&lt;/math&gt;.

As it developed, algebra was extended to other non-numerical objects, such as [[vector (mathematics)|vectors]], [[matrix (mathematics)|matrices]], and [[polynomial]]s. Then the structural properties of these non-numerical objects were abstracted to define [[algebraic structure]]s such as [[group (mathematics)|groups]], [[ring (mathematics)|rings]], and [[field (mathematics)|fields]].

Before the 16th century, mathematics was divided into only two subfields, [[arithmetic]] and [[geometry]]. Even though some methods, which had been developed much earlier, may be considered nowadays as algebra, the emergence of algebra and, soon thereafter, of [[infinitesimal calculus]] as subfields of mathematics only dates from 16th or 17th century. From the second half of 19th century on, many new fields of mathematics appeared, most of which made use of both arithmetic and geometry, and almost all of which used algebra.

Today, algebra has grown until it includes many branches of mathematics, as can be seen in the [[Mathematics Subject Classification]]&lt;ref&gt;[http://www.ams.org/mathscinet/msc/msc2010.html 2010 Mathematics Subject Classification]&lt;/ref&gt;
where none of the first level areas (two digit entries) is called ''algebra''. Today algebra includes section 08-General algebraic systems, 12-[[Field theory (mathematics)|Field theory]] and [[polynomial]]s, 13-[[Commutative algebra]], 15-[[Linear algebra|Linear]] and [[multilinear algebra]]; [[matrix theory]], 16-[[associative algebra|Associative rings and algebras]], 17-[[Nonassociative ring]]s and [[Non-associative algebra|algebra]]s, 18-[[Category theory]]; [[homological algebra]], 19-[[K-theory]] and 20-[[Group theory]]. Algebra is also used extensively in 11-[[Number theory]] and 14-[[Algebraic geometry]].

== Etymology ==
The word ''algebra'' comes from the [[Arabic language]] ({{lang|ar|الجبر}} ''{{transl|ar|al-jabr}}'' &quot;restoration&quot;) from the title of the book ''[[The Compendious Book on Calculation by Completion and Balancing|Ilm al-jabr wa'l-muḳābala]]'' by [[Muḥammad ibn Mūsā al-Khwārizmī|al-Khwarizmi]]. The word entered the English language during [[Middle English|Late Middle English]] from either Spanish, Italian, or [[Medieval Latin]]. Algebra originally referred to a surgical procedure, and still is used in that sense in Spanish, while the mathematical meaning was a later development.&lt;ref name=oed&gt;{{cite web|title=algebra|url=http://www.oxforddictionaries.com/us/definition/english/algebra|work=Oxford English Dictionary|publisher=Oxford University Press}}&lt;/ref&gt;

== History ==
{{Main|History of algebra|Timeline of algebra}}

The start of algebra as an area of mathematics may be dated to the end of 16th century, with [[François Viète]]'s work. Until the 19th century, algebra consisted essentially of the [[theory of equations]]. In the following, &quot;Prehistory of algebra&quot; is about the results of the theory of equations that precede the emergence of algebra as an area of mathematics.

=== Prehistory of algebra ===

[[File:Image-Al-Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala.jpg|thumb|A page from [[:en:Muhammad ibn Musa al-Khwarizmi|Al-Khwārizmī]]'s ''[[The Compendious Book on Calculation by Completion and Balancing|al-Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala]]'']]

The roots of algebra can be traced to the ancient [[Babylonian mathematics|Babylonians]],&lt;ref&gt;{{cite book |last=Struik |first=Dirk J. |year=1987 |title=A Concise History of Mathematics |location=New York |publisher=Dover Publications |isbn=0-486-60255-9 }}&lt;/ref&gt; who developed an advanced arithmetical system with which they were able to do calculations in an [[algorithm]]ic fashion. The Babylonians developed formulas to calculate solutions for problems typically solved today by using [[linear equation]]s, [[quadratic equation]]s, and [[indeterminate equation|indeterminate linear equations]]. By contrast, most [[Egyptian mathematics|Egyptians]] of this era, as well as [[Greek mathematics|Greek]] and [[Chinese mathematics]] in the [[1st millennium BC]], usually solved such equations by geometric methods, such as those described in the ''[[Rhind Mathematical Papyrus]]'', [[Euclid's Elements|Euclid's ''Elements'']], and ''[[The Nine Chapters on the Mathematical Art]]''. The geometric work of the Greeks, typified in the ''Elements'', provided the framework for generalizing formulae beyond the solution of particular problems into more general systems of stating and solving equations, although this would not be realized until [[Mathematics in medieval Islam|mathematics developed in medieval Islam]].&lt;ref&gt;{{harvnb|Boyer|1991}}&lt;/ref&gt;

By the time of [[Plato]], [[Greek mathematics]] had undergone a drastic change. The [[Ancient Greece|Greeks]] created a geometric algebra where terms were represented by sides of geometric objects, usually lines, that had letters associated with them.&lt;ref name=citeboyer&gt;{{Harv|Boyer|1991|loc=&quot;Europe in the Middle Ages&quot; p. 258}} &quot;In the arithmetical theorems in Euclid's ''Elements'' VII-IX, numbers had been represented by line segments to which letters had been attached, and the geometric proofs in al-Khwarizmi's ''Algebra'' made use of lettered diagrams; but all coefficients in the equations used in the ''Algebra'' are specific numbers, whether represented by numerals or written out in words. The idea of generality is implied in al-Khwarizmi's exposition, but he had no scheme for expressing algebraically the general propositions that are so readily available in geometry.&quot;&lt;/ref&gt; [[Diophantus]] (3rd century AD) was an [[Alexandria]]n [[Greek mathematics|Greek mathematician]] and the author of a series of books called ''[[Arithmetica]]''. These texts deal with solving [[algebraic equation]]s,&lt;ref&gt;{{cite book |authorlink=Florian Cajori |first=Florian |last=Cajori |year=2010 |url=http://books.google.com/?id=gZ2Us3F7dSwC&amp;pg=PA34&amp;dq#v=onepage&amp;q=&amp;f=false |title=A History of Elementary Mathematics – With Hints on Methods of Teaching |page=34 |isbn=1-4460-2221-8 }}&lt;/ref&gt; and have led, in [[number theory]] to the modern notion of [[Diophantine equation]].

Earlier traditions discussed above had a direct influence on [[Muhammad ibn Mūsā al-Khwārizmī]] (c. 780–850). He later wrote ''[[The Compendious Book on Calculation by Completion and Balancing]]'', which established algebra as a mathematical discipline that is independent of [[geometry]] and [[arithmetic]].&lt;ref&gt;{{Cite journal|title=Al Khwarizmi: The Beginnings of Algebra|author=Roshdi Rashed|publisher=[[Saqi Books]]|date=November 2009|isbn=0-86356-430-5|ref=harv|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

The [[Hellenistic civilization|Hellenistic]] mathematicians [[Hero of Alexandria]] and [[Diophantus]] &lt;ref&gt;[http://library.thinkquest.org/25672/diiophan.htm Diophantus, Father of Algebra]&lt;/ref&gt; as well as [[Indian mathematics|Indian mathematicians]] such as [[Brahmagupta]] continued the traditions of Egypt and Babylon, though Diophantus' ''[[Arithmetica]]'' and Brahmagupta's ''[[Brahmasphutasiddhanta]]'' are on a higher level.&lt;ref&gt;[http://www.algebra.com/algebra/about/history/ History of Algebra]&lt;/ref&gt; For example, the first complete arithmetic solution (including zero and negative solutions) to [[quadratic equation]]s was described by Brahmagupta in his book ''Brahmasphutasiddhanta''. Later, Arabic and Muslim mathematicians developed algebraic methods to a much higher degree of sophistication. Although Diophantus and the Babylonians used mostly special ''ad hoc'' methods to solve equations, Al-Khwarizmi contribution was fundamental. He solved linear and quadratic equations without algebraic symbolism, [[negative numbers]] or [[zero]], thus he has to distinguish several types of equations.&lt;ref name=&quot;Meri2004&quot;&gt;{{cite book|author=Josef W. Meri|title=Medieval Islamic Civilization|url=http://books.google.com/books?id=H-k9oc9xsuAC&amp;pg=PA31|accessdate=25 November 2012|year=2004|publisher=Psychology Press|isbn=978-0-415-96690-0|page=31}}&lt;/ref&gt;

In the context where algebra is identified with the [[theory of equations]], the [[Greeks|Greek]] mathematician [[Diophantus]] has traditionally been known as the &quot;father of algebra&quot; but in more recent times there is much debate over whether al-Khwarizmi, who founded the discipline of ''al-jabr'', deserves that title instead.&lt;ref&gt;{{cite book |first=Carl B. |last=Boyer |title=A History of Mathematics |edition=Second |location= |publisher=Wiley |year=1991 |pages=178, 181 |isbn=0-471-54397-7 }}&lt;/ref&gt; Those who support Diophantus point to the fact that the algebra found in ''Al-Jabr'' is slightly more elementary than the algebra found in ''Arithmetica'' and that ''Arithmetica'' is syncopated while ''Al-Jabr'' is fully rhetorical.&lt;ref&gt;{{cite book |first=Carl B. |last=Boyer |title=A History of Mathematics |edition=Second |location= |publisher=Wiley |year=1991 |page=228 |isbn=0-471-54397-7 }}&lt;/ref&gt; Those who support Al-Khwarizmi point to the fact that he introduced the methods of &quot;[[Reduction (mathematics)|reduction]]&quot; and &quot;balancing&quot; (the transposition of subtracted terms to the other side of an equation, that is, the cancellation of [[like terms]] on opposite sides of the equation) which the term ''al-jabr'' originally referred to,&lt;ref name=Boyer-229&gt;{{Harv|Boyer|1991|loc=&quot;The Arabic Hegemony&quot; p. 229}} &quot;It is not certain just what the terms ''al-jabr'' and ''muqabalah'' mean, but the usual interpretation is similar to that implied in the translation above. The word ''al-jabr'' presumably meant something like &quot;restoration&quot; or &quot;completion&quot; and seems to refer to the transposition of subtracted terms to the other side of an equation; the word ''muqabalah'' is said to refer to &quot;reduction&quot; or &quot;balancing&quot; – that is, the cancellation of like terms on opposite sides of the equation.&quot;&lt;/ref&gt; and that he gave an exhaustive explanation of solving quadratic equations,&lt;ref&gt;{{Harv|Boyer|1991|loc=&quot;The Arabic Hegemony&quot; p. 230}} &quot;The six cases of equations given above exhaust all possibilities for linear and quadratic equations having positive root. So systematic and exhaustive was al-Khwarizmi's exposition that his readers must have had little difficulty in mastering the solutions.&quot;&lt;/ref&gt; supported by geometric proofs, while treating algebra as an independent discipline in its own right.&lt;ref&gt;Gandz and Saloman (1936), ''The sources of al-Khwarizmi's algebra'', Osiris i, p. 263–277: &quot;In a sense, Khwarizmi is more entitled to be called &quot;the father of algebra&quot; than Diophantus because Khwarizmi is the first to teach algebra in an elementary form and for its own sake, Diophantus is primarily concerned with the theory of numbers&quot;.&lt;/ref&gt; His algebra was also no longer concerned &quot;with a series of [[problem]]s to be resolved, but an [[Expository writing|exposition]] which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study&quot;. He also studied an equation for its own sake and &quot;in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems&quot;.&lt;ref name=Rashed-Armstrong&gt;{{Cite book | last1=Rashed | first1=R. | last2=Armstrong | first2=Angela | year=1994 | title=The Development of Arabic Mathematics | publisher=[[Springer Science+Business Media|Springer]] | isbn=0-7923-2565-6 | oclc=29181926 | pages=11–2 | ref=harv | postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

The Persian mathematician [[Omar Khayyam]] is credited with identifying the foundations of [[algebraic geometry]] and found the general geometric solution of the [[cubic equation]]. Another Persian mathematician, [[Sharaf al-Dīn al-Tūsī]], found algebraic and numerical solutions to various cases of cubic equations.&lt;ref&gt;{{MacTutor|id=Al-Tusi_Sharaf|title=Sharaf al-Din al-Muzaffar al-Tusi}}&lt;/ref&gt; He also developed the concept of a [[Function (mathematics)|function]].&lt;ref&gt;{{Cite journal|last=Victor J. Katz|first=Bill Barton|title=Stages in the History of Algebra with Implications for Teaching|journal=Educational Studies in Mathematics|publisher=[[Springer Science+Business Media|Springer Netherlands]]|volume=66|issue=2|date=October 2007|doi=10.1007/s10649-006-9023-7|pages=185–201 [192]|last2=Barton|first2=Bill|ref=harv|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt; The Indian mathematicians [[Mahavira (mathematician)|Mahavira]] and [[Bhaskara II]], the Persian mathematician [[Al-Karaji]],&lt;ref name=&quot;Boyer al-Karkhi ax2n&quot;&gt;{{Harv|Boyer|1991|loc=&quot;The Arabic Hegemony&quot; p. 239}} &quot;Abu'l Wefa was a capable algebraist as well as a trigonometer.&amp;nbsp;... His successor al-Karkhi evidently used this translation to become an Arabic disciple of Diophantus – but without Diophantine analysis!&amp;nbsp;... In particular, to al-Karkhi is attributed the first numerical solution of equations of the form ax&lt;sup&gt;2n&lt;/sup&gt; + bx&lt;sup&gt;n&lt;/sup&gt; = c (only equations with positive roots were considered),&quot;&lt;/ref&gt; and the Chinese mathematician [[Zhu Shijie]], solved various cases of cubic, [[quartic equation|quartic]], [[quintic equation|quintic]] and higher-order [[polynomial]] equations using numerical methods. In the 13th century, the solution of a cubic equation by [[Fibonacci]] is representative of the beginning of a revival in European algebra. As the Islamic world was declining, the European world was ascending. And it is here that algebra was further developed.

=== History of algebra ===
[[File:Gerolamo Cardano (colour).jpg|thumb|200px|In 1545, the Italian mathematician [[Girolamo Cardano]] published ''[[Ars Magna (Gerolamo Cardano)|Ars magna]]'' -''The great art'', a 40-chapter masterpiece in which he gave for the first time a method for solving the general [[cubic function|cubic]] and [[quartic equation]]s.]]

[[François Viète]]'s work at the close of the 16th century marks the start of the classical discipline of algebra. In 1637, [[René Descartes]] published ''[[La Géométrie]]'', inventing [[analytic geometry]] and introducing modern algebraic notation. Another key event in the further development of algebra was the general algebraic solution of the cubic and quartic equations, developed in the mid-16th century. The idea of a [[determinant]] was developed by [[Japanese mathematics|Japanese mathematician]] [[Kowa Seki]] in the 17th century, followed independently by [[Gottfried Leibniz]] ten years later, for the purpose of solving systems of simultaneous linear equations using [[matrix (mathematics)|matrices]]. [[Gabriel Cramer]] also did some work on matrices and determinants in the 18th century. Permutations were studied by [[Joseph-Louis Lagrange]] in his 1770 paper ''Réflexions sur la résolution algébrique des équations'' devoted to solutions of algebraic equations, in which he introduced [[Resolvent (Galois theory)|Lagrange resolvents]]. [[Paolo Ruffini]] was the first person to develop the theory of [[permutation group]]s, and like his predecessors, also in the context of solving algebraic equations.

[[Abstract algebra]] was developed in the 19th century, deriving from the interest in solving equations, initially focusing on what is now called [[Galois theory]], and on [[constructible number|constructibility]] issues.&lt;ref&gt;&quot;[http://www.math.hawaii.edu/~lee/algebra/history.html The Origins of Abstract Algebra]&quot;. University of Hawaii Mathematics Department.&lt;/ref&gt; The &quot;[[Abstract algebra#Modern algebra|modern algebra]]&quot; has deep nineteenth-century roots in the work, for example, of [[Richard Dedekind]] and [[Leopold Kronecker]] and profound interconnections with other branches of mathematics such as [[algebraic number theory]] and [[algebraic geometry]].&lt;ref&gt;&quot;[http://www.msri.org/calendar/workshops/WorkshopInfo/245/show_workshop The History of Algebra in the Nineteenth and Twentieth Centuries]&quot;. Mathematical Sciences Research Institute.&lt;/ref&gt; [[George Peacock]] was the founder of axiomatic thinking in arithmetic and algebra. [[Augustus De Morgan]] discovered [[relation algebra]] in his ''Syllabus of a Proposed System of Logic''. [[Josiah Willard Gibbs]] developed an algebra of vectors in three-dimensional space, and [[Arthur Cayley]] developed an algebra of matrices (this is a noncommutative algebra).&lt;ref&gt;&quot;[http://www.cambridge.org/catalogue/catalogue.asp?ISBN=9781108005043 The Collected Mathematical Papers]&quot;.Cambridge University Press.&lt;/ref&gt;

== Topics containing the word &quot;algebra&quot; ==
Areas of mathematics:
* [[Elementary algebra]], the part of algebra that is usually taught in elementary courses of mathematics.
* [[Abstract algebra]], in which [[algebraic structure]]s such as [[group (mathematics)|groups]], [[ring (mathematics)|rings]] and [[field (mathematics)|fields]] are [[axiomatization|axiomatically]] defined and investigated.
* [[Linear algebra]], in which the specific properties of [[linear equation]]s, [[vector space]]s and [[matrix (mathematics)|matrices]] are studied.
* [[Commutative algebra]], the study of [[commutative ring]]s.
* [[Computer algebra]], the implementation of algebraic methods as [[algorithm]]s and [[computer program]]s.
* [[Homological algebra]], the study of algebraic structures that are fundamental to study [[topological space]]s.
* [[Universal algebra]], in which properties common to all algebraic structures are studied.
* [[Algebraic number theory]], in which the properties of numbers are studied from an algebraic point of view.
* [[Algebraic geometry]], a branch of geometry, in its primitive form specifying curves and surfaces by solutions of polynomial equations.
* [[Algebraic combinatorics]], in which algebraic methods are used to study combinatorial questions.

Many mathematical structures are called '''algebras''':
* [[Algebra over a field]] or more generally [[Algebra (ring theory)|algebra over a ring]].&lt;br&gt;Many classes of algebras over a field or over a ring have a specific name:
** [[Associative algebra]]
** [[Non-associative algebra]]
** [[Lie algebra]]
** [[Hopf algebra]]
** [[C*-algebra]]
** [[Symmetric algebra]]
** [[Exterior algebra]]
** [[Tensor algebra]]
* In [[measure theory]],
** [[Sigma-algebra]]
** [[Algebra over a set]]
* In [[category theory]]
** [[F-algebra]] and [[F-coalgebra]]
** [[T-algebra]]
* In [[logic]],
** [[Relational algebra]], in which a set of [[finitary relation]]s that is [[closure (mathematics)|closed]] under certain operators.
** [[Boolean algebra]], a structure abstracting the computation with the [[truth value]]s ''false'' and ''true''. See also [[Boolean algebra (structure)]].
** [[Heyting algebra]]

== Elementary algebra ==
{{main|Elementary algebra}}
[[File:algebraic equation notation.svg|thumb|right|Algebraic expression notation:&lt;br/&gt;&amp;nbsp; 1 – power (exponent)&lt;br/&gt;&amp;nbsp; 2 – coefficient&lt;br/&gt;&amp;nbsp; 3 – term&lt;br/&gt;&amp;nbsp; 4 – operator&lt;br/&gt;&amp;nbsp; 5 – constant term&lt;br/&gt;&amp;nbsp; ''x'' ''y'' ''c'' – variables/constants]]
'''Elementary algebra''' is the most basic form of algebra. It is taught to students who are presumed to have no knowledge of [[mathematics]] beyond the basic principles of [[arithmetic]]. In arithmetic, only [[number]]s and their arithmetical operations (such as +, −, ×, ÷) occur. In algebra, numbers are often represented by symbols called [[variable (mathematics)|variables]] (such as ''a'', ''n'', ''x'', ''y'' or ''z''). This is useful because:
* It allows the general formulation of arithmetical laws (such as ''a'' + ''b'' = ''b'' + ''a'' for all ''a'' and ''b''), and thus is the first step to a systematic exploration of the properties of the [[real number|real number system]].
* It allows the reference to &quot;unknown&quot; numbers, the formulation of [[equation]]s and the study of how to solve these. (For instance, &quot;Find a number ''x'' such that 3''x'' + 1 = 10&quot; or going a bit further &quot;Find a number ''x'' such that ''ax'' + ''b'' = ''c''&quot;. This step leads to the conclusion that it is not the nature of the specific numbers that allows us to solve it, but that of the operations involved.)
* It allows the formulation of [[function (mathematics)|functional]] relationships. (For instance, &quot;If you sell ''x'' tickets, then your profit will be 3''x'' − 10 dollars, or ''f''(''x'') = 3''x'' − 10, where ''f'' is the function, and ''x'' is the number to which the function is applied&quot;.)

=== Polynomials ===
[[File:Polynomialdeg3.svg|The [[graph of a function|graph]] of a polynomial function of degree 3.|thumb|upright]]
{{main|Polynomial}}

A '''polynomial''' is an [[expression (mathematics)|expression]] that is the sum of a finite number of non-zero [[term (mathematics)|terms]], each term consisting of the product of a constant and a finite number of [[Variable (mathematics)|variables]] raised to whole number powers. For example, ''x''&lt;sup&gt;2&lt;/sup&gt; + 2''x'' − 3 is a polynomial in the single variable ''x''. A '''polynomial expression''' is an expression that may be rewritten as a polynomial, by using commutativity, associativity and distributivity of addition and multiplication. For example, (''x'' − 1)(''x'' + 3) is a polynomial expression, that, properly speaking, is not a polynomial. A '''polynomial function''' is a function that is defined by a polynomial, or, equivalently, by a polynomial expression. The two preceding examples define the same polynomial function.

Two important and related problems in algebra are the [[factorization of polynomials]], that is, expressing a given polynomial as a product of other polynomials that can not be factored any further, and the computation of [[polynomial greatest common divisor]]s. The example polynomial above can be factored as (''x'' − 1)(''x'' + 3). A related class of problems is finding algebraic expressions for the [[root of a function|roots]] of a polynomial in a single variable.

===Teaching algebra===
{{see also|Mathematics education}}
It has been suggested that elementary algebra should be taught as young as eleven years old,&lt;ref&gt;{{Cite web |title=Hull's Algebra |work=New York Times |date=July 16, 1904 |url=http://query.nytimes.com/mem/archive-free/pdf?res=F10714FB395E12738DDDAF0994DF405B848CF1D3 |format=[[pdf]] |accessdate=September 21, 2012}}&lt;/ref&gt; though in recent years it is more common for public lessons to begin at the eighth grade level (≈&amp;nbsp;13&amp;nbsp;y.o.&amp;nbsp;±) in the United States.&lt;ref&gt;{{Cite web |last=Quaid |first=Libby |title=Kids misplaced in algebra |work=[[Associated Press]] |date=September 22, 2008 |url=http://www.usatoday.com/news/nation/2008-09-22-357650952_x.htm |format=Report |accessdate=September 23, 2012}}&lt;/ref&gt;

Since 1997, [[Virginia Tech]] and some other universities have begun using a personalized model of teaching algebra that combines instant feedback from specialized computer software with one-on-one and small group tutoring, which has reduced costs and increased student achievement.&lt;ref&gt;{{cite news|url=http://www.nytimes.com/2012/09/07/us/ut-arlington-adopts-new-way-to-tackle-algebra.html|title=THE TEXAS TRIBUNE; U.T.-Arlington Adopts New Way to Tackle Algebra|last=Hamilton|first=Reeve|date=7 September 2012|work=The New York Times|accessdate=10 September 2012}}&lt;/ref&gt;

== Abstract algebra ==
{{Main|Abstract algebra|Algebraic structure}}

'''Abstract algebra''' extends the familiar concepts found in elementary algebra and [[arithmetic]] of [[number]]s to more general concepts. Here are listed fundamental concepts in abstract algebra.

'''[[Set (mathematics)|Sets]]''': Rather than just considering the different types of [[number]]s, abstract algebra deals with the more general concept of ''sets'': a collection of all objects (called [[Element (mathematics)|elements]]) selected by property specific for the set. All collections of the familiar types of numbers are sets. Other examples of sets include the set of all two-by-two [[Matrix (mathematics)|matrices]], the set of all second-degree [[polynomials]] (''ax''&lt;sup&gt;2&lt;/sup&gt; + ''bx'' + ''c''), the set of all two dimensional [[Vector (geometric)|vectors]] in the plane, and the various [[finite groups]] such as the [[cyclic group]]s, which are the groups of integers [[modular arithmetic|modulo]] ''n''. [[Set theory]] is a branch of [[logic]] and not technically a branch of algebra.

'''[[Binary operation]]s''': The notion of [[addition]] (+) is abstracted to give a ''binary operation'', ∗ say. The notion of binary operation is meaningless without the set on which the operation is defined. For two elements ''a'' and ''b'' in a set ''S'', ''a'' ∗ ''b'' is another element in the set; this condition is called [[Closure (mathematics)|closure]]. [[Addition]] (+), [[subtraction]] (-), [[multiplication]] (×), and [[Division (mathematics)|division]] (÷) can be binary operations when defined on different sets, as are addition and multiplication of matrices, vectors, and polynomials.

'''[[Identity element]]s''': The numbers zero and one are abstracted to give the notion of an ''identity element'' for an operation. Zero is the identity element for addition and one is the identity element for multiplication. For a general binary operator ∗ the identity element ''e'' must satisfy ''a'' ∗ ''e'' = ''a'' and ''e'' ∗ ''a'' = ''a''. This holds for addition as ''a'' + 0 = ''a'' and 0 + ''a'' = ''a'' and multiplication ''a'' × 1 = ''a'' and 1 × ''a'' = ''a''. Not all sets and operator combinations have an identity element; for example, the set of positive natural numbers (1, 2, 3,&amp;nbsp;...) has no identity element for addition.

'''[[Inverse elements]]''': The negative numbers give rise to the concept of ''inverse elements''. For addition, the inverse of ''a'' is written −''a'', and for multiplication the inverse is written ''a''&lt;sup&gt;−1&lt;/sup&gt;. A general two-sided inverse element ''a''&lt;sup&gt;−1&lt;/sup&gt; satisfies the property that ''a'' ∗ ''a''&lt;sup&gt;−1&lt;/sup&gt; = 1 and ''a''&lt;sup&gt;−1&lt;/sup&gt; ∗ ''a'' = 1 .

'''[[Associativity]]''': Addition of integers has a property called associativity. That is, the grouping of the numbers to be added does not affect the sum. For example: {{nowrap|1=(2 + 3) + 4 = 2 + (3 + 4)}}. In general, this becomes (''a'' ∗ ''b'') ∗ ''c'' = ''a'' ∗ (''b'' ∗ ''c''). This property is shared by most binary operations, but not subtraction or division or [[octonion multiplication]].

'''[[Commutative operation|Commutativity]]''': Addition and multiplication of real numbers are both commutative. That is, the order of the numbers does not affect the result. For example: 2 + 3 = 3 + 2. In general, this becomes ''a'' ∗ ''b'' = ''b'' ∗ ''a''. This property does not hold for all binary operations. For example, [[matrix multiplication]] and [[Quaternion|quaternion multiplication]] are both non-commutative.

=== Groups ===
{{main|Group (mathematics)}} {{see also|Group theory|Examples of groups}}

Combining the above concepts gives one of the most important structures in mathematics: a '''[[group (mathematics)|group]]'''. A group is a combination of a set ''S'' and a single [[binary operation]] ∗, defined in any way you choose, but with the following properties:
* An identity element ''e'' exists, such that for every member ''a'' of ''S'', ''e'' ∗ ''a'' and ''a'' ∗ ''e'' are both identical to ''a''.
* Every element has an inverse: for every member ''a'' of ''S'', there exists a member ''a''&lt;sup&gt;−1&lt;/sup&gt; such that ''a'' ∗ ''a''&lt;sup&gt;−1&lt;/sup&gt; and ''a''&lt;sup&gt;−1&lt;/sup&gt; ∗ ''a'' are both identical to the identity element.
* The operation is associative: if ''a'', ''b'' and ''c'' are members of ''S'', then (''a'' ∗ ''b'') ∗ ''c'' is identical to ''a'' ∗ (''b'' ∗ ''c'').

If a group is also [[commutativity|commutative]]—that is, for any two members ''a'' and ''b'' of ''S'', ''a'' ∗ ''b'' is identical to ''b'' ∗ ''a''—then the group is said to be [[Abelian group|abelian]].

For example, the set of integers under the operation of addition is a group. In this group, the identity element is 0 and the inverse of any element ''a'' is its negation, −''a''. The associativity requirement is met, because for any integers ''a'', ''b'' and ''c'', (''a'' + ''b'') + ''c'' = ''a'' + (''b'' + ''c'')

The nonzero [[rational number]]s form a group under multiplication. Here, the identity element is 1, since 1 × ''a'' = ''a'' × 1 = ''a'' for any rational number ''a''. The inverse of ''a'' is 1/''a'', since ''a'' × 1/''a'' = 1.

The integers under the multiplication operation, however, do not form a group. This is because, in general, the multiplicative inverse of an integer is not an integer. For example, 4 is an integer, but its multiplicative inverse is ¼, which is not an integer.

The theory of groups is studied in [[group theory]]. A major result in this theory is the [[classification of finite simple groups]], mostly published between about 1955 and 1983, which separates the [[finite set|finite]] [[simple group]]s into roughly 30 basic types.

[[Semigroup]]s, [[quasigroup]]s, and [[monoid]]s are structures similar to groups, but more general. They comprise a set and a closed binary operation, but do not necessarily satisfy the other conditions. A [[semigroup]] has an ''associative'' binary operation, but might not have an identity element. A [[monoid]] is a semigroup which does have an identity but might not have an inverse for every element. A [[quasigroup]] satisfies a requirement that any element can be turned into any other by either a unique left-multiplication or right-multiplication; however the binary operation might not be associative.

All groups are monoids, and all monoids are semigroups.

{| class=&quot;wikitable&quot;
|-
| colspan=11|Examples
|-
!Set:
| colspan=2|[[Natural numbers]] '''N'''
| colspan=2|[[Integers]] '''Z'''
| colspan=4|[[Rational numbers]] '''Q''' (also [[Real numbers|real]] '''R''' and [[Complex numbers|complex]] '''C''' numbers)
| colspan=2|Integers [[modular arithmetic|modulo]] 3: '''Z'''&lt;sub&gt;3&lt;/sub&gt; = {0, 1, 2}
|-
!Operation
| +
| × (w/o zero)
| +
| × (w/o zero)
| +
| −
| × (w/o zero)
| ÷ (w/o zero)
| +
| × (w/o zero)
|-
!Closed
| Yes
| Yes
| Yes
| Yes
| Yes
| Yes
| Yes
| Yes
| Yes
| Yes
|-
| Identity
| 0
| 1
| 0
| 1
| 0
| N/A
| 1
| N/A
| 0
| 1
|-
| Inverse
| N/A
| N/A
| −''a''
| N/A
| −''a''
| N/A
| 1/''a''
| N/A
| 0, 2, 1, respectively
| N/A, 1, 2, respectively
|-
| Associative
| Yes
| Yes
| Yes
| Yes
| Yes
| No
| Yes
| No
| Yes
| Yes
|-
| Commutative
| Yes
| Yes
| Yes
| Yes
| Yes
| No
| Yes
| No
| Yes
| Yes
|-
| Structure
| [[monoid]]
| [[monoid]]
| [[abelian group]]
| [[monoid]]
| [[abelian group]]
| [[quasigroup]]
| [[abelian group]]
| [[quasigroup]]
| [[abelian group]]
| [[abelian group]] ('''Z'''&lt;sub&gt;2&lt;/sub&gt;)
|}

=== Rings and fields ===
{{main|ring (mathematics)|field (mathematics)}} {{see also|Ring theory|Glossary of ring theory|Field theory (mathematics)|glossary of field theory}}

Groups just have one binary operation. To fully explain the behaviour of the different types of numbers, structures with two operators need to be studied. The most important of these are [[Ring (mathematics)|rings]], and [[Field (mathematics)|fields]].

A '''[[Ring (mathematics)|ring]]''' has two binary operations (+) and (×), with × distributive over +. Under the first operator (+) it forms an ''abelian group''. Under the second operator (×) it is associative, but it does not need to have identity, or inverse, so division is not required. The additive (+) identity element is written as 0 and the additive inverse of ''a'' is written as −''a''.

'''[[Distributivity]]''' generalises the ''distributive law'' for numbers, and specifies the order in which the operators should be applied, (called the [[Order of operations|precedence]]). For the integers {{nowrap|1=(''a'' + ''b'') × ''c'' = ''a'' × ''c'' + ''b'' × ''c''}} and {{nowrap|1=''c'' × (''a'' + ''b'') = ''c'' × ''a'' + ''c'' × ''b'',}} and × is said to be ''distributive'' over +.

The integers are an example of a ring. The integers have additional properties which make it an '''[[integral domain]]'''.

A '''[[Field (mathematics)|field]]''' is a ''ring'' with the additional property that all the elements excluding 0 form an ''abelian group'' under ×. The multiplicative (×) identity is written as 1 and the multiplicative inverse of ''a'' is written as ''a''&lt;sup&gt;−1&lt;/sup&gt;.

The rational numbers, the real numbers and the complex numbers are all examples of fields.

== See also ==
&lt;!-- Please place all see also references to the following pages.--&gt;
{{portal|Algebra}}
* [[Outline of algebra]]
* [[Outline of linear algebra]]

== Notes ==
{{reflist|30em}}

== References ==
*{{Citation
 | first=Carl B.
 | last=Boyer
 | authorlink=Carl Benjamin Boyer
 | title=A History of Mathematics
 | edition=Second Edition
 | publisher=John Wiley &amp; Sons, Inc.
 | year=1991
 | isbn=0-471-54397-7
 }}
* Donald R. Hill, ''Islamic Science and Engineering'' (Edinburgh University Press, 1994).
* Ziauddin Sardar, Jerry Ravetz, and Borin Van Loon, ''Introducing Mathematics'' (Totem Books, 1999).
* George Gheverghese Joseph, ''The Crest of the Peacock: Non-European Roots of Mathematics'' ([[Penguin Books]], 2000).
* John J O'Connor and Edmund F Robertson, [http://www-history.mcs.st-andrews.ac.uk/Indexes/Algebra.html ''History Topics: Algebra Index'']. In ''[[MacTutor History of Mathematics archive]]'' ([[University of St Andrews]], 2005).
* I.N. Herstein: ''Topics in Algebra''. ISBN 0-471-02371-X
* R.B.J.T. Allenby: ''Rings, Fields and Groups''. ISBN 0-340-54440-6
* [[L. Euler]]: ''[http://web.mat.bham.ac.uk/C.J.Sangwin/euler/ Elements of Algebra]'', ISBN 978-1-899618-73-6
* {{cite book|last=Asimov|first=Isaac|title=Realm of Algebra|year=1961|publisher=Houghton Mifflin|authorlink=Isaac Asimov}}

== External links ==
{{Wiktionary|algebra}}
{{Wikibooks|Algebra}}
* [http://www.khanacademy.org/math/algebra Khan Academy: Conceptual videos and worked examples]
* [https://www.khanacademy.org/math/algebra/introduction-to-algebra/overview_hist_alg/v/origins-of-algebra Khan Academy: Origins of Algebra, free online micro lectures]
* [http://algebrarules.com Algebrarules.com: An open source resource for learning the fundamentals of Algebra]
* [http://www.gresham.ac.uk/event.asp?PageId=45&amp;EventId=620 4000 Years of Algebra], lecture by Robin Wilson, at [[Gresham College]], October 17, 2007 (available for MP3 and MP4 download, as well as a text file).
* {{sep entry|algebra|Algebra|Vaughan Pratt}}


{{Algebra |expanded}}
{{Areas of mathematics |collapsed}}

[[Category:Algebra| ]]</text>
      <sha1>s9fuq3dj92ruc1sqp6m13jafvh38ukl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Operand</title>
    <ns>0</ns>
    <id>22656</id>
    <revision>
      <id>601212731</id>
      <parentid>595469279</parentid>
      <timestamp>2014-03-25T16:25:12Z</timestamp>
      <contributor>
        <username>Daniele Pugliesi</username>
        <id>7681380</id>
      </contributor>
      <comment>/* Order of operations */</comment>
      <text xml:space="preserve" bytes="7169">{{Refimprove|date=November 2008}}

In [[mathematic]]s, an '''operand''' is the object of a [[mathematical operation]], a quantity on which an operation is performed.&lt;ref&gt;American Heritage Dictionary&lt;/ref&gt;

==Example ==
The following [[arithmetic]] expression shows an example of operators and operands:

:&lt;math&gt;3 + 6 = 9\;&lt;/math&gt;

In the above example, '+' is the symbol for the operation called [[addition]]. 

The '''operand''' '3' is one of the inputs (quantities) followed by the addition [[Operator_(mathematics)|operator]], and the operand '6' is the other input necessary for the operation.

The result of the operation is 9. (The number '9' is also called the sum of the addends, 3 and 6.)

An operand, then, is also referred to as &quot;one of the inputs (quantities) for an operation&quot;.

==Notation==

===Expressions as operands===
Operands may be complex, and may consist of expressions also made up of operators with operands.

:&lt;math&gt;(3 + 5) \times 2\;&lt;/math&gt;

In the above expression '(3&amp;nbsp;+&amp;nbsp;5)' is the first operand for the multiplication operator and '2' the second. The operand '(3&amp;nbsp;+&amp;nbsp;5)' is an expression in itself, which contains an addition operator, with the operands '3' and '5'.

===Order of operations===
{{See also|Order of operations}}
Rules of precedence affect which values form operands for which operators:

:&lt;math&gt;3 + 5 \times 2&lt;/math&gt;

In the above expression, the multiplication operator has the higher precedence than the addition operator, so the multiplication operator has operands of '5' and '2'. The addition operator has operands of '3' and '5&amp;nbsp;×&amp;nbsp;2'.

===Positioning of operands===
Depending on the [[mathematical notation]] being used the position of an operator in relation to its operand(s) may vary. In everyday usage [[infix notation]] is the most common, however other notations also exist, such as the [[prefix notation|prefix]] and [[postfix notation|postfix]] notations. These alternate notations are most common within [[computer science]].

Below is a comparison of three different notations — all represent an addition of the numbers '1' and '2'

:&lt;math&gt;1 + 2\;&lt;/math&gt; (infix notation)

:&lt;math&gt;+\;1\;2&lt;/math&gt; (prefix notation)

:&lt;math&gt;1\;2\;+&lt;/math&gt; (postfix notation)

===Infix Notation and the Order of Operation===
{{main|Order of operations}}

With infix notation, one easy mnemonic for remembering the order of operation is:

'''P'''lease '''e'''xcuse '''m'''y '''d'''ear '''A'''unt '''S'''ally.

The first letter (in '''boldtype''') of each word in the above mnemonic stands for the following:

:'''p''' = parentheses
:'''e''' = exponents
:'''m''' = multiplication
:'''d''' = division
:'''a''' = addition
:'''s''' = subtraction

In a mathematical expression, the order of operation is carried out from left to right. Start with the left most value and seek the first operation to be carried out in accordance with the order specified above (i.e., start with parentheses and end with subtraction{{dubious|Correctness of PEMDAS explanation|reason=MD and AS need to be grouped together|date=May 2013}}). For example, in the expression

:&lt;math&gt;4 \times 2^2 - (2 + 2^2)&lt;/math&gt;,

the first operation to be acted upon is any and all expressions found inside a parenthesis. So beginning at the left and moving to the right, find the first (and in this case, the only) parenthesis, that is, (2&amp;nbsp;+&amp;nbsp;2&lt;sup&gt;2&lt;/sup&gt;). Within the parenthesis itself is found the expression 2&lt;sup&gt;2&lt;/sup&gt;. The reader is required to find the value of 2&lt;sup&gt;2&lt;/sup&gt; before going any further. The value of 2&lt;sup&gt;2&lt;/sup&gt; is 4. Having found this value, the remaining expression looks like this:

:&lt;math&gt;4 \times 2^2 - (2 + 4)&lt;/math&gt;

The next step is to calculate the value of expression inside the parenthesis itself, that is, (2&amp;nbsp;+&amp;nbsp;4)&amp;nbsp;=&amp;nbsp;6. Our expression now looks like this:

:&lt;math&gt;4 \times 2^2 - 6&lt;/math&gt;

Having calculated the parenthetical part of the expression, we start over again beginning with the left most value and move right. The next order of operation (according to the rules) is exponents. Start at the left most value, that is, 4, and scan your eyes to the right and search for the first exponent you come across. The first (and only) expression we come across that is expressed with an exponent is 2&lt;sup&gt;2&lt;/sup&gt;.  We find the value of 2&lt;sup&gt;2&lt;/sup&gt;, which is 4. What we have left is the expression

:&lt;math&gt;4 \times 4 - 6\;&lt;/math&gt;.

The next order of operation is multiplication. 4&amp;nbsp;×&amp;nbsp;4 is 16. Now our expression looks like this:

:&lt;math&gt;16 - 6\;&lt;/math&gt;

The next order of operation according to the rules is division. However, there is no division operator sign (÷) in the expression, 16&amp;nbsp;−&amp;nbsp;6. So we move on to the next order of operation, i.e., addition. But there is no addition operator sign (+) in the expression 16&amp;nbsp;−&amp;nbsp;6. So we move on to the next and final order of operation, which is subtraction.

:&lt;math&gt;16 - 6 = 10\;&lt;/math&gt;. 

So the correct value for our original expression, 4&amp;nbsp;×&amp;nbsp;2&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;(2&amp;nbsp;+&amp;nbsp;2&lt;sup&gt;2&lt;/sup&gt;), is 10. 

It is important to carry out the order of operation in accordance with rules set by convention. If the reader evaluates an expression but does not follow the correct order of operation, the reader will come forth with a different value. The different value will be the incorrect value because the order of operation was not followed. The reader will arrive at the correct value for the expression if and only if each operation is carried out in the proper order.

===Arity===
The number of operands of an operator is called its [[arity]]. Based on arity, operators are classified as nullary (no operands), [[unary operation|unary]] (1 operand), [[binary operation|binary]] (2 operands), [[ternary operation|ternary]] (3 operands) etc.

==Computer science==
In computer [[programming language]]s, the definitions of [[operator (programming)|operator]] and '''operand''' are almost the same as in mathematics.

In computing, an '''operand''' is the part of a computer instruction which specifies what data is to be manipulated or operated on, while at the same time representing the data itself. A computer instruction describes an operation such as add or multiply X, while the operand (or operands, as there can be more than one) specify on which X to operate as well as the value of X.

Additionally, in [[assembly language]], an '''operand''' is a value (an argument) on which the [[instruction (computer science)|instruction]], named by [[mnemonic]], operates.  The operand may be a [[processor register]], a [[memory address]], a literal constant, or a label.  A simple example (in the [[x86]] architecture) is

:[[MOV (x86 instruction)|MOV]] &amp;nbsp; DS, AX

where the value in register operand 'AX' is to be moved into register 'DS'. Depending on the [[instruction set|instruction]], there may be zero, one, two, or more operands.

==See also==
{{portal|Mathematics}}
*[[Instruction_set#Number_of_operands|Instruction set]]

==References==
{{Reflist}}

[[Category:Algebra]]
[[Category:Mathematical notation]]
[[Category:Operators (programming)]]
[[Category:Machine code]]</text>
      <sha1>g8nomlbpzaoxq3fyqf641nltwj9lx57</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Kernel (set theory)</title>
    <ns>0</ns>
    <id>330095</id>
    <revision>
      <id>610598796</id>
      <parentid>543724780</parentid>
      <timestamp>2014-05-29T06:14:29Z</timestamp>
      <contributor>
        <ip>137.158.153.203</ip>
      </contributor>
      <comment>/* Definition */</comment>
      <text xml:space="preserve" bytes="3985">{{refimprove|date=December 2009}}

In [[set theory]], the '''kernel''' of a [[function (mathematics)|function]] ''f'' may be taken to be either

*the [[equivalence relation]] on the function's [[domain of a function|domain]] that roughly expresses the idea of &quot;equivalent as far as the function ''f'' can tell&quot;,&lt;ref name=&quot;bergman&quot;&gt;{{citation|title=Universal Algebra: Fundamentals and Selected Topics|series=Pure and Applied Mathematics|volume=301|first=Clifford|last=Bergman|publisher=CRC Press|year=2011|isbn=9781439851296|url=http://books.google.com/books?id=QXi3BZWoMRwC&amp;pg=PA14|pages=14–16}}.&lt;/ref&gt; or
*the corresponding [[partition of a set|partition]] of the domain.

==Definition==
For the formal definition, let ''X'' and ''Y'' be [[Set (mathematics)|sets]] and let ''f'' be a function from ''X'' to ''Y''.
Elements ''x''&lt;sub&gt;1&lt;/sub&gt; and ''x''&lt;sub&gt;2&lt;/sub&gt; of ''X'' are ''equivalent'' if ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;) and ''f''(''x''&lt;sub&gt;2&lt;/sub&gt;) are [[equal (math)|equal]], i.e. are the same element of ''Y''.
The kernel of ''f'' is the equivalence relation thus defined.&lt;ref name=&quot;bergman&quot;/&gt;

==Quotients==
Like any equivalence relation, the kernel can be [[Ideal (ring theory)|modded out]] to form a [[quotient set]], and the quotient set is the partition:

:&lt;math&gt;\left\{\, \left\{\, w \in X \mid f(x)=f(w) \,\right\}  \mid x \in X \,\right\}.&lt;/math&gt;

This quotient set &lt;math&gt;X/\mathord{=_f}&lt;/math&gt; is called the ''[[coimage]]'' of the function &lt;math&gt;f&lt;/math&gt;, and denoted &lt;math&gt;\operatorname{coim} f&lt;/math&gt; (or a variation).
The coimage is [[natural isomorphism|naturally isomorphic]] (in the set-theoretic sense of a [[bijection]]) to the [[image (mathematics)|image]], &lt;math&gt;\operatorname{im} f&lt;/math&gt;; specifically, the [[equivalence class]] of &lt;math&gt;x&lt;/math&gt; in &lt;math&gt;X&lt;/math&gt; (which is an element of &lt;math&gt;\operatorname{coim} f&lt;/math&gt;) corresponds to &lt;math&gt;f(x)&lt;/math&gt; in &lt;math&gt;Y&lt;/math&gt; (which is an element of &lt;math&gt;\operatorname{im} f&lt;/math&gt;).

==As a subset of the square==
Like any [[binary relation]], the kernel of a function may be thought of as a [[subset]] of the [[Cartesian product]] ''X'' × ''X''.
In this guise, the kernel may be denoted &quot;ker ''f''&quot; (or a variation) and may be defined symbolically as

: &lt;math&gt;\operatorname{ker} f := \{(x,x') \mid f(x) = f(x')\}&lt;/math&gt;.&lt;ref name=&quot;bergman&quot;/&gt;

The study of the properties of this subset can shed light on &lt;math&gt;f&lt;/math&gt;.

==In algebraic structures==
If ''X'' and ''Y'' are [[algebraic structure]]s of some fixed type (such as [[group (mathematics)|group]]s, [[ring (algebra)|ring]]s, or [[vector space]]s), and if the function ''f'' from ''X'' to ''Y'' is a [[homomorphism]], then ker ''f'' will be a [[subalgebra]] of the [[direct product]] ''X'' × ''X''. Subalgebras of ''X'' × ''X'' that are also equivalence relations (called ''[[congruence relation]]s'') are important in [[abstract algebra]], because they define the most general notion of [[quotient algebra]].&lt;ref name=&quot;bergman&quot;/&gt; Thus the coimage of ''f'' is a quotient algebra of ''X'' much as the image of ''f'' is a subalgebra of ''Y''; and the bijection between them becomes an [[isomorphism]] in the algebraic sense as well (this is the most general form of the [[first isomorphism theorem]] in algebra). The use of kernels in this context is discussed further in the article [[Kernel (algebra)]].

==In topological spaces==
If ''X'' and ''Y'' are [[topological space]]s and ''f'' is a [[continuous function]] between them, then the topological properties of ker ''f'' can shed light on the spaces ''X'' and ''Y''.
For example, if ''Y'' is a [[Hausdorff space]], then ker ''f'' must be a [[closed set]].
Conversely, if ''X'' is a Hausdorff space and ker ''f'' is a closed set, then the coimage of ''f'', if given the [[quotient space (topology)|quotient space]] topology, must also be a Hausdorff space.

==References==
{{reflist}}

{{DEFAULTSORT:Kernel (Set Theory)}}
[[Category:Algebra]]
[[Category:Topology]]
[[Category:Abstract algebra]]</text>
      <sha1>b3c5rvau8h38c519fb9lq1vxsmc3gb2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Generalized arithmetic progression</title>
    <ns>0</ns>
    <id>2240347</id>
    <revision>
      <id>607157865</id>
      <parentid>580285300</parentid>
      <timestamp>2014-05-05T11:51:36Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <text xml:space="preserve" bytes="1918">{{Cleanup-rewrite|date=May 2009}}

In [[mathematics]], a '''multiple arithmetic progression''', '''generalized arithmetic progression''', ''k''-'''dimensional arithmetic progression''' or a '''linear set''', is a set of [[integer]]s or [[tuple]]s of integers constructed as an [[arithmetic progression]] is, but allowing several possible differences. So, for example, we start at 17 and may add a multiple of 3 ''or'' of 5, repeatedly.  In algebraic terms we look at integers

:&lt;math&gt;a + mb + nc + \ldots&lt;/math&gt;

where &lt;math&gt;a, b, c&lt;/math&gt; and so on are fixed, and &lt;math&gt;m, n&lt;/math&gt; and so on are confined to some ranges

:&lt;math&gt;0&lt;/math&gt; &lt;span style=&quot;font-size:125%&quot;&gt;&amp;nbsp;&amp;le;&amp;nbsp;&lt;/span&gt; &lt;math&gt;m&lt;/math&gt; &lt;span style=&quot;font-size:125%&quot;&gt;&amp;nbsp;&amp;le;&amp;nbsp;&lt;/span&gt; &lt;math&gt;M&lt;/math&gt;

and so on, for a finite progression. The number&amp;nbsp; &lt;math&gt;k&lt;/math&gt; , that is the number of permissible differences, is called the ''dimension'' of the generalized progression.

More generally, let

:&lt;math&gt;L(C;P)&lt;/math&gt;

be the set of all elements &lt;math&gt;x&lt;/math&gt; in &lt;math&gt;N^n&lt;/math&gt; of the form

:&lt;math&gt;x = c_0 + \sum_{i=1}^m k_i x_i,&lt;/math&gt;

with &lt;math&gt;c_0&lt;/math&gt; in &lt;math&gt;C&lt;/math&gt;, &lt;math&gt;x_1, \ldots, x_m&lt;/math&gt; in &lt;math&gt;P&lt;/math&gt;, and &lt;math&gt;k_1, \ldots, k_m&lt;/math&gt; in &lt;math&gt;N&lt;/math&gt;. &lt;math&gt;L&lt;/math&gt; is said to be a ''linear set'' if &lt;math&gt;C&lt;/math&gt; consists of exactly one element, and &lt;math&gt;P&lt;/math&gt; is finite.

A subset of &lt;math&gt;N^n&lt;/math&gt; is said to be '''semilinear'''{{Anchor|semilinear set}} if it is a finite union of linear sets.

==See also==
* [[Freiman's theorem]]

==References==
*{{cite book| last=Nathanson | first=Melvyn B. | year=1996 | title=Additive Number Theory: Inverse Problems and Geometry of Sumsets | volume=165 | series=[[Graduate Texts in Mathematics]] | publisher=Springer | isbn=0-387-94655-1 | zbl=0859.11003 }}

{{DEFAULTSORT:Generalized Arithmetic Progression}}
[[Category:Algebra]]
[[Category:Combinatorics]]</text>
      <sha1>4a16ouqf2xkixr7ozedvv5vfmin26ec</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Schur algebra</title>
    <ns>0</ns>
    <id>26339669</id>
    <revision>
      <id>599203752</id>
      <parentid>595911472</parentid>
      <timestamp>2014-03-11T22:47:54Z</timestamp>
      <contributor>
        <username>Colonies Chris</username>
        <id>577301</id>
      </contributor>
      <minor/>
      <comment>sp, date &amp; link fixes; unlinking common words, replaced: ’s → 's, Kac-Moody → Kac–Moody (2) using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="8619">In mathematics, '''Schur algebras''', named after [[Issai Schur]], are certain finite-dimensional [[associative algebra|algebras]] closely associated with [[Schur&amp;ndash;Weyl duality]] between [[general linear group|general linear]] and [[symmetric group|symmetric]] groups. They are used to relate the [[representation theory|representation theories]] of those two [[group (mathematics)|groups]]. Their use was promoted by the influential monograph of [[J. A. Green]] first published in 1980.&lt;ref&gt;[[J. A. Green]], ''Polynomial Representations of GL&lt;sub&gt;n&lt;/sub&gt;'', Springer Lecture Notes 830, Springer-Verlag 1980. {{MR|2349209}}, ISBN 978-3-540-46944-5, ISBN 3-540-46944-3&lt;/ref&gt; The name &quot;Schur algebra&quot; is due to Green. In the modular case (over infinite [[field (mathematics)|fields]] of positive characteristic) Schur algebras were used by Gordon James and [[Karin Erdmann]] to show that the (still open) problems of computing decomposition numbers for general linear groups and symmetric groups are actually equivalent.&lt;ref&gt;Karin Erdmann, Decomposition numbers for symmetric groups and composition factors of Weyl modules. ''Journal of Algebra'' 180 (1996), 316&amp;ndash;320. {{doi|10.1006/jabr.1996.0067}} {{MR|1375581}}&lt;/ref&gt; Schur algebras were used by Friedlander and [[Andrei Suslin|Suslin]] to prove finite generation of [[cohomology]] of finite [[group scheme]]s.&lt;ref&gt;[[Eric Friedlander]] and [[Andrei Suslin]], Cohomology of finite group schemes over a field. ''Inventiones Mathematicae'' 127 (1997), 209--270. {{MR|1427618}} {{doi|10.1007/s002220050119}}&lt;/ref&gt;

== Construction ==
The Schur algebra &lt;math&gt;S_k(n, r)&lt;/math&gt; can be defined for any [[commutative ring]] &lt;math&gt;k&lt;/math&gt; and integers &lt;math&gt;n, r \geq 0&lt;/math&gt;. Consider the [[associative algebra|algebra]] &lt;math&gt;k[x_{ij}]&lt;/math&gt; of [[polynomials]] (with coefficients in &lt;math&gt;k&lt;/math&gt;) in &lt;math&gt;n^2&lt;/math&gt; commuting variables &lt;math&gt;x_{ij}&lt;/math&gt;, 1 ≤ ''i'', ''j'' ≤ &lt;math&gt;n&lt;/math&gt;. Denote by &lt;math&gt;A_k(n, r)&lt;/math&gt; the homogeneous polynomials of degree &lt;math&gt;r&lt;/math&gt;. Elements of &lt;math&gt;A_k(n, r)&lt;/math&gt; are ''k''-linear combinations of monomials formed by multiplying together &lt;math&gt;r&lt;/math&gt; of the generators &lt;math&gt;x_{ij}&lt;/math&gt; (allowing repetition). Thus

: &lt;math&gt;k[x_{ij}] = \bigoplus_{r\ge 0} A_k(n, r).&lt;/math&gt;

Now, &lt;math&gt;k[x_{ij}]&lt;/math&gt; has a natural [[coalgebra]] structure with comultiplication &lt;math&gt;\Delta&lt;/math&gt; and counit &lt;math&gt;\varepsilon&lt;/math&gt; the algebra homomorphisms given on generators by

: &lt;math&gt; \Delta(x_{ij}) = \textstyle\sum_l x_{il} \otimes x_{lj}, \quad \varepsilon(x_{ij}) = \delta_{ij}\quad\ &lt;/math&gt;  &amp;nbsp;&amp;nbsp;  ([[Kronecker delta|Kronecker's delta]]).

Since comultiplication is an algebra homomorphism, &lt;math&gt;k[x_{ij}]&lt;/math&gt; is a [[bialgebra]]. One easily
checks that &lt;math&gt;A_k(n, r)&lt;/math&gt; is a subcoalgebra of the bialgebra &lt;math&gt;k[x_{ij}]&lt;/math&gt;, for every ''r''&amp;nbsp;≥&amp;nbsp;0.

'''Definition.''' The Schur algebra (in degree &lt;math&gt;r&lt;/math&gt;) is the algebra &lt;math&gt;S_k (n, r) = \mathrm{Hom}_k( A_k (n, r), k)&lt;/math&gt;. That is, &lt;math&gt;S_k(n,r)&lt;/math&gt; is the linear dual of &lt;math&gt;A_k(n,r)&lt;/math&gt;.

It is a general fact that the linear [[dual space|dual]] of a coalgebra &lt;math&gt;A&lt;/math&gt; is an algebra in a natural way, where the multiplication in the algebra is induced by dualizing the comultiplication in the coalgebra. To see this, let 
: &lt;math&gt;\Delta(a) = \textstyle \sum a_i \otimes b_i&lt;/math&gt;
and, given linear functionals &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g&lt;/math&gt; on &lt;math&gt;A&lt;/math&gt;, define their product to be the linear functional given by 
: &lt;math&gt;\textstyle a \mapsto \sum f(a_i) g(b_i).&lt;/math&gt;
The identity element for this multiplication of functionals is the counit in &lt;math&gt;A&lt;/math&gt;.

== Main properties ==

* One of the most basic properties expresses &lt;math&gt;S_k(n,r)&lt;/math&gt; as a centralizer algebra. Let &lt;math&gt;V = k^n&lt;/math&gt; be the space of rank &lt;math&gt;n&lt;/math&gt; column vectors over &lt;math&gt;k&lt;/math&gt;, and form the [[tensor product|tensor]] power

: &lt;math&gt;V^{\otimes r} = V \otimes \cdots \otimes V \quad (r\text{ factors}). \, &lt;/math&gt;
Then the [[symmetric group]] &lt;math&gt;\mathfrak{S}_r&lt;/math&gt; on &lt;math&gt;r&lt;/math&gt; letters acts naturally on the tensor space by place permutation, and one has an isomorphism 
: &lt;math&gt;S_k(n,r) \cong \mathrm{End}_{\mathfrak{S}_r} (V^{\otimes r}).&lt;/math&gt;
In other words, &lt;math&gt;S_k(n,r)&lt;/math&gt; may be viewed as the algebra of [[endomorphisms]] of tensor space commuting with the action of the [[symmetric group]].

* &lt;math&gt;S_k(n,r)&lt;/math&gt; is free over &lt;math&gt;k&lt;/math&gt; of rank given by the [[binomial coefficient]] &lt;math&gt;\tbinom{n^2+r-1}{r}&lt;/math&gt;.

* Various bases of &lt;math&gt;S_k(n,r)&lt;/math&gt; are known, many of which are indexed by pairs of semistandard [[Young tableau]]x of shape &lt;math&gt;\lambda&lt;/math&gt;, as &lt;math&gt;\lambda&lt;/math&gt; varies over the set of [[partition (number theory)|partitions]] of &lt;math&gt;r&lt;/math&gt; into no more than &lt;math&gt;n&lt;/math&gt; parts.

* In case ''k'' is an infinite field, &lt;math&gt;S_k(n,r)&lt;/math&gt; may also be identified with the enveloping algebra (in the sense of H. Weyl) for the action of the [[general linear group]] &lt;math&gt;\mathrm{GL}_n(k)&lt;/math&gt; acting on tensor space (via the diagonal action on tensors, induced from the natural action of &lt;math&gt;\mathrm{GL}_n(k)&lt;/math&gt; on &lt;math&gt;V = k^n&lt;/math&gt; given by matrix multiplication).

* Schur algebras are &quot;defined over the integers&quot;. This means that they satisfy the following change of scalars property:

: &lt;math&gt;S_k(n,r) \cong S_{\mathbb{Z}}(n,r) \otimes _{\mathbb{Z}} k&lt;/math&gt;
:for any commutative ring &lt;math&gt;k&lt;/math&gt;.

* Schur algebras provide natural examples of quasihereditary algebras&lt;ref&gt;Edward Cline, Brian Parshall, and Leonard Scott, Finite-dimensional algebras and highest weight categories. ''Journal für die Reine und Angewandte Mathematik'' [Crelle's Journal]  391 (1988), 85&amp;ndash;99. {{MR|0961165}}&lt;/ref&gt; (as defined by Cline, Parshall, and Scott), and thus have nice [[homological algebra|homological]] properties. In particular, Schur algebras have finite [[global dimension]].

== Generalizations ==
* '''Generalized Schur algebras''' (associated to any reductive [[algebraic group]]) were introduced by Donkin in the 1980s.&lt;ref&gt;Stephen Donkin, On Schur algebras and related algebras, I. ''Journal of Algebra'' 104 (1986), 310&amp;ndash;328.  {{doi|10.1016/0021-8693(86)90218-8}} {{MR|0866778}}&lt;/ref&gt; These are also quasihereditary.

* Around the same time, Dipper and James&lt;ref&gt;Richard Dipper and Gordon James,  The q-Schur algebra. ''Proceedings of the London Math. Society'' (3) 59 (1989), 23&amp;ndash;50. {{doi|10.1112/plms/s3-59.1.23}} {{MR|0997250}}&lt;/ref&gt; introduced the '''quantized Schur algebras''' (or '''q-Schur algebras''' for short), which are a type of q-deformation of the classical Schur algebras described above, in which the symmetric group is replaced by the corresponding [[Hecke algebra]] and the general linear group by an appropriate [[quantum group]].

* There are also '''generalized q-Schur algebras''', which are obtained by generalizing the work of Dipper and James in the same way that Donkin generalized the classical Schur algebras.&lt;ref&gt;Stephen Doty, Presenting generalized q-Schur algebras. ''Representation Theory'' 7 (2003), 196--213 (electronic). {{doi|10.1090/S1088-4165-03-00176-6}}&lt;/ref&gt;

* There are further generalizations, such as the '''affine q-Schur algebras'''&lt;ref&gt;R. M. Green, The affine q-Schur algebra. ''Journal of Algebra'' 215 (1999), 379--411. {{doi|10.1006/jabr.1998.7753}}&lt;/ref&gt; related to affine [[Kac–Moody algebra|Kac–Moody]] [[Lie algebra]]s and other generalizations, such as the '''cyclotomic q-Schur algebras'''&lt;ref&gt;Richard Dipper, Gordon James, and Andrew Mathas, Cyclotomic q-Schur algebras. ''Math. Zeitschrift'' 229 (1998), 385--416. {{doi|10.1007/PL00004665}} {{MR|1658581}}&lt;/ref&gt; related to Ariki-Koike algebras (which are q-deformations of certain [[complex reflection group]]s).

The study of these various classes of generalizations forms an active area of contemporary research.

== References ==
{{Reflist}}

== Further reading ==
* Stuart Martin, ''Schur Algebras and Representation Theory'', Cambridge University Press 1993. {{MR|2482481}}, ISBN 978-0-521-10046-5
* Andrew Mathas, [http://www.ams.org/bookstore-getitem/item=ULECT-15 Iwahori-Hecke algebras and Schur algebras of the symmetric group], University Lecture Series, vol.15, American Mathematical Society, 1999. {{MR|1711316}}, ISBN 0-8218-1926-7
* [[Hermann Weyl]], ''The Classical Groups. Their Invariants and Representations''. Princeton University Press, Princeton, N.J., 1939. {{MR|0000255}}, ISBN 0-691-05756-7

[[Category:Algebra]]
[[Category:Representation theory]]</text>
      <sha1>ptbqtjq5vbwyvz6k1ddtbl178d3mlvv</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Outline of algebra</title>
    <ns>0</ns>
    <id>6473570</id>
    <revision>
      <id>542417236</id>
      <parentid>541961943</parentid>
      <timestamp>2013-03-06T19:04:52Z</timestamp>
      <contributor>
        <username>SchreiberBike</username>
        <id>5839411</id>
      </contributor>
      <comment>[[WP:DPL|You can help.]] Repaired link to the disambiguation page, [[Degree]].</comment>
      <text xml:space="preserve" bytes="6626">The following outline is provided as an overview of and topical guide to algebra:

'''[[Algebra]]''' is one of the main branches of [[mathematics]] and is concerned with the study of [[structure (algebraic)|structure]], [[relation (mathematics)|relation]]  and [[quantity]]. Algebra studies the effects of [[addition|add]]ing and [[multiplication|multiply]]ing [[number]]s, [[Variable (mathematics)|variables]], and [[polynomial]]s, along with their [[factorization]] and determining their [[root of a function|root]]s. In addition to working directly with numbers, algebra also covers [[symbols]], variables, and [[Set (mathematics)|set]] [[element (mathematics)|elements]]. Addition and multiplication are general [[operation (mathematics)|operations]], but their precise definitions lead to structures such as [[group (mathematics)|groups]], [[ring (mathematics)|rings]], and [[field (mathematics)|fields]].

==Overview==
{{Main|Algebra}}

* [[Arithmetic]]
* [[Equation]]s
* An [[algebraic equation]] is an equation involving only algebraic expressions in the unknowns. These are further classified by [[Degree of a polynomial|degree]].
* A [[linear equation]] is algebraic equation of degree one.
* A [[Polynomial#Polynomial_equations|polynomial equation]] is an equation in which a polynomial is set equal to another polynomial.
* A [[transcendental equation]] is an equation involving a transcendental function of one of its  variables.
* A [[functional equation]] is an equation in which the unknowns are [[Function (mathematics)|functions]] rather than simple quantities.
* A [[differential equation]] is an equation involving [[derivative]]s.
* An [[integral equation]] is an equation involving [[integral]]s.
* A [[Diophantine equation]] is an equation where the unknowns are required to be [[integer]]s.
* [[Polynomials]]
* [[Variable (mathematics)|Variables]]

==Branches==
* [[Pre-algebra]]
* [[Elementary algebra]]
* [[Abstract algebra]]
* [[Linear algebra]]
* [[Universal algebra]]

==History==
{{Main|History of algebra}}

==General algebra concepts==
* [[Algebra]] &amp;ndash; 
* [[Cubic equation]] &amp;ndash; 
* [[Fundamental theorem of algebra]] &amp;ndash; 
* [[Linear equation]] &amp;ndash; 
* [[Quadratic equation]] &amp;ndash; 
* [[Quartic equation]] &amp;ndash; 
* [[Quintic equation]] &amp;ndash; 
* [[Polynomial]] &amp;ndash; 
&lt;!--
===Boolean algebra===
[[Algebra of sets]] [[Talk:Algebra of sets| ]] -
[[Algebraic normal form]] [[Talk:Algebraic normal form| ]] -
[[Ampheck]] [[Talk:Ampheck| ]] -
[[And-inverter graph]] [[Talk:And-inverter graph| ]] -
[[George Boole|Boole, George]] [[Talk:George Boole| ]] -
[[Boolean algebra (structure)]] [[Talk:Boolean algebra (structure)| ]] -
[[Boolean algebras canonically defined]] [[Talk:Boolean algebras canonically defined| ]] -
[[Boolean conjunctive query]] -
[[Boolean domain]] [[Talk:Boolean domain| ]] -
[[Boolean function]] [[Talk:Boolean function| ]] -
[[Boolean algebra (logic)]] [[Talk:Boolean algebra (logic)| ]] -
[[Implicant|Boolean implicant]] [[Talk:Implicant| ]] -
[[Boolean prime ideal theorem]] [[Talk:Boolean prime ideal theorem| ]] -
[[Boolean-valued function]] [[Talk:Boolean-valued function| ]] -
[[Boolean-valued model]] [[Talk:Boolean-valued model| ]] -
[[Boolean satisfiability problem]] [[Talk:Boolean satisfiability problem| ]] -
[[Boole's syllogistic]] [[Talk:Boole's syllogistic| ]] -
[[Canonical form (Boolean algebra)]] [[Talk:normal form (Boolean algebra)| ]] -
[[Characteristic function]] [[Talk:Characterisitic function| ]] -
[[Compactness theorem]] [[Talk:compactness theorem| ]] -
[[Complete Boolean algebra]] [[Talk:Complete Boolean algebra| ]] -
[[Consensus theorem]] [[Talk:Consensus theorem| ]] -
[[Augustus De Morgan|De Morgan, Augustus]] [[Talk:Augustus De Morgan| ]] -
[[De Morgan's laws]] [[Talk:de Morgan's laws| ]] -
[[Duality (order theory)]] [[Talk:duality (order theory)| ]] -
[[Entitative graph]] [[Talk:Entitative graph| ]] -
[[Minilog|Espresso heuristic logic minimizer]] -
[[Existential graph]] [[Talk:Existential graph| ]] -
[[First-order logic]] [[Talk:First-order logic| ]] -
[[Formal system]] [[Talk:formal system| ]] -
[[Free Boolean algebra]] [[Talk:free Boolean algebra| ]] -
[[Heyting algebra]] [[Talk:Heyting algebra| ]] -
[[Indicator function]] [[Talk:Indicator function| ]] -
[[Interior algebra]] [[Talk:interior algebra| ]] -
[[William Stanley Jevons|Jevons, William Stanley]] [[Talk:William Stanley Jevons| ]] -
[[Johnston diagram]] [[Talk:Johnston diagram| ]] -
[[Karnaugh map]] [[Talk:Karnaugh map| ]] -
[[Laws of Form]] [[Talk:Laws of Form| ]] -
[[Lindenbaum–Tarski algebra]] [[Talk:Lindenbaum–Tarski algebra| ]] -
[[Logic gate]] [[Talk:logic gate| ]] -
[[Logical connective]] [[Talk:logical connective| ]] -
[[Logical graph]] [[Talk:Logical graph| ]] -
[[Logical matrix]] [[Talk:Logical matrix| ]] -
[[Logical value]] [[Talk:Logical value| ]] -
[[Minimal negation operator]] [[Talk:Minimal negation operator| ]] -
[[Monadic Boolean algebra]] [[Talk:monadic Boolean algebra| ]] -
[[Charles Peirce|Peirce, Charles Sanders]] [[Talk:Charles Peirce| ]] -
[[Peirce's law]] [[Talk:Peirce's law| ]] -
[[Propositional calculus]] [[Talk:propositional calculus| ]] -
[[Sole sufficient operator]] [[Talk:Sole sufficient operator| ]] -
[[Marshall Harvey Stone|Stone, Marshall Harvey]] [[Talk:Marshall Harvey Stone| ]] -
[[Stone duality]] [[Talk:Stone duality| ]] -
[[Stone's representation theorem for Boolean algebras]] [[Talk:Stone's representation theorem for Boolean algebras| ]] -
[[Stone's representation theorem for Boolean algebras|Stone space]] -
[[Topological Boolean algebra]] [[Talk:topological Boolean algebra| ]] -
[[Truth table]] [[Talk:truth table| ]] -
[[Two-element Boolean algebra]] [[Talk:Two-element Boolean algebra| ]] -
[[John Venn|Venn, John]] [[Talk:John Venn| ]] -
[[Venn diagram]] [[Talk:Venn diagram| ]] -
[[Zeroth-order logic]] [[Talk:Zeroth-order logic| ]]
--&gt;

==See also==
{{Portal|Algebra}}

* [[Table of mathematical symbols]]

==External links==
*[http://www.gresham.ac.uk/event.asp?PageId=45&amp;EventId=620 '4000 Years of Algebra'], lecture by Robin Wilson, at [[Gresham College]], 17 October 2007 (available for MP3 and MP4 download, as well as a text file).
* [http://www.exampleproblems.com ExampleProblems.com] Example problems and solutions from [http://www.exampleproblems.com/wiki/index.php/Algebra basic] and [http://www.exampleproblems.com/wiki/index.php/Abstract_Algebra abstract] algebra.

{{Outline footer}}
{{Sister project links|Algebra}}
{{Use dmy dates|date=September 2011}}

[[Category:Algebra| List]]
[[Category:Mathematics-related lists|Algebra]]
[[Category:Outlines|Algebra]]</text>
      <sha1>5tjieb4q1e7yykm5feu5a42t41px64o</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Timeline of geometry</title>
    <ns>0</ns>
    <id>19374248</id>
    <revision>
      <id>600141403</id>
      <parentid>597808251</parentid>
      <timestamp>2014-03-18T11:18:23Z</timestamp>
      <contributor>
        <username>Edward</username>
        <id>4261</id>
      </contributor>
      <minor/>
      <comment>link [[square root of 2]] using [[User:Edward/Find link|Find link]]</comment>
      <text xml:space="preserve" bytes="10730">{{Use dmy dates|date=September 2010}}
A [[timeline]] of '''[[algebra]]''' and '''[[geometry]]'''

==Before 1000 BC==
* ca. 2000 BC — [[Scotland]], [[Carved Stone Balls]] exhibit a variety of symmetries including all of the symmetries of [[Platonic solid]]s.
* 1800 BC — [[Moscow Mathematical Papyrus]], findings volume of a frustum
* 1650 BC — [[Rhind Mathematical Papyrus]], copy of a lost scroll from around 1850 BC, the scribe [[Ahmes]] presents one of the first known approximate values of [[pi|π]] at 3.16, the first attempt at [[squaring the circle]], earliest known use of a sort of [[cotangent]], and knowledge of solving first order linear equations

==1st millennium BC==
* 800 BC — [[Baudhayana]], author of the Baudhayana [[Sulba Sutras|Sulba Sutra]], a [[Vedic Sanskrit]] geometric text, contains [[quadratic equations]], and calculates the [[square root of 2]] correct to five decimal places
* ca. 600 BC — the other [[Vedic civilization|Vedic]] “[[Sulba Sutras]]” (“rule of chords” in [[Sanskrit]]) use [[Pythagorean triples]], contain of a number of geometrical proofs, and approximate [[pi|π]] at 3.16
* 5th century BC — [[Hippocrates of Chios]] utilizes [[Lune (mathematics)|lunes]] in an attempt to [[squaring the circle|square the circle]]
* 5th century BC — [[Apastamba]], author of the Apastamba [[Sulba Sutras|Sulba Sutra]], another [[Vedic Sanskrit]] geometric text, makes an attempt at [[squaring the circle]] and also calculates the [[square root]] of 2 correct to five decimal places
* 530 BC — [[Pythagoras]] studies propositional [[geometry]] and vibrating lyre strings; his group also discover the [[irrational number|irrationality]] of the [[square root]] of [[two]],
* 370 BC — [[Eudoxus of Cnidus|Eudoxus]] states the [[method of exhaustion]] for [[area]] determination
* 300 BC — [[Euclid]] in his ''[[Euclid's Elements|Elements]]'' studies [[geometry]] as an [[axiomatic system]], proves the [[Infinite set|infinitude]] of [[prime number]]s and presents the [[Euclidean algorithm]]; he states the law of reflection in  ''Catoptrics'', and he proves the [[fundamental theorem of arithmetic]]
* 260 BC — [[Archimedes]] [[method of exhaustion|proved]] that the value of [[pi|π]] lies between 3&amp;nbsp;+&amp;nbsp;1/7 (approx. 3.1429) and 3&amp;nbsp;+&amp;nbsp;10/71 (approx. 3.1408), that the area of a circle was equal to π multiplied by the square of the radius of the circle and that the area enclosed by a parabola and a straight line is 4/3 multiplied by the area of a triangle with equal base and height. He also gave a very accurate estimate of the value of the square root of 3.
* 225 BC — [[Apollonius of Perga]] writes  ''On [[Conic section|Conic Sections]]'' and names the [[ellipse]], [[parabola]], and [[hyperbola]],
* 150 BC — [[Jainism|Jain]] mathematicians in [[History of India|India]] write the “Sthananga Sutra”, which contains work on the theory of numbers, arithmetical operations, [[geometry]], operations with [[fractions]], simple equations, [[cubic equations]], quartic equations, and [[permutations]] and [[combinations]]
* 140 BC — [[Hipparchus]] develops the bases of [[trigonometry]].

==1st millennium==
* ca. 340 — [[Pappus of Alexandria]] states his [[Pappus's hexagon theorem|hexagon theorem]] and his [[Pappus's centroid theorem|centroid theorem]]
* 500 — [[Aryabhata]] writes the “Aryabhata-Siddhanta”, which first introduces the trigonometric functions and methods of calculating their approximate numerical values. It defines the concepts of [[sine]] and [[cosine]], and also contains the [[Aryabhata's sine table|earliest tables of sine]] and cosine values (in 3.75-degree intervals from 0 to 90 degrees)
* 600s — [[Bhaskara I]] gives a rational approximation of the sine function
* 700s — [[Virasena]] gives explicit rules for the [[Fibonacci sequence]], gives the derivation of the [[volume]] of a [[frustum]] using an [[Infinity|infinite]] procedure, and also deals with the [[logarithm]] to [[base 2]] and knows its laws
* 700s — [[Shridhara]] gives the rule for finding the volume of a sphere and also the formula for solving quadratic equations
* 820 — [[Al-Mahani]] conceived the idea of reducing [[Geometry|geometrical]] problems such as [[doubling the cube]] to problems in algebra.
* ca. 900 — [[Abu Kamil]] of Egypt had begun to understand what we would write in symbols as &lt;math&gt;x^n \cdot x^m = x^{m+n}&lt;/math&gt;
* 975 — [[Al-Batani]] — Extended the Indian concepts of sine and cosine to other trigonometrical ratios, like tangent, secant and their inverse functions. Derived the formula: &lt;math&gt; \sin \alpha = \tan \alpha / \sqrt{1+\tan^2 \alpha} &lt;/math&gt; and &lt;math&gt; \cos \alpha = 1 / \sqrt{1 + \tan^2 \alpha}&lt;/math&gt;.

==1000–1500==
*ca. 1000 — [[Law of sines]] is discovered by [[Islamic mathematics|Muslim mathematicians]], but it is uncertain who discovers it first between [[Abu-Mahmud al-Khujandi]], [[Abu Nasr Mansur]], and [[Abū al-Wafā' al-Būzjānī|Abu al-Wafa]].
* ca. 1100 — [[Omar Khayyám]] “gave a complete classification of [[cubic equation]]s with geometric solutions found by means of intersecting [[conic section]]s.” He became the first to find general [[geometry|geometric]] solutions of [[cubic equation]]s and laid the foundations for the development of [[analytic geometry]] and [[non-Euclidean geometry]]. He also extracted [[root of a function|roots]] using the [[decimal]] system ([[Hindu-Arabic numeral system]]).
* 1135 — [[Sharafeddin Tusi]] followed al-Khayyam's application of algebra to geometry, and wrote a treatise on [[cubic equation]]s which “represents an essential contribution to another [[algebra]] which aimed to study [[curve]]s by means of [[equation]]s, thus inaugurating the beginning of [[algebraic geometry]].”&lt;ref name=MacTutor&gt;[http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/Arabic_mathematics.html Arabic mathematics], ''[[MacTutor History of Mathematics archive]]'', [[University of St Andrews]], Scotland&lt;/ref&gt;
* ca. 1250 — [[Nasir Al-Din Al-Tusi]] attempts to develop a form of [[non-Euclidean geometry]].
* 15th century — [[Nilakantha Somayaji]], a [[Kerala school of astronomy and mathematics|Kerala school]] mathematician, writes the “Aryabhatiya Bhasya”, which contains work on infinite-series expansions, problems of algebra, and spherical geometry

==17th century==
* 1600s – Putumana Somayaji writes the &quot;Paddhati&quot;, which presents a detailed discussion of various trigonometric series
* 1619 –  [[Johannes Kepler]] discovers two of the [[Kepler-Poinsot polyhedra]].

==18th century==
* 1722 –  [[Abraham de Moivre]] states [[de Moivre's formula]] connecting [[trigonometric function]]s and [[complex number]]s,
* 1733 –  [[Giovanni Gerolamo Saccheri]] studies what geometry would be like if [[parallel postulate|Euclid's fifth postulate]] were false,
* 1796 –  [[Carl Friedrich Gauss]] proves that the [[heptadecagon|regular 17-gon]] can be constructed using only a [[compass and straightedge]]
* 1797 –  [[Caspar Wessel]] associates vectors with [[complex number]]s and studies complex number operations in geometrical terms,

==19th century==
* 1806 –  [[Louis Poinsot]] discovers the two remaining [[Kepler-Poinsot polyhedra]].
* 1829 –  [[Bolyai]], [[Carl Friedrich Gauss|Gauss]], and [[Nikolai Ivanovich Lobachevsky|Lobachevsky]] invent hyperbolic [[non-Euclidean geometry]],
* 1837 –  [[Pierre Wantzel]] proves that doubling the cube and [[trisecting the angle]] are impossible with only a compass and straightedge, as well as the full completion of the problem of [[Constructible polygon|constructibility]] of regular polygons
* 1843 –  [[William Rowan Hamilton|William Hamilton]] discovers the calculus of [[quaternion]]s and deduces that they are non-commutative,
* 1854 –  [[Bernhard Riemann]] introduces [[Riemannian geometry]],
* 1854 –  [[Arthur Cayley]] shows that [[quaternion]]s can be used to represent rotations in four-dimensional [[space]],
* 1858 –  [[August Ferdinand Möbius]] invents the [[Möbius strip]],
* 1870 –  [[Felix Klein]] constructs an analytic geometry for Lobachevski's geometry thereby establishing its self-consistency and the logical independence of Euclid's fifth postulate,
* 1873 –  [[Charles Hermite]] proves that [[e (mathematical constant)|e]] is transcendental,
* 1878 – Charles Hermite solves the general quintic equation by means of elliptic and modular functions
* 1882 –  [[Ferdinand von Lindemann]] proves that π is transcendental and that therefore the circle cannot be squared with a compass and straightedge,
* 1882 –  Felix Klein invents the [[Klein bottle]],
* 1899 –  [[David Hilbert]] presents a set of self-consistent geometric axioms in ''Foundations of Geometry''

==20th century==
* 1901 –  [[Élie Cartan]] develops the [[exterior derivative]],
* 1905 – [[Albert Einstein|Einstein's]] theory of [[special relativity]].
* 1912 –  [[Luitzen Egbertus Jan Brouwer]] presents the [[Brouwer fixed-point theorem]],
* 1916 – [[Albert Einstein|Einstein's]] theory of [[general relativity]].
* 1930 –  [[Casimir Kuratowski]] shows that the [[three-cottage problem]] has no solution,
* 1931 –  [[Georges de Rham]] develops theorems in [[cohomology]] and [[characteristic class]]es,
* 1933 –  [[Karol Borsuk]] and [[Stanislaw Ulam]] present the [[Borsuk-Ulam Theorem|Borsuk-Ulam antipodal-point theorem]],
* 1955 –  [[H. S. M. Coxeter]] et al. publish the complete list of [[uniform polyhedron]],
* 1975 –  [[Benoit Mandelbrot]], [[fractal]]s theory,
* 1981 – [[Mikhail Gromov (mathematician)|Mikhail Gromov]] develops the theory of [[hyperbolic group]]s, revolutionizing both infinite group theory and global differential geometry,
* 1983 –  the [[classification of finite simple groups]], a collaborative work involving some hundred mathematicians and spanning thirty years, is completed,
* 1991 –  [[Alain Connes]] and [[John Lott (mathematician)|John Lott]] develop [[non-commutative geometry]],
* 1998 –  [[Thomas Callister Hales]] (almost certainly) proves the [[Kepler conjecture]],

==21st century==
* 2003 – [[Grigori Perelman]] proves the [[Poincaré conjecture]],
* 2007 – a team of researches throughout North America and Europe used networks of computers to map [[E8 (mathematics)]].&lt;ref&gt;Elizabeth A. Thompson, MIT News Office, ''Math research team maps E8'' http://www.huliq.com/15695/mathematicians-map-e8&lt;/ref&gt;

==References==
{{Reflist}}&lt;!--added above categories/infobox footers by script-assisted edit--&gt;

{{DEFAULTSORT:Timeline Of Algebra And Geometry}}
[[Category:Mathematics timelines|Algebra and geometry]]
[[Category:Algebra| Timeline]]
[[Category:Geometry| ]]</text>
      <sha1>iv3k819daixyjfdpqerxrsnhe4u62mz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cluster algebra</title>
    <ns>0</ns>
    <id>27038423</id>
    <revision>
      <id>610408830</id>
      <parentid>610408568</parentid>
      <timestamp>2014-05-27T22:13:27Z</timestamp>
      <contributor>
        <username>R.e.b.</username>
        <id>217736</id>
      </contributor>
      <comment>/* References */ Adding/improving reference(s)</comment>
      <text xml:space="preserve" bytes="12248">'''Cluster algebras''' are a class of [[commutative ring]]s introduced by {{harvs|txt|last=Fomin|author1-link=Sergey Fomin|last2=Zelevinsky|author2-link=Andrei Zelevinsky|year=2002|year2=2003|year3=2007}}.  A cluster algebra of rank ''n'' is an [[integral domain]] ''A'', together with some subsets of size ''n'' called clusters whose union generates the algebra ''A'' and which satisfy various conditions.

==Definitions==

Suppose that ''F'' is an integral domain, such as the field '''Q'''(''x''&lt;sub&gt;1&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''&lt;/sub&gt;) of rational functions in ''n'' variables over the rational numbers '''Q'''. 

A '''cluster''' of '''rank''' ''n'' consists of a set of ''n'' elements {''x'', ''y'', ...} of ''F'', usually assumed to be an algebraically independent set of generators of a field extension ''F''.

A '''seed''' consists of a cluster {''x'',''y'',...}  of ''F'', together with an '''exchange matrix'''  ''B'' with integer entries ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; indexed by  pairs of elements ''x'', ''y'' of the cluster. The matrix is sometimes assumed to be skew symmetric, so that ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; = –''b''&lt;sub&gt;''y'',''x''&lt;/sub&gt;. More generally the matrix might be skew symmetrizable, meaning there are positive integers ''d''&lt;sub&gt;''x''&lt;/sub&gt; associated with the elements of the cluster such that ''d''&lt;sub&gt;''x''&lt;/sub&gt;''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; = –''d''&lt;sub&gt;''y''&lt;/sub&gt;''b''&lt;sub&gt;''y'',''x''&lt;/sub&gt;. It is common to picture a seed as a [[quiver (mathematics)|quiver]] with vertices the generating set, by drawing ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; arrows from ''x'' to ''y'' if this number is positive. When ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; is skew symmetrizable the quiver has no loops or 2-cycles.

A '''mutation''' of a seed, depending on a choice of vertex ''y'' of the cluster,  is a new seed given by a generalization of [[Tilting theory| tilting]] as follows. Exchange the values of ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; and ''b''&lt;sub&gt;''y'',''x''&lt;/sub&gt; for all ''x'' in the cluster. If  ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt; &gt; 0 and ''b''&lt;sub&gt;''y'',''z''&lt;/sub&gt; &gt; 0  then replace ''b''&lt;sub&gt;''x'',''z''&lt;/sub&gt; by  ''b''&lt;sub&gt;''x'',''y''&lt;/sub&gt;''b''&lt;sub&gt;''y'',''z''&lt;/sub&gt; + ''b''&lt;sub&gt;''x'',''z''&lt;/sub&gt;. Finally replace ''y'' by a new generator ''w'', where 
:&lt;math&gt;wy=\prod_{t,b_{t,y}&gt;0}t^{b_{t,y}} + \prod_{t,b_{t,y}&lt;0}t^{-b_{t,y}}&lt;/math&gt;
where the products run through the elements ''t'' in the cluster of the seed such that ''b''&lt;sub&gt;''t'',''y''&lt;/sub&gt; is positive or negative respectively. The inverse of a mutation is also a mutation: in other words, if ''A'' is a mutation of ''B'', then ''B'' is a mutation of ''A''. 

A '''cluster algebra''' is constructed from a seed as follows. 
If we repeatedly mutate the seed in all possible ways, we get a finite or infinite graph of seeds, where two seeds are joined if one can be obtained by mutating the other. The underlying algebra of the cluster algebra is the algebra generated by all the clusters of all the seeds in this graph. The cluster algebra  also comes with the extra structure of the seeds of this graph.

A cluster algebra is said to be of '''finite type''' if it has only a finite number of seeds. {{harvtxt|Fomin|Zelevinsky|2003}} showed that the cluster algebras of finite type can be classified in terms of the  [[Dynkin diagram]]s of finite-dimensional [[simple Lie algebra]]s.

==Examples==

===Cluster algebras of rank 1===

If {''x''} is the cluster of a seed of rank 1, then the only mutation takes this to {2''x''&lt;sup&gt;–1&lt;/sup&gt;}. So a cluster algebra of rank 1 is just a ring ''k''[''x'',''x''&lt;sup&gt;–1&lt;/sup&gt;] of Laurent polynomials, and it has just two clusters, {''x''} and {2''x''&lt;sup&gt;–1&lt;/sup&gt;}. In particular it is of finite type and is associated with the Dynkin diagram A&lt;sub&gt;1&lt;/sub&gt;.

===Cluster algebras of rank 2===

Suppose that we start with the cluster {''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;} and take the exchange matrix with ''b''&lt;sub&gt;12&lt;/sub&gt;=–b&lt;sub&gt;21&lt;/sub&gt;=1. Then mutation gives a sequence of variables ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;, ''x''&lt;sub&gt;4&lt;/sub&gt;,... such that the clusters are given by adjacent pairs {''x''&lt;sub&gt;''n''&lt;/sub&gt;,''x''&lt;sub&gt;''n''+1&lt;/sub&gt;}. The variables are related by 
:&lt;math&gt;\displaystyle x_{n-1}x_{n+1}=1+x_n&lt;/math&gt;
so are given by the sequence

:&lt;math&gt; x_1, x_2, x_3=\frac{1+x_2}{x_1}, x_4=\frac{1+x_3}{x_2}=\frac{1+x_1+x_2}{x_1x_2},  &lt;/math&gt;
: &lt;math&gt; x_5=\frac{1+x_4}{x_3}=\frac{1+x_1}{x_2},
x_6=\frac{1+x_5}{x_4}=x_1,x_7=\frac{1+x_6}{x_5}=x_2,\ldots&lt;/math&gt;
which repeats with period 5. So this cluster algebra has exactly 5 clusters, and in particular is of finite type. It is associated with the Dynkin diagram A&lt;sub&gt;2&lt;/sub&gt;.

There are similar examples with ''b''&lt;sub&gt;12&lt;/sub&gt; = 1, –''b''&lt;sub&gt;21&lt;/sub&gt; = 2 or 3, where the analogous sequence of cluster variables repeats with period 6 or 8. These are also of finite type, and are associated with the Dynkin diagrams B&lt;sub&gt;2&lt;/sub&gt; and G&lt;sub&gt;2&lt;/sub&gt;. However if |''b''&lt;sub&gt;12&lt;/sub&gt;''b''&lt;sub&gt;21&lt;/sub&gt;| ≥ 4 then the sequence of cluster variables is not periodic and the cluster algebra is of infinite type.

===Cluster algebras of rank 3===

Suppose we start with the quiver ''x''&lt;sub&gt;1&lt;/sub&gt;→''x''&lt;sub&gt;2&lt;/sub&gt;→''x''&lt;sub&gt;3&lt;/sub&gt;. Then the 14 clusters are:
:&lt;math&gt;\left\{ x_1,x_2,x_3 \right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{1+x_2}{x_1},x_2,x_3 \right\},&lt;/math&gt;
:&lt;math&gt;\left\{x_1, \frac{x_1 + x_3}{x_2},x_3 \right\},&lt;/math&gt; 
:&lt;math&gt;\left\{x_1,x_2,\frac{1+x_2}{x_3}\right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{1+x_2}{x_1}, \frac{x_1 +(1+x_2)x_3}{x_1 x_2},x_3 \right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{1+x_2}{x_1},x_2,\frac{1+x_2}{x_3} \right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{x_1+(1+x_2)x_3}{x_1x_2},\frac{x_1 + x_3}{x_2},x_3 \right\},&lt;/math&gt;
:&lt;math&gt;\left\{x_1,\frac{x_1+x_3}{x_2},\frac{(1+x_2)x_1+x_3}{x_2x_3} \right\},&lt;/math&gt; 
:&lt;math&gt;\left\{x_1,\frac{(1+x_2)x_1 + x_3}{x_2 x_3},\frac{1+x_2}{x_3} \right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{1+x_2}{x_1},\frac{x_1+(1+x_2)x_3}{x_1 x_2},\frac{(1+x_2)x_1 +(1+x_2)x_3}{x_1 x_2x_3}\right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{1+x_2}{x_1},\frac{(1+x_2)x_1 +(1+x_2)x_3}{x_1 x_2x_3},
\frac{1+x_2}{x_3} \right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{x_1+(1+x_2)x_3}{x_1x_2},\frac{x_1+x_3}{x_2},\frac{(1+x_2)x_1+(1+x_2)x_3}{x_1 x_2 x_3} \right\},&lt;/math&gt;
:&lt;math&gt;\left\{\frac{(1+x_2)x_1 +(1+x_2)x_3}{x_1 x_2x_3},\frac{x_1+x_3}{x_2},\frac{(1+x_2)x_1+x_3}{x_2 x_3} \right\},&lt;/math&gt; 
:&lt;math&gt;\left\{\frac{(1+x_2)x_1+(1+x_2)x_3}{x_1 x_2 x_3},\frac{(1+x_2)x_1+x_3}{x_2 x_3},\frac{1+x_2}{x_3} \right\}.&lt;/math&gt;

There are 6 cluster variables other than the 3 initial ones ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt; given by
:&lt;math&gt;\frac{1+x_2}{x_1},\frac{x_1 + x_3}{x_2},\frac{1+x_2}{x_3}, \frac{x_1+(1+x_2)x_3}{x_1x_2}, \frac{(1+x_2)x_1+x_3}{x_2 x_3}, \frac{(1+x_2)x_1 +(1+x_2)x_3}{x_1 x_2x_3}&lt;/math&gt;. 
They correspond to the 6 positive roots of the Dynkin diagram A&lt;sub&gt;3&lt;/sub&gt;: more precisely the denominators are monomials in ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;, corresponding to the expression of positive roots as the sum of simple roots. 
The 3+6 cluster variables generate a cluster algebra of finite type, associated with the Dynkin diagram A&lt;sub&gt;3&lt;/sub&gt;.
The 14 clusters are the vertices of the cluster graph, which is an [[associahedron]].

===Grassmannians===
Simple examples are given by the algebras of homogeneous functions on the [[Grassmannian]]s. The Plücker coordinates provide some of the distinguished elements.

[[File:Cluster algebra.svg|thumb|300px|Mutation between two triangulations of the heptagon]]
For the Grassmannian of planes in ℂ&lt;sup&gt;''n''&lt;/sup&gt;, the situation is even more simple. In that case, the Plücker coordinates provide all the distinguished elements and the clusters can be completely described using [[Polygon triangulation|triangulation]]s of a [[regular polygon]] with ''n'' vertices. More precisely, clusters are in one-to-one correspondence with triangulations and the distinguished elements are in one-to-one correspondence with diagonals (line segments joining two vertices of the polygon). One can distinguish between diagonals in the boundary, which belong to every cluster, and diagonals in the interior. This corresponds to a general distinction between coefficient variables and cluster variables.

===Cluster algebras arising from surfaces===
Suppose '''S''' is a compact connected oriented Riemann surface and '''M''' is a non-empty finite set of points in '''S''' that contains at least one point from each boundary component of '''S''' (the boundary of '''S''' is not assumed to be either empty or non-empty). The pair ('''S''','''M''') is often referred to as a ''bordered surface with marked points''. It has been shown by Fomin-Shapiro-Thurston that if '''S''' is not a closed surface, or if '''M''' has more than one point, then the (tagged) arcs on ('''S''','''M''') parameterize the set of cluster variables of certain cluster algebra ''A''('''S''','''M'''), which depends only on ('''S''','''M''') and the choice of some coefficient system, in such a way that the set of (tagged) triangulations of ('''S''','''M''') is in one-to-one correspondence with the set of clusters of ''A''('''S''','''M'''), two (tagged) triangulations being related by a ''flip'' if and only if the clusters they correspond to are related by cluster mutation.

==References==
*{{Citation | last1=Berenstein | first1=Arkady | last2=Fomin | first2=Sergey | last3=Zelevinsky | first3=Andrei | title=Cluster algebras. III. Upper bounds and double Bruhat cells | doi=10.1215/S0012-7094-04-12611-9 | mr=2110627 | year=2005 | journal=[[Duke Mathematical Journal]] | volume=126 | issue=1 | pages=1–52}}
*{{Citation | last1=Fomin | first1=Sergey | last2=Shapiro | first2=Michael | last3=Thurston | first3=Dylan | title=Cluster algebras and triangulated surfaces, part I: Cluster complexes. | year=2008 | journal=[[Acta Mathematica]] | volume=201 | issue= | pages=83-146 | doi=10.1007/s11511-008-0030-7}}
*{{Citation | last1=Fomin | first1=Sergey | last2=Zelevinsky | first2=Andrei | title=Cluster algebras. I. Foundations | doi=10.1090/S0894-0347-01-00385-X | mr=1887642 | year=2002 | journal=[[Journal of the American Mathematical Society]]   | volume=15 | issue=2 | pages=497–529}}
*{{Citation | last1=Fomin | first1=Sergey | last2=Zelevinsky | first2=Andrei | title=Cluster algebras. II. Finite type classification | doi=10.1007/s00222-003-0302-y | mr=2004457 | year=2003 | journal=[[Inventiones Mathematicae]]   | volume=154 | issue=1 | pages=63–121}}
*{{Citation | last1=Fomin | first1=Sergey | last2=Zelevinsky | first2=Andrei | title=Cluster algebras. IV. Coefficients | doi=10.1112/S0010437X06002521 | mr=2295199 | year=2007 | journal=Compositio Mathematica   | volume=143 | issue=1 | pages=112–164}}
*{{Citation | last1=Fomin | first1=Sergey | last2=Reading | first2=Nathan | editor1-last=Miller | editor1-first=Ezra | editor2-last=Reiner | editor2-first=Victor | editor3-last=Sturmfels | editor3-first=Bernd | editor3-link=Bernd Sturmfels | title=Geometric combinatorics |chapter=Root systems and generalized associahedra |publisher=Amer. Math. Soc. | location=Providence, R.I. | series=IAS/Park City Math. Ser. | mr=2383126 | year=2007 | volume=13 | isbn=978-0-8218-3736-8|arxiv=math/0505518}}
*{{citation|MR=3155783 
|last=Marsh|first= Robert J.
|title=Lecture notes on cluster algebras. 
|series=Zurich Lectures in Advanced Mathematics|publisher= European Mathematical Society (EMS)|place= Zürich|year= 2013|isbn=  978-3-03719-130-9|doi=10.4171/130 }}
*{{Citation | last1=Reiten | first1=Idun | title=Tilting theory and cluster algebras | url=http://arxiv.org/abs/1012.6014 | series=Trieste Proceedings of Workshop | year=2010}}
* {{citation | first = Andrei | last = Zelevinsky | title = What Is . . . a Cluster Algebra? | journal = AMS Notices | volume = 54 | issue = 11 | pages = 1494–1495 | year = 2007 | url = http://www.ams.org/notices/200711/tx071101494p.pdf }}.

==External links==

*Fomin's [http://www.math.lsa.umich.edu/~fomin/cluster.html Cluster algebra portal]
*[http://www.math.lsa.umich.edu/~fomin/papers.html Fomin's papers on cluster algebras]
*[http://front.math.ucdavis.edu/author/A.Zelevinsky Zelevinsky's papers on cluster algebras]
[[Category:Algebra]]</text>
      <sha1>r28xz6y4y3guvnp0nymv4qt0p9wt0uc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Recurrence relation</title>
    <ns>0</ns>
    <id>146806</id>
    <revision>
      <id>611191120</id>
      <parentid>611086578</parentid>
      <timestamp>2014-06-02T06:07:50Z</timestamp>
      <contributor>
        <username>InverseHypercube</username>
        <id>452348</id>
      </contributor>
      <minor/>
      <comment>/* Computer Science */</comment>
      <text xml:space="preserve" bytes="38903">{{redirect-distinguish|Difference equation|differential equation}}
In [[mathematics]], a '''recurrence relation''' is an [[equation]] that [[recursion|recursively]] defines a [[sequence]], once one or more initial terms are given: each further term of the sequence is defined as a [[Function (mathematics)|function]] of the preceding terms.

The term '''[[#Relationship_to_difference_equations_narrowly_defined|difference equation]]''' sometimes (and for the purposes of this article) refers to a specific type of recurrence relation.  However, &quot;difference equation&quot; is frequently used to refer to ''any'' recurrence relation.

An example of a recurrence relation is the [[logistic map]]:

:&lt;math&gt;x_{n+1} = r x_n (1 - x_n),&lt;/math&gt;

with a given constant ''r''; given the initial term ''x''&lt;sub&gt;0&lt;/sub&gt; each subsequent term is determined by this relation.

Some simply defined recurrence relations can have very complex ([[chaos theory|chaotic]]) behaviours, and they are a part of the field of mathematics known as [[Nonlinearity|nonlinear analysis]].

Solving a recurrence relation means obtaining a [[closed-form solution]]: a non-recursive function of ''n''.

== Fibonacci numbers ==
The [[Fibonacci number]]s are the archetype of a linear, homogeneous recurrence relation with constant coefficients (see below).  They are defined using the linear recurrence relation

:&lt;math&gt;F_n = F_{n-1}+F_{n-2}&lt;/math&gt;

with seed values:

:&lt;math&gt;F_0 = 0&lt;/math&gt;
:&lt;math&gt;F_1 = 1&lt;/math&gt;

Explicitly, recurrence yields the equations:
:&lt;math&gt;F_2 = F_1 + F_0&lt;/math&gt;
:&lt;math&gt;F_3 = F_2 + F_1&lt;/math&gt;
:&lt;math&gt;F_4 = F_3 + F_2&lt;/math&gt;
etc.

We obtain the sequence of Fibonacci numbers which begins:
:0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...

It can be solved by methods described below yielding the [[Binet formula|closed-form expression]] which involve powers of the two roots of the characteristic polynomial ''t''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''t''&amp;nbsp;+&amp;nbsp;1; the [[generating function]] of the sequence is the [[rational function]]
:  &lt;math&gt;\frac{t}{1-t-t^2}.&lt;/math&gt;

== Structure ==

===Linear homogeneous recurrence relations with constant coefficients===
An order ''d'' '''linear homogeneous recurrence relation with constant coefficients'''  is an equation of the form

:&lt;math&gt;a_n = c_1a_{n-1} + c_2a_{n-2}+\cdots+c_da_{n-d},&lt;/math&gt;

where the ''d'' coefficients ''c&lt;sub&gt;i&lt;/sub&gt;'' (for all ''i'') are constants.

More precisely, this is an infinite list of simultaneous linear equations, one for each ''n''&amp;gt;''d''−1. A sequence which satisfies a relation of this form is called a '''linear recurrence sequence''' or LRS. There are ''d'' [[Degrees of freedom (statistics)|degrees of freedom]] for LRS, i.e., the initial values &lt;math&gt;a_0,\dots,a_{d-1}&lt;/math&gt; can be taken to be any values but then the linear recurrence determines the sequence uniquely.

The same coefficients yield the [[characteristic polynomial]] (also &quot;auxiliary polynomial&quot;)

:&lt;math&gt;p(t)= t^d - c_1t^{d-1} - c_2t^{d-2}-\cdots-c_{d}&lt;/math&gt;
whose ''d'' roots play a crucial role in finding and understanding the sequences satisfying the recurrence. If the roots ''r''&lt;sub&gt;1&lt;/sub&gt;, ''r''&lt;sub&gt;2&lt;/sub&gt;, ... are all distinct, then the solution to the recurrence takes the form
:&lt;math&gt;a_n = k_1 r_1^n + k_2 r_2^n + \cdots + k_d r_d^n,&lt;/math&gt;
where the coefficients ''k&lt;sub&gt;i&lt;/sub&gt;'' are determined in order to fit the initial conditions of the recurrence. When the same roots occur multiple times, the terms in this formula corresponding to the second and later occurrences of the same root are multiplied by increasing powers of ''n''. For instance, if the characteristic polynomial can be factored as (''x''−''r'')&lt;sup&gt;3&lt;/sup&gt;, with the same root ''r'' occurring three times, then the solution would take the form
:&lt;math&gt;a_n = k_1 r^n + k_2 n r^n + k_3 n^2 r^n.&lt;/math&gt;&lt;ref&gt;{{citation|contribution=2.1.1 Constant coefficients – A) Homogeneous equations|title=Mathematics for the Analysis of Algorithms|first1=Daniel H.|last1=Greene|first2=Donald E.|last2=Knuth|author2-link=Donald Knuth| edition=2nd| publisher=Birkhäuser |page=17 | year=1982}}.&lt;/ref&gt;

As well as the [[Fibonacci number]]s, other sequences generated by linear homogeneous recurrences include the [[Lucas number]]s and [[Lucas sequence]]s, the [[Jacobsthal number]]s, the [[Pell number]]s and more generally the solutions to [[Pell's equation]].

=== Rational generating function ===
Linear recursive sequences are precisely the sequences whose [[generating function]] is a [[rational function]]: the denominator is the polynomial obtained from the auxiliary polynomial by reversing the order of the coefficients, and the numerator is determined by the initial values of the sequence.

The simplest cases are periodic sequences, &lt;math&gt;a_n = a_{n-d}, n\geq d&lt;/math&gt;, which have sequence &lt;math&gt;a_0,a_1,\dots,a_{d-1},a_0,\dots&lt;/math&gt; and generating function a sum of geometric series:
:&lt;math&gt;\begin{align}\frac{a_0 + a_1 x^1 + \cdots + a_{d-1}x^{d-1}}{1-x^d} =&amp; \left(a_0 + a_1 x^1 + \cdots + a_{d-1}x^{d-1}\right) + \left(a_0 + a_1 x^1 + \cdots + a_{d-1}x^{d-1}\right)x^d + \\&amp; \left(a_0 + a_1 x^1 + \cdots + a_{d-1}x^{d-1}\right)x^{2d} + \cdots.\end{align}&lt;/math&gt;

More generally, given the recurrence relation:
:&lt;math&gt;a_n = c_1a_{n-1} + c_2a_{n-2}+\cdots+c_da_{n-d}&lt;/math&gt;
with generating function

:&lt;math&gt;a_0 + a_1x^1 + a_2 x^2 + \cdots,&lt;/math&gt;

the series is annihilated at ''a&lt;sub&gt;d&lt;/sub&gt;'' and above by the polynomial:

:&lt;math&gt;1- c_1x^1 - c_2 x^2 - \cdots - c_dx^d.&lt;/math&gt;

That is, multiplying the generating function by the polynomial yields

:&lt;math&gt;b_n = a_n - c_1 a_{n-1} - c_2 a_{n-2} - \cdots - c_d a_{n-d}&lt;/math&gt;

as the coefficient on &lt;math&gt;x^n&lt;/math&gt;, which vanishes (by the recurrence relation) for ''n'' ≥ ''d''.  Thus

:&lt;math&gt;\left (a_0 + a_1x^1 + a_2 x^2 + \cdots \right ) \left  (1- c_1x^1 - c_2 x^2 - \cdots - c_dx^d \right) = \left (b_0 + b_1x^1 + b_2 x^2 + \cdots + b_{d-1} x^{d-1} \right )&lt;/math&gt;

so dividing yields

:&lt;math&gt;a_0 + a_1x^1 + a_2 x^2 + \cdots  =\frac{b_0 + b_1x^1 + b_2 x^2 + \cdots + b_{d-1} x^{d-1}}{1- c_1x^1 - c_2 x^2 - \cdots - c_dx^d},&lt;/math&gt;

expressing the generating function as a rational function.

The denominator is &lt;math&gt;x^d p\left(x^{-1}\right),&lt;/math&gt; a transform of the auxiliary polynomial (equivalently, reversing the order of coefficients); one could also use any multiple of this, but this normalization is chosen both because of the simple relation to the auxiliary polynomial, and so that &lt;math&gt;b_0 = a_0&lt;/math&gt;.

=== Relationship to difference equations narrowly defined ===
Given an ordered [[sequence]] &lt;math&gt;\left\{a_n\right\}_{n=1}^\infty&lt;/math&gt; of [[real numbers]]: the '''first difference''' &lt;math&gt;\Delta(a_n)&lt;/math&gt; is defined as

:&lt;math&gt;\Delta(a_n) = a_{n+1} - a_n\,&lt;/math&gt;.

The '''second difference''' &lt;math&gt;\Delta^2(a_n)&lt;/math&gt; is defined as

:&lt;math&gt;\Delta^2(a_n) = \Delta(a_{n+1}) - \Delta(a_n)&lt;/math&gt;,

which can be simplified to

:&lt;math&gt;\Delta^2(a_n) = a_{n+2} - 2a_{n+1} + a_n&lt;/math&gt;.

More generally: the '''''k''&lt;sup&gt;th&lt;/sup&gt; difference''' of the sequence ''a&lt;sub&gt;n&lt;/sub&gt;'' is written as &lt;math&gt;\Delta^k(a_n)&lt;/math&gt; is defined recursively as

:&lt;math&gt;\Delta^k(a_n) = \Delta^{k-1}(a_{n+1}) - \Delta^{k-1}(a_n)=\sum_{t=0}^k \binom{k}{t} (-1)^t a_{n+k-t}&lt;/math&gt;.

(The sequence and its differences are related by a [[binomial transform]].) The more restrictive definition of '''difference equation''' is an equation composed of ''a&lt;sub&gt;n&lt;/sub&gt;'' and its ''k''&lt;sup&gt;th&lt;/sup&gt; differences. (A widely used broader definition treats &quot;difference equation&quot; as synonymous with &quot;recurrence relation&quot;.  See for example [[rational difference equation]] and [[matrix difference equation]].)

Actually, it is easily seen that 
&lt;math&gt;a_{n+k} = 
{n\choose 0}a_n + {n\choose 1} \Delta(a_n)  + \cdots + {n\choose k} \Delta^n(a_n).
&lt;/math&gt;
Thus, a difference equation can be defined as an equation that involves 
''a&lt;sub&gt;n&lt;/sub&gt;'', ''a&lt;sub&gt;n-1&lt;/sub&gt;'', ''a&lt;sub&gt;n-2&lt;/sub&gt;'' etc. (or equivalenty
''a&lt;sub&gt;n&lt;/sub&gt;'', ''a&lt;sub&gt;n+1&lt;/sub&gt;'', ''a&lt;sub&gt;n+2&lt;/sub&gt;'' etc.)

Since difference equations are a very common form of recurrence, some authors use the two terms interchangeably. For example, the difference equation

:&lt;math&gt;3\Delta^2(a_n) + 2\Delta(a_n) + 7a_n = 0&lt;/math&gt;

is equivalent to the recurrence relation

:&lt;math&gt;3a_{n+2} = 4a_{n+1} - 8a_n&lt;/math&gt;

Thus one can solve many recurrence relations by rephrasing them as difference equations, and then solving the difference equation, analogously to how one solves [[ordinary differential equations]].  However, the [[Ackermann number]]s are an example of a recurrence relation that do not map to a difference equation, much less points on the solution to a differential equation.

See [[time scale calculus]] for a unification of the theory of difference equations with that of [[differential equations]].

[[Summation equation]]s relate to difference equations as [[integral equation]]s relate to differential equations.

====From sequences to grids====
Single-variable or one-dimensional recurrence relations are about sequences (i.e. functions defined on one-dimensional grids). Multi-variable or n-dimensional recurrence relations are about n-dimensional grids. Functions defined on n-grids can also be studied with '''partial difference equations'''.&lt;ref&gt;[http://books.google.com/books?id=1klnDGelHGEC Partial difference equations], Sui Sun Cheng, CRC Press, 2003, ISBN 978-0-415-29884-1&lt;/ref&gt;

== Solving ==

=== General methods ===
For order 1, the recurrence
:&lt;math&gt;a_{n}=r a_{n-1}&lt;/math&gt;
has the solution ''a&lt;sub&gt;n&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;''r&lt;sup&gt;n&lt;/sup&gt;'' with ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;1 and the most general solution is ''a&lt;sub&gt;n&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;''kr&lt;sup&gt;n&lt;/sup&gt;'' with ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''k''. The  characteristic polynomial equated to zero (the [[Characteristic polynomial|characteristic equation]]) is simply ''t''&amp;nbsp;−&amp;nbsp;''r''&amp;nbsp;=&amp;nbsp;0.

Solutions to such recurrence relations of higher order are found by systematic means, often using the fact that ''a&lt;sub&gt;n&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;''r&lt;sup&gt;n&lt;/sup&gt;'' is a solution for the recurrence exactly when ''t''&amp;nbsp;=&amp;nbsp;''r'' is a root of the characteristic polynomial. This can be approached directly or using [[generating function]]s ([[formal power series]]) or matrices.

Consider, for example, a recurrence relation of the form

:&lt;math&gt;a_{n}=Aa_{n-1}+Ba_{n-2}.&lt;/math&gt;

When does it have a solution of the same general form as ''a&lt;sub&gt;n&lt;/sub&gt;'' = ''r&lt;sup&gt;n&lt;/sup&gt;''? Substituting this guess ([[ansatz]]) in the recurrence relation, we find that

:&lt;math&gt;r^{n}=Ar^{n-1}+Br^{n-2}&lt;/math&gt;

must be true for '''all'''&amp;nbsp;''n''&amp;nbsp;&gt;&amp;nbsp;1.

Dividing through by ''r''&lt;sup&gt;''n''−2&lt;/sup&gt;, we get that all these equations reduce to the same thing:

:&lt;math&gt;r^2=Ar+B,&lt;/math&gt;
:&lt;math&gt;r^2-Ar-B=0,&lt;/math&gt;

which is the characteristic equation of the recurrence relation.  Solve for ''r'' to obtain the two roots ''λ''&lt;sub&gt;1&lt;/sub&gt;, ''λ''&lt;sub&gt;2&lt;/sub&gt;: these roots are known as the [[characteristic root]]s or '''eigenvalues''' of the characteristic equation.  Different solutions are obtained depending on the nature of the roots:  If these roots are distinct, we have the general solution

:&lt;math&gt;a_n = C\lambda_1^n+D\lambda_2^n&lt;/math&gt;

while if they are identical (when ''A''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;4''B''&amp;nbsp;=&amp;nbsp;0), we have

:&lt;math&gt;a_n = C\lambda^n+Dn\lambda^n&lt;/math&gt;

This is the most general solution; the two constants ''C'' and ''D'' can be chosen based on two given initial conditions ''a''&lt;sub&gt;0&lt;/sub&gt; and ''a''&lt;sub&gt;1&lt;/sub&gt; to produce a specific solution.

In the case of complex eigenvalues (which also gives rise to complex values for the solution parameters ''C'' and ''D''), the use of complex numbers can be eliminated by rewriting the solution in trigonometric form.  In this case we can write the eigenvalues as &lt;math&gt;\lambda_1, \lambda_2 = \alpha \pm \beta i.&lt;/math&gt; Then it can be shown that

:&lt;math&gt;a_n = C\lambda_1^n+D\lambda_2^n&lt;/math&gt;

can be rewritten as&lt;ref&gt;Chiang, Alpha C., ''Fundamental Methods of Mathematical Economics'', third edition, McGraw-Hill, 1984.&lt;/ref&gt;{{rp|576–585}}

:&lt;math&gt;a_n = 2 M^n \left( E \cos(\theta n) + F \sin(\theta n)\right) = 2 G M^n \cos(\theta n - \delta),&lt;/math&gt;

where

:&lt;math&gt;\begin{array}{lcl}
  M = \sqrt{\alpha^2+\beta^2} &amp; \cos (\theta) =\tfrac{\alpha}{M} &amp; \sin( \theta) = \tfrac{\beta}{M} \\
  C,D = E \mp F i &amp; &amp; \\
  G = \sqrt{E^2+F^2} &amp; \cos (\delta ) = \tfrac{E}{G} &amp; \sin (\delta )= \tfrac{F}{G}
\end{array}&lt;/math&gt;

Here ''E'' and ''F'' (or equivalently, ''G'' and δ)  are real constants which depend on the initial conditions. Using 
:&lt;math&gt;\lambda_1+\lambda_2=2 \alpha = A,&lt;/math&gt;
:&lt;math&gt;\lambda_1 \cdot \lambda_2=\alpha^2+\beta^2=-B,&lt;/math&gt;

one may simplify the solution given above as

:&lt;math&gt;a_n = (-B)^{\frac{n}{2}} \left( E \cos(\theta n) + F \sin(\theta n)\right),&lt;/math&gt;

where ''a''&lt;sub&gt;1&lt;/sub&gt; and ''a''&lt;sub&gt;2&lt;/sub&gt; are the initial conditions and

:&lt;math&gt;\begin{align}
  E &amp;=\frac{-A a_1 + a_2}{B} \\
 F &amp;=-i \frac{A^2 a_1 - A a_2 +2 a_1 B}{B \sqrt{A^2+4B}} \\
\theta &amp;=a\cos \left (\frac{A}{2 \sqrt{-B}} \right )
\end{align}&lt;/math&gt;

In this way there is no need to solve for λ&lt;sub&gt;1&lt;/sub&gt; and λ&lt;sub&gt;2&lt;/sub&gt;.

In all cases—real distinct eigenvalues, real duplicated eigenvalues, and complex conjugate eigenvalues—the equation is [[stability theory|stable]] (that is, the variable ''a'' converges to a fixed value (specifically, zero)); if and only if ''both'' eigenvalues are smaller than one in [[absolute value]].  In this second-order case, this condition on the eigenvalues can be shown&lt;ref&gt;Papanicolaou, Vassilis, &quot;On the asymptotic stability of a class of linear difference equations,&quot; ''Mathematics Magazine'' 69(1), February 1996, 34&amp;ndash;43.&lt;/ref&gt; to be equivalent to |''A''|&amp;nbsp;&lt;&amp;nbsp;1&amp;nbsp;−&amp;nbsp;''B''&amp;nbsp;&lt;&amp;nbsp;2, which is equivalent to |''B''|&amp;nbsp;&lt;&amp;nbsp;1 and |''A''|&amp;nbsp;&lt;&amp;nbsp;1&amp;nbsp;−&amp;nbsp;''B''.

The equation in the above example was [[homogeneous differential equation|homogeneous]], in that there was no constant term.  If one starts with the non-homogeneous recurrence

:&lt;math&gt;b_{n}=Ab_{n-1}+Bb_{n-2}+K&lt;/math&gt;

with constant term ''K'', this can be converted into homogeneous form as follows:  The [[steady state]] is found by setting ''b&lt;sub&gt;n&lt;/sub&gt;'' =&amp;nbsp;''b''&lt;sub&gt;''n''−1&lt;/sub&gt; =&amp;nbsp;''b''&lt;sub&gt;''n''−2&lt;/sub&gt; =&amp;nbsp;''b''* to obtain

:&lt;math&gt; b^{*} = \frac{K}{1-A-B}.&lt;/math&gt;

Then the non-homogeneous recurrence can be rewritten in homogeneous form as

:&lt;math&gt;[b_n - b^*]=A[b_{n-1}-b^*]+B[b_{n-2}-b^*],&lt;/math&gt;

which can be solved as above.

The stability condition stated above in terms of eigenvalues for the second-order case remains valid for the general ''n''&lt;sup&gt;th&lt;/sup&gt;-order case:  the equation is stable if and only if all eigenvalues of the characteristic equation are less than one in absolute value.

===Solving via linear algebra===
A linearly recursive sequence y of order n

:&lt;math&gt;y_{n+k} - c_{n-1}y_{n-1+k} - c_{n-2}y_{n-2+k} + \cdots - c_{0}y_{k} = 0&lt;/math&gt;

is identical to

:&lt;math&gt;y_{n} = c_{n-1}y_{n-1} + c_{n-2}y_{n-2} + \cdots +c_{0}y_{0}.&lt;/math&gt;

Expanded with n-1 identities of kind &lt;math&gt;y_{n-k} = y_{n-k},&lt;/math&gt; this n-th order equation is translated into a system of n first order linear equations,

:&lt;math&gt;\vec y_{n} = \begin{bmatrix}y_n\\y_{n-1}\\ \vdots \\ y_1 \end{bmatrix} = \begin{bmatrix}-c_{n-1} &amp; -c_{n-2} &amp; \cdots &amp; -c_{0} \\ 1 &amp; 0 &amp; \cdots &amp;0\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 0\end{bmatrix}\begin{bmatrix}y_{n-1} \\y_{n-2} \\ \vdots \\ y_0 \end{bmatrix} = C\ \vec y_{n-1} = C^n \vec y_0.&lt;/math&gt;

Observe that the vector &lt;math&gt;\vec y_n&lt;/math&gt; can be computed by ''n'' applications of the [[companion matrix]], C, to the initial state vector, &lt;math&gt;y_0&lt;/math&gt;. Thereby, n-th entry of the sought sequence y, is the top component of &lt;math&gt;\vec y_n, y_n = \vec y_n[n]&lt;/math&gt;.

[[Eigendecomposition of a matrix#Example|Eigendecomposition]], &lt;math&gt;\vec y_n = \vec C^n\, \vec y_0 = c_1\,\lambda_1^n\,\vec e_1 + c_2\,\lambda_2^n\,\vec e_2 + \cdots + c_n\,\lambda_n^n\,\vec e_n&lt;/math&gt; into eigenvalues, &lt;math&gt;\lambda_1, \lambda_2, \ldots, \lambda_n&lt;/math&gt;, and eigenvectors, &lt;math&gt;\vec e_1, \vec e_2, \ldots, \vec e_n&lt;/math&gt;, is used to compute &lt;math&gt;\vec y_n.&lt;/math&gt; Thanks to the crucial fact that system ''C'' time-shifts every eigenvector, ''e'', by simply scaling its components ''λ'' times,

:&lt;math&gt;C\,\vec e_i = \lambda_i \vec e_i = C \begin{bmatrix}e_{i,n} \\e_{i,n-1} \\ \vdots \\ e_{i,1}\end{bmatrix} = \begin{bmatrix}\lambda_i\,e_{i,n} \\ \lambda_i\,e_{i,n-1} \\ \vdots \\ \lambda_i\,e_{i,1}\end{bmatrix}&lt;/math&gt;

that is, time-shifted version of eigenvector,''e'', has components ''λ'' times larger, the eighenvector components are powers of ''λ'', &lt;math&gt;\vec e_i = \begin{bmatrix}\lambda_i^{n-1} &amp; \cdots &amp; \lambda_i^2 &amp; \lambda_i &amp; 1\end{bmatrix}^T,&lt;/math&gt; and, thus, recurrent linear homogeneous equation solution is a combination of exponential functions, &lt;math&gt;\vec y_n = \sum_1^n {c_i\,\lambda_i^n\,\vec e_i}&lt;/math&gt;. The components &lt;math&gt;c_i&lt;/math&gt; can be determined out of initial conditions:

:&lt;math&gt;\vec y_0 = \begin{bmatrix}y_{0}\\ y_{-1} \\ \vdots\\ y_{-n+1}\end{bmatrix} = \sum_{i=1}^n {c_i\,\lambda_i^0\,\vec e_i} = \begin{bmatrix}\vec e_1 &amp; \vec e_2 &amp; \cdots &amp; \vec e_n\end{bmatrix}\,\begin{bmatrix}c_1 \\ c_2 \\ \cdots \\ c_n\end{bmatrix} = E\, \begin{bmatrix}c_1 \\ c_2 \\ \cdots \\ c_n\end{bmatrix}&lt;/math&gt;

Solving for coefficients,

:&lt;math&gt;\begin{bmatrix}c_1 \\ c_2 \\ \cdots \\ c_n\end{bmatrix} = E^{-1} \vec y_0 = \begin{bmatrix}\lambda_1^{n-1} &amp; \lambda_2^{n-1} &amp; \cdots &amp;  \lambda_n^{n-1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \lambda_1 &amp; \lambda_2 &amp; \cdots &amp;  \lambda_n \\ 1 &amp; 1 &amp; \cdots &amp; 1\end{bmatrix}^{-1}\,\begin{bmatrix}y_{0}\\ y_{-1} \\ \vdots\\ y_{-n+1}\end{bmatrix}.&lt;/math&gt;

This also works with arbitrary boundary conditions &lt;math&gt;\underbrace{y_a, y_b, \ldots}_\text{n}&lt;/math&gt;, not necessary the initial ones,

:&lt;math&gt;\begin{bmatrix}y_a \\ y_b \\ \vdots\end{bmatrix}  = \begin{bmatrix}\vec y_a[n] \\ \vec y_b[n] \\ \vdots\end{bmatrix} = \begin{bmatrix}\sum_{i=1}^n {c_i\,\lambda_i^a\,\vec e_i[n]} \\ \sum_{i=1}^n {c_i\,\lambda_i^b\,\vec e_i[n]} \\ \vdots\end{bmatrix}  =\begin{bmatrix}\sum_{i=1}^n {c_i\,\lambda_i^a\,\lambda_i^{n-1}} \\ \sum_{i=1}^n {c_i\,\lambda_i^b\,\lambda_i^{n-1}} \\ \vdots\end{bmatrix} = &lt;/math&gt;

:&lt;math&gt; = \begin{bmatrix}\sum {c_i\,\lambda_i^{a+n-1}} \\ \sum {c_i\,\lambda_i^{b+n-1}} \\ \vdots\end{bmatrix} = \begin{bmatrix}\lambda_1^{a+n-1} &amp; \lambda_2^{a+n-1} &amp; \cdots &amp; \lambda_n^{a+n-1} \\ \lambda_1^{b+n-1} &amp; \lambda_2^{b+n-1} &amp; \cdots &amp; \lambda_n^{b+n-1} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \end{bmatrix}\,\begin{bmatrix}c_1 \\ c_2 \\ \vdots \\ c_n\end{bmatrix}.&lt;/math&gt;

This description is really no different from general method above, however it is more succinct. It also works nicely for situations like
:&lt;math&gt;\begin{cases}
  a_n =a_{n-1}-b_{n-1}\\
  b_n =2a_{n-1}+b_{n-1}.
\end{cases}
&lt;/math&gt;
where there are several linked recurrences {{Citation needed|date=June 2010}}.

===Solving with z-transforms===
Certain difference equations - in particular, [[Z-transform#Linear constant-coefficient difference equation|linear constant coefficient]] difference equations - can be solved using [[z-transform]]s. The ''z''-transforms are a class of [[integral transform]]s that lead to more convenient algebraic manipulations and more straightforward solutions. There are cases in which obtaining a direct solution would be all but impossible, yet solving the problem via a thoughtfully chosen integral transform is straightforward.

===Theorem===
Given a linear homogeneous recurrence relation with constant coefficients of order ''d'', let ''p''(''t'') be the [[characteristic polynomial]] (also &quot;auxiliary polynomial&quot;)

:&lt;math&gt;t^d - c_1t^{d-1} - c_2t^{d-2}-\cdots-c_{d} = 0 \,&lt;/math&gt;

such that each ''c&lt;sub&gt;i&lt;/sub&gt;'' corresponds to each ''c&lt;sub&gt;i&lt;/sub&gt;'' in the original recurrence relation (see the general form above). Suppose &amp;lambda; is a root of ''p''(''t'') having multiplicity ''r''. This is to say that (''t''−λ)&lt;sup&gt;''r''&lt;/sup&gt; divides ''p''(''t''). The following two properties hold:

# Each of the ''r'' sequences &lt;math&gt;\lambda^n, n\lambda^n, n^2\lambda^n,\dots,n^{r-1}\lambda^n&lt;/math&gt; satisfies the recurrence relation.
# Any sequence satisfying the recurrence relation can be written uniquely as a linear combination of solutions constructed in part 1 as ''λ'' varies over all distinct roots of&amp;nbsp;''p''(''t'').

As a result of this theorem a linear homogeneous recurrence relation with constant coefficients can be solved in the following manner:

# Find the characteristic polynomial ''p''(''t'').
# Find the roots of ''p''(''t'') counting multiplicity.
# Write ''a&lt;sub&gt;n&lt;/sub&gt;'' as a linear combination of all the roots (counting multiplicity as shown in the theorem above) with unknown coefficients ''b&lt;sub&gt;i&lt;/sub&gt;''.

::&lt;math&gt;a_n = \left (b_1\lambda_1^n + b_2n\lambda_1^n + b_3n^2\lambda_1^n+\cdots+b_r n^{r-1}\lambda_1^n \right )+\cdots+ \left (b_{d-q+1}\lambda_{*}^n + \cdots + b_{d}n^{q-1}\lambda_{*}^n \right )&lt;/math&gt;

:This is the general solution to the original recurrence relation. (''q'' is the multiplicity of λ&lt;sub&gt;*&lt;/sub&gt;)

:4. Equate each &lt;math&gt;a_0, a_1, \dots,a_d&lt;/math&gt; from part 3 (plugging in ''n'' = 0, ..., ''d'' into the general solution of the recurrence relation) with the known values &lt;math&gt;a_0, a_1, \dots,a_d&lt;/math&gt; from the original recurrence relation. However, the values ''a&lt;sub&gt;n&lt;/sub&gt;'' from the original recurrence relation used do not usually have to be contiguous: excluding exceptional cases, just ''d'' of them are needed (i.e., for an original linear homogeneous recurrence relation of order 3 one could use the values ''a''&lt;sub&gt;0&lt;/sub&gt;, ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;4&lt;/sub&gt;).  This process will produce a linear system of ''d'' equations with ''d'' unknowns. Solving these equations for the unknown coefficients &lt;math&gt;b_1, b_2, \dots,b_d &lt;/math&gt; of the general solution and plugging these values back into the general solution will produce the particular solution to the original recurrence relation that fits the original recurrence relation's initial conditions (as well as all subsequent values &lt;math&gt;a_0,a_1,a_2,\dots&lt;/math&gt; of the original recurrence relation).

The method for solving linear [[differential equations]] is similar to the method above—the &quot;intelligent guess&quot; ([[ansatz]]) for linear differential equations with constant coefficients is ''e''&lt;sup&gt;λ''x''&lt;/sup&gt; where λ is a complex number that is determined by substituting the guess into the differential equation.

This is not a coincidence.  Considering the [[Taylor series]] of the solution to a linear differential equation:

:&lt;math&gt;\sum_{n=0}^\infin \frac{f^{(n)}(a)}{n!} (x-a)^n&lt;/math&gt;

it can be seen that the coefficients of the series are given by the ''n''&lt;sup&gt;th&lt;/sup&gt; derivative of ''f''(''x'') evaluated at the point ''a''. The differential equation provides a linear difference equation relating these coefficients.

This equivalence can be used to quickly solve for the recurrence relationship for the coefficients in the power series solution of a linear differential equation.

The rule of thumb (for equations in which the polynomial multiplying the first term is non-zero at zero) is that:

:&lt;math&gt;y^{[k]} \to  f[n+k]&lt;/math&gt;
and more generally
:&lt;math&gt;x^m*y^{[k]} \to n(n-1)(n-m+1)f[n+k-m]&lt;/math&gt;

'''Example:''' The recurrence relationship for the Taylor series coefficients of the equation:

:&lt;math&gt; (x^2 + 3x -4)y^{[3]} -(3x+1)y^{[2]} + 2y = 0&lt;/math&gt;

is given by

:&lt;math&gt; n(n-1)f[n+1] + 3nf[n+2] -4f[n+3] -3nf[n+1] -f[n+2]+ 2f[n] = 0&lt;/math&gt;

or

:&lt;math&gt;-4f[n+3] +2nf[n+2] + n(n-4)f[n+1] +2f[n] = 0.&lt;/math&gt;

This example shows how problems generally solved using the power series solution method taught in normal differential equation classes can be solved in a much easier way.

'''Example:''' The differential equation

:&lt;math&gt;ay'' + by' +cy = 0&lt;/math&gt;

has solution

:&lt;math&gt; y=e^{ax}.&lt;/math&gt;

The conversion of the differential equation to a difference equation of the Taylor coefficients is

:&lt;math&gt;af[n + 2] + bf[n + 1] + cf[n] = 0.&lt;/math&gt;

It is easy to see that the ''n''th derivative of ''e''&lt;sup&gt;''ax''&lt;/sup&gt; evaluated at 0 is ''a''&lt;sup&gt;''n''&lt;/sup&gt;

===Solving non-homogeneous recurrence relations===
If the recurrence is inhomogeneous, a particular solution can be found by the [[method of undetermined coefficients]] and the solution is the sum of the solution of the homogeneous and the particular solutions.  Another method to solve an inhomogeneous recurrence is the method of ''symbolic differentiation''.  For example, consider the following recurrence:

:&lt;math&gt;a_{n+1} = a_{n} + 1&lt;/math&gt;

This is an inhomogeneous recurrence.  If we substitute ''n'' ↦ ''n''+1, we obtain the recurrence

:&lt;math&gt;a_{n+2} = a_{n+1} + 1&lt;/math&gt;

Subtracting the original recurrence from this equation yields

:&lt;math&gt;a_{n+2} - a_{n+1} = a_{n+1} - a_n&lt;/math&gt;

or equivalently

:&lt;math&gt;a_{n+2} = 2 a_{n+1} - a_n&lt;/math&gt;

This is a homogeneous recurrence which can be solved by the methods explained above.  In general, if a linear recurrence has the form

:&lt;math&gt; a_{n+k} = \lambda_{k-1} a_{n+k-1} + \lambda_{k-2} a_{n+k-2} + \cdots + \lambda_1 a_{n+1} + \lambda_0 a_{n} + p(n) &lt;/math&gt;

where &lt;math&gt;\lambda_0, \lambda_1, \dots, \lambda_{k-1}&lt;/math&gt; are constant coefficients and ''p''(''n'') is the inhomogeneity, then if ''p''(''n'') is a polynomial with degree ''r'', then this inhomogeneous recurrence can be reduced to a homogeneous recurrence by applying the method of symbolic differencing ''r'' times.

If
:&lt;math&gt;P(x) = \sum_{n=0}^\infty p_n x^n&lt;/math&gt;
is the generating function of the inhomogeneity, the generating function
:&lt;math&gt;A(x) = \sum_{n=0}^\infty a(n) x^n&lt;/math&gt;
of the inhomogeneous recurrence 
:&lt;math&gt; a_n = \sum_{i=1}^s c_i a_{n-i}+p_n,\quad n\ge n_r,&lt;/math&gt;
with constant coefficients {{math|c&lt;sub&gt;i&lt;/sub&gt;}} is derived from
:&lt;math&gt; \left (1-\sum_{i=1}^sc_ix^i \right )A(x)=P(x)+\sum_{n=0}^{n_r-1}[a_n-p_n]x^n-\sum_{i=1}^s c_ix^i\sum_{n=0}^{n_r-i-1}a_nx^n.&lt;/math&gt;
If ''P''(''x'') is a rational generating function, ''A''(''x'') is also one. The case discussed above, where ''p&lt;sub&gt;n&lt;/sub&gt;'' = ''K'' is a constant, emerges as one example of this formula, with ''P''(''x'') = ''K''/(1−''x''). Another example, the recurrence &lt;math&gt;a_n=10 a_{n-1}+n&lt;/math&gt; with linear inhomogeneity, arises in the definition of the [[schizophrenic number]]s. The solution of homogeneous recurrences is incorporated as ''p'' = ''P'' = 0.

Moreover, for the general first-order linear inhomogeneous recurrence relation with variable coefficient(s)

:&lt;math&gt;a_{n+1} = f_n a_n + g_n, \qquad f_n \neq 0,&lt;/math&gt;
there is also a nice method to solve it:&lt;ref&gt;http://faculty.pccu.edu.tw/%7Emeng/Math15.pdf&lt;/ref&gt;
:&lt;math&gt;a_{n+1} - f_n a_n = g_n&lt;/math&gt;
:&lt;math&gt;\frac{a_{n+1}}{\prod_{k=0}^n f_k} - \frac{f_n a_n}{\prod_{k=0}^n f_k} = \frac{g_n}{\prod_{k=0}^n f_k}&lt;/math&gt;
:&lt;math&gt;\frac{a_{n+1}}{\prod_{k=0}^n f_k} - \frac{a_n}{\prod_{k=0}^{n-1} f_k} = \frac{g_n}{\prod_{k=0}^n f_k}&lt;/math&gt;

Let 
:&lt;math&gt;A_n = \frac{a_n}{\prod_{k=0}^{n-1} f_k},&lt;/math&gt;
Then 
:&lt;math&gt;A_{n+1} - A_n = \frac{g_n}{\prod_{k=0}^n f_k}&lt;/math&gt;
:&lt;math&gt;\sum_{m=0}^{n-1}(A_{m+1} - A_m) = A_n - A_0 = \sum_{m=0}^{n-1}\frac{g_m}{\prod_{k=0}^m f_k}&lt;/math&gt;
:&lt;math&gt;\frac{a_n}{\prod_{k=0}^{n-1} f_k} = A_0 + \sum_{m=0}^{n-1}\frac{g_m}{\prod_{k=0}^m f_k}&lt;/math&gt;
:&lt;math&gt;a_n = \left(\prod_{k=0}^{n-1} f_k \right) \left(A_0 + \sum_{m=0}^{n-1}\frac{g_m}{\prod_{k=0}^m f_k}\right)&lt;/math&gt;

===General linear homogeneous recurrence relations===
Many linear homogeneous recurrence relations may be solved by means of the [[generalized hypergeometric series]]. Special cases of these lead to recurrence relations for the [[orthogonal polynomials]], and many [[special function]]s. For example, the solution to

:&lt;math&gt;J_{n+1}=\frac{2n}{z}J_n-J_{n-1}&lt;/math&gt;

is given by

:&lt;math&gt;J_n=J_n(z), \,&lt;/math&gt;

the [[Bessel function]], while

:&lt;math&gt;(b-n)M_{n-1} +(2n-b-z)M_n - nM_{n+1}=0 \,&lt;/math&gt;

is solved by

:&lt;math&gt;M_n=M(n,b;z) \,&lt;/math&gt;

the [[confluent hypergeometric series]].

===Solving a first order rational difference equation===
{{Main|Rational difference equation}}

A first order rational difference equation has the form &lt;math&gt;w_{t+1} = \tfrac{aw_t+b}{cw_t+d}&lt;/math&gt;. Such an equation can be solved by writing &lt;math&gt;w_t&lt;/math&gt; as a nonlinear transformation of another variable &lt;math&gt;x_t&lt;/math&gt; which itself evolves linearly.  Then standard methods can be used to solve the linear difference equation in &lt;math&gt;x_t&lt;/math&gt;.

==Stability==

===Stability of linear higher-order recurrences===
The linear recurrence of order ''d'',

:&lt;math&gt;a_n = c_1a_{n-1} + c_2a_{n-2}+\dots+c_da_{n-d},  \, &lt;/math&gt;

has the [[Characteristic polynomial|characteristic equation]]

:&lt;math&gt;\lambda^d - c_1 \lambda^{d-1} - c_2 \lambda^{d-2} - \dots - c_d \lambda^0 =0. \, &lt;/math&gt;

The recurrence is [[Stability theory|stable]], meaning that the iterates converge asymptotically to a fixed value, if and only if the [[eigenvalues]] (i.e., the roots of the characteristic equation), whether real or complex, are all less than [[unity (mathematics)|unity]] in absolute value.

===Stability of linear first-order matrix recurrences===
{{Main|Matrix difference equation}}

In the first-order matrix difference equation

:&lt;math&gt;[x_t - x^*] = A[x_{t-1}-x^*]\,&lt;/math&gt;

with state vector ''x'' and transition matrix ''A'', ''x'' converges asymptotically to the steady state vector ''x''* if and only if all eigenvalues of the transition matrix ''A'' (whether real or complex) have an [[absolute value]] which is less than&amp;nbsp;1.

===Stability of nonlinear first-order recurrences===
Consider the nonlinear first-order recurrence

:&lt;math&gt;x_n=f(x_{n-1}).&lt;/math&gt;

This recurrence is [[stability theory|locally stable]], meaning that it [[limit of a sequence|converges]] to a fixed point ''x''* from points sufficiently close to ''x''*, if the slope of ''f''  in the neighborhood of ''x''* is smaller than [[unity (mathematics)|unity]] in absolute value:  that is,

: &lt;math&gt;| f' (x^*) | &lt; 1. \, &lt;/math&gt;

A nonlinear recurrence could have multiple fixed points, in which case some fixed points may be locally stable and others locally unstable; for continuous ''f''  two adjacent fixed points cannot both be locally stable.

A nonlinear recurrence relation could also have a cycle of period ''k'' for ''k'' &gt; 1.  Such a cycle is stable, meaning that it attracts a set of initial conditions of positive measure, if the composite function

:&lt;math&gt;g(x) := f \circ f \circ \cdot \cdot \cdot \circ f(x)&lt;/math&gt;

with ''f'' appearing ''k'' times is locally stable according to the same criterion:

: &lt;math&gt;| g' (x^*) | &lt; 1,&lt;/math&gt;

where ''x''* is any point on the cycle.

In a [[chaos theory|chaotic]] recurrence relation, the variable ''x'' stays in a bounded region but never converges to a fixed point or an attracting cycle; any fixed points or cycles of the equation are unstable.  See also [[logistic map]], [[dyadic transformation]], and [[tent map]].

== Relationship to differential equations ==
When solving an [[ordinary differential equation]] [[numerical ordinary differential equations|numerically]], one typically encounters a recurrence relation. For example, when solving the [[initial value problem]]

:&lt;math&gt;y'(t) = f(t,y(t)), \ \ y(t_0)=y_0,&lt;/math&gt;

with [[Euler's method]] and a step size ''h'', one calculates the values

:&lt;math&gt;y_0=y(t_0), \ \ y_1=y(t_0+h), \ \ y_2=y(t_0+2h), \ \dots&lt;/math&gt;

by the recurrence

:&lt;math&gt;\, y_{n+1} = y_n + hf(t_n,y_n). &lt;/math&gt;

Systems of linear first order differential equations can be discretized exactly analytically using the methods shown in the [[discretization]] article.

== Applications ==

=== Biology ===
Some of the best-known difference equations have their origins in the attempt to model [[population]] dynamics. For example, the [[Fibonacci number]]s were once used as a model for the growth of a rabbit population.

The [[logistic map]] is used either directly to model population growth, or as a starting point for more detailed models. In this context, coupled difference equations are often used to model the interaction of two or more populations. For example, the Nicholson-Bailey model for a host-[[parasite]] interaction is given by

:&lt;math&gt;N_{t+1} = \lambda N_t e^{-aP_t} \,&lt;/math&gt;
:&lt;math&gt;P_{t+1} = N_t(1-e^{-aP_t}), \,&lt;/math&gt;

with ''N''&lt;sub&gt;''t''&lt;/sub&gt; representing the hosts, and ''P''&lt;sub&gt;''t''&lt;/sub&gt; the parasites, at time&amp;nbsp;''t''.

[[Integrodifference equation]]s are a form of recurrence relation important to spatial [[ecology]]. These and other difference equations are particularly suited to modeling [[voltinism|univoltine]] populations.

=== Digital signal processing ===
In [[digital signal processing]], recurrence relations can model feedback in a system, where outputs at one time become inputs for future time. They thus arise in [[infinite impulse response]] (IIR) [[digital filter]]s.

For example, the equation for a &quot;feedforward&quot; IIR [[comb filter]] of delay ''T'' is:

:&lt;math&gt;y_t = (1 - \alpha) x_t + \alpha y_{t - T}&lt;/math&gt;

Where &lt;math&gt;x_t&lt;/math&gt; is the input at time ''t'', &lt;math&gt;y_t&lt;/math&gt; is the output at time ''t'', and α controls how much of the delayed signal is fed back into the output.  From this we can see that

:&lt;math&gt;y_t = (1 - \alpha) x_t + \alpha ((1-\alpha) x_{t-T} + \alpha y_{t - 2T})&lt;/math&gt;
:&lt;math&gt;y_t = (1 - \alpha) x_t + (\alpha-\alpha^2) x_{t-T}  + \alpha^2 y_{t - 2T})&lt;/math&gt;

etc.

===Economics===
Recurrence relations, especially linear recurrence relations, are used extensively in both theoretical and empirical economics.&lt;ref&gt;Sargent, Thomas J., ''Dynamic Macroeconomic Theory'', Harvard University Press, 1987.&lt;/ref&gt;  In particular, in macroeconomics one might develop a model of various broad sectors of the economy (the financial sector, the goods sector, the labor market, etc.) in which some agents' actions depend on lagged variables.  The model would then be solved for current values of key variables (interest rate, real GDP, etc.) in terms of [[exogeny|exogenous variables]] and lagged [[endogeneity (economics)|endogenous]] variables.  See also [[time series analysis]].

===Computer science===

Recurrence relations are also of fundamental importance in [[analysis of algorithms]].&lt;ref&gt;Cormen, T. et al, ''Introduction to Algorithms'', MIT Press, 2009&lt;/ref&gt;&lt;ref&gt;R. Sedgewick, F. Flajolet, ''An Introduction to the Analysis of Algorithms'', Addison-Wesley, 2013&lt;/ref&gt; If an [[algorithm]] is designed so that it will break a problem into smaller subproblems ([[Divide and conquer algorithm|divide and conquer]]), its running time is described by a recurrence relation.

A simple example is the time an algorithm takes to search an element in an ordered vector with &lt;math&gt;n&lt;/math&gt; elements,  in the worst case.

A naive algorithm will search from left to right, one element at a time. The worst possible scenario is when the required element is the last, so the number of comparisons is &lt;math&gt;n&lt;/math&gt;.

A better algorithm is called [[Binary search algorithm|binary search]]. However, it requires a sorted vector. It will first check if the element is at the middle of the vector. If not, then it will check if the middle element is greater or lesser than the sought element. At this point, half of the vector can be discarded, and the algorithm can be run again on the other half. The number of comparisons will be given by

:&lt;math&gt;c_1=1&lt;/math&gt;
:&lt;math&gt;c_n=1+c_{n/2}&lt;/math&gt;

which will be close to &lt;math&gt;\log_2(n)&lt;/math&gt;.

==See also==
{{colbegin}}
*[[Iterated function]]
*[[Matrix difference equation]]
*[[Orthogonal polynomials]]
*[[Recursion]]
*[[Recursion (computer science)]]
*[[Lagged Fibonacci generator]]
*[[Master theorem]]
*[[Circle points segments proof]]
*[[Continued fraction]]
*[[Time scale calculus]]
*[[Integrodifference equation]]
* [[Combinatorial principles]]
*[[Infinite impulse response]]
{{colend}}

== References ==
{{reflist}}
* {{cite book|first1=Paul M. | last1=Batchelder
|title=An introduction to linear difference equations
|year=1967
|publisher=Dover Publications
}}
* {{cite book|first1=Kenneth S. | last1=Miller
|title=Linear difference equations
|year=1968 | publisher=W. A. Benjamin
}}
* {{cite book| first1=Alfred | last1=Brousseau
|title=Linear Recursion and Fibonacci Sequences
|year=1971
|publisher=Fibonacci Association
|url=http://www.fq.math.ca/linear.html
}}
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 1990. ISBN 0-262-03293-7. Chapter 4: Recurrences, pp.&amp;nbsp;62–90.
*{{cite book| first1=Ronald L. | last1=Graham |
first2=Donald E. | last2=Knuth
|first3=Oren | last3=Patashnik
|title=[[Concrete Mathematics]]: A Foundation for Computer Science
|edition=2
|publisher=Addison-Welsey
|year=1994 |isbn=0-201-55802-5
}}
*{{cite book |year=2010 | url=http://eu.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000338.html
|first1=Walter |last1=Enders
|title=Applied Econometric Times Series
|edition=3
}}
*{{cite book| first1=Paul | last1=Cull | first2=Mary | last2=Flahive
|first3=Robbie | last3=Robson
|title=Difference Equations: From Rabbits to Chaos
|publisher=Springer | year=2005 | isbn=0-387-23234-6 }} chapter 7.
* {{cite book| first1=Ian | last1=Jacques
|title=Mathematics for Economics and Business
|edition=Fifth
|publisher=Prentice Hall
|year=2006
|isbn=0-273-70195-9
|pages=551–568
}} Chapter 9.1: Difference Equations.
* {{cite web| first1=Andrei D. | last1=Polyanin
|title=Difference and Functional Equations: Exact Solutions
|url=http://eqworld.ipmnet.ru/en/solutions/fe.htm}} at EqWorld - The World of Mathematical Equations.
* {{cite web| first1=Andrei D. | last1=Polyanin
|title=Difference and Functional Equations: Methods
|url=http://eqworld.ipmnet.ru/en/education/edu-fe.htm}} at EqWorld - The World of Mathematical Equations.

== External links ==
* {{springer|title=Recurrence relation|id=p/r080150}}
* {{MathWorld | urlname= RecurrenceEquation | title= Recurrence Equation}}
* {{cite web |first1= John H. |last1=Mathews |url=http://math.fullerton.edu/mathews/c2003/ZTransformDEMod.html
|title= Homogeneous Difference Equations}}
* [http://books.google.com/books?id=pOBXUoVZ9EEC&amp;pg=PA95&amp;lpg=PA95&amp;dq=%22difference+equation%22+%22recurrence+relation%22&amp;source=web&amp;ots=1kZStOrPh5&amp;sig=VYKkC__C9AfmfrhjFhhLg_Q5YPk&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=5&amp;ct=result Introductory Discrete Mathematics]
* {{cite web |title= OEIS Index Rec|url=http://oeis.org/wiki/Index_to_OEIS:_Section_Rec}} [[OEIS]] index to a few thousand examples of linear recurrences, sorted by order (number of terms) and signature (vector of values of the constant coefficients)

[[Category:Algebra]]
[[Category:Recurrence relations| ]]
[[Category:Theory of computation]]</text>
      <sha1>cvn4rhb43xnu4z379h87atzrtupjbrc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rational difference equation</title>
    <ns>0</ns>
    <id>27419729</id>
    <revision>
      <id>506503149</id>
      <parentid>422606310</parentid>
      <timestamp>2012-08-09T04:12:56Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>/* First approach */[[WP:TSN|Typo fixing]], typos fixed: ,  → , using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="4024">A '''rational difference equation''' is a nonlinear [[difference equation]] of the form&lt;ref&gt;[http://books.google.com/books?id=4Kb3lO31NcAC&amp;printsec=frontcover&amp;dq=on+third+order+rational+difference+equations&amp;source=bl&amp;ots=JSV5xuGLO3&amp;sig=Y_oeukThSmjZhsLRbloxDPuHnSg&amp;hl=en&amp;ei=artgTOvYOcL-8Ab2lMTgCQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=6&amp;ved=0CCsQ6AEwBQ#v=onepage&amp;q&amp;f=false Dynamics of third-order rational difference equations with open problems and Conjectures]&lt;/ref&gt;&lt;ref name=&quot;Ladas-Kulenovic&quot;&gt;[http://books.google.com/books?id=zW7N4r64aZgC&amp;printsec=frontcover&amp;dq=on+second+order+rational+difference+equations&amp;hl=en&amp;ei=5b9gTPvTLoH78AaA6fyQCQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CC8Q6AEwAA#v=onepage&amp;q&amp;f=false Dynamics of Second-order rational difference equations with open problems and Conjectures]&lt;/ref&gt; 

: &lt;math&gt;x_{n+1} = \frac{\alpha+\sum_{i=0}^k \beta_ix_{n-i}}{A+\sum_{i=0}^k B_ix_{n-i}},&lt;/math&gt;

where the initial conditions &lt;math&gt;x_{0}, x_{-1},\dots, x_{-k}&lt;/math&gt; are such that the denominator is never zero for any &lt;math&gt;n&lt;/math&gt;.

==First-order rational difference equation==
A '''first-order rational difference equation''' is a nonlinear [[difference equation]] of the form

: &lt;math&gt;w_{t+1} = \frac{aw_t+b}{cw_t+d}.&lt;/math&gt;

When &lt;math&gt;a,b,c,d&lt;/math&gt; and the initial condition &lt;math&gt;w_{0}&lt;/math&gt; are real numbers, this difference equation is called a '''Riccati difference equation'''.&lt;ref name=&quot;Ladas-Kulenovic&quot;/&gt;

Such an equation can be solved by writing &lt;math&gt;w_t&lt;/math&gt; as a nonlinear transformation of another variable &lt;math&gt;x_t&lt;/math&gt; which itself evolves linearly.  Then standard methods can be used to solve the linear [[Recurrence relation#Solving|difference equation]] in &lt;math&gt;x_t&lt;/math&gt;.

== Solving a first-order equation==
===First approach===

One approach &lt;ref&gt;Brand, Louis, &quot;A sequence defined by a difference equation,&quot; ''[[American Mathematical Monthly]]'' 62, September 1955, 489&amp;ndash;492.&lt;/ref&gt; to developing the transformed variable &lt;math&gt;x_t&lt;/math&gt;, when &lt;math&gt;ad-bc \neq 0&lt;/math&gt;, is to write

: &lt;math&gt;y_{t+1}= \alpha - \frac{\beta}{y_t}&lt;/math&gt;

where &lt;math&gt;\alpha = (a+d)/c&lt;/math&gt;  and &lt;math&gt;\beta = (ad-bc)/c^{2}&lt;/math&gt; and where &lt;math&gt;w_t = y_t -d/c&lt;/math&gt;.  Further writing  &lt;math&gt;y_t = x_{t+1}/x_t&lt;/math&gt; can be shown to yield

: &lt;math&gt;x_{t+2} - \alpha x_{t+1} + \beta x_t =0. \,&lt;/math&gt;

===Second approach===

This approach &lt;ref&gt;Mitchell, Douglas W., &quot;An analytic Riccati solution for two-target discrete-time control,&quot; ''[[Journal of Economic Dynamics and Control]]'' 24, 2000, 615&amp;ndash;622.&lt;/ref&gt;  gives a first-order difference equation for &lt;math&gt;x_t&lt;/math&gt; instead of a second-order one, for the case in which &lt;math&gt;(d-a)^{2}+4bc&lt;/math&gt; is non-negative.  Write  &lt;math&gt;x_t = 1/(\eta + w_t)&lt;/math&gt;  implying &lt;math&gt;w_t = (1- \eta x_t)/x_t&lt;/math&gt;, where &lt;math&gt;\eta&lt;/math&gt; is given by &lt;math&gt;\eta = (d-a+r)/2c&lt;/math&gt; and where &lt;math&gt;r=\sqrt{(d-a)^{2}+4bc}&lt;/math&gt;.  Then it can be shown that &lt;math&gt;x_t&lt;/math&gt; evolves according to

: &lt;math&gt;x_{t+1} = \frac{(d-\eta c)x_t}{\eta c+a} + \frac{c}{\eta c+a}.&lt;/math&gt;

==Application==

It was shown in &lt;ref&gt;Balvers, Ronald J., and Mitchell, Douglas W., &quot;Reducing the dimensionality of linear quadratic control problems,&quot; ''[[Journal of Economic Dynamics and Control]]'' 31, 2007, 141&amp;ndash;159.&lt;/ref&gt; that a dynamic [[matrix Riccati equation]] of the form

: &lt;math&gt; H_{t-1} = K +A'H_tA - A'H_tC(C'H_tC)^{-1}C'H_tA, \,&lt;/math&gt;

which can arise in some discrete-time [[optimal control]] problems, can be solved using the second approach above if the matrix ''C'' has only one more row than column.

==References==

&lt;references/&gt;

==See also==

* Newth, Gerald, &quot;World order from chaotic beginnings,&quot; ''[[Mathematical Gazette]]'' 88, March 2004, 39-45,  for a [[trigometry|trigonometric]] approach.

* Simons, Stuart, &quot;A non-linear difference equation,&quot; ''Mathematical Gazette'' 93, November 2009, 500-504.

{{DEFAULTSORT:Rational Difference Equation}}
[[Category:Algebra|Algebra]]
[[Category:Recurrence relations|Recurrence relations]]</text>
      <sha1>07qh1f28uxxr739fo8oslqvykwx8sif</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Book:Algebraic Mathematics and Logics</title>
    <ns>108</ns>
    <id>23222766</id>
    <revision>
      <id>479066935</id>
      <parentid>452313063</parentid>
      <timestamp>2012-02-27T04:32:37Z</timestamp>
      <contributor>
        <username>Prosfilaes</username>
        <id>49272</id>
      </contributor>
      <comment>Disambiguated: [[Algebroid]] → [[Lie algebroid]], [[Supergroup]] → [[Supergroup (physics)]]</comment>
      <text xml:space="preserve" bytes="3041">{{saved book
 |title=Algebraic Logic and Algebraic Mathematics
 |subtitle=
 |cover-image=
 |cover-color=
}}

== Algebraic Logic and Algebraic Mathematics ==

;Algebraic Logic
:[[Boolean algebra]]
:[[Algebraic logic]]
:[[Łukasiewicz logic]]
:[[Intuitionistic logic]]
:[[Mathematical logic]]
:[[Heyting arithmetic]]
:[[Metatheory]]
:[[Metalogic]]

;Quantum Logics and Quantum Computers
:[[Many-valued logic]]
:[[Quantum logic]]
:[[Quantum computer]]

;Abstract Algebra
:[[Abstract algebra]]
:[[Universal algebra]]
:[[Heyting algebra]]
:[[MV-algebra]]
:[[Group algebra]]
:[[Lie algebra]]
:[[Affine Lie algebra]]
:[[Lie group]]
:[[Lie algebroid|Algebroid]]

;Quantum Algebra and Geometry
:[[Quantum affine algebra]]
:[[Clifford algebra]]
:[[Von Neumann algebra]]
:[[C*-algebra]]
:[[Kac–Moody algebra]]
:[[Hopf algebra]]
:[[Quantum group]]
:[[Group representation]]
:[[Unitary representation]]
:[[Representation theory of the Lorentz group]]
:[[Stone–von Neumann theorem]]
:[[Peter–Weyl theorem]]
:[[Quasi-Hopf algebra]]
:[[Quasitriangular Hopf algebra]]
:[[Ribbon Hopf algebra]]
:[[Quasi-triangular Quasi-Hopf algebra]]
:[[Quantum inverse scattering method]]
:[[Yangian]]
:[[Exterior algebra]]
:[[Superalgebra]]
:[[Supergroup (physics)|Supergroup]]
:[[Noncommutative quantum field theory]]
:[[Standard Model]]
:[[Noncommutative standard model]]
:[[Noncommutative geometry]]

;Algebraic Geometry and Analytic Geometry
:[[Algebraic geometry]]
:[[List of algebraic geometry topics]]
:[[Duality (projective geometry)|Duality]]
:[[Universal algebraic geometry]]
:[[Grothendieck topology]]
:[[Grothendieck–Hirzebruch–Riemann–Roch theorem]]
:[[Algebraic geometry and analytic geometry]]
:[[Differential geometry]]

;Algebraic Topology, Group Theory and Groupoids
:[[Algebraic topology]]
:[[Groupoid]]
:[[Group theory]]
:[[Abelian group]]
:[[Galois group]]
:[[Grothendieck group]]
:[[Esquisse d'un Programme]]
:[[Galois theory]]
:[[Grothendieck's Galois theory]]
:[[Galois cohomology]]
:[[Homological algebra]]
:[[Homology theory]]
:[[Homotopical algebra]]
:[[De Rham cohomology]]
:[[Crystalline cohomology]]
:[[Cohomology]]
:[[K-theory]]
:[[Algebraic K-theory]]
:[[Topological K-theory]]

;Category Theory and Categorical Logic
:[[Category theory]]
:[[Category (mathematics)|Category]]
:[[Glossary of category theory]]
:[[Dual (category theory)|Dual]]
:[[Abelian category]]
:[[Functor]]
:[[Yoneda lemma]]
:[[Limit (category theory)|Limit]]
:[[Adjoint functors]]
:[[Natural transformation|Natural transformations]]
:[[Variety (universal algebra)|Variety]]
:[[Domain theory]]
:[[Enriched category]]
:[[Topos]]
:[[Descent (category theory)|Descent]]
:[[Stack (descent theory)|Stack]]
:[[Categorical logic]]
:[[Timeline of category theory and related mathematics]]
:[[List of important publications in mathematics]]

;Higher Dimensional Algebras
:[[Higher-dimensional algebra]]
:[[Higher category theory]]
:[[Duality (mathematics)|Duality]]
 
[[Category:Wikipedia books on mathematics|Algebraic Mathematics and Logics]]
[[Category:Algebra|β]]</text>
      <sha1>dr7km37refsubjztc4shlugil3ghbg0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Polynomial arithmetic</title>
    <ns>0</ns>
    <id>16275025</id>
    <revision>
      <id>609061866</id>
      <parentid>606962037</parentid>
      <timestamp>2014-05-18T07:25:21Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:FrescoBot/Links|link syntax]]</comment>
      <text xml:space="preserve" bytes="4693">'''Polynomial arithmetic''' is a branch of [[algebra]] dealing with some properties of [[Polynomial|polynomials]] which share strong analogies with properties of [[number theory]] relative to integers.
It includes basic mathematical operations such as [[addition]], [[subtraction]], and [[multiplication]], as well as more elaborate operations like [[Euclidean division]], and properties related to roots of polynomials. The latter are essentially connected to the fact that the set ''K''[''X''] of [[univariate]] polynomials with coefficients in a [[Field_(mathematics)|field]] ''K'' is a [[commutative ring]], such as the ring of integers &lt;math&gt;\mathbb{Z}&lt;/math&gt;.

==Elementary operations on polynomials==
Addition and subtraction of two polynomials are performed by adding or subtracting corresponding [[coefficients]]. If

: &lt;math&gt;f(x) = \sum_{i=0}^n a_ix^i; g(x) = \sum_{i=0}^m b_ix^i&lt;/math&gt;

then addition is defined as

: &lt;math&gt;f(x)+g(x)= \sum_{i=0}^m (a_i+b_i)x^i &lt;/math&gt;  where m &gt; n

Multiplication is performed much the same way as addition and subtraction, but instead by multiplying the corresponding coefficients. If &lt;math&gt;f(x) = \sum_{i=0}^n a_ix^i; g(x) = \sum_{i=0}^m b_ix^i&lt;/math&gt; then multiplication is defined as &lt;math&gt;f(x)\times g(x)=\sum_{i=0}^{n+m} c_ix^i&lt;/math&gt; where &lt;math&gt;c_k=a_0b_k+a_1b_{k-1}+\cdots+a_{k-1}b_1+a_kb_0&lt;/math&gt;. Note that we treat &lt;math&gt;a_i&lt;/math&gt; as zero for &lt;math&gt;i&gt;n&lt;/math&gt; and that the degree of the product is equal to the [[sum]] of the degrees of the two polynomials.

==Advanced polynomial arithmetics and comparison with number theory==
Many fascinating properties of polynomials can be found when, thanks to the basic operations that can be performed on two polynomials and the underlying [[commutative ring]] structure of the set they live in, one tries to apply reasonings similar to those known from number theory.

To see this, one first needs to introduce two concepts: the notion of [[Polynomial_roots|root]] of a polynomial and that of [[Divisibility_(ring_theory)|divisibility]] for pairs of polynomials. 

If one considers a polynom &lt;math&gt;P&lt;/math&gt; of a single variable ''X'' in a field ''K'' (typically &lt;math&gt;\mathbb{R}&lt;/math&gt; or &lt;math&gt;\mathbb{C}&lt;/math&gt;), and with coefficients in that field, a root &lt;math&gt;r&lt;/math&gt; of &lt;math&gt;P&lt;/math&gt; is an element of ''K'' such that

: &lt;math&gt;P(r)=0&lt;/math&gt;

The second concept, divisibility of polynomials, allows to see a first analogy with number theory: a polynomial &lt;math&gt;B&lt;/math&gt; is said to divide another polynomial &lt;math&gt;A&lt;/math&gt; when the latter can be written as

: &lt;math&gt;A = BC &lt;/math&gt;

with C being ALSO a polynomial. This definition is similar to divisibility for integers, and the fact that &lt;math&gt;B&lt;/math&gt; divides &lt;math&gt;A&lt;/math&gt; is also denoted &lt;math&gt;B|A&lt;/math&gt;.

The relation between both concepts above arises when noticing the following property: &lt;math&gt;r&lt;/math&gt; is a root of &lt;math&gt;P&lt;/math&gt; if and only if &lt;math&gt;(X-r)|P&lt;/math&gt;. Whereas one logical inclusion (&quot;if&quot;) is obvious, the other (&quot;only if&quot;) relies on a more elaborate concept, the [[Polynomial_long_division#Euclidean_division|Euclidean division of polynomials]], here again strongly reminding of the [[Euclidean division]] of integers.

From this it follows that one can define [[Prime_polynomial|prime polynomials]], as polynomials that cannot be divided by any other polynomials but 1 and themselves (up to an overall constant factor) - here again the analogously with prime integers is manifest, and allows that some of the main definitions and theorems related to prime numbers and number theory have their counterpart in polynomial algebra. The most important result is the [[Fundamental_theorem_of_algebra| fundamental theorem of algebra]], allowing for factorization of any polynomial as a product of prime ones. Worth mentioning is also the [[Bézout's_identity#B.C3.A9zout.27s_identity_for_polynomials|Bézout's identity]] in the context of polynomials. It states that two given polynomials P and Q have as [[Polynomial greatest common divisor|greatest common divisor (GCD)]] a third polynomial D (D is then unique as GCD of P and Q up to a finite constant factor), if and only if there exists polynomials U and V such that
: &lt;math&gt; UP+VQ = D &lt;/math&gt;.

==See also==
* [[Polynomial long division]]
* [[Polynomial greatest common divisor]]

==References==
* Stallings, William; : &quot;Cryptography And Network Security: Principles and Practice&quot;, pages 121-126. Prentice Hall, 1999.
{{Refimprove|date=March 2008}}

==External links==
*J.A. Beachy and W.D. Blair; : &quot;[http://www.math.niu.edu/~beachy/aaol/polynomials.html Polynomials]&quot;, from &quot;Abstract algebra&quot;, 2nd edition, 1996.

[[Category:Polynomials]]
[[Category:Algebra]]</text>
      <sha1>50kb6qzc0lppmwwu5mu4x2f1gfoej1h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Board puzzles with algebra of binary variables</title>
    <ns>0</ns>
    <id>29138951</id>
    <revision>
      <id>611645413</id>
      <parentid>605866890</parentid>
      <timestamp>2014-06-05T07:37:10Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor/>
      <comment>Typo/[[WP:AWB/GF|general]] fixing, replaced: the this → this using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="13422">{{multiple issues|
{{original research|date=December 2012}}
{{refimprove|date=October 2010}}
}}

'''Board puzzles with algebra of binary variables''' ask players to locate the hidden objects based on a set of clue cells and their neighbors marked as variables (unknowns). A variable with value of 1 corresponds to a cell with an object. Contrary, a variable with value of 0 corresponds to an empty cell—no hidden object.

==Overview==
These puzzles are based on algebra with binary variables taking a pair of values, for example, (no, yes), (false, true), (not exists, exists), ('''0''',&amp;nbsp;'''1'''). It invites the player quickly establish some equations, and  inequalities for the solution. The [[Partition of a set|partitioning]] can be used to reduce the complexity of the problem. Moreover, if the puzzle is prepared in a way that there exists [[Uniqueness|a unique solution only]], this fact can be used to eliminate some variables without calculation.

The problem can be modeled as [[Linear program#Integer unknowns|binary integer linear programming]] which is a special case of integer linear programming.&lt;ref&gt;Schrijver 1986&lt;/ref&gt;

==History==
One of the famous puzzle in this class is [[Minesweeper (Windows)|Microsoft's Minesweeper]]. Its ancestors and variants are summarized in the article [[Minesweeper (computer game)|Minesweeper Computer Game]]. Another version of this game is called [[Tentaizu (puzzle)|Tentaizu]] which is appeared in Southwest Airlines' magazine Spirit in 2008–2009. Tentaizu is published as an application in Google's Android Market in 2010.

==Algebra with binary variables==

Below the letters in the mathematical statements are used as variables where each can take the value either '''0''' or '''1''' only. A simple example of an equation with binary variables is given below:

:'''''a''''' '''+''' '''''b''''' '''=''' '''0'''

Here there are two variables '''''a''''' and '''''b''''' but one equation. The solution is constrained by the fact that '''''a''''' and '''''b''''' can take only values '''0''' or '''1'''. There is only one solution here, both '''''a''''' '''= 0''', and '''''b''''' '''= 0'''. Another simple example is given below:

:'''''a''''' '''+''' '''''b''''' '''=''' '''2'''

The solution is straightforward: '''''a''''' and '''''b''''' must be '''1''' to make '''''a''''' '''+''' '''''b''''' equal to '''2'''.

Another interesting case is shown below:

:'''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''=''' '''2'''

:'''''a''''' '''+''' '''''b''''' '''≤''' '''1'''

Here, the first statement is an equation and the second statement is an inequality indicating the three possible cases:

#'''''a''''' '''= 1''' and '''''b''''' '''= 0''',
#'''''a''''' '''= 0''' and '''''b''''' '''= 1''', and
#'''''a''''' '''= 0''' and '''''b''''' '''= 0''',

The last case causes a contradiction on '''''c''''' by forcing '''''c''''' '''= 2''', which is not possible. Therefore, either first or second case is correct. This leads to the fact that '''''c''''' must be '''1'''.

The modification of a large equation into smaller form is not difficult. However, an equation set with binary variables cannot be always solved by applying linear algebra. The following is an example for applying the subtraction of two equations:

:'''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''+''' '''''d''''' '''= 3'''

:'''''c''''' '''+''' '''''d''''' '''= 1'''

The first statement has four variables whereas the second statement has only two variables. The latter one means that the sum of '''''c''''' and '''''d''''' is '''1'''. Using this fact on the first statement, the equations above can be reduced to

:'''''a''''' '''+''' '''''b''''' '''= 2'''

:'''''c''''' '''+''' '''''d''''' '''= 1'''

==The algebra on a board==
[[File:tentaizu 4x4 example.png|thumb|right|alt=tentaizu_4x4_example|Figure 1: An example puzzle on 4x4 board]]

A game based on the algebra with binary variables can be visualized in many different ways. One generic way is to represent the right side of an equation as a clue in a cell (clue cell), and the neighbors of a clue cell as variables. A simple case is shown in Figure 1. The neighbors can be assumed to be the up/down, left/right, and corner cells that are sharing an edge or a corner. The white cells may contain a hidden object or nothing. In other words, they are the binary variables.  They take place on the left side of the equations. Each clue cell, a cell with blue background in Figure 1, contains a positive number corresponding to the number of its neighbors that have hidden objects. The total number of the objects on the board can be given as an additional clue. The same board with variables marked is shown in Figure 2.

===The reduction into a set of equations with binary variables===
The main equation is written by using the total number of the hidden objects given. From the first figure this corresponds to the following equation

:'''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''+''' '''''d''''' '''+''' '''''e''''' '''+''' '''''f''''' '''+''' '''''g''''' '''+''' '''''h''''' '''+''' '''''i''''' '''+''' '''''j''''' '''+''' '''''k''''' '''+''' '''''m''''' '''= 3'''

The other equations are composed one by one for each clue cells:

:'''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''+''' '''''e''''' '''+''' '''''f''''' '''+''' '''''h''''' '''+''' '''''i''''' '''+''' '''''j''''' '''= 1'''

:'''''f''''' '''+''' '''''g''''' '''+''' '''''j''''' '''+''' '''''m''''' '''= 1'''

:'''''h''''' '''+''' '''''i''''' '''+''' '''''j''''' '''+''' '''''k''''' '''= 2'''

:'''''i''''' '''+''' '''''j''''' '''+''' '''''m''''' '''= 2'''

Although there are several ways to solve the equations above, the following explicit way can be applied:

#It is known from the equation set that '''''i''''' '''+''' '''''j''''' '''+''' '''''m''''' '''= 2'''. However, since '''''j''''' and '''''m''''' are neighbors of a cell with number '''1''', the following is true: '''''j''''' '''+''' '''''m''''' '''≤ 1'''. This means that the variable '''''i''''' must be '''1'''.
#Since '''''i''''' '''= 1''' and the variable '''''i''''' is the neighbor to the clue cell with number '''1''', the variables '''''a''''', '''''b''''', '''''c''''', '''''e''''', '''''f''''', '''''h''''', and '''''j''''' must be zero. The same result can be obtained by replacing '''''i''''' '''= 1''' into the second equation as follows: '''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''+''' '''''e''''' '''+''' '''''f''''' '''+''' '''''h''''' '''+''' '''''j''''' '''= 0'''. This is equivalent to '''''a''''' '''= 0''', '''''b''''' '''= 0''', '''''c''''' '''= 0''', '''''e''''' '''= 0''', '''''f''''' '''= 0''', '''''h''''' '''= 0''', '''''j''''' '''= 0'''.
#Figure 3 is obtained after Step 1 and Step 2. The grayed cells with '–' are the variables with value '''0'''. The cell with the symbol '''Δ''' corresponds to the variable with value '''1'''. The variable '''''k''''' is the only neighbor of the left most clue cell with value '''2'''. This clue cell has one neighbor with an object and only one remaining cell with variable '''''k'''''. Therefore '''''k''''' must be '''1'''.
#Similarly, the variable '''''m''''' must be '''1''' too because it is the only remaining variable neighbor to the right most clue cell with value '''2'''.
#Since '''''k''''' '''= 1''', '''''m''''' '''= 1''' and '''''i''''' '''= 1''', we complete the marking of three hidden objects therefore '''''d''''' '''= 0''', and '''''g''''' '''= 0'''. The final solution is given in Figure 4.

{|
| [[Image:tentaizu 4x4 example with variables.png|thumb|upright|alt=tentaizu_4x4_example_with_variables|Figure 2: Binary variables are marked]]
| [[Image:tentaizu 4x4 example with variables solved partially.png|thumb|upright|alt=tentaizu_4x4_example_solved_partially|Figure 3: The example solved partially]]
| [[Image:tentaizu 4x4 example solved.png|thumb|upright|alt=tentaizu_4x4_example_with_variables_solved|Figure 4: The example solved]]
|}

===Use of uniqueness===

In the example above (Figure 2), the variables '''''a''''', '''''b''''', '''''c''''', and '''''e''''' are the neighbors of the clue cell '''1''' and they are not neighbors of any other cell. It is obvious that the followings are possible solutions:

*'''''a''''' '''= 1''', '''''b''''' '''= 0''', '''''c''''' '''= 0''', '''''e''''' '''= 0'''

*'''''a''''' '''= 0''', '''''b''''' '''= 1''', '''''c''''' '''= 0''', '''''e''''' '''= 0'''

*'''''a''''' '''= 0''', '''''b''''' '''= 0''', '''''c''''' '''= 1''', '''''e''''' '''= 0'''

*'''''a''''' '''= 0''', '''''b''''' '''= 0''', '''''c''''' '''= 0''', '''''e''''' '''= 1'''

However, if the puzzle is prepared so that we should have one only one (unique) solution, we can set that all these variables '''''a''''', '''''b''''', '''''c''''', and '''''e''''' must be 0. Otherwise there become more than one solutions.

===Use of partitioning===
[[File:tentaizu 4x4 example partitioned.png|thumb|right|alt=tentaizu_4x4_example_partitioned|Figure 5: An example for partitioning]]

Some puzzle configurations may allow the player to use partitioning&lt;ref&gt;Halmos 1960&lt;/ref&gt; for complexity reduction. An example is given in Figure 5. Each partition corresponds to a number of the objects hidden. The sum of the hidden objects in the partitions must be equal to the total number of objects hidden on the board. One possible way to determine a partitioning is to choose the lead clue cells which have no common neighbors. The cells outside of the red transparent zones in Figure 5 must be empty. In other words, there are no hidden objects in the all-white cells. Since there must be a hidden object within the upper partition zone, the third row from top shouldn't contain a hidden object. This leads to the fact that the two variable cells on the bottom row around the clue cell must have hidden objects. The rest of the solution is straightforward.

===Use of try-and-check method===
[[File:tentaizu 4x4 example for inconsistency.png|thumb|left|alt=tentaizu_4x4_example_for_inconsistency|Figure 6: An example for try-and-check method]]

At some cases, the player can set a variable cell as '''1''' and check if any inconsistency occurs. The example in Figure 6 shows an inconsistency check. The cell marked with an hidden object '''Δ''' is under the test. Its marking leads to the set all the variables (grayed cells) to be '''0'''. This follows the inconsistency. The clue cell marked red with value '''1''' does not have any remaining neighbor that can include a hidden object. Therefore, the cell under the test must not include a hidden object. In algebraic form we have two equations:

:'''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''+''' '''''d''''' '''= 1'''

:'''''a''''' '''+''' '''''b''''' '''+''' '''''c''''' '''+''' '''''d''''' '''+''' '''''e''''' '''+''' '''''f''''' '''+''' '''''g''''' '''= 1'''

Here '''''a''''', '''''b''''', '''''c''''', and '''''d''''' correspond to the top four grayed cells in Figure 6. The cell with '''''Δ''''' is represented by the variable '''''f''''', and the other two grayed cells are marked as '''''e''''' and '''''g'''''. If we set '''''f''''' '''= 1''', then '''''a''''' '''= 0''', '''''b''''' '''= 0''', '''''c''''' '''= 0''', '''''d''''' '''= 0''', '''''e''''' '''= 0''', '''''g''''' '''= 0'''. The first equation above will have the left hand side equal to '''0''' while the right hand side has '''1'''. A contradiction.

Try-and-check may need to be applied consequently in more than one step on some puzzles in order to reach a conclusion. This is equivalent to [[binary search algorithm]]&lt;ref&gt;Drozdek 2000&lt;/ref&gt; to eliminate possible paths which lead to inconsistency.

==Complexity==

Because of binary variables, the equation set for the solution does not possess linearity property. In other words, the rank of the equation matrix may not always address the right complexity.

The complexity of this class of puzzles can be adjusted in several ways. One of the simplest method is to set a ratio of the number of the clue cells to the total number of the cells on the board. However, this may result a largely varying complexity range for a fixed ratio. Another method is to reduce clue cells based on some problem solving strategies step by step. The complex strategies may be enabled for high complexity levels such as subtracting an equation with another one, or the higher depth of try-and-check steps. When the board size increases, the range of the problem cases increases. The ratio of the number of hidden objects to the total number of cells affects the complexity of the puzzle too.

==See also==
*[[Minesweeper (Windows)|Microsoft's Minesweeper]]
*[[Minesweeper (computer game)|Minesweeper Computer Game]]
*[[Tentaizu (puzzle)]]

== Notes ==
{{Reflist|2}}

==References==
*Paul Halmos, ''Naive set theory''. Princeton, NJ: D. Van Nostrand Company, 1960. Reprinted by Springer-Verlag, New York, 1974. ISBN 0-387-90092-6 (Springer-Verlag edition).
*[[Alexander Schrijver]], ''Theory of Linear and Integer Programming''. John Wiley &amp; Sons, 1986. Reprinted in 1999. ISBN 0-471-98232-6.
*Adam Drozdek, ''Data Structures and Algorithms in C++'', Brooks/Cole, second edition, 2000. ISBN 0-534-37597-9.

== External links ==
* [http://tentaizuhexontheweb.appspot.com Tentaizu Hex (free of charge web app)]

[[Category:Puzzles]]
[[Category:Algebra]]
[[Category:Variables]]
[[Category:Equations]]</text>
      <sha1>1hbdd2i4cnmzy44d3ivnm3k596a5ouj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Regular chain</title>
    <ns>0</ns>
    <id>18814716</id>
    <revision>
      <id>538885272</id>
      <parentid>538884633</parentid>
      <timestamp>2013-02-18T15:08:02Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <minor/>
      <comment>/* See also */dab + spelling</comment>
      <text xml:space="preserve" bytes="7802">In [[computer algebra]], a '''regular chain''' is a particular kind of triangular set in a multivariate polynomial ring over a field. It enhances the notion of [[Wu's method of characteristic set|characteristic set]].

== Introduction ==

Given a linear system, one can convert it to a triangular system via [[Gaussian elimination]]. For the non-linear case, 
given a polynomial system F over a field, one can convert (decompose or triangularize) it to a finite set of triangular sets, in the sense that the [[algebraic variety]] ''V''(F) is described by these triangular sets. 
A triangular set may merely describe the empty set. To fix this degenerated case, the notion of regular chain was introduced, independently by Kalkbrener (1993), Yang and Zhang (1994). Regular chains also appear in Chou and Gao (1992). Regular chains are special triangular sets which are used in different algorithms for computing unmixed-dimensional decompositions of algebraic varieties. Without using factorization, these decompositions have better properties that the ones produced by [[Wu's method|Wu's algorithm]]. Kalkbrener's original definition was based on the following observation: every irreducible variety is uniquely determined by one of its [[generic point]]s and varieties can be represented by describing the generic points of their irreducible components. These generic points are given by regular chains.

== Examples ==

Denote '''Q''' the rational number field. In '''Q'''[x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;] with variable ordering x&lt;sub&gt;1&lt;/sub&gt; &lt; x&lt;sub&gt;2&lt;/sub&gt; &lt; x&lt;sub&gt;3&lt;/sub&gt;,
: &lt;math&gt;T = \{ x_2^2-x_1^2, x_2(x_3-x_1)\}&lt;/math&gt;
is a triangular set and also a regular chain. Two generic points given by ''T'' are (a, a, a) and (a, -a, a) where ''a'' is transcendental over '''Q'''.
Thus there are two irreducible components, given by { x&lt;sub&gt;2&lt;/sub&gt; - x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt; - x&lt;sub&gt;1&lt;/sub&gt; } and { x&lt;sub&gt;2&lt;/sub&gt; + x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt; - x&lt;sub&gt;1&lt;/sub&gt; }, respectively.
Note that: (1) the [[content (algebra)|content]] of the second polynomial is x&lt;sub&gt;2&lt;/sub&gt;, which does not contribute to the generic points represented and thus can be removed; (2) the [[dimension]] of each component is 1, the number of free variables in the regular chain.

== Formal definitions ==

The variables in the polynomial ring 
:&lt;math&gt;R = k[x_1, \ldots, x_n]&lt;/math&gt; 
are always sorted as x&lt;sub&gt;1&lt;/sub&gt; &lt; ... &lt; x&lt;sub&gt;n&lt;/sub&gt;. 
A non-constant polynomial ''f'' in  &lt;math&gt;R&lt;/math&gt; can be seen as a univariate polynomial in its greatest variable.
The greatest variable in ''f'' is called its main variable, denoted by ''mvar''(f). Let ''u'' be 
the main variable of ''f'' and write it as 
:&lt;math&gt;f = a_eu^e + \cdots + a_0&lt;/math&gt;, 
where ''e'' is the degree of ''f'' w.r.t. ''u''  and &lt;math&gt;a_e&lt;/math&gt; is 
the leading coefficient of ''f'' w.r.t. ''u''.  Then the initial of ''f''
is &lt;math&gt;a_e&lt;/math&gt; and ''e'' is its main degree.

*Triangular set

A non-empty subset ''T'' of &lt;math&gt;R&lt;/math&gt; is a triangular set, 
if the polynomials in ''T'' are non-constant and have distinct main variables. 
Hence, a triangular set is finite, and has cardinality at most ''n''.

*Regular chain

Let T = {t&lt;sub&gt;1&lt;/sub&gt;, ..., t&lt;sub&gt;s&lt;/sub&gt;} be a triangular set such that 
''mvar''(t&lt;sub&gt;1&lt;/sub&gt;) &lt; ... &lt; ''mvar''(t&lt;sub&gt;s&lt;/sub&gt;), 
&lt;math&gt;h_i&lt;/math&gt; be the initial of ''t''&lt;sub&gt;i&lt;/sub&gt; and ''h'' be the product of h&lt;sub&gt;i&lt;/sub&gt;'s. 
Then ''T'' is a ''regular chain'' if 
: &lt;math&gt;\mathrm{resultant}(h, T) = 
\mathrm{resultant}(\cdots(\mathrm{resultant}(h, t_s),\ldots, t_i)\cdots)\neq 0&lt;/math&gt;, 
where each [[resultant]] is computed with respect to the main variable of ''t''&lt;sub&gt;i&lt;/sub&gt;, respectively.  
This definition is from Yang and Zhang, which is of much algorithmic flavor.

*Quasi-component and saturated ideal of a regular chain

The ''quasi-component'' ''W''(''T'') described by the regular chain ''T'' is 
:&lt;math&gt;W(T)=V(T)\setminus V(h)&lt;/math&gt;, that is,
the set difference of the varieties ''V''(''T'') and ''V''(''h''). 
The attached algebraic object of a regular chain is its ''saturated ideal'' 
:&lt;math&gt;\mathrm{sat}(T)=(T):h^\infty&lt;/math&gt;. 
A classic result is that the [[Zariski closure]] of ''W''(''T'') equals the variety defined by sat(''T''), that is,
:&lt;math&gt;\overline{W(T)}=V(\mathrm{sat}(T))&lt;/math&gt;,
and its dimension is n - |T|, the difference of the number of variables and the number of polynomials in ''T''.

*Triangular decompositions    

In general, there are two ways to decompose a polynomial system ''F''. The first one is to decompose lazily, that is, only to represent its [[generic point]]s in the (Kalkbrener) sense,
: &lt;math&gt;\sqrt{(F)}=\cap_{i=1}^{e}\sqrt{\mathrm{sat}(T_i)}&lt;/math&gt;.
The second is to describe all zeroes in the [[Daniel Lazard|Lazard]] sense,
: &lt;math&gt;V(F)=\cup_{i=1}^{e}W(T_i)&lt;/math&gt;.
There are various algorithms available for triangular decompositions in either sense.

== Properties ==

Let ''T'' be a regular chain in the polynomial ring ''R''.

* The saturated ideal sat(''T'') is an ''unmixed ideal'' with dimension n − |''T''|.

* A regular chain holds a strong elimination property in the sense that:
: &lt;math&gt; \mathrm{sat}(T \cap k[x_1, \ldots , x_i]) = \mathrm{sat}(T) \cap k[x_1,\ldots , x_i] &lt;/math&gt;.

* A polynomial ''p'' is in sat(''T'') if and only if p is pseudo-reduced to zero by ''T'', that is,
: &lt;math&gt;p\in\mathrm{sat}(T)\iff \mathrm{prem}(p, T)=0&lt;/math&gt;.
:Hence the membership test for sat(''T'') is algorithmic.

* A polynomial p is a '''[[zero-divisor]]''' modulo sat(''T'') if and only if &lt;math&gt;\mathrm{prem}(p, T)\neq0&lt;/math&gt; and &lt;math&gt;\mathrm{resultant}(p, T)=0&lt;/math&gt;.
:Hence the regularity test for sat(''T'') is algorithmic.

* Given a prime ideal ''P'', there exists a regular chain ''C'' such that ''P'' = sat(''C''). 

* If the first element of a regular chain ''C'' is an irreducible polynomial and the others are linear in their main variable, then sat(''C'') is a prime ideal.

* Conversely, if ''P'' is a prime ideal, then, after almost all linear changes of variables, there exists a regular chain ''C'' of the preceding shape such that ''P'' = sat(''C'').

* A triangular set is a regular chain if and only if it is a [[Wu's method of characteristic set#Ritt characteristic set|Ritt characteristic set]] of its saturated ideal.

== See also ==
*[[Wu's method of characteristic set]]
*[[Gröbner basis]]
*[[RegularChains]], a software to compute with regular chains
*[[Regular semi-algebraic system]]
*[[Triangular decomposition]]

== Further references ==

* P. Aubry, D. Lazard, M. Moreno Maza. On the theories of triangular sets. Journal of Symbolic Computation, 28(1&amp;ndash;2):105&amp;ndash;124, 1999.
* F. Boulier and F. Lemaire and M. Moreno Maza. Well known theorems on triangular systems and the D5 principle. Transgressive Computing 2006, Granada, Spain.
* E. Hubert. Notes on triangular sets and triangulation-decomposition algorithms I: Polynomial systems.  LNCS, volume 2630, Springer-Verlag Heidelberg.
* F. Lemaire and M. Moreno Maza and Y. Xie. The RegularChains library. Maple Conference 2005. 
* M. Kalkbrener: Algorithmic Properties of Polynomial Rings. J. Symb. Comput. 26(5): 525&amp;ndash;581 (1998).
* M. Kalkbrener: A Generalized Euclidean Algorithm for Computing Triangular Representations of Algebraic Varieties. J. Symb. Comput. 15(2): 143&amp;ndash;167 (1993).
* D. Wang. Computing Triangular Systems and Regular Systems. Journal of Symbolic Computation 30(2) (2000) 221&amp;ndash;236.
* Yang, L., Zhang, J. (1994). Searching dependency between algebraic equations: an algorithm applied to automated reasoning. Artificial Intel ligence in Mathematics, pp.&amp;nbsp;14715,  Oxford University Press.

[[Category:Equations]]
[[Category:Algebra]]
[[Category:Polynomials]]
[[Category:Algebraic geometry]]
[[Category:Computer algebra]]</text>
      <sha1>mrt7slumtlphfv9ic8ffedun5qxhnai</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Regular semi-algebraic system</title>
    <ns>0</ns>
    <id>29317092</id>
    <revision>
      <id>580424443</id>
      <parentid>394915934</parentid>
      <timestamp>2013-11-06T08:38:33Z</timestamp>
      <contributor>
        <username>Crown Prince</username>
        <id>13045129</id>
      </contributor>
      <comment>Disambiguated: [[Characteristic set]] → [[Wu's method of characteristic set]]</comment>
      <text xml:space="preserve" bytes="3095">In [[computer algebra]], a '''regular semi-algebraic system''' is a particular kind of triangular  system  of multivariate polynomials over a real closed field.

== Introduction ==
[[Regular chain]]s and [[triangular decomposition]]s are fundamental and well-developed tools for describing the complex solutions of polynomial systems. The notion of a regular semi-algebraic system is an adaptation of the concept of a regular chain focusing on solutions of the real analogue: semi-algebraic systems.

Any semi-algebraic system &lt;math&gt;S&lt;/math&gt; can be decomposed into finitely many regular semi-algebraic systems &lt;math&gt;S_1, \ldots, S_e&lt;/math&gt; such that a point (with real coordinates) is a solution of &lt;math&gt;S&lt;/math&gt; if and only if it is a solution of one of the systems &lt;math&gt;S_1, \ldots, S_e&lt;/math&gt;.&lt;ref&gt;Changbo Chen, James H. Davenport, John P. May, Marc Moreno-Maza, Bican Xia, Rong Xiao. Triangular decomposition of semi-algebraic systems.  Proceedings of 2010 International Symposium on Symbolic and Algebraic Computation (ISSAC 2010), ACM Press, pp. 187–194, 2010.&lt;/ref&gt;

== Formal definition ==

Let &lt;math&gt;T&lt;/math&gt; be a [[regular chain]] of &lt;math&gt;{{\mathbf{k}}}[x_1, \ldots, x_n]&lt;/math&gt;
for some ordering of the variables &lt;math&gt;\mathbf{x} = x_1, \ldots, x_n&lt;/math&gt; and a [[real closed field]] &lt;math&gt;{{\mathbf{k}}}&lt;/math&gt;.
Let &lt;math&gt;\mathbf{u} = u_1, \ldots, u_d&lt;/math&gt; and &lt;math&gt;\mathbf{y} = y_1, \ldots, y_{n-d}&lt;/math&gt;
designate respectively the variables of &lt;math&gt;\mathbf{x}&lt;/math&gt; that are free and algebraic with respect to &lt;math&gt;T&lt;/math&gt;. Let &lt;math&gt;P \subset {{\mathbf{k}}}[\mathbf{x}]&lt;/math&gt; be finite such that
each polynomial in &lt;math&gt;P&lt;/math&gt; is regular w.r.t.\ the saturated ideal of &lt;math&gt;T&lt;/math&gt;.
Define &lt;math&gt;P_{&gt;} :=\{p&gt;0\mid p\in P\}&lt;/math&gt;. Let &lt;math&gt;\mathcal{Q}&lt;/math&gt; be a quantifier-free 
formula of &lt;math&gt;{{\mathbf{k}}}[\mathbf{x}]&lt;/math&gt; involving only the variables of &lt;math&gt;\mathbf{u}&lt;/math&gt;. We say that &lt;math&gt;R := [\mathcal{Q}, T, P_{&gt;}]&lt;/math&gt; is a '''regular semi-algebraic system''' if the following three conditions hold.

* &lt;math&gt;\mathcal{Q}&lt;/math&gt; defines a non-empty open semi-algebraic set &lt;math&gt;S&lt;/math&gt; of &lt;math&gt;{{\mathbf{k}}}^d&lt;/math&gt;,
* the regular system &lt;math&gt;[T, P] &lt;/math&gt; specializes well at every point  &lt;math&gt;u&lt;/math&gt; of &lt;math&gt;S&lt;/math&gt;,
* at each point &lt;math&gt;u&lt;/math&gt; of &lt;math&gt;S&lt;/math&gt;, the specialized system &lt;math&gt;[T(u), P(u)_{&gt;}&lt;/math&gt; has at least one real zero.

The zero set of &lt;math&gt;R&lt;/math&gt;, denoted by &lt;math&gt;{Z}_{{{\mathbf{k}}}}(R)&lt;/math&gt;,
is defined as the set of points &lt;math&gt;(u, y) \in {{{\mathbf{k}}}}^d \times {{{\mathbf{k}}}}^{n-d}&lt;/math&gt; such that &lt;math&gt;\mathcal{Q}(u)&lt;/math&gt; is true and  &lt;math&gt;t(u, y)=0&lt;/math&gt;,  &lt;math&gt;p(u, y)&gt;0&lt;/math&gt;,  for all &lt;math&gt;t\in T&lt;/math&gt;and all &lt;math&gt;p\in P&lt;/math&gt;.

== See also ==
*[[Wu's method of characteristic set|Characteristic set]]
*[[Triangular decomposition]]
*[[Regular chain]]
*[[RegularChains]]
*[[Real algebraic geometry]]

== References ==
{{Reflist}}

[[Category:Equations]]
[[Category:Algebra]]
[[Category:Polynomials]]
[[Category:Algebraic geometry]]
[[Category:Computer algebra]]</text>
      <sha1>hxduls86zr2bzy8kpmy67u4wmby5o4q</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Triangular decomposition</title>
    <ns>0</ns>
    <id>29321051</id>
    <revision>
      <id>599070814</id>
      <parentid>580424458</parentid>
      <timestamp>2014-03-11T01:28:56Z</timestamp>
      <contributor>
        <ip>99.241.166.168</ip>
      </contributor>
      <comment>changed latex to {{math|...}} and minor formatting changes (bad latex syntax)</comment>
      <text xml:space="preserve" bytes="10762">In [[computer algebra]], a '''triangular decomposition''' of a polynomial system {{mvar|S}} is a set of simpler polynomial systems {{math|''S''&lt;sub&gt;1&lt;/sub&gt;, ..., ''S&lt;sub&gt;e&lt;/sub&gt;''}} such that a point is a solution of {{mvar|S}} if and only if it is a solution of one of the systems {{math|''S''&lt;sub&gt;1&lt;/sub&gt;, ..., ''S&lt;sub&gt;e&lt;/sub&gt;''}}.

When the purpose is to describe the solution set of {{mvar|S}} in the [[algebraic closure]] of its coefficient field, those simpler systems are [[regular chain]]s. If the coefficient of {{mvar|S}} are real numbers, then the real solutions of  {{mvar|S}} can be obtained by a triangular decomposition into [[regular semi-algebraic system]]s. In both cases, each of these simpler systems has a triangular shape and remarkable properties, which justifies the terminology.

== Formal definitions ==
Let {{math|'''k'''}} be a field and {{math|''x''&lt;sub&gt;1&lt;/sub&gt; &lt; ... &lt; ''x&lt;sub&gt;n&lt;/sub&gt;''}} be ordered variables. We denote by {{math|''R'' {{=}} '''k'''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;'']}} the corresponding polynomial ring. For {{math|''F'' ⊂ ''R''}}, regarded as a system of polynomial equations, there are two notions of a '''triangular decomposition''' over the [[algebraic closure]] of {{math|'''k'''}}. The first one is to decompose lazily, by representing only the [[generic point]]s of the algebraic set {{math|''V''(''F'')}} in the so-called sense of Kalkbrener.

: &lt;math&gt;\sqrt{(F)}=\bigcap_{i=1}^{e}\sqrt{\mathrm{sat}(T_i)}.&lt;/math&gt;

The second is to describe explicitly all the points of {{math|''V''(''F'')}} in the so-called sense of in [[Daniel Lazard|Lazard]] and [[Wu Wenjun|Wen-Tsun Wu]].

: &lt;math&gt;V(F)=\bigcup_{i=1}^{e}W(T_i).&lt;/math&gt;

In both cases {{math|''T''&lt;sub&gt;1&lt;/sub&gt;, ..., ''T&lt;sub&gt;e&lt;/sub&gt;''}} are finitely many [[regular chain]]s of {{mvar|R}} and &lt;math&gt;\sqrt{\mathrm{sat}(T_i)}&lt;/math&gt; denotes the radical of the '''saturated ideal''' of {{math|''T&lt;sub&gt;i&lt;/sub&gt;''}} while {{math|''W''(''T&lt;sub&gt;i&lt;/sub&gt;'')}} denotes the '''quasi-component''' of {{math|''T&lt;sub&gt;i&lt;/sub&gt;''}}. Please refer to [[regular chain]] for definitions of these notions.

Assume from now on that {{math|'''k'''}} is a [[real closed field]]. Consider {{mvar|S}} a semi-algebraic system with polynomials in {{mvar|R}}. There exist&lt;ref&gt;Changbo Chen, James H. Davenport, John P. May, Marc Moreno-Maza, Bican Xia, Rong Xiao. Triangular decomposition of semi-algebraic systems.  Proceedings of 2010 International Symposium on Symbolic and Algebraic Computation (ISSAC 2010), ACM Press, pp. 187--194, 2010.&lt;/ref&gt; finitely many [[regular semi-algebraic system]]s {{math|''S''&lt;sub&gt;1&lt;/sub&gt;, ..., ''S&lt;sub&gt;e&lt;/sub&gt;''}} such that we have

:&lt;math&gt;Z_{\mathbf{k}}(S) = Z_{\mathbf{k}}(S_1) \cup \cdots \cup Z_{\mathbf{k}}(S_e) &lt;/math&gt;

where {{math|''Z''&lt;sub&gt;'''k'''&lt;/sub&gt;(''S'')}} denotes the points of {{math|'''k'''&lt;sup&gt;''n''&lt;/sup&gt;}} which solve {{mvar|S}}. The regular semi-algebraic systems {{math|''S''&lt;sub&gt;1&lt;/sub&gt;, ..., ''S&lt;sub&gt;e&lt;/sub&gt;''}} form a '''triangular decomposition''' of the semi-algebraic system {{mvar|S}}.

== History ==
The '''Characteristic Set Method''' is the first factorization-free algorithm  which was proposed for decomposing an algebraic variety into equidimensional components.  Moreover, the Author, [[Wu Wenjun|Wen-Tsun Wu]], realized an implementation of this method and reported experimental data in his 1987 pioneer article titled &quot;A zero structure theorem for polynomial equations solving&quot;.&lt;ref&gt;Wu, W. T. (1987). A zero structure theorem for polynomial equations solving. MM Research Preprints, 1, 2–12&lt;/ref&gt; To put this work into context, let us recall what was the common idea of an algebraic set decomposition at the time this article was written.

Let {{math|'''K'''}} be an [[algebraically closed field]] and {{math|'''k'''}} be a subfield of {{math|'''K'''}}. A subset {{math|''V'' ⊂ '''K'''&lt;sup&gt;''n''&lt;/sup&gt;}} is an (affine) [[algebraic variety]] over {{math|'''k'''}} if there exists a polynomial set {{math|''F'' ⊂ '''k'''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;'']}} such that the zero set {{math|''V''(''F'') ⊂ '''K'''&lt;sup&gt;''n''&lt;/sup&gt;}} of {{mvar|F}} equals {{mvar|V}}.

Recall that {{mvar|V}} is said '''irreducible''' if for all  algebraic varieties {{math|''V''&lt;sub&gt;1&lt;/sub&gt;, ''V''&lt;sub&gt;2&lt;/sub&gt; ⊂ '''K'''&lt;sup&gt;''n''&lt;/sup&gt;}} the relation {{math|''V'' {{=}} ''V''&lt;sub&gt;1&lt;/sub&gt; ∪ ''V''&lt;sub&gt;2&lt;/sub&gt;}} implies either {{math|''V'' {{=}} ''V''&lt;sub&gt;1&lt;/sub&gt;}} or {{math|''V'' {{=}} ''V''&lt;sub&gt;2&lt;/sub&gt;}}. A first algebraic variety decomposition result is the famous [[Lasker–Noether theorem]] which implies the following.

:'''Theorem (Lasker - Noether).''' For each algebraic variety {{math|''V'' ⊂ '''K'''&lt;sup&gt;''n''&lt;/sup&gt;}} there exist finitely many irreducible algebraic varieties {{math|''V''&lt;sub&gt;1&lt;/sub&gt;, ..., ''V&lt;sub&gt;e&lt;/sub&gt;'' ⊂ '''K'''&lt;sup&gt;''n''&lt;/sup&gt;}} such that we have
::&lt;math&gt;  V = V_1 \cup \cdots \cup V_e. &lt;/math&gt;
:Moreover, if  {{math|''V&lt;sub&gt;i&lt;/sub&gt;'' ⊈ ''V&lt;sub&gt;j&lt;/sub&gt;''}} holds for {{math|1 ≤ ''i'' &lt; ''j'' ≤ ''e''}} then the set {{math|{''V''&lt;sub&gt;1&lt;/sub&gt;, ..., ''V&lt;sub&gt;e&lt;/sub&gt;''} }}is unique and forms the '''irreducible decomposition''' of {{mvar|V}}.

The varieties {{math|''V''&lt;sub&gt;1&lt;/sub&gt;, ..., ''V&lt;sub&gt;e&lt;/sub&gt;''}} in the above Theorem are called the '''irreducible components''' of {{mvar|V}} and can be regarded as a natural output for a decomposition algorithm, or, in other words, for an algorithm solving a system of equations in {{math|'''k'''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;'']}}.

In order to lead to a computer program, this algorithm specification  should prescribe how irreducible components are represented.  Such an encoding is introduced  by [[Joseph Ritt]]&lt;ref&gt;Ritt, J. (1966). Differential Algebra. New York, Dover Publications&lt;/ref&gt;  through the following result.

:'''Theorem (Ritt).''' If {{math|''V'' ⊂ '''K'''&lt;sup&gt;''n''&lt;/sup&gt;}} is a non-empty and irreducible variety then one can compute a reduced triangular set {{mvar|C}} contained in the ideal &lt;math&gt;\langle F \rangle&lt;/math&gt; generated by {{mvar|F}} in {{math|'''k'''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;'']}} and such that all polynomials {{mvar|g}} in &lt;math&gt;\langle F \rangle&lt;/math&gt; reduces to zero by pseudo-division w.r.t {{mvar|C}}.

We call the set {{mvar|C}} in Ritt's Theorem a '''Ritt characteristic set''' of the ideal &lt;math&gt;\langle F \rangle&lt;/math&gt;. Please refer to [[regular chain]] for the notion of a triangular set.

[[Joseph Ritt]] described  a method for solving polynomial systems  based on polynomial factorization over field extensions and computation of characteristic sets of prime ideals.

Deriving a practical implementation of this method, however,  was and remains a difficult problem. In the 80's, when the [[Wu's method of characteristic set|Characteristic set]] Method was introduced,  polynomial factorization was an active research area and certain fundamental questions on this subject were solved  recently&lt;ref&gt;A. M. Steel Conquering inseparability: Primary decomposition and multivariate factorization over algebraic function fields of positive characteristic&lt;/ref&gt;

Nowadays, decomposing an algebraic variety into irreducible components is not essential to process most application problems, since weaker notions of decompositions, less costly to compute, are sufficient.

The '''Characteristic Set Method''' relies on the following variant of Ritt's Theorem.

:'''Theorem (Wen-Tsun Wu).''' For any finite polynomial set {{math|''F'' ⊂ '''k'''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;'']}}, one can compute a reduced triangular set &lt;math&gt;C \subset \langle F \rangle&lt;/math&gt; such that  all polynomial {{mvar|g}} in {{mvar|F}} reduces to zero by pseudo-division w.r.t {{mvar|C}}.

Different concepts and algorithms extended the work of [[Wu Wenjun|Wen-Tsun Wu]]. In the early 90's, the notion of a [[regular chain]], introduced independently by Michael Kalkbrener in 1991 in his PhD Thesis and, by Lu Yang and Jingzhong Zhang&lt;ref&gt;Yang, L., Zhang, J. (1994). Searching dependency between algebraic equations: an algorithm applied to automated reasoning. Artificial Intelligence in Mathematics, pp.&amp;nbsp;14715,  Oxford University Press.&lt;/ref&gt; led to important algorithmic discoveries.

In Kalkbrener's vision,&lt;ref&gt;M. Kalkbrener: A Generalized Euclidean Algorithm for Computing Triangular Representations of Algebraic Varieties. J. Symb. Comput. 15(2): 143&amp;ndash;167 (1993)&lt;/ref&gt; regular chains are used to represent the generic zeros of the irreducible components of an algebraic variety.  In the original work of Yang and Zhang, they are used to decide whether a hypersurface intersects a quasi-variety (given by a regular chain).  [[Regular chain]]s have, in fact, several interesting properties and are the key notion in many algorithms for decomposing systems of algebraic or differential equations.

Regular chains have been investigated in many papers.&lt;ref&gt;S.C. Chou and X.S. Gao. On the dimension of an arbitrary ascending chain.  Chinese Bull. of Sci., 38:799--804, 1991.&lt;/ref&gt;&lt;ref&gt;Michael Kalkbrener. Algorithmic properties of polynomial rings. J. Symb. Comput.}, 26(5):525--581, 1998.&lt;/ref&gt;&lt;ref&gt;P. Aubry, D. Lazard, M. Moreno Maza. On the theories of triangular sets. Journal of Symbolic Computation, 28(1&amp;ndash;2):105&amp;ndash;124, 1999.&lt;/ref&gt;

The abundant literature on the subject can be explained by the many equivalent definitions of a regular chain. Actually, the original formulation of Kalkbrener is quite  different from that of Yang and Zhang.  A bridge between these two notions, the point of view of Kalkbrener and that of Yang and Zhang, appears in Dongming Wang's paper.&lt;ref&gt;D. Wang. Computing Triangular Systems and Regular Systems. Journal of Symbolic Computation 30(2) (2000) 221&amp;ndash;236&lt;/ref&gt;

There are various algorithms available for obtaining triangular decomposition of {{math|''V''(''F'')}} both in the sense of Kalkbrener and in the sense of Lazard and [[Wu Wenjun|Wen-Tsun Wu]]. The '''Lextriangular Algorithm''' by [[Daniel Lazard]]&lt;ref&gt;D. Lazard, ''Solving zero-dimensional algebraic systems''. Journal of Symbolic Computation '''13''', 1992&lt;/ref&gt; and the '''Triade Algorithm''' by Marc Moreno Maza&lt;ref&gt;M. Moreno Maza: On triangular decomposition of algebraic varieties. MEGA 2000 (2000).&lt;/ref&gt; together with the '''Characteristic Set Method''' are available in various computer algebra systems, including [[Axiom (computer algebra system)|Axiom]].

== See also ==
*[[Wu's method of characteristic set]]
*[[Regular chain]]
*[[RegularChains]], a software to compute regular chains
*[[Regular semi-algebraic system]]

== References ==

{{Reflist}}

[[Category:Equations]]
[[Category:Algebra]]
[[Category:Polynomials]]
[[Category:Algebraic geometry]]
[[Category:Computer algebra]]
[[Category:Computer algebra systems]]</text>
      <sha1>hqdckn62krjslbgagppud787ynktcjc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Indeterminate equation</title>
    <ns>0</ns>
    <id>4529537</id>
    <revision>
      <id>562715601</id>
      <parentid>562705629</parentid>
      <timestamp>2013-07-03T16:59:36Z</timestamp>
      <contributor>
        <username>Duoduoduo</username>
        <id>10585217</id>
      </contributor>
      <comment>tweak</comment>
      <text xml:space="preserve" bytes="1687">An '''indeterminate equation''', in [[mathematics]], is an equation for which there is more than one solution; for example, 2''x'' = ''y'' is a simple indeterminate equation, as are a''x'' + b''y'' = c and ''x''&lt;sup&gt;2&lt;/sup&gt; = 1.  Indeterminate equations cannot be solved uniquely. Prominent examples include the following:

'''[[Univariate]] [[polynomial equation]]''':
:&lt;math&gt;a_nx^n+a_{n-1}x^{n-1}+\dots +a_2x^2+a_1x+a_0 = 0,&lt;/math&gt;

which has multiple solutions for the variable ''x'' in the [[complex plane]] unless it can be rewritten in the form &lt;math&gt;a_n(x-b)^n=0&lt;/math&gt;.

'''Non-degenerate [[Conic section|conic equation]]:

:&lt;math&gt;Ax^2 + Bxy + Cy^2 +Dx + Ey + F = 0,&lt;/math&gt;

where at least one of the given [[Parameter#Mathematical functions|parameters]] ''A'', ''B'', and ''C'' is non-zero, and ''x'' and ''y'' are real variables.

'''[[Pell's equation]]''':
:&lt;math&gt;\ x^2 - Py^2 = 1,&lt;/math&gt;

where ''P'' is a given integer that is not a [[square number]], and in which the variables ''x'' and ''y'' are required to be integers.

'''The equation of [[Pythagorean triple]]s''':
:&lt;math&gt;x^2+y^2=z^2,&lt;/math&gt;

in which the variables ''x'', ''y'', and ''z'' are required to be positive integers.

'''The equation of the [[Fermat-Catalan conjecture]]''':
:&lt;math&gt;a^m+b^n=c^k,&lt;/math&gt;

in which the variables ''a'', ''b'', ''c'' are required to be [[coprime]] positive integers and the variables ''m'', ''n'', and ''k'' are required to be positive integers the sum of whose reciprocals is less than 1.

== See also ==

* [[Indeterminate system]]
* [[Indeterminate (variable)]]
* [[Linear algebra]]

== References ==
{{Unreferenced|date=August 2008}}

[[Category:Algebra]]

{{algebra-stub}}</text>
      <sha1>3yt5u52xan9gbqjehx44hed8gcv77zf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Infrastructure (number theory)</title>
    <ns>0</ns>
    <id>23936022</id>
    <revision>
      <id>587274064</id>
      <parentid>498131829</parentid>
      <timestamp>2013-12-22T20:15:34Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>[[WP:AWB/GF|General fixes]], removed orphan tag using [[Project:AWB|AWB]] (9803)</comment>
      <text xml:space="preserve" bytes="14305">In [[mathematics]], an '''infrastructure''' is a [[Group (mathematics)|group]]-like structure appearing in [[global field]]s.

== Historic development ==

In 1972, [[Daniel Shanks|D. Shanks]] first discovered the infrastructure of a [[Quadratic field|real quadratic number field]] and applied his [[baby-step giant-step]] algorithm to compute the [[Dirichlet's unit theorem#The regulator|regulator]] of such a field in &lt;math&gt;\mathcal{O}(D^{1/4+\varepsilon})&lt;/math&gt; binary operations (for every &lt;math&gt;\varepsilon &gt; 0&lt;/math&gt;), where &lt;math&gt;D&lt;/math&gt; is the [[Quadratic field#Discriminant|discriminant]] of the quadratic field; previous methods required &lt;math&gt;\mathcal{O}(D^{1/2+\varepsilon})&lt;/math&gt; binary operations.&lt;ref name=&quot;shanks-infrastructure&quot;&gt;D. Shanks: The infrastructure of a real quadratic field and its applications. Proceedings of the Number Theory Conference (Univ. Colorado, Boulder, Colo., 1972), pp. 217-224. University of Colorado, Boulder, 1972. {{MR|389842}}&lt;/ref&gt; Ten years later, [[Hendrik Lenstra|H. W. Lenstra]] published&lt;ref name=&quot;lenstra-infrastructure&quot;&gt;H. W. Lenstra Jr.: On the calculation of regulators and class numbers of quadratic fields. Number theory days, 1980 (Exeter, 1980), 123&amp;ndash;150, London Math. Soc. Lecture Note Ser., 56, Cambridge University Press, Cambridge, 1982. {{MR|697260}}&lt;/ref&gt; a mathematical framework describing the infrastructure of a real quadratic number field in terms of &quot;circular groups&quot;. It was also described by R. Schoof&lt;ref name=&quot;schoof-infrastructure1&quot;&gt;R. J. Schoof: Quadratic fields and factorization. Computational methods in number theory, Part II, 235&amp;ndash;286, Math. Centre Tracts, 155, Math. Centrum, Amsterdam, 1982. {{MR|702519}}&lt;/ref&gt; and H. C. Williams,&lt;ref name=&quot;williams-infrastructure1&quot;&gt;H. C. Williams: Continued fractions and number-theoretic computations. Number theory (Winnipeg, Man., 1983). Rocky Mountain J. Math. 15 (1985), no. 2, 621&amp;ndash;655. {{MR|823273}}&lt;/ref&gt; and later extended by H. C. Williams, G. W. Dueck and B. K. Schmid to certain [[Cubic field|cubic number fields]] of [[Dirichlet's unit theorem|unit rank]] one&lt;ref name=&quot;williams-dueck-schmid&quot;&gt;H. C. Williams, G. W. Dueck, B. K. Schmid: A rapid method of evaluating the regulator and class number of a pure cubic field. Math. Comp. 41 (1983), no. 163, 235&amp;ndash;286. {{MR|701638}}&lt;/ref&gt;&lt;ref name=&quot;williams-dueck&quot;&gt;G. W. Dueck, H. C. Williams: Computation of the class number and class group of a complex cubic field. Math. Comp. 45 (1985), no. 171, 223&amp;ndash;231. {{MR|790655}}&lt;/ref&gt; and by J. Buchmann and H. C. Williams to all number fields of unit rank one.&lt;ref name=&quot;buchmann-williams-infrastructure&quot;&gt;J. Buchmann, H. C. Williams: On the infrastructure of the principal ideal class of an algebraic number field of unit rank one. Math. Comp. 50 (1988), no. 182, 569&amp;ndash;579. {{MR|929554}}&lt;/ref&gt; In his [[Habilitation|habilitation thesis]], J. Buchmann presented a baby-step giant-step algorithm to compute the regulator of a number field of ''arbitrary'' unit rank.&lt;ref name=&quot;buchmann-habil&quot;&gt;J. Buchmann: Zur Komplexität der Berechnung von Einheiten und Klassenzahlen algebraischer Zahlkörper. Habilitationsschrift, Düsseldorf, 1987. [http://www.cdc.informatik.tu-darmstadt.de/~buchmann/Lecture%20Notes/habil.pdf PDF]&lt;/ref&gt; The first description of infrastructures in number fields of arbitrary unit rank was given by R. Schoof using [[Arakelov divisor]]s in 2008.&lt;ref&gt;R. Schoof: Computing Arakelov class groups. (English summary) Algorithmic number theory: lattices, number fields, curves and cryptography, 447&amp;ndash;495, Math. Sci. Res. Inst. Publ., 44, Cambridge University Press, 2008. {{MR|2467554}} [http://www.mat.uniroma2.it/~schoof/papers.html PDF]&lt;/ref&gt;

The infrastructure was also described for other [[global field]]s, namely for [[algebraic function field]]s over [[finite field]]s. This was done first by A. Stein and H. G. Zimmer in the case of real [[Hyperelliptic curve|hyperelliptic]] function fields.&lt;ref name=&quot;stein-zimmer&quot;&gt;A. Stein, H. G. Zimmer: An algorithm for determining the regulator and the fundamental unit of hyperelliptic congruence function field. In &quot;Proceedings of the 1991 International Symposium on Symbolic and Algebraic Computation, ISSAC '91,&quot; Association for Computing Machinery, (1991), 183&amp;ndash;184.&lt;/ref&gt; It was extended to certain cubic function fields of unit rank one by R. Scheidler and A. Stein.&lt;ref name=&quot;stein-scheidler-cubicinfra&quot;&gt;R. Scheidler, A. Stein: Unit computation in purely cubic function fields of unit rank 1. (English summary) Algorithmic number theory (Portland, OR, 1998), 592&amp;ndash;606, Lecture Notes in Comput. Sci., 1423, Springer, Berlin, 1998. {{MR|1726104}}&lt;/ref&gt;&lt;ref name=&quot;scheidler-infrapurecubic&quot;&gt;R. Scheidler: Ideal arithmetic and infrastructure in purely cubic function fields. (English, French summary) J. Théor. Nombres Bordeaux 13 (2001), no. 2, 609&amp;ndash;631. {{MR|1879675}}&lt;/ref&gt; In 1999, S. Paulus and H.-G. Rück related the infrastructure of a real quadratic function field to the divisor class group.&lt;ref name=&quot;paulus-rueck&quot;&gt;S. Paulus, H.-G. Rück: Real and imaginary quadratic representations of hyperelliptic function fields. (English summary) Math. Comp. 68 (1999), no. 227, 1233&amp;ndash;1241. {{MR|1627817}}&lt;/ref&gt; This connection can be generalized to arbitrary function fields and, combining with R. Schoof's results, to all global fields.&lt;ref name=&quot;fontein-infrastructure&quot;&gt;{{cite journal | first=F. | last=Fontein | title=The Infrastructure of a Global Field of Arbitrary Unit Rank | journal=Math. Comp. | volume=80 | year=2011 | number=276 | pages=2325–2357 | doi=	10.1090/S0025-5718-2011-02490-7 | arxiv=0809.1685 }}&lt;/ref&gt;

== The one-dimensional case ==

=== An abstract definition ===

A '''one-dimensional (abstract) infrastructure''' &lt;math&gt;(X, d)&lt;/math&gt; consists of a [[real number]] &lt;math&gt;R &gt; 0&lt;/math&gt;, a [[finite set]] &lt;math&gt;X \neq \emptyset&lt;/math&gt; together with an [[Injective function|injective]] map &lt;math&gt;d : X \to \mathbb{R}/R\mathbb{Z}&lt;/math&gt;.&lt;ref name=&quot;fontein-pohlighellman&quot;&gt;F. Fontein: Groups from cyclic infrastructures and Pohlig-Hellman in certain infrastructures. (English summary) Adv. Math. Commun. 2 (2008), no. 3, 293&amp;ndash;307. {{MR|2429459}}&lt;/ref&gt; The map &lt;math&gt;d&lt;/math&gt; is often called the ''distance map''.

By interpreting &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; as a [[circle]] of [[circumference]] &lt;math&gt;R&lt;/math&gt; and by identifying &lt;math&gt;X&lt;/math&gt; with &lt;math&gt;d(X)&lt;/math&gt;, one can see a one-dimensional infrastructure as a circle with a finite set of points on it.

=== Baby steps ===

A '''baby step''' is an [[unary operation]] &lt;math&gt;bs : X \to X&lt;/math&gt; on a one-dimensional infrastructure &lt;math&gt;(X, d)&lt;/math&gt;. Visualizing the infrastructure as a circle, a baby step assigns each point of &lt;math&gt;d(X)&lt;/math&gt; the next one. Formally, one can define this by assigning to &lt;math&gt;x \in X&lt;/math&gt; the real number &lt;math&gt;f_x := \inf\{ f' &gt; 0 \mid d(x) + f' \in d(X) \}&lt;/math&gt;; then, one can define &lt;math&gt;bs(x) := d^{-1}(d(x) + f_x)&lt;/math&gt;.

=== Giant steps and reduction maps ===

Observing that &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; is naturally an [[Abelian group]], one can consider the sum &lt;math&gt;d(x) + d(y) \in \mathbb{R}/R\mathbb{Z}&lt;/math&gt; for &lt;math&gt;x, y \in X&lt;/math&gt;. In general, this is not an element of &lt;math&gt;d(X)&lt;/math&gt;. But instead, one can take an element of &lt;math&gt;d(X)&lt;/math&gt; which lies ''nearby''. To formalize this concept, assume that there is a map &lt;math&gt;red : \mathbb{R}/R\mathbb{Z} \to X&lt;/math&gt;; then, one can define &lt;math&gt;gs(x, y) := red(d(x) + d(y))&lt;/math&gt; to obtain a [[binary operation]] &lt;math&gt;gs : X \times X \to X&lt;/math&gt;, called the '''giant step''' operation. Note that this operation is in general ''not'' [[Associativity|associative]].

The main difficulty is how to choose the map &lt;math&gt;red&lt;/math&gt;. Assuming that one wants to have the condition &lt;math&gt;red \circ d = \mathrm{id}_X&lt;/math&gt;, a range of possibilities remain.   One possible choice&lt;ref name=&quot;fontein-pohlighellman&quot; /&gt; is given as follows: for &lt;math&gt;v \in \mathbb{R}/R\mathbb{Z}&lt;/math&gt;, define &lt;math&gt;f_v := \inf\{ f \ge 0 \mid v - f \in d(X) \}&lt;/math&gt;; then one can define &lt;math&gt;red(v) := d^{-1}(v - f_v)&lt;/math&gt;. This choice, seeming somewhat arbitrary, appears in a natural way when one tries to obtain infrastructures from global fields.&lt;ref name=&quot;fontein-infrastructure&quot; /&gt; Other choices are possible as well, for example choosing an element &lt;math&gt;x \in d(X)&lt;/math&gt; such that &lt;math&gt;|d(x) - v|&lt;/math&gt; is minimal (here, &lt;math&gt;|d(x) - v|&lt;/math&gt; is stands for &lt;math&gt;\inf\{ |f - v| \mid f \in d(x) \}&lt;/math&gt;, as &lt;math&gt;d(x)&lt;/math&gt; is of the form &lt;math&gt;v + R\mathbb{Z}&lt;/math&gt;); one possible construction in the case of real quadratic hyperelliptic function fields is given by S. D. Galbraith, M. Harrison and D. J. Mireles Morales.&lt;ref name=&quot;galbraith-harrison-mirelesmorales&quot;&gt;S. D. Galbraith, M. Harrison, D. J. Mireles Morales: Efficient hyperelliptic arithmetic using balanced representation for divisors. (English summary) Algorithmic number theory, 342&amp;ndash;356, Lecture Notes in Comput. Sci., 5011, Springer, Berlin, 2008. {{MR|2467851}}&lt;/ref&gt;

=== Relation to real quadratic fields ===

D. Shanks observed the infrastructure in real quadratic number fields when he was looking at cycles of reduced [[binary quadratic form]]s. Note that there is a close relation between reducing binary quadratic forms and [[continued fraction]] expansion; one step in the continued fraction expansion of a certain [[quadratic irrational]]ity gives an [[unary operation]] on the set of reduced forms, which cycles through all reduced forms in one equivalence class. Arranging all these reduced forms in a cycle, Shanks noticed that one can quickly jump to reduced forms further away from the beginning of the circle by [[Composition of binary quadratic forms|composing]] two such forms and reducing the result. He called this [[binary operation]] on the set of reduced forms a '''giant step''', and the operation to go to the next reduced form in the cycle a '''baby step'''.

=== Relation to &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; ===

The set &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; has a natural group operation and the giant step operation is defined in terms of it. Hence, it makes sense to compare the arithmetic in the infrastructure to the arithmetic in &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt;. It turns out that the group operation of &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; can be described using giant steps and baby steps, by representing elements of &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; by elements of &lt;math&gt;X&lt;/math&gt; together with a relatively small real number; this has been first described by D. Hühnlein and S. Paulus&lt;ref name=&quot;huehnlein-paulus&quot;&gt;D. Hühnlein, S. Paulus: On the implementation of cryptosystems based on real quadratic number fields (extended abstract). Selected areas in cryptography (Waterloo, ON, 2000), 288&amp;ndash;302, Lecture Notes in Comput. Sci., 2012, Springer, 2001. {{MR|1895598}}&lt;/ref&gt; and by M. J. Jacobson, Jr., R. Scheidler and H. C. Williams&lt;ref name=&quot;jacobson-scheidler-williams&quot;&gt;M. J. Jacobson Jr., R. Scheidler, H. C. Williams: The efficiency and security of a real quadratic field based key exchange protocol. Public-key cryptography and computational number theory (Warsaw, 2000), 89&amp;ndash;112, de Gruyter, Berlin, 2001 {{MR|1881630}}&lt;/ref&gt; in the case of infrastructures obtained from real quadratic number fields. They used floating point numbers to represent the real numbers, and called these representations CRIAD-representations resp. &lt;math&gt;(f, p)&lt;/math&gt;-representations. More generally, one can define a similar concept for all one-dimensional infrastructures; these are sometimes called &lt;math&gt;f&lt;/math&gt;-representations.&lt;ref name=&quot;fontein-pohlighellman&quot; /&gt;

A '''set of &lt;math&gt;f&lt;/math&gt;-representations''' is a subset &lt;math&gt;fRep&lt;/math&gt; of &lt;math&gt;X \times \mathbb{R}/R\mathbb{Z}&lt;/math&gt; such that the map &lt;math&gt;\Psi_{fRep} : fRep \to \mathbb{R}/R\mathbb{Z}, \; (x, f) \mapsto d(x) + f&lt;/math&gt; is a bijection and that &lt;math&gt;(x, 0) \in fRep&lt;/math&gt; for every &lt;math&gt;x \in X&lt;/math&gt;. If &lt;math&gt;red : \mathbb{R}/R\mathbb{Z} \to X&lt;/math&gt; is a reduction map, &lt;math&gt;fRep_{red} := \{ (x, f) \in X \times \mathbb{R}/R\mathbb{Z} \mid red(d(x) + f) = x \}&lt;/math&gt; is a set of &lt;math&gt;f&lt;/math&gt;-representations; conversely, if &lt;math&gt;fRep&lt;/math&gt; is a set of &lt;math&gt;f&lt;/math&gt;-representations, one can obtain a reduction map by setting &lt;math&gt;red(f) = \pi_1(\Psi_{fRep}^{-1}(f))&lt;/math&gt;, where &lt;math&gt;\pi_1 : X \times \mathbb{R}/R\mathbb{Z} \to X, \; (x, f) \mapsto x&lt;/math&gt; is the projection on $X$. Hence, sets of &lt;math&gt;f&lt;/math&gt;-representations and reduction maps are in a [[one-to-one correspondence]].

Using the bijection &lt;math&gt;\Psi_{fRep} : fRep \to \mathbb{R}/R\mathbb{Z}&lt;/math&gt;, one can pull over the group operation on &lt;math&gt;\mathbb{R}/R\mathbb{Z}&lt;/math&gt; to &lt;math&gt;fRep&lt;/math&gt;, hence turning &lt;math&gt;fRep&lt;/math&gt; into an abelian group &lt;math&gt;(fRep, +)&lt;/math&gt; by &lt;math&gt;x + y := \Psi_{fRep}^{-1}(\Psi_{fRep}(x) + \Psi_{fRep}(y))&lt;/math&gt;, &lt;math&gt;x, y \in fRep&lt;/math&gt;. In certain cases, this group operation can be explicitly described without using &lt;math&gt;\Psi_{fRep}&lt;/math&gt; and &lt;math&gt;d&lt;/math&gt;.

In case one uses the reduction map &lt;math&gt;red : \mathbb{R}/R\mathbb{Z} \to X, \; v \mapsto d^{-1}(v - \inf\{ f \ge 0 \mid v - f \in d(X) \})&lt;/math&gt;, one obtains &lt;math&gt;fRep_{red} = \{ (x, f) \mid f \ge 0, \; \forall f' \in [0, f) : d(x) + f' \not\in d(X) \}&lt;/math&gt;. Given &lt;math&gt;(x, f), (x', f') \in fRep_{red}&lt;/math&gt;, one can consider &lt;math&gt;(x'', f'')&lt;/math&gt; with &lt;math&gt;x'' = gs(x, x')&lt;/math&gt; and &lt;math&gt;f'' = f + f' + (d(x) + d(x') - d(gs(x, x'))) \ge 0&lt;/math&gt;; this is in general no element of &lt;math&gt;fRep_{red}&lt;/math&gt;, but one can reduce it as follows: one computes &lt;math&gt;bs^{-1}(x'')&lt;/math&gt; and &lt;math&gt;f'' - (d(x'') - d(bs^{-1}(x'')))&lt;/math&gt;; in case the latter is not negative, one replaces &lt;math&gt;(x'', f'')&lt;/math&gt; with &lt;math&gt;(bs^{-1}(x''), f'' - (d(x'') - d(bs^{-1}(x''))))&lt;/math&gt; and continues. If the value was negative, one has that &lt;math&gt;(x'', f'') \in fRep_{red}&lt;/math&gt; and that &lt;math&gt;\Psi_{fRep_{red}}(x, f) + \Psi_{fRep_{red}}(x', f') = \Psi_{fRep_{red}}(x'', f'')&lt;/math&gt;, i.e. &lt;math&gt;(x, f) + (x', f') = (x'', f'')&lt;/math&gt;.

== References ==
{{Reflist}}

{{DEFAULTSORT:Infrastructure (Number Theory)}}
[[Category:Algebra]]
[[Category:Algebraic structures]]</text>
      <sha1>io98wmeoe5q7c4fg7bnp8lpd4msaxyl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Closed-form expression</title>
    <ns>0</ns>
    <id>585143</id>
    <revision>
      <id>612244753</id>
      <parentid>612224739</parentid>
      <timestamp>2014-06-09T18:06:03Z</timestamp>
      <contributor>
        <username>Waldir</username>
        <id>182472</id>
      </contributor>
      <comment>/* See also */ +1</comment>
      <text xml:space="preserve" bytes="9691">{{Redirect|Closed formula|&quot;closed formula&quot; in the sense of a logic formula with no free variables|Sentence (mathematical logic)}}
{{ref improve|date=June 2014}}
{{Merge from|Analytical expression|discuss=Talk:Closed-form expression#Proposed merger from Analytical expression|date=May 2011}}

In [[mathematics]], an [[expression (mathematics)|expression]] is said to be a '''closed-form expression''' if it can be expressed analytically in terms of a [[finite set|finite]] number of certain &quot;well-known&quot; [[function (mathematics)|function]]s.  Typically, these well-known functions are defined to be [[Elementary function (differential algebra)|elementary functions]]—constants, one variable ''x'', elementary operations of [[arithmetic]] (+ − × ÷), ''n''th roots, exponent and logarithm (which thus also include trigonometric functions and inverse trigonometric functions). Often problems are said to be '''tractable''' if they can be solved in terms of a closed-form expression.

Closed–form expressions are an important sub-class of [[analytic expression]]s, which contain a bounded or unbounded number of applications of well-known functions. Unlike the broader analytic expressions, the closed-form expressions do not include [[Series (mathematics)#Infinite series|infinite series]] or [[continued fraction]]s; neither includes [[integral]]s or [[limit of a sequence|limits]]. Indeed, by the [[Stone–Weierstrass theorem]], any [[continuous function]] on the unit interval can be expressed as a limit of polynomials, so any class of functions containing the polynomials and closed under limits will necessarily include all continuous functions.

Similarly,  an [[equation]] or [[system of equations]] is said to have a '''closed-form solution''' if, and only if, at least one [[equation solving|solution]] can be expressed as a closed-form expression; and it is said to have an '''analytic solution''' if and only if at least one solution can be expressed as an analytic expression. There is a subtle distinction between a &quot;closed-form ''function''&quot; and a &quot;[[#Closed-form number|closed-form ''number'']]&quot; in the discussion of a &quot;closed-form solution&quot;, discussed in {{Harv|Chow|1999}} and [[#Closed-form number|below]]. A closed-form or analytic solution is sometimes referred to as an '''explicit solution'''.

An area of study in mathematics referred to broadly as [[Galois theory]] involves proving that no closed-form expression exists in certain contexts, based on the central example of closed-form solutions to polynomials.

Equations or systems too complex for closed-form or analytical solutions can often be analysed by [[mathematical model]]ling and [[computer simulation]].

== Examples ==

=== Roots of polynomials ===
The solutions of any [[quadratic equation]] with [[complex number|complex]] [[coefficients]] can be expressed in closed form in terms of [[addition]], [[subtraction]], [[multiplication]], [[Division (mathematics)|division]], and [[square root]] extraction, each of which is an elementary function. For example, the quadratic equation:

&lt;div style=&quot;text-align: center;&quot;&gt;&lt;math&gt;ax^2+bx+c=0,\,&lt;/math&gt;&lt;/div&gt;

is closed-form since its solutions can be expressed in terms of elementary functions:

&lt;div style=&quot;text-align: center;&quot;&gt;&lt;math&gt;x={-b\pm\sqrt{b^2-4ac} \over 2a}&lt;/math&gt;&lt;/div&gt;

Similarly solutions of cubic and quartic (third and fourth degree) equations can be expressed using arithmetic, square roots, and [[cube root]]s, or alternatively using arithmetic and trigonometric functions. However, there are [[quintic equation]]s without closed-form solutions using elementary functions, such as ''x''&lt;sup&gt;5&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;''x''&amp;nbsp;+&amp;nbsp;1&amp;nbsp;=&amp;nbsp;0; see [[Galois theory]].

=== Non-closed-form expressions with closed-form solutions ===

The expression:

&lt;div style=&quot;text-align: center;&quot;&gt;&lt;math&gt;f(x) = \sum_{i=0}^\infty {x \over 2^i}&lt;/math&gt;&lt;/div&gt;

is not closed-form because the summation entails an infinite number of elementary operations. However, by summing a geometric series this expression can be expressed in the closed-form:&lt;ref&gt;{{cite web|last=Holton|first=Glyn|title=Numerical Solution, Closed-Form Solution|url=http://www.riskglossary.com/link/closed_form_solution.htm|accessdate=31 December 2012}}&lt;/ref&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;&lt;math&gt;f(x) = 2x&lt;/math&gt;&lt;/div&gt;

=== Integrals ===
{{main|Differential Galois theory}}
The integral of a closed-form expression may or may not itself be expressible as a closed-form expression. This study is referred to as [[differential Galois theory]], by analogy with algebraic Galois theory.

The basic theorem of differential Galois theory is due to [[Joseph Liouville]] in the 1830s and 1840s and hence referred to as '''[[Liouville's theorem (differential algebra)|Liouville's theorem]]'''.

A standard example of an elementary function whose antiderivative does not have a closed-form expression is:

&lt;div style=&quot;text-align: center;&quot;&gt;&lt;math&gt;e^{-x^2}&lt;/math&gt;&lt;/div&gt;

whose antiderivative is (up to constants) the [[error function]]:

&lt;div style=&quot;text-align: center;&quot;&gt;&lt;math&gt;\operatorname{erf}(x) = \frac{2}{\sqrt{\pi}}\int_{0}^x e^{-t^2}\,\mathrm dt.&lt;/math&gt;&lt;/div&gt;

== Alternative definitions ==
Changing the definition of &quot;well-known&quot; to include additional functions can change the set of equations with closed-form solutions.  Many [[cumulative distribution function]]s cannot be expressed in closed form, unless one considers [[special functions]] such as the [[error function]] or [[gamma function]] to be well known.  It is possible to solve the quintic equation if general [[hypergeometric function]]s are included, although the solution is far too complicated algebraically to be useful.  For many practical computer applications, it is entirely reasonable to assume that the gamma function and other special functions are well-known, since numerical implementations are widely available.

== Closed-form number ==
{{see also|Transcendence theory}}
Three subfields of the complex numbers '''C''' have been suggested as encoding the notion of a &quot;closed-form number&quot;; in increasing order of generality, these are the [[EL number]]s, [[Liouville number (closed form expression)|Liouville numbers]], and [[elementary number]]s. The '''Liouville numbers''', denoted '''L''' (not to be confused with [[Liouville number]]s in the sense of rational approximation), form the smallest ''[[algebraically closed]]'' subfield of '''C''' closed under exponentiation and logarithm (formally, intersection of all such subfields)—that is, numbers which involve ''explicit'' exponentiation and logarithms, but allow explicit and ''implicit'' polynomials (roots of polynomials); this is defined in {{Harv|Ritt|1948|loc=p. 60}}. '''L''' was originally referred to as '''elementary numbers''', but this term is now used more broadly to refer to numbers defined in explicitly or implicitly in terms of algebraic operations, exponentials, and logarithms. A narrower definition proposed in {{Harv|Chow|1999|loc=pp. 441–442}}, denoted '''E''', and referred to as '''EL numbers''', is the smallest subfield of '''C''' closed under exponentiation and logarithm—this need not be algebraically closed, and correspond to ''explicit'' algebraic, exponential, and logarithmic operations. &quot;EL&quot; stands both for &quot;Exponential-Logarithmic&quot; and as an abbreviation for &quot;elementary&quot;.

Whether a number is a closed-form number is related to whether a number is [[transcendental number|transcendental]]. Formally, Liouville numbers and elementary numbers contain the [[algebraic number]]s, and they include some but not all transcendental numbers. In contrast, EL numbers do not contain all algebraic numbers, but do include some transcendental numbers. Closed-form numbers can be studied via [[transcendence theory]], in which a major result is the [[Gelfond–Schneider theorem]], and a major open question is [[Schanuel's conjecture]].

== Numerical computations ==
For purposes of numeric computations, being in closed form is not in general necessary, as many limits and integrals can be efficiently computed.

== Conversion from numerical forms ==
There is software that attempts to find closed-form expressions for numerical values, including RIES,&lt;ref&gt;{{cite web|last=Munafo|first=Robert|title=RIES - Find Algebraic Equations, Given Their Solution|url=http://mrob.com/pub/ries/index.html|accessdate=30 April 2012}}&lt;/ref&gt; &lt;tt&gt;identify&lt;/tt&gt; in [[Maple (software)|Maple]],&lt;ref&gt;{{cite web|title=identify|url=http://www.maplesoft.com/support/help/Maple/view.aspx?path=identify|work=Maple Online Help|publisher=Maplesoft|accessdate=30 April 2012}}&lt;/ref&gt; Plouffe's Inverter,&lt;ref&gt;{{cite web|title=Plouffe's Inverter|url=http://pi.lacim.uqam.ca/eng/server_en.html|accessdate=30 April 2012}}&lt;/ref&gt; and the [[Inverse Symbolic Calculator]].&lt;ref&gt;{{cite web|title=Inverse Symbolic Calculator|url=http://oldweb.cecm.sfu.ca/projects/ISC/|accessdate=30 April 2012}}&lt;/ref&gt;

==See also==
* [[Algebraic solution]]
* [[Analytic expression]]
* [[Finitary operation]]
* [[Numerical solution]]
* [[Computer simulation]]
* [[Symbolic regression]]

==References==
{{reflist}}
{{refbegin}}
* {{ Citation | title = Integration in finite terms | last = Ritt | first = J. F. | authorlink = Joseph Ritt | year = 1948 }}
* {{Citation | title = What is a Closed-Form Number? | first = Timothy Y. | last = Chow | volume = 106 | number = 5 | pages = 440–448 | jstor = 2589148 | journal = [[American Mathematical Monthly]] |date=May 1999 }}
{{refend}}

== External links ==
* {{MathWorld | urlname = Closed-FormSolution | title = Closed-Form Solution}}

{{DEFAULTSORT:Closed-Form Expression}}
[[Category:Algebra]]
[[Category:Special functions]]</text>
      <sha1>7kgbfdvoznz2ltkg01l0t1i4g3fi8y1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Idempotent matrix</title>
    <ns>0</ns>
    <id>2974863</id>
    <revision>
      <id>596955593</id>
      <parentid>576615153</parentid>
      <timestamp>2014-02-24T19:10:39Z</timestamp>
      <contributor>
        <username>Bill Shillito</username>
        <id>605362</id>
      </contributor>
      <comment>Added an example of a 2x2 and a 3x3 idempotent matrix.</comment>
      <text xml:space="preserve" bytes="4648">{{More footnotes|date=February 2012}}

In [[algebra]], an '''idempotent matrix''' is a [[matrix (mathematics)|matrix]] which, when multiplied by itself, yields itself.&lt;ref&gt;{{cite book |last=Chiang |first=Alpha C. |title=Fundamental Methods of Mathematical Economics |publisher=McGraw–Hill |edition=3rd |year=1984 |page=80 |location=New York |isbn=0070108137 }}&lt;/ref&gt;&lt;ref name=Greene&gt;{{cite book |last=Greene |first=William H. |title=Econometric Analysis |publisher=Prentice–Hall |location=Upper Saddle River, NJ |edition=5th |year=2003 |pages=808–809 |isbn=0130661899 }}&lt;/ref&gt; That is, the matrix ''M'' is idempotent if and only if ''MM''&amp;nbsp;=&amp;nbsp;''M''. For this product ''MM'' to be [[Matrix multiplication|defined]], ''M'' must necessarily be a [[square matrix]]. Viewed this way, idempotent matrices are [[idempotent element]]s of [[matrix ring]]s.

==Example==
Examples of a &lt;math&gt;2\times 2&lt;/math&gt; and a &lt;math&gt;3\times 3&lt;/math&gt; idempotent matrix are &lt;math&gt;\begin{bmatrix}1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}&lt;/math&gt; and &lt;math&gt;\begin{bmatrix}2 &amp; -2 &amp; -4 \\ -1 &amp; 3 &amp; 4 \\ 1 &amp; -2 &amp; -3 \end{bmatrix}&lt;/math&gt;, respectively.

==Properties==
With the exception of the [[identity matrix]], an idempotent matrix is [[singular matrix|singular]]; that is, its number of independent rows (and columns) is less than its number of rows (and columns).  This can be seen from writing ''MM = M'', assuming that ''M'' has full rank (is non-singular), and pre-multiplying by ''M''&lt;sup&gt;−1&lt;/sup&gt; to obtain ''M'' = ''M''&lt;sup&gt;−1&lt;/sup&gt;''M'' = ''I''.

When an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. This holds since [''I''&amp;nbsp;−&amp;nbsp;''M''][''I''&amp;nbsp;−&amp;nbsp;''M''] =&amp;nbsp;''I''&amp;nbsp;−&amp;nbsp;''M''&amp;nbsp;−&amp;nbsp;''M''&amp;nbsp;+&amp;nbsp;''M''&lt;sup&gt;2&lt;/sup&gt; =&amp;nbsp;''I''&amp;nbsp;−&amp;nbsp;''M''&amp;nbsp;−&amp;nbsp;''M''&amp;nbsp;+&amp;nbsp;''M'' =&amp;nbsp;''I''&amp;nbsp;−&amp;nbsp;''M''.

An idempotent matrix is always [[diagonalizable]] and its [[eigenvalue]]s are either 0 or 1.&lt;ref&gt;{{cite book |first=Roger A. |last=Horn |first2=Charles R. |last2=Johnson |title=Matrix analysis |publisher=Cambridge University Press |year=1990 |page={{Google books quote|id=PlYQN0ypTwEC|page=148|text=every idempotent matrix is diagonalizable|p. 148}} |isbn=0521386322 }}&lt;/ref&gt; The [[trace (linear algebra)|trace]] of an idempotent matrix — the sum of the elements on its main diagonal — equals the [[rank (linear algebra)|rank]] of the matrix and thus is always an integer. This provides an easy way of computing the rank, or alternatively an easy way of determining the trace of a matrix whose elements are not specifically known (which is helpful in [[econometrics]], for example, in establishing the degree of [[bias (statistics)|bias]] in using a [[variance|sample variance]] as an estimate of a [[variance|population variance]]).

==Applications==

Idempotent matrices arise frequently in [[regression analysis]] and [[econometrics]]. For example, in [[ordinary least squares]], the regression problem is to choose a vector &lt;math&gt;\beta&lt;/math&gt; of coefficient estimates so as to minimize the sum of squared residuals (mispredictions) ''e''&lt;sub&gt;''i''&lt;/sub&gt;: in matrix form,

:&lt;math&gt;\text{Minimize } (y - X \beta)^T(y - X \beta) \, &lt;/math&gt;

where ''y'' is a vector of [[Dependent and independent variables#Use in statistics|dependent variable]] observations, and ''X'' is a matrix each of whose columns is a column of observations on one of the [[Dependent and independent variables#Use in statistics|independent variables]]. The resulting estimator is

:&lt;math&gt;\beta = (X^TX)^{-1}X^Ty \, &lt;/math&gt;

where superscript ''T'' indicates a [[transpose]], and the vector of residuals is&lt;ref name=Greene/&gt;

:&lt;math&gt;e = y - X \beta = y - X(X^TX)^{-1}X^Ty = [I - X(X^TX)^{-1}X^T]y = My. \, &lt;/math&gt;

Here both ''M'' and &lt;math&gt;X(X^TX)^{-1}X^T&lt;/math&gt;(the latter being known as the [[hat matrix]]) are idempotent matrices, a fact which allows simplification when the sum of squared residuals is computed:

:&lt;math&gt; e^Te = (My)^T(My) = y^TM^TMy = y^TMMy = y^TMy. \, &lt;/math&gt;

The idempotency of ''M'' plays a role in other calculations as well, such as in determining the variance of the estimator &lt;math&gt;\beta&lt;/math&gt;.

An idempotent linear operator ''P'' is a projection operator on the [[Column space|range space]] ''R(P)'' along its [[null space]] ''N(P)''. ''P'' is an [[orthogonal projection]] operator if and only if it is idempotent and [[Symmetric matrix|symmetric]].

==See also==
* [[Idempotence]]
* [[Nilpotent]]
* [[Projection (linear algebra)]]
* [[Hat matrix]]

==References==
{{reflist}}

[[Category:Algebra]]
[[Category:Econometrics]]
[[Category:Matrices]]</text>
      <sha1>bgff4y7xo6wq4aiedipp5nbhemjmy0n</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Graph algebra (social sciences)</title>
    <ns>0</ns>
    <id>27145862</id>
    <revision>
      <id>610154056</id>
      <parentid>580044884</parentid>
      <timestamp>2014-05-26T03:00:57Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Social sciences methodology to [[:Category:Social science methodology]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2014 May 14]].</comment>
      <text xml:space="preserve" bytes="573">{{Multiple issues|notability=April 2010|refimprove=April 2010}}
'''Graph algebra''' is [[systems-centric]] modeling tool for the [[social sciences]].   It was first developed by Sprague, Pzeworski, and Cortes&lt;ref&gt;Cortés, Fernando, Adam Przeworski, and John Sprague. 1974. Systems Analysis for Social Scientists. New York: John Wiley &amp; Sons.&lt;/ref&gt; as a hybridized version of engineering plots to describe social phenomena.

== Notes and references ==

{{Reflist}}

{{DEFAULTSORT:Graph Algebra (Social Sciences)}}
[[Category:Algebra]]
[[Category:Social science methodology]]</text>
      <sha1>l2lr25i2bbo653tnugtzbsxwx4p3lqr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Groupoid algebra</title>
    <ns>0</ns>
    <id>30780490</id>
    <revision>
      <id>601851692</id>
      <parentid>491882483</parentid>
      <timestamp>2014-03-29T19:01:44Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes, removed orphan tag using [[Project:AWB|AWB]] (10067)</comment>
      <text xml:space="preserve" bytes="2853">In [[mathematics]], the concept of '''groupoid algebra''' generalizes the notion of [[group algebra]].&lt;ref&gt;Khalkhali (2009), {{Google books quote|id=UInc5AyTAikC|page=48|text=groupoid algebra|p. 48}}&lt;/ref&gt;

== Definition ==
Given a [[groupoid]] &lt;math&gt;(G, \cdot)&lt;/math&gt; and a [[field (mathematics)|field]] &lt;math&gt;K&lt;/math&gt;, it is possible to define the groupoid algebra &lt;math&gt;KG&lt;/math&gt; as the [[Algebra over a field|algebra]] over &lt;math&gt;K&lt;/math&gt; formed by the [[vector space]] having the elements of &lt;math&gt;G&lt;/math&gt; as [[generator (mathematics)|generator]]s and having the [[multiplication]] of these elements defined by &lt;math&gt;g * h = g \cdot h&lt;/math&gt;, whenever this product is defined, and &lt;math&gt;g * h = 0&lt;/math&gt; otherwise. The product is then extended by [[linearity]].&lt;ref&gt;Dokuchaev, Exel &amp; Piccione (2000), p. 7&lt;/ref&gt;

== Examples ==
Some examples of groupoid algebras are the following:&lt;ref&gt;da Silva &amp; Weinstein (1999), {{Google books quote|id=2fcC1EGKz08C|page=97|text=groupoid algebras|p. 97}}&lt;/ref&gt;
* [[Group algebra]]s
* [[Matrix ring|Matrix algebra]]s
* [[Algebra of functions|Algebras of functions]]

== Properties ==
* When a groupoid has a [[Wikt:finite|finite]] number of [[Object (category theory)|objects]] and a finite number of [[morphism (category theory)|morphisms]], the groupoid algebra is a [[direct sum]] of [[tensor product]]s of group algebras and matrix algebras.&lt;ref&gt;Khalkhali &amp; Marcolli (2008), {{Google books quote|id=HsTkPOj0iusC|page=210|text=Groupoid algebra of a finite groupoid|p. 210}}&lt;/ref&gt;

== See also ==
* [[Hopf algebra]]
* [[Partial group algebra]]

== Notes ==
&lt;references /&gt;

== References ==
* {{cite book |last1=Khalkhali |first1=Masoud |authorlink1= |last2= |first2= |authorlink2= |title=Basic Noncommutative Geometry |url= |edition= |series=EMS Series of Lectures in Mathematics |volume= |year=2009 |publisher=European Mathematical Society |location= |isbn=978-3-03719-061-6 }}
* {{cite book |last1=da Silva |first1=Ana Cannas |authorlink1= |last2=Weinstein |first2=Alan |authorlink2= |title=Geometric models for noncommutative algebras |url= |edition=2 |series=Berkeley mathematics lecture notes |volume=10 |year=1999 |publisher=AMS Bookstore |location= |isbn=978-0-8218-0952-5 }}
* {{cite journal |last1=Dokuchaev |first1=M. |last2=Exel |first2=R. |last3=Piccione |first3=P. |year=2000 |title=Partial Representations and Partial Group Algebras |journal=Journal of Algebra |volume=226 |issue= |pages=505–532 |publisher=Elsevier |issn=0021-8693 |doi= 10.1006/jabr.1999.8204|url=|arxiv= math/9903129
}}
* {{cite book |last1=Khalkhali |first1=Masoud |authorlink1= |last2=Marcolli |first2=Matilde |authorlink2= |title=An invitation to noncommutative geometry |url= |edition= |series= |volume= |year=2008 |publisher=World Scientific |location= |isbn=978-981-270-616-4 }}

[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>71lzif7oamd72gzpzipe0t1dvw0ta5k</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Graph dynamical system</title>
    <ns>0</ns>
    <id>20335837</id>
    <revision>
      <id>608098646</id>
      <parentid>591517004</parentid>
      <timestamp>2014-05-11T19:21:40Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>Task 3: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <text xml:space="preserve" bytes="10168">In [[mathematics]], the concept of '''graph dynamical systems''' can be used to capture a wide range of processes taking place on graphs or networks. A major theme in the mathematical and computational analysis of GDSs is to relate their structural properties (e.g. the network connectivity) and the global dynamics that result. 

The work on GDSs considers finite graphs and finite state spaces. As such, the research typically involves techniques from, e.g., [[graph theory]], [[combinatorics]], [[algebra]], and [[dynamical systems]] rather than differential geometry. In principle, one could define and study GDSs over an infinite graph (e.g. [[cellular automata]] or [[Stochastic cellular automata|Probabilistic Cellular Automata]] over &lt;math&gt;\mathbb{Z}^k&lt;/math&gt; or [[interacting particle systems]] when some randomness is included), as well as GDSs with infinite state space (e.g. &lt;math&gt;\mathbb{R}&lt;/math&gt; as in coupled map lattices); see, for example, Wu.&lt;ref name=wu-05&gt;{{cite journal |doi=10.1088/0951-7715/18/3/007 |last=Wu |first=Chai Wah |year=2005 |title=Synchronization in networks of nonlinear dynamical systems coupled via a directed graph |journal=Nonlinearity |volume= 18 |issue= 3|pages=1057–1064 |ref=Wu:05}}&lt;/ref&gt; In the following, everything is implicitly assumed to be finite unless stated otherwise.

==Formal definition==

A graph dynamical system is constructed from the following components:

&lt;blockquote&gt;
* A finite ''graph'' ''Y'' with vertex set v[''Y''] = {1,2, ... , n}. Depending on the context the graph can be directed or undirected.

* A state ''x&lt;sub&gt;v&lt;/sub&gt;'' for each vertex ''v'' of ''Y'' taken from a finite set ''K''. The ''system state'' is the ''n''-tuple ''x'' = (''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ... , ''x&lt;sub&gt;n&lt;/sub&gt;''), and ''x''[''v''] is the tuple consisting of the states associated to the vertices in the 1-neighborhood of ''v'' in ''Y'' (in some fixed order).

* A ''vertex function'' ''f&lt;sub&gt;v&lt;/sub&gt;'' for each vertex ''v''. The vertex function maps the state of vertex ''v'' at time ''t'' to the vertex state at time ''t''&amp;nbsp;+&amp;nbsp;1 based on the states associated to the 1-neighborhood of ''v'' in ''Y''.

* An ''update scheme'' specifying the mechanism by which the mapping of individual vertex states is carried out so as to induce a discrete dynamical system with map ''F'': ''K&lt;sup&gt;n&lt;/sup&gt; → K&lt;sup&gt;n&lt;/sup&gt;''.
&lt;/blockquote&gt;

The ''phase space'' associated to a dynamical system with map ''F'': ''K&lt;sup&gt;n&lt;/sup&gt; → K&lt;sup&gt;n&lt;/sup&gt;'' is the finite directed graph with vertex set ''K&lt;sup&gt;n&lt;/sup&gt;'' and directed edges (''x'', ''F''(''x'')). The structure of the phase space is governed by the properties of the graph ''Y'', the vertex functions (''f&lt;sub&gt;i&lt;/sub&gt;'')''&lt;sub&gt;i&lt;/sub&gt;'', and the update scheme. The research in this area seeks to infer phase space properties based on the structure of the system constituents. The analysis has a local-to-global character.

== Generalized cellular automata (GCA) ==

If, for example, the update scheme consists of applying the vertex functions synchronously one obtains the class of ''generalized cellular automata'' (CA). In this case, the global map ''F'': ''K&lt;sup&gt;n&lt;/sup&gt; → K&lt;sup&gt;n&lt;/sup&gt;'' is given by

&lt;math&gt;F(x)_v = f_v(x[v]) \;.&lt;/math&gt;

This class is referred to as generalized cellular automata since the classical or standard [[Cellular automaton|cellular automata]] are typically defined and studied over regular graphs or grids, and the vertex functions are typically assumed to be identical.

'''Example:''' Let ''Y'' be the circle graph on vertices {1,2,3,4} with edges {1,2}, {2,3}, {3,4} and {1,4}, denoted Circ&lt;sub&gt;4&lt;/sub&gt;. Let ''K'' = {0,1} be the state space for each vertex and use the function nor&lt;sub&gt;3&lt;/sub&gt; : ''K&lt;sup&gt;3&lt;/sup&gt;'' → ''K'' defined by nor&lt;sub&gt;3&lt;/sub&gt;(''x,y,z'')&amp;nbsp;=&amp;nbsp;(1&amp;nbsp;+&amp;nbsp;''x'')(1&amp;nbsp;+&amp;nbsp;''y'')(1&amp;nbsp;+&amp;nbsp;''z'') with arithmetic modulo 2 for all vertex functions. Then for example the system state (0,1,0,0) is mapped to (0,&amp;nbsp;0,&amp;nbsp;0,&amp;nbsp;1) using a synchronous update. All the transitions are shown in the phase space below. 

[[File:circ-4-nor.jpg|frame | center | 326 |]]

== Sequential dynamical systems (SDS) ==

If the vertex functions are applied asynchronously in the sequence specified by a word ''w'' = (''w''&lt;sub&gt;1&lt;/sub&gt;, ''w''&lt;sub&gt;2&lt;/sub&gt;, ... , ''w&lt;sub&gt;m&lt;/sub&gt;'') or permutation &lt;math&gt;\pi&lt;/math&gt; = ( &lt;math&gt;\pi_1&lt;/math&gt;, &lt;math&gt;\pi_2,\dots,\pi_n&lt;/math&gt;) of ''v''[''Y''] one obtains the class of ''[[Sequential dynamical system]]s'' (SDS).&lt;ref name=Mortveit-08&gt;{{cite book |last=Mortveit |first=Henning S. |author2=Reidys, Christian M. | year=2008 |title=An introduction to sequential dynamical systems |publisher=[[Springer Verlag]] |location=New York |isbn=978-0-387-30654-4 | series=Universitext| ref=Mortveit:08}}&lt;/ref&gt; In this case it is convenient to introduce the ''Y''-local maps ''F&lt;sub&gt;i&lt;/sub&gt;'' constructed from the vertex functions by

: &lt;math&gt;F_i (x) = (x_1, x_2,\ldots, x_{i-1}, f_i(x[i]), x_{i+1}, \ldots , x_n) \;. &lt;/math&gt;

The SDS map ''F'' = [''F&lt;sub&gt;Y&lt;/sub&gt;'' , ''w''] : ''K&lt;sup&gt;n&lt;/sup&gt;'' → ''K&lt;sup&gt;n&lt;/sup&gt;'' is the function composition

: &lt;math&gt;[F_Y ,w] = F_{w(m)} \circ F_{w(m-1)} \circ \cdots \circ F_{w(2)} \circ F_{w(1)} \;. &lt;/math&gt;

If the update sequence is a permutation one frequently speaks of a ''permutation SDS'' to emphasize this point. 

'''Example:''' Let ''Y'' be the circle graph on vertices {1,2,3,4} with edges {1,2}, {2,3}, {3,4} and {1,4}, denoted Circ&lt;sub&gt;4&lt;/sub&gt;. Let ''K''={0,1} be the state space for each vertex and use the function nor&lt;sub&gt;3&lt;/sub&gt; : ''K''&lt;sup&gt;3&lt;/sup&gt; → ''K'' defined by nor&lt;sub&gt;3&lt;/sub&gt;(''x,&amp;nbsp;y,&amp;nbsp;z'') = (1&amp;nbsp;+&amp;nbsp;''x'')(1&amp;nbsp;+&amp;nbsp;''y'')(1&amp;nbsp;+&amp;nbsp;''z'') with arithmetic modulo 2 for all vertex functions. Using the update sequence (1,2,3,4) then the system state (0,&amp;nbsp;1,&amp;nbsp;0,&amp;nbsp;0) is mapped to (0,&amp;nbsp;0,&amp;nbsp;1,&amp;nbsp;0). All the system state transitions for this sequential dynamical system are shown in the phase space below. 

[[File:circ-4-nor-1234.jpg|frame | center | 326 |]]

== Stochastic graph dynamical systems ==

From, e.g., the point of view of applications it is interesting to consider the case where one or more of the components of a GDS contains stochastic elements. Motivating applications could include processes that are not fully understood (e.g. dynamics within a cell) and where certain aspects for all practical purposes seem to behave according to some probability distribution. There are also applications governed by deterministic principles whose description is so complex or unwieldy that it makes sense to consider probabilistic approximations.  

Every element of a graph dynamical system can be made stochastic in several ways. For example, in a sequential dynamical system the update sequence can be made stochastic. At each iteration step one may choose the update sequence ''w'' at random from a given distribution of update sequences with corresponding probabilities. The matching probability space of update sequences induces a probability space of SDS maps. A natural object to study in this regard is the Markov chain on state space induced by this collection of SDS maps. This case is referred to as ''update sequence stochastic GDS'' and is motivated by, e.g., processes where &quot;events&quot; occur at random according to certain rates (e.g. chemical reactions), synchronization in parallel computation/discrete event simulations, and in computational paradigms described later&lt;!-- Make sure this cross ref stays/works. --&gt;. 

This specific example with stochastic update sequence illustrates two general facts for such systems: when passing to a stochastic graph dynamical system one is generally led to (1) a study of Markov chains (with specific structure governed by the constituents of the GDS), and (2) the resulting Markov chains tend to be large having an exponential number of states. A central goal in the study of stochastic GDS is to be able to derive reduced models. 

One may also consider the case where the vertex functions are stochastic, i.e., ''function stochastic GDS''. For example, Random Boolean networks are examples of function stochastic GDS using a synchronous update scheme and where the state space is ''K'' = {0,&amp;nbsp;1}. Finite [[probabilistic cellular automata]] (PCA) is another example of function stochastic GDS. In principle the class of Interacting particle systems (IPS) covers finite and infinite [[probabilistic cellular automata|PCA]], but in practice the work on IPS is largely concerned with the infinite case since this allows one to introduce more interesting topologies on state space.

==Applications==

Graph dynamical systems constitute a natural framework for capturing distributed systems such as biological networks and epidemics over social networks, many of which are frequently referred to as complex systems.

==See also==

*[[Sequential dynamical system]]s
*[[Finite state machine]]s
*[[Cellular automaton|Cellular automata]]
*[[Stochastic cellular automata|Probabilistic Cellular Automata]]
*[[Hopfield net]]works
*[[Boolean network]]s
*[[Petri net]]s
*[[Dynamic network analysis]] (a [[social science]] topic)
*[[Chemical reaction network theory]]
*[[Kauffman network]]s

==References==

{{reflist}}

==External links==
*[http://legacy.samsi.info/200809/algebraic/presentations/discrete/friday/samsi-05-dec-08.pdf Graph Dynamical Systems – A Mathematical Framework for Interaction-Based Systems, Their Analysis and Simulations by Henning Mortveit]

==Further reading==
* {{cite journal |doi=10.1088/0951-7715/22/2/010 |last=Macauley |first=Matthew |author2=Mortveit, Henning S. |year=2009 |title=Cycle equivalence of graph dynamical systems |journal=Nonlinearity |volume=22 |issue=2 |pages=421&amp;ndash;436 |ref=Macauley:09a}}
*{{cite book |last=Golubitsky |first=Martin |authorlink=Marty Golubitsky|author2=Stewart, Ian |year=2003 |title =The Symmetry Perspective |publisher=Birkhauser |location=Basel | ref=Golubitsky:03 |isbn=0-8176-2171-7}}

{{DEFAULTSORT:Graph Dynamical System}}
[[Category:Dynamical systems]]
[[Category:Algebra]]
[[Category:Graph theory]]
[[Category:Combinatorics]]</text>
      <sha1>isxa90svze2ijv4whe9i21mw8msrdyu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Factorization of polynomials over finite fields</title>
    <ns>0</ns>
    <id>30059275</id>
    <revision>
      <id>598424052</id>
      <parentid>598423804</parentid>
      <timestamp>2014-03-06T17:28:28Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <minor/>
      <comment>/* Example */ Two sections had the same heading</comment>
      <text xml:space="preserve" bytes="26513">In [[mathematics]] and [[computer algebra]] the [[factorization of polynomials|factorization of a polynomial]] consists of decomposing it into a [[product (mathematics)|product]] of [[irreducible polynomial|irreducible factors]]. This decomposition is theoretically possible and is unique for [[polynomial]]s with [[coefficient]]s in any [[field (mathematics)|field]], but rather strong restrictions on the field of the coefficients are needed to allow the computation of the factorization by means of an [[algorithm]]. In practice, algorithms have been designed only for polynomials with coefficients in a [[finite field]], in the [[field of rationals]] or in a [[finitely generated field extension]] of one of them.

The case of the '''factorization of [[univariate]] polynomials over a finite field''', which is the subject of this article, is especially important, because all the algorithms (including the case of multivariate polynomials over the rational numbers), which are sufficiently efficient to be implemented, reduce the problem to this case (see [[Polynomial factorization]]). It is also interesting for various applications of finite fields, such as [[coding theory]] ([[cyclic redundancy]] codes and [[BCH code]]s), [[cryptography]] ([[public key cryptography]] by the means of [[elliptic curve cryptography|elliptic curves]]), and [[computational number theory]].

As the reduction of the factorization of [[multivariate polynomial]]s to that of univariate polynomials does not have any specificity in the case of coefficients in a finite field, only polynomials with one variable are considered in this article.

==Background==
===Finite field===
{{main|Finite field}}
The theory of finite fields, whose origins can be traced back to the works of [[Gauss]] and [[Galois]], has played a part in various branches of mathematics. Due to the applicability of the concept in other topics of mathematics and sciences like computer science there has been a resurgence of interest in finite fields and this is partly due to important applications in [[coding theory]] and [[cryptography]]. Applications of finite fields introduce some of these developments in [[cryptography]], [[computer algebra]] and [[coding theory]]. 

A finite field or [[Galois field]] is a field with a [[Wikt:finite|finite]] order (number of elements). The order of a finite field is always a [[prime]] or a power of prime. For each [[prime power]] ''q'' = ''p&lt;sup&gt;r&lt;/sup&gt;'', there exists exactly one finite field with ''q'' elements, [[up to]] isomorphism. This field is denoted ''GF''(''q'') or '''F'''&lt;sub&gt;''q''&lt;/sub&gt;. If ''p'' is prime, ''GF''(''p'') is the [[prime field]] of order ''p''; it is the field of [[residue class#Ring of congruence classes|residue class]]es modulo ''p'', and its ''p'' elements are denoted 0, 1, ..., ''p''−1. Thus ''a''&amp;nbsp;=&amp;nbsp;''b'' in ''GF''(''p'') means the same as ''a'' ≡ ''b'' (mod ''p'').

===Irreducible polynomials===
Let ''F'' be a finite field. As for general fields, a non-constant polynomial ''f'' in ''F''[''x''] is said to be [[irreducible polynomial|irreducible]] over ''F'' if it is not the product of two polynomials of positive degree. A polynomial of positive degree that is not irreducible over ''F'' is called ''reducible over'' ''F''.

Irreducible polynomials allow to construct the finite fields of non prime order. In fact, for a prime power ''q'', let '''F'''&lt;sub&gt;''q''&lt;/sub&gt; be the finite field with ''q'' elements, unique up to an isomorphism. A polynomial ''f'' of degree ''n'' greater than one, which is irreducible over '''F'''&lt;sub&gt;''q''&lt;/sub&gt;, defines a field extension of degree ''n'' which is isomorphic to the field with ''q''&lt;sup&gt;''n''&lt;/sup&gt; elements: the elements of this extension are the polynomials of degree lower than ''n''; addition, subtraction and multiplication by an element of '''F'''&lt;sub&gt;''q''&lt;/sub&gt; are those of the polynomials; the product of two elements it the remainder of the division by ''f'' of their product as polynomials; the inverse of an element may be computed by the extended GCD algorithm (see [[Polynomial greatest common divisor|Arithmetic of algebraic extensions]]).

It follows that, to compute in a finite field of non prime order, one needs to generate an irreducible polynomial. For this, the common method is to take a polynomial at random and test it for irreducibility. For sake of efficiency of the multiplication in the field, it is usual to search for polynomials of the shape ''x''&lt;sup&gt;''n''&lt;/sup&gt; + ''ax'' + ''b''.{{Citation needed|date=February 2014}} 

Irreducible polynomials over finite fields are also useful for [[Pseudorandom]] number generators using feedback shift registers and [[discrete logarithm]] over '''F'''&lt;sub&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sub&gt;.

====Example====
The polynomial ''P'' = ''x''&lt;sup&gt;4&lt;/sup&gt; + 1 is irreducible over '''Q''' but not over any finite field. 

* On any field extension of '''F'''&lt;sub&gt;2&lt;/sub&gt;, ''P'' = (''x''+1)&lt;sup&gt;4&lt;/sup&gt;. 

*On every other finite field, at least one of −1, 2 and −2 is a square, because the product of two non squares is a square and so we have
#If &lt;math&gt;-1=a^2,&lt;/math&gt; then &lt;math&gt;P=(x^2+a)(x^2-a).&lt;/math&gt;
#If &lt;math&gt;2=b^2,&lt;/math&gt; then &lt;math&gt;P=(x^2+bx+1)(x^2-bx+1).&lt;/math&gt;
#If &lt;math&gt;-2=c^2,&lt;/math&gt; then &lt;math&gt;P=(x^2+cx-1)(x^2-cx-1).&lt;/math&gt;

===Complexity===
Polynomial factoring algorithms use basic polynomial operations such as products, divisions, gcd, powers of one polynomial modulo another, etc. A [[Multiplication_algorithm#Polynomial_multiplication|multiplication]] of two polynomials of degree at most ''n'' can be done in [[Big O notation|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)]] operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; using &quot;classical&quot; arithmetic, or in ''O''(''n''log(''n'')) operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; using [[Multiplication_algorithm#Fast_multiplication_algorithms_for_large_inputs|&quot;fast&quot; arithmetic]]. A [[Euclidean division]] (division with remainder) can be performed within the same time bounds. The cost of a [[polynomial greatest common divisor]] between two polynomials of degree at most ''n'' can be taken as ''O''(''n''&lt;sup&gt;2&lt;/sup&gt;) operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; using classical methods, or as ''O''(''n''log&lt;sup&gt;2&lt;/sup&gt;(''n'')) operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; using fast methods.  For polynomials ''h'', ''g'' of degree at most ''n'', the exponentiation ''h&lt;sup&gt;q&lt;/sup&gt;'' mod ''g'' can be done with ''O''(log(''q'')) polynomial products, using [[exponentiation by squaring]] method, that is ''O''(''n''&lt;sup&gt;2&lt;/sup&gt;log(''q'')) operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; using classical methods, or ''O''(''n''log(''q'')log(''n'')) operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; using fast methods.

In the algorithms that follow, the complexities are expressed in terms of number of arithmetic operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;, using classical algorithms for the arithmetic of polynomials.

==Factoring algorithms==
Many algorithms for factoring polynomials over finite fields include the following three stages:

===Square-free factorization===
The algorithm determines a [[square-free polynomial|square-free]] factorization for polynomials whose coefficients come from the finite field '''F'''&lt;sub&gt;''q''&lt;/sub&gt; of order ''q'' = ''p&lt;sup&gt;m&lt;/sup&gt;'' with ''p'' a prime.  This algorithm firstly determines the [[derivative]] and then computes the gcd of the polynomial and its derivative. If it is not one then the gcd is again divided into the original polynomial, provided that the derivative is not zero (a case that exists for non-constant polynomials defined over finite fields).

This algorithm uses the fact that, if the derivative of a polynomial is zero, then it is a polynomial in ''x''&lt;sup&gt;''p''&lt;/sup&gt;, which is the ''p''th power of the polynomial obtained by substituting ''x'' by ''x''&lt;sup&gt;1/''p''&lt;/sup&gt;.

This algorithm works also over a field of [[characteristic (algebra)|characteristic]] zero, with the only difference that it never enters in the blocks of instructions where ''p''th roots are computed. However, in this case, [[Square-free polynomial#Yun's algorithm|Yun's algorithm]] is much more efficient because it computes the greatest common divisors of polynomials of lower degrees. A consequence is that, when factoring a polynomial over the integers, the algorithm which follows is not used: one compute first the square-free factorization over the integers, and to factor the resulting polynomials, one chooses a ''p'' such that they remain square-free modulo ''p''.

   '''Algorithm''': '''SFF''' (Square-Free Factorization)
   '''Input''': A [[monic polynomial]] ''f'' in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']
   '''Output''': Square-free factorization of ''f''

   ''i''←1; ''R'' ← 1; ''g'' ← ''f''&amp;prime;;
   '''if''' ''g'' ≠ 0 '''then''' {
      ''c'' ← '''gcd'''(''f'', ''g'');
      ''w'' ← ''f''/''c'';
      '''while''' ''w'' ≠ 1 do {
            ''y'' ← '''gcd'''(''w'', ''c''); ''z'' ← ''w''/''y'';
            ''R'' ← ''R''·''z''&lt;sup&gt;''i''&lt;/sup&gt;; ''i'' ← i+1; 
            ''w'' ← ''y''; ''c'' ← ''c''/''y'' }
      '''if''' ''c'' ≠ 1 '''then''' {
            ''c'' ← ''c''&lt;sup&gt;1/''p''&lt;/sup&gt;;
            '''Output'''(''R''·'''SFF'''(''c'')&lt;sup&gt;''p''&lt;/sup&gt;) }
      else  '''Output'''(''R'')
   '''else''' {
            ''f'' ← ''f''&lt;sup&gt;1/''p''&lt;/sup&gt;;
            '''Output'''('''SFF'''(''f'')&lt;sup&gt;''p''&lt;/sup&gt;) }
   '''end'''.

====Example of a square-free factorization====
Let 

:&lt;math&gt; f = x^{11} + 2 x^9 + 2x^8 + x^6 + x^5 + 2x^3 + 2x^2 +1 \in \mathbf{F}_3[x],&lt;/math&gt;
to be factored over the field with three elements.

The algorithm computes first 

:&lt;math&gt; c = \gcd(f, f') = x^9 + 2x^6 + x^3 + 2.&lt;/math&gt;

Since the derivative is non-zero we have {{math|1=''w'' = ''f''/''c'' = ''x''&lt;sup&gt;2&lt;/sup&gt; + 2}} and we enter the while loop.  After one loop we have {{math|1=''y'' = ''x'' + 2}}, {{math|1=''z'' = ''x'' + 1}} and {{math|1=''R'' = ''x'' + 1}} with updates {{math|1=''i'' = 2}}, {{math|1=''w'' = ''x'' + 2}} and {{math|1=''c'' = ''x''&lt;sup&gt;8&lt;/sup&gt; + ''x''&lt;sup&gt;7&lt;/sup&gt; + ''x''&lt;sup&gt;6&lt;/sup&gt; + ''x''&lt;sup&gt;2&lt;/sup&gt;+''x''+1}}. The second time through the loop gives {{math|1=''y'' = ''x'' + 2}}, {{math|1=''z'' = 1}}, {{math|1=''R'' = ''x'' + 1}}, with updates {{math|1=''i'' = 3}}, {{math|1=''w'' = ''x'' + 2}} and {{math|1=''c'' = ''x''&lt;sup&gt;7&lt;/sup&gt; + 2''x''&lt;sup&gt;6&lt;/sup&gt; + ''x'' + 2}}. The third time through the loop also does not change {{math|1=''R''}}. For the fourth time through the loop we get {{math|1= ''y'' = 1}}, {{math|1=''z'' = ''x'' + 2}}, {{math|1=''R'' = (''x'' + 1)(''x'' + 2)&lt;sup&gt;4&lt;/sup&gt;}}, with updates {{math|1=''i'' = 5}}, {{math|1=''w'' = 1}} and {{math|1=''c'' = ''x''&lt;sup&gt;6&lt;/sup&gt; + 1}}. Since ''w'' = 1, we exit the while loop. Since ''c'' ≠ 1, it must be a perfect cube. The cube root of ''c'', obtained by replacing ''x''&lt;sup&gt;3&lt;/sup&gt; by ''x'' is ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1, and calling the square-free procedure recursively determines that it is square-free. Therefore, cubing it and combining it with the value of ''R'' to that point gives the square-free decomposition

:&lt;math&gt; f= (x+1)(x^2+1)^3(x+2)^4.&lt;/math&gt;

===Distinct-degree factorization===
This algorithm splits a square-free polynomial into a product of polynomials whose irreducible factors all have the same degree. Let ''f'' ∈ '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] of degree ''n'' be the polynomial to be factored. 

    '''Algorithm''' Distinct-degree factorization(DDF)
    '''Input''': A monic square-free polynomial  ''f'' ∈ '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']
    '''Output''': The set of all pairs (''g'', ''d''), such that 
              ''f'' has an irreducible factor of degree ''d'' and
              ''g'' is the product of all monic irreducible factors of ''f'' of degree ''d''.
    '''Begin'''
        &lt;math&gt;i:=1;\qquad S:=\emptyset,\qquad f^*:=f;&lt;/math&gt;
        '''while''' &lt;math&gt;\deg f^*\ge 2i&lt;/math&gt; '''do''' 
            &lt;math&gt;g=\gcd(f^*, x^{q^i}-x)&lt;/math&gt;
            '''if''' ''g'' ≠ 1, '''then''' 
              &lt;math&gt;S:=S\cup{(g,i)}&lt;/math&gt;;
              ''f*'' := ''f*''/''g'';
            '''end if'''
            ''i'' := ''i''+1;
        '''end while''';
        '''if''' ''f*'' ≠ 1, '''then''' &lt;math&gt;S:= S\cup{(f^*,\deg f^*)}&lt;/math&gt;;
        '''if''' ''S'' = ∅
            '''then return''' {(''f'', 1)}
            '''else return''' ''S''
     '''End'''
The correctness of the algorithm is based on the following: 

&lt;blockquote&gt;'''Lemma.''' For ''i'' ≥ 1 the polynomial 

:&lt;math&gt;x^{q^i}-x \in \mathbf{F}_q[x]&lt;/math&gt; 

is the product of all monic irreducible polynomials in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] whose degree divides ''i''.&lt;/blockquote&gt;

At first glance, this is not efficient since it involves computing the GCD of polynomials of a degree which is exponential in the degree of the input polynomial. However 

:&lt;math&gt;g=\gcd \left (f^*, x^{q^i}-x \right )&lt;/math&gt; 

may be replaced by 

:&lt;math&gt;g=\gcd \left (f^*, \left (x^{q^i}-x \mod f^* \right ) \right ).&lt;/math&gt; 

Therefore we have to compute:

:&lt;math&gt;x^{q^i}-x \mod f^*,&lt;/math&gt; 

there are two methods:

&lt;blockquote&gt;'''Method I.''' Start from the value of 

:&lt;math&gt;x^{q^{i-1}}\mod f^* &lt;/math&gt; 

computed at the preceding step and to compute its ''q''-th power modulo the new ''f*'', using [[exponentiation by squaring]] method. This needs 

:&lt;math&gt;O \left (\log(q) \deg(f)^2 \right )&lt;/math&gt; 

arithmetic operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; at each step, and thus 

:&lt;math&gt;O \left (\log(q) \deg(f)^3 \right )&lt;/math&gt; 

arithmetic operations for the whole algorithm.&lt;/blockquote&gt;

&lt;blockquote&gt;'''Method II.''' Using the fact that the ''q''-th power is a linear map over '''F'''&lt;sub&gt;''q''&lt;/sub&gt; we may compute its matrix with 

:&lt;math&gt;O \left (\deg(f)^2(\log(q)+\deg(f)) \right )&lt;/math&gt; 

operations. Then at each iteration of the loop, compute the product of a matrix by a vector (with ''O''(deg(''f'')&lt;sup&gt;2&lt;/sup&gt;) operations). This induces a total number of operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt; which is 

:&lt;math&gt;O \left (\deg(f)^2 (\log(q)+\deg(f)) \right ).&lt;/math&gt; 

Thus this second method is more efficient and is usually preferred. Moreover, the matrix that is computed in this method is used, by most algorithms, for equal-degree factorization (see below); thus using it for the distinct-degree factorization saves further computing time.&lt;/blockquote&gt;

===Equal-degree factorization===
{{main|Cantor–Zassenhaus algorithm}}
In this section, we consider the factorization of a monic squarefree univariate polynomial ''f'', of degree ''n'', over a finite field '''F'''&lt;sub&gt;''q''&lt;/sub&gt;, which has ''r'' ≥ 2 pairwise distinct irreducible factors &lt;math&gt; f_1,\ldots,f_r&lt;/math&gt; each of degree ''d''.

We first describe an algorithm by Cantor and Zassenhaus (1981) and then a variant that has a slightly better complexity. Both are probabilistic algorithms whose running time depends on random choices ([[Las Vegas algorithm]]s), and have a good average running time. In next section we describe an algorithm by Shoup (1990), which is also an equal-degree factorization algorithm, but is deterministic. All these algorithms require an odd order ''q'' for the field of coefficients. For more factorization algorithms see e.g. Knuth's book [[The Art of Computer Programming]] volume 2.

     Algorithm Cantor–Zassenhaus algorithm.
     Input: A finite field '''F'''&lt;sub&gt;''q''&lt;/sub&gt; of odd order ''q''.
            A monic square free polynomial ''f'' in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] of degree ''n'' = ''rd'', 
                 which has ''r'' ≥ 2 irreducible factors each of degree ''d''
     Output: The set of monic irreducible factors of ''f''.

     Factors:={''f''};
     while Size(Factors) &lt; ''r'' do,
        Choose ''h'' in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] with deg(''h'') &lt; ''n'' at random;
        &lt;math&gt;g:=h^{\frac{q^d-1}{2}}- 1 \pmod f&lt;/math&gt;
        for each ''u'' in Factors with deg(''u'') &gt; ''d'' do
            if gcd(''g'', ''u'') ≠ 1 and gcd(''g'', ''u'') ≠ ''u'', then
              Factors:= Factors&lt;math&gt;\,\setminus\, \{u\}\cup\{(\gcd(g,u),u/\gcd(g,u))\}&lt;/math&gt;;
            endif;
     endwhile
     return Factors.

The correctness of this algorithm relies on the fact that the ring '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']/''f'' is a direct product of the fields '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']/''f&lt;sub&gt;i&lt;/sub&gt;'' where ''f&lt;sub&gt;i&lt;/sub&gt;'' runs on the irreducible factors of ''f''. As all these fields have ''q&lt;sup&gt;d&lt;/sup&gt;'' elements, the component of ''g'' in any of these fields is zero with probability 

:&lt;math&gt;\frac{q^d-1}{2q^d} \sim \tfrac{1}{2}.&lt;/math&gt; 

This implies that the polynomial gcd(''g'', ''u'') is the product of the factors of ''g'' for which the component of ''g'' is zero.

It has been shown that the average number of iterations of the while loop of the algorithm is less than &lt;math&gt;2.5 \log_2 r&lt;/math&gt;, giving an average number of arithmetic operations in'''F'''&lt;sub&gt;''q''&lt;/sub&gt; which is &lt;math&gt;O(dn^2\log(r)\log(q))&lt;/math&gt;.&lt;ref&gt;{{citation|first1=Philippe|last1=Flajolet|first2=Jean-Marc | last2=Steayaert |title=A branching process arising in dynamic hashing, trie searching and polynomial factorization| booktitle = Automata, languages and programming (Aarhus, 1982)| series = Lecture Notes in Comput. Sci.|volume = 140|pages = 239–251|publisher = Springer|year=1982}}&lt;/ref&gt;

In the typical case where ''d''log(''q'') &gt; ''n'', this complexity may be reduced to 

:&lt;math&gt;O(n^2(\log(r)\log(q)+n))&lt;/math&gt; 

by choosing ''h'' in the kernel of the linear map 

:&lt;math&gt; v \to v^q-v \pmod f&lt;/math&gt; 

and replacing the instruction 

:&lt;math&gt;g:=h^{\frac{q^d-1}{2}}- 1 \pmod f&lt;/math&gt; 

by 

:&lt;math&gt;g:=h^{\frac{q-1}{2}}- 1 \pmod f.&lt;/math&gt; 

The proof of validity is the same as above, replacing the direct product of the fields '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']/''f&lt;sub&gt;i&lt;/sub&gt;'' by the direct product of their subfields with ''q'' elements. The complexity is decomposed in &lt;math&gt;O(n^2\log(r)\log(q))&lt;/math&gt; for the algorithm itself, &lt;math&gt;O(n^2(\log(q)+n))&lt;/math&gt; for the computation of the matrix of the linear map (which may be already computed in the square-free factorization) and ''O''(''n''&lt;sup&gt;3&lt;/sup&gt;) for computing its kernel. It may be noted that this algorithm works also if the factors have not the same degree (in this case the number ''r'' of factors, needed for stopping the while loop, is found as the dimension of the kernel). Nevertheless, the complexity is slightly better if square-free factorization is done before using this algorithm (as ''n'' may decrease with square-free factorization, this reduces the complexity of the critical steps).

===Victor Shoup's algorithm===
Like the algorithms of the preceding section, [[Victor Shoup]]'s algorithm is an equal-degree factorization algorithm.&lt;ref&gt;Victor Shoup, On the deterministic complexity of factoring polynomials over finite fields, Information Processing Letters 33:261-267, 1990&lt;/ref&gt; Unlike them, it is a deterministic algorithm. However, it is less efficient, in practice, that the algorithms of preceding section. For Shoup's algorithm, the input is restricted to polynomials over prime fields '''F'''&lt;sub&gt;''q''&lt;/sub&gt;.

Let ''g'' = ''g''&lt;sub&gt;1&lt;/sub&gt; ... ''g&lt;sub&gt;k&lt;/sub&gt;'' be the desired factorization, where the ''g&lt;sub&gt;i&lt;/sub&gt;'' are distinct monic irreducible polynomials of degree ''d''. Let ''n'' = deg(''g'') = ''kd''. We consider the [[ring (mathematics)|ring]] ''R'' = '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']/''g'' and denote also by ''x'' the image of ''x'' in ''R''. The ring ''R'' is the direct product of the fields ''R&lt;sub&gt;i&lt;/sub&gt;'' = '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x'']/''g&lt;sub&gt;i&lt;/sub&gt;'', and we denote by ''p&lt;sub&gt;i&lt;/sub&gt;'' the natural [[homomorphism]] from the ''R'' onto ''R&lt;sub&gt;i&lt;/sub&gt;''. The [[Galois group]] of ''R&lt;sub&gt;i&lt;/sub&gt;'' over '''F'''&lt;sub&gt;''q''&lt;/sub&gt; is cyclic of order ''d'', generated by the  [[field automorphism]] ''u'' → ''u&lt;sup&gt;p&lt;/sup&gt;''. It follows that the roots of ''g&lt;sub&gt;i&lt;/sub&gt;'' in ''R&lt;sub&gt;i&lt;/sub&gt;'' are 

:&lt;math&gt; p_i(x), p_i(x^q), p_i \left (x^{q^2} \right ), p_i \left (x^{q^{d-1}} \right ).&lt;/math&gt; 

If ''q'' &gt; ''n'', the [[Newton's identities]] allow to compute the ''s&lt;sub&gt;i&lt;/sub&gt;'' with 

Like in the preceding algorithm, this algorithm uses the same [[subalgebra]] ''B'' of ''R'' as the [[Berlekamp's algorithm]], sometimes called the &quot;Berlekamp subagebra&quot; and defined as 

:&lt;math&gt;\begin{align}
B &amp;= \left \{\alpha \in R \ : \ p_1(\alpha), \cdots, p_k(\alpha) \in \mathbf{F}_q \right \} \\
&amp;= \{u\in R \ : \ u^q=u\}
\end{align}&lt;/math&gt;

A subset ''S'' of ''B'' is said a [[separating set]] if, for every 1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;&lt;&amp;nbsp;''j''&amp;nbsp;≤&amp;nbsp;''k'' there exists ''s''&amp;nbsp;∈&amp;nbsp;''S'' such that &lt;math&gt;p_i(s) \ne p_j(s)&lt;/math&gt;. In the preceding algorithm, a separating set is constructed by choosing at random the elements of ''S''. In Shoup's algorithm, the separating set is constructed in the following way. Let ''s'' in ''R''[''Y''] be such that 

:&lt;math&gt;\begin{align}
s&amp;=(Y-x) \left (Y-x^q \right )\cdots \left (Y-x^{q^{d-1}} \right ) \\
&amp;=s_0+\cdots+s_{d-1}Y^{d-1}+Y^d
\end{align}&lt;/math&gt; 

Then &lt;math&gt;\{s_0,\dots ,s_{d-1}\}&lt;/math&gt; is a separating set because &lt;math&gt;p_i(s)=g_i&lt;/math&gt; for ''i'' =1, ..., ''k'' (the two monic polynomials have the same roots). As the ''g&lt;sub&gt;i&lt;/sub&gt;'' are pairwise distinct, for every pair of distinct indexes (''i'', ''j''), at least one of the coefficients ''s&lt;sub&gt;h&lt;/sub&gt;'' will satisfy &lt;math&gt;p_i(s_h)\ne p_j(s_h).&lt;/math&gt;

Having a separating set, Shoup's algorithm proceeds as the last algorithm of the preceding section, simply by replacing the instruction &quot;choose at random ''h'' in the kernel of the linear map &lt;math&gt; v \to v^q-v \pmod f&lt;/math&gt;&quot; by &quot;choose ''h'' + ''i'' with ''h'' in ''S'' and ''i'' in {1, ..., ''k''−1}&quot;.

==Rabin's test of irreducibility==
Like distinct-degree factorization algorithm, Rabin's algorithm&lt;ref&gt;{{cite journal |last1=Rabin |first1=Michael |year=1980 |title=Probabilistic algorithms in finite fields |journal=SIAM Journal on Computing |volume=9 |issue=2 |pages=273–280 |doi=10.1137/0209024 }}&lt;/ref&gt; is based on the Lemma stated above. Distinct-degree factorization algorithm tests every ''d'' not greater than half the degree of the input polynomial. Rabin's algorithm takes advantage that the factors are not needed for considering fewer ''d''. Otherwise, it is similar to distinct-degree factorization algorithm. It is based on the following fact.

Let ''p''&lt;sub&gt;1&lt;/sub&gt;, ..., ''p&lt;sub&gt;k&lt;/sub&gt;'', be all the prime divisors of ''n'', and denote &lt;math&gt;n/p_i=n_i&lt;/math&gt;, for 1 ≤ ''i'' ≤ ''k'' polynomial ''f'' in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] of degree ''n'' is irreducible in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] if and only if &lt;math&gt; \gcd \left (f,x^{q^{n_i}}-x \right )=1&lt;/math&gt;, for 1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;≤&amp;nbsp;''k'', and ''f'' divides &lt;math&gt;x^{q^n}-x&lt;/math&gt;. In fact, if ''f'' has a factor of degree not dividing ''n'', then ''f'' does not divide &lt;math&gt;x^{q^n}-x&lt;/math&gt;; if ''f'' has a factor of degree dividing ''n'', then this factor divides at least one of the &lt;math&gt;x^{q^{n_i}}-x.&lt;/math&gt;

  '''Algorithm''' Rabin Irreducibility Test
  '''Input''': A monic polynomial ''f'' in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;[''x''] of degree ''n'', 
         ''p''&lt;sub&gt;1&lt;/sub&gt;, ..., ''p&lt;sub&gt;k&lt;/sub&gt;'' all distinct prime divisors of ''n''.
  '''Output''': Either &quot;''f'' is irreducible&quot; or &quot;''f'' is reducible&quot;.
  '''Begin'''
      '''for''' ''j'' = 1 to ''k'' '''do''' 
         &lt;math&gt;n_j=n/p_j&lt;/math&gt;;
      '''for''' ''i'' = 1 to ''k'' '''do''' 
         &lt;math&gt;h:=x^{q^{n_i}}-x \bmod f&lt;/math&gt;;
         ''g'' := gcd(''f'', ''h'');
         '''if''' ''g'' ≠ 1, '''then return''' 'f is reducible' '''and STOP''';
      '''end for''';
      &lt;math&gt;g:= x^{q^{n}}-x \bmod f&lt;/math&gt;;
      '''if''' ''g'' = 0, '''then return''' &quot;f is irreducible&quot;, 
          '''else return''' &quot;''f'' is reducible&quot;
  '''end.'''

The basic idea of this algorithm is to compute &lt;math&gt; x^{q^{n_i}} \bmod f&lt;/math&gt; starting from the smallest &lt;math&gt; n_1,\ldots,n_k&lt;/math&gt; by repeated squaring or using the [[Finite_field#Frobenius_automorphisms|Frobenius automorphism]], and then to take the correspondent gcd. Using the elementary polynomial arithmetic, the computation of the matrix of the Frobenius automorphism needs &lt;math&gt;O(n^2 (n+\log q))&lt;/math&gt; operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;, the computation of 

:&lt;math&gt;x^{q^{n_i}}-x \pmod f&lt;/math&gt; 

needs ''O''(''n''&lt;sup&gt;3&lt;/sup&gt;) further operations, and the algorithm itself needs ''O''(''kn''&lt;sup&gt;2&lt;/sup&gt;) operations, giving a total of &lt;math&gt;O(n^2 (n+\log q))&lt;/math&gt; operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;. Using fast arithmetic (complexity &lt;math&gt;O(n\log n)&lt;/math&gt; for multiplication and division, and &lt;math&gt;O(n(\log n)^2)&lt;/math&gt; for GCD computation), the computation of the &lt;math&gt;x^{q^{n_i}}-x \bmod f&lt;/math&gt; by repeated squaring is &lt;math&gt;O(n^2\log n\log q)&lt;/math&gt;, and the algorithm itself is &lt;math&gt;O(kn(\log n)^2)&lt;/math&gt;, giving a total of &lt;math&gt;O(n^2\log n\log q)&lt;/math&gt; operations in '''F'''&lt;sub&gt;''q''&lt;/sub&gt;.

==See also==
* [[Berlekamp's algorithm]]
* [[Cantor–Zassenhaus algorithm]]
* [[Polynomial factorization]]

==References==
*KEMPFERT,H (1969) On the ''Factorization of Polynomials'' Department of Mathematics, The Ohio State University,Columbus,Ohio 43210
*Shoup,Victor (1996) ''Smoothness and Factoring Polynomials over Finite Fields'' Computer Science Department University of Toronto
* Von Zur Gathen, J., Panario, D. (2001) Factoring Polynomials Over Finite Fields: A Survey . Fachbereich Mathematik-Informatik, Universitat Paderborn. Department of Computer Science, University of Toronto.
*Gao Shuhong, Panario Daniel,''Test and Construction of Irreducible Polynomials over Finite Fields'' Department of mathematical Sciences, Clemson University, South Carolina, 29634-1907, USA. and Department of computer science University of Toronto, Canada M5S-1A4
*Shoup, Victor (1989) New Algorithms for Finding Irreducible Polynomials over Finite Fields Computer Science Department University of Wisconsin&amp;ndash;Madison
*Geddes, K.O. (1990) Algorithms for Computer Algebra

==External links==
* Some irreducible polynomials http://www.math.umn.edu/~garrett/m/algebra/notes/07.pdf
* Field and Galois Theory :http://www.jmilne.org/math/CourseNotes/FT.pdf
* Galois Field:http://designtheory.org/library/encyc/topics/gf.pdf
* Factoring polynomials over finite fields: http://www.science.unitn.it/~degraaf/compalg/polfact.pdf

==Notes==
{{Reflist}}

[[Category:Polynomials]]
[[Category:Algebra]]
[[Category:Computer algebra]]
[[Category:Coding theory]]
[[Category:Cryptography]]
[[Category:Computational number theory]]</text>
      <sha1>61d7luqcspwaklx704ck0zyse1hak81</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Irreducible polynomial</title>
    <ns>0</ns>
    <id>188725</id>
    <revision>
      <id>610132259</id>
      <parentid>610131484</parentid>
      <timestamp>2014-05-25T23:14:58Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <comment>/* top */ clarification: the assertion is also about unknown algorithms</comment>
      <text xml:space="preserve" bytes="12114">In [[mathematics]], a [[polynomial]] is said to be '''irreducible''' if it cannot be [[factorization|factored]] into the product of two or more non-trivial polynomials whose coefficients are of a specified type. Thus in the common context of polynomials with [[rational number|rational]] [[coefficient]]s, a polynomial is irreducible if it cannot be expressed as the product of two or more such polynomials, each of them having a lower [[polynomial degree|degree]] than the original one. For example, while &lt;math&gt;x^2-1 = (x-1)(x+1)&lt;/math&gt; is reducible over the rationals, &lt;math&gt;x^2+1&lt;/math&gt; is not.

For any [[field (mathematics)|field]] ''F'', a polynomial with coefficients in ''F'' is said to be '''irreducible''' over ''F'' if it is non-constant and cannot be factored into the product of two or more non-constant polynomials with coefficients in ''F''. The property of irreducibility depends on the field ''F''; a polynomial may be irreducible over some fields but reducible over others.  Some simple examples are discussed below.  

A polynomial with integer coefficients, or, more generally, with coefficients in a [[unique factorization domain]] ''F'' is said to be '''irreducible''' over ''F'' if it is not [[unit (ring theory)|invertible]] nor zero and cannot be factored into the product of two non-invertible polynomials with coefficients in ''F''. This definition generalizes the definition given for the case of coefficients in a field, because, in this case, the non constant polynomials are exactly the polynomials that are
non-invertible and non zero.

It is helpful to compare irreducible polynomials to [[prime number]]s: prime numbers (together with the corresponding negative numbers of equal magnitude) are the irreducible [[integer]]s. They exhibit many of the general properties of the concept of 'irreducibility' that equally apply to irreducible polynomials, such as the essentially unique factorization into prime or irreducible factors:

Every polynomial with coefficients in a field or a unique factorization domain ''F'' can be factorized into polynomials that are irreducible over ''F''. This factorization is unique [[up to]] [[permutation]] of the factors and the multiplication of the factors by invertible constants from ''F''. This property of unique factorization is commonly expressed by saying that the polynomial rings over a field or a unique factorization domain are unique factorization domains. However, the existence of such a factorization does not mean that the factorization of a given polynomial may always be computed: there are fields over which no [[algorithm]] can exist for factoring polynomials.&lt;ref&gt;{{citation |author1=Fröhlich, A.|author2=Shepherson, J. C.|title = On the factorisation of polynomials in a finite number of steps|journal = Mathematische Zeitschrift|volume = 62|issue=1|year = 1955|issn = 0025-5874|doi=10.1007/BF01180640}}
&lt;/ref&gt; There do exist factorization algorithms for the polynomials that have coefficients in the rational numbers, in a [[finite field]] or in a [[finitely generated field extension]] of these fields. They are described in the article [[Polynomial factorization]]. 

If an [[univariate]] polynomial ''p'' has a root (in some [[field extension]]) which is also a root of an irreducible polynomial ''q'', then ''p'' is a multiple of ''q'', and thus all roots of ''q'' are roots of ''p''; this is [[Abel's irreducibility theorem]]. This implies that the roots of an irreducible polynomial may not be distinguished through algebraic relations. This result is one of the starting points of [[Galois theory]], which has been introduced by [[Évariste Galois]] to study the relationship between the roots of a polynomial.

== Simple examples ==

The following six polynomials demonstrate some elementary properties of reducible and irreducible polynomials:

:&lt;math&gt;p_1(x)=x^2+4x+4\,={(x+2)(x+2)}&lt;/math&gt;,
:&lt;math&gt;p_2(x)=x^2-4\,={(x-2)(x+2)}&lt;/math&gt;,
:&lt;math&gt;p_3(x)=9x^2-3\,=3(3x^2-1)\,=3(x\sqrt{3}-1)(x\sqrt{3}+1)&lt;/math&gt;,
:&lt;math&gt;p_4(x)=x^2-4/9\,=(x-2/3)(x+2/3)&lt;/math&gt;,
:&lt;math&gt;p_5(x)=x^2-2\,=(x-\sqrt{2})(x+\sqrt{2})&lt;/math&gt;,
:&lt;math&gt;p_6(x)=x^2+1\,={(x-i)(x+i)}&lt;/math&gt;.

Over the ring &lt;math&gt;\mathbb Z&lt;/math&gt; of [[integer]]s, the first three polynomials are reducible (the third one is reducible because the factor 3 is not invertible in the integers), the last two are irreducible. (The fourth, of course, is not a polynomial over the integers.)

Over the field &lt;math&gt;\mathbb Q&lt;/math&gt; of [[rational number]]s, the first two and the fourth polynomials are reducible, but the other three polynomials are irreducible (as a polynomial over the rationals, 3 is a [[unit (ring theory)|unit]], and, therefore, does not count as a factor).

Over the field &lt;math&gt;\mathbb R&lt;/math&gt; of [[real number]]s, the first five polynomials are reducible, but &lt;math&gt;p_6(x)&lt;/math&gt; is still irreducible.

Over the field &lt;math&gt;\mathbb C&lt;/math&gt; of [[complex number]]s, all six polynomials are reducible. In fact, every nonzero polynomial &lt;math&gt;p(x)&lt;/math&gt; over &lt;math&gt;\mathbb C&lt;/math&gt; can be factored as

:&lt;math&gt; p(x) = a(x-z_1)\cdots (x-z_n)&lt;/math&gt;

where &lt;math&gt;n&lt;/math&gt; is the degree, &lt;math&gt;a&lt;/math&gt; the leading coefficient and &lt;math&gt;z_1,\dots,z_n&lt;/math&gt; the zeros of &lt;math&gt;p(x)&lt;/math&gt;. Thus, the only non-constant irreducible polynomials over &lt;math&gt;\mathbb C&lt;/math&gt; are [[linear function|linear polynomials]]. This is the [[Fundamental theorem of algebra]].

The existence of irreducible polynomials of degree greater than one (without zeros in the original field) historically motivated the [[field extension|extension]] of that original number field so that even these polynomials can be reduced into linear factors: from [[rational number]]s (&amp;nbsp;&lt;math&gt;\mathbb{Q}&lt;/math&gt;&amp;nbsp;), to the [[real number|real]] subset of the [[algebraic number]]s (&amp;nbsp;&lt;math&gt;\mathcal{A}\cap\mathbb{R}&lt;/math&gt;&amp;nbsp;), and finally to the algebraic subset of the [[complex number]]s (&amp;nbsp;&lt;math&gt;\mathcal{A}\cap\mathbb{C}&lt;/math&gt;&amp;nbsp;). After the invention of [[calculus]] those latter two subsets were later extended to all [[real number]]s (&amp;nbsp;&lt;math&gt;\mathbb{R}&lt;/math&gt;&amp;nbsp;) and all [[complex number]]s (&amp;nbsp;&lt;math&gt;\mathbb{C}&lt;/math&gt;&amp;nbsp;).

For algebraic purposes, the extension from rational numbers to real numbers is too &quot;radical&quot;: it introduces [[transcendental number]]s, which are not the solutions of algebraic equations with rational coefficients. These numbers are not needed for the algebraic purpose of factorizing polynomials (but they are necessary for the use of real numbers in [[Mathematical analysis|analysis]]). The set of [[algebraic numbers]] (&amp;nbsp;&lt;math&gt;\mathcal{A}&lt;/math&gt;&amp;nbsp;) is the [[algebraic closure]] of the rationals, and contains the roots of all polynomials (including ''i'' for instance). This is a [[countable]] field and is strictly contained in the complex numbers &amp;ndash; the difference being that this field (&amp;nbsp;&lt;math&gt;\mathcal{A}&lt;/math&gt;&amp;nbsp;) is &quot;algebraically complete&quot; (as are the [[complex number]]s, &lt;math&gt;\mathbb{C}&lt;/math&gt;&amp;nbsp;) but not analytically complete since it lacks the aforementioned transcendentals. 

The above paragraph generalizes in that there is a purely algebraic process to [[field extension|extend]] a given field ''F'' with a given polynomial &lt;math&gt;p(x)&lt;/math&gt; to a larger field where this polynomial &lt;math&gt;p(x)&lt;/math&gt; can be reduced into linear factors.  The study of such extensions is the starting point of [[Galois theory]].

=== Real and complex numbers ===
As shown in the examples above, only [[linear|linear polynomial]]s are irreducible over the field of complex numbers (this is a consequence of the [[fundamental theorem of algebra]]). Since [[Complex conjugate root theorem|the complex roots of a real polynomial are in conjugate pairs]], the irreducible polynomials over the field of real numbers are the linear polynomials and the quadratic polynomials with no real roots. For example,
&lt;math&gt;x^4 + 1&lt;/math&gt; factors over the real numbers as &lt;math&gt;(x^2 + \sqrt{2}x + 1)(x^2 - \sqrt{2}x + 1).&lt;/math&gt;

=== Generalization ===
If ''R'' is an [[integral domain]], an element ''f'' of ''R'' which is neither zero nor a unit is called [[irreducible element|irreducible]] if there are no non-units ''g'' and ''h'' with ''f'' = ''gh''. One can show that every [[prime element]] is irreducible;&lt;ref&gt;Consider p a prime that is reducible: p=ab. Then p | ab =&gt; p | a or p | b. Say p | a =&gt; a = pc, then we have: p=ab=pcb =&gt; p(1-cb)=0. Because R is a domain we have: cb=1. So b is a unit and p is irreducible&lt;/ref&gt; the converse is not true in general but holds in [[unique factorization domain]]s. The [[polynomial ring]] ''F''[''x''] over a field ''F'' (or any unique-factorization domain) is again a unique factorization domain. Inductively, this means that the polynomial ring in ''n'' indeterminants (over a ring ''R'') is a unique factorization domain if the same is true for ''R''.

=== Finite fields ===
Factorization over a [[finite field]] behaves similarly to factorization over the rational or the complex field. However, polynomials with integer coefficients that are irreducible over the field &lt;math&gt;\mathbb Q&lt;/math&gt; can be reducible over a finite field. For example, the polynomial &lt;math&gt;x^2+1&lt;/math&gt; is irreducible over &lt;math&gt;\mathbb Q&lt;/math&gt; but reducible over the field &lt;math&gt;\mathbb F_2&lt;/math&gt; of two elements. Indeed, over &lt;math&gt;\mathbb F_2&lt;/math&gt;, we have

:&lt;math&gt; (x^2+1) = (x+1)^2 &lt;/math&gt;

The irreducibility of a polynomial over the integers &lt;math&gt;\mathbb Z&lt;/math&gt; is related to that over the field &lt;math&gt;\mathbb F_p&lt;/math&gt; of &lt;math&gt;p&lt;/math&gt; elements (for a prime &lt;math&gt;p&lt;/math&gt;). Namely, if a polynomial over &lt;math&gt;\mathbb Z&lt;/math&gt; with leading coefficient &lt;math&gt;1&lt;/math&gt; is reducible over &lt;math&gt;\mathbb Z&lt;/math&gt; then it is reducible over &lt;math&gt;\mathbb F_p&lt;/math&gt; for any prime &lt;math&gt;p&lt;/math&gt;. The converse, however, is not true,&lt;ref&gt;{{cite book|title=Abtract Algebra|year=2004|publisher=John Wiley &amp; Sons, Inc.|isbn=0-471-43334-9|page=309|author=David Dummit|author2=Richard Foote|chapter=chapter 9, Proposition 12}}&lt;/ref&gt; there are polynomials of arbitrary large degree that are irreducible over the integers and reducible over every finite field. A simple example of such a polynomial is &lt;math&gt;x^4+1,&lt;/math&gt; which is irreducible over the integers and reducible over every finite field.

== See also ==
* [[Gauss's lemma (polynomial)]]
* [[Rational root theorem]], a method of finding whether a polynomial has a linear factor with rational coefficients
* [[Eisenstein's criterion]]
* [[Perron method]]
* [[Hilbert's irreducibility theorem]]
* [[Cohn's irreducibility criterion]]
* [[Irreducible component]] of a [[topological space]]
* [[Factorization of polynomials over finite fields]]
* [[Quartic function#Factorization into quadratics]]
* [[Cubic function#Factorization]]
* [[Casus irreducibilis]], the irreducible cubic with three real roots
* [[Quadratic equation#Quadratic factorization]]

== References ==
* {{citation | first1 = Alfred J. | last1 = Menezes | authorlink1 = Alfred Menezes | first2 = Paul C. | last2 = Van Oorschot | authorlink2 = Paul van Oorschot | first3 = Scott A. | last3 = Vanstone | authorlink3 = Scott Vanstone | title = Handbook of applied cryptography | publisher = [[CRC Press]] | year = 1997 | isbn = 978-0-8493-8523-0}}, [http://books.google.com/books?id=nSzoG72E93MC&amp;pg=PA154 pp. 154].
* {{citation | first1 = Rudolf | last1 = Lidl | first2 = Harald | last2 = Niederreiter | title = Finite fields | edition = 2nd | publisher = [[Cambridge University Press]] | year = 1997 | isbn = 978-0-521-39231-0}}, [http://books.google.ca/books?id=xqMqxQTFUkMC&amp;pg=PA91 pp. 91].

== External links ==
* {{MathWorld | title = Irreducible Polynomial | urlname = IrreduciblePolynomial}}
* {{PlanetMath | urlname = IrreduciblePolynomial2 | title = Irreducible Polynomial}}
* [http://theory.cs.uvic.ca/inf/neck/PolyInfo.html Information on Primitive and Irreducible Polynomials], The (Combinatorial) Object Server.

== Notes ==
{{reflist}}

{{DEFAULTSORT:Irreducible Polynomial}}
[[Category:Polynomials]]
[[Category:Abstract algebra]]
[[Category:Algebra]]</text>
      <sha1>8d1fopax1pf8b1uquqomek91h7ocfbb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>System of polynomial equations</title>
    <ns>0</ns>
    <id>27420015</id>
    <revision>
      <id>597317679</id>
      <parentid>591789361</parentid>
      <timestamp>2014-02-27T02:57:25Z</timestamp>
      <contributor>
        <username>PhilippHeßler</username>
        <id>20869585</id>
      </contributor>
      <minor/>
      <comment>Link to grevlex section on the page on monomial orders.</comment>
      <text xml:space="preserve" bytes="26931">A '''system of polynomial equations''' is a set of [[simultaneous equations]] ''f''&lt;sub&gt;1&lt;/sub&gt; = 0, ..., ''f''&lt;sub&gt;''h''&lt;/sub&gt; = 0 where the ''f''&lt;sub&gt;''i''&lt;/sub&gt; are [[Polynomial ring|polynomials]] in several variables, say ''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;, over some [[Field (mathematics)|field]] ''k''.

Usually, the field ''k'' is either the field of [[rational number]]s or a [[finite field]], although most of the theory applies to any field.

A ''solution'' is a set of the values for the ''x''&lt;sub&gt;''i''&lt;/sub&gt; which belong to some [[algebraically closed]] [[field extension]] ''K'' of ''k''. When ''k'' is the field of [[rational number]]s, ''K'' is the field of [[complex number]]s.

==Examples and extensions==

===Trigonometric equations===
A trigonometric equation is an equation ''g'' = 0 where ''g'' is a [[trigonometric polynomial]]. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing sin(''x'') and cos(''x'') by two new variables ''s'' and ''c'' and adding the new equation ''s''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''c''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;1&amp;nbsp;=&amp;nbsp;0.

For example the equation

:&lt;math&gt; \sin(x)^3+\cos(3x)=0 \, &lt;/math&gt;

is equivalent to the polynomial system

:&lt;math&gt; \begin{cases}
s^3+4c^3-3c&amp;=0\\
s^2+c^2-1&amp;=0
\end{cases}
&lt;/math&gt;

=== Solutions in a finite field ===
When solving a system over a finite field ''k'' with ''q'' elements, one is primarily interested in the solutions in ''k''. As the elements of ''k'' are exactly the solutions of the equation ''x''&lt;sup&gt;''q''&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''x''&amp;nbsp;=&amp;nbsp;0, it suffices, for restricting the solutions to ''k'', to add the equation ''x''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''q''&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''x''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0 for each variable&amp;nbsp;''x''&lt;sub&gt;''i''&lt;/sub&gt;.

=== Coefficients in a number field or in a finite field with non-prime order ===

The elements of a [[number field]] are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers. 

For example, if a system contains &lt;math&gt;\sqrt{2}&lt;/math&gt;, a system over the rational numbers is obtained by adding the equation ''r''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;2&amp;nbsp;=&amp;nbsp;0 and replacing &lt;math&gt;\sqrt{2}&lt;/math&gt; by ''r''&lt;sub&gt;2&lt;/sub&gt; in the other equations.

In the case of a finite field, the same transformation allows always to suppose that the field ''k'' has a prime order.

==Basic properties and definitions==
A system is [[Overdetermined system|overdetermined]] if the number of equations is higher than the number of variables. A system is ''inconsistent'' if it has no solutions. By [[Hilbert's Nullstellensatz]] this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems are inconsistent. For example the system &amp;nbsp;''x''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;1 =&amp;nbsp;0, ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;1&amp;nbsp;=&amp;nbsp;0 is overdetermined but not inconsistent.

A system is [[Underdetermined system|underdetermined]] if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension ''K'' of&amp;nbsp;''k''.

A system is [[Zero-dimensional space|zero-dimensional]] if it has a finite number of solutions in an algebraically closed extension ''K'' of&amp;nbsp;''k''. This terminology comes from the fact that the [[algebraic variety]] of the solutions has [[Dimension of an algebraic variety|dimension]] zero. A system with infinitely many solutions is said to be ''positive-dimensional''.

A zero-dimensional system with as many equations as variables is said to be ''well-behaved''.&lt;ref&gt;Songxin Liang, J. Gerhard, D.J. Jeffrey, G. Moroz, ''A Package for Solving Parametric Polynomial Systems''. Communications in Computer Algebra (2009)&lt;/ref&gt; 
[[Bézout's theorem]] asserts that a well-behaved system whose equations have degrees ''d''&lt;sub&gt;1&lt;/sub&gt;, ..., ''d''&lt;sub&gt;''n''&lt;/sub&gt; has at most ''d''&lt;sub&gt;1&lt;/sub&gt;...''d''&lt;sub&gt;''n''&lt;/sub&gt; solutions. This bound is sharp. If all the degrees are equal to ''d'', this bound becomes ''d''&lt;sup&gt;''n''&lt;/sup&gt; and is exponential in the number of variables. 

This exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with Bézout's bound higher than, say 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).

== What is solving? ==
The first thing to do for solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a [[Gröbner basis]] of the left hand side of the equations. The system is ''inconsistent'' if this Gröbner basis is reduced to 1. The system is ''zero-dimensional'' if, for every variable there is a [[Gröbner basis|leading monomial]] of some element of the Gröbner basis which is a pure power of this variable. For this test, the best [[monomial order]] is usually the [[monomial order#Graded reverse lexicographic order|graded reverse lexicographic]] one (grevlex).

If the system is ''positive-dimensional'', it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean &quot;finding a description of the solutions from which the relevant properties of the solutions are easy to extract&quot;. There is no commonly accepted such description. In fact there are a lot of different &quot;relevant properties&quot;, which involve almost every subfield of [[algebraic geometry]].

A natural example of an open question about solving positive-dimensional systems is the following: ''decide if a polynomial system over the [[rational number]]s has a finite number of real solutions and compute them''. The only published algorithm which allows one to solve this question is [[cylindrical algebraic decomposition]], which is not efficient enough, in practice, to be used for this.

For zero-dimensional systems, solving consists in computing all the solutions. There are two different ways of outputting the solutions. The most common, possible only for real or complex solutions consists in outputting numeric approximations of the solutions. Such a solution is called ''numeric''. A solution is ''certified'' if it is provided with a bound on the error of the approximations which separates the different solutions.

The other way to represent the solutions is said to be ''algebraic''. It uses the fact that, for a zero-dimensional system, the solutions belong to the [[algebraic closure]] of the field ''k'' of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation of the solutions which need only to solve only one univariate polynomial for each solution have to be preferred: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.

== Algebraic representation of the solutions==

=== Regular chains ===
{{main|Regular chain}}
The usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists in a sequence of polynomials ''f''&lt;sub&gt;1&lt;/sub&gt;(''x''&lt;sub&gt;1&lt;/sub&gt;), ''f''&lt;sub&gt;2&lt;/sub&gt;(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;), ..., ''f''&lt;sub&gt;''n''&lt;/sub&gt;(''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;) such that, for every ''i'' such that 1 ≤ ''i'' ≤ ''n''
* ''f''&lt;sub&gt;''i''&lt;/sub&gt; is a polynomial in ''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''i''&lt;/sub&gt; only, which has a degree ''d''&lt;sub&gt;''i''&lt;/sub&gt; &gt; 0 in ''x''&lt;sub&gt;''i''&lt;/sub&gt; ;
* the coefficient of ''x''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''d''&lt;sub&gt;''i''&lt;/sub&gt;&lt;/sup&gt; in ''f''&lt;sub&gt;''i''&lt;/sub&gt; is a polynomial in ''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''i''&amp;nbsp;&amp;minus;&amp;nbsp;1&lt;/sub&gt; which does not have any common zero with ''f''&lt;sub&gt;1&lt;/sub&gt;, ..., ''f''&lt;sub&gt;''i''&amp;nbsp;&amp;minus;&amp;nbsp;1&lt;/sub&gt;.

To such a [[regular chain]] is associated a triangular system of equations
:&lt;math&gt;
\begin{cases}
f_1(x_1)= 0\\
f_2(x_1,x_2)=0\\
\cdots\\
f_n(x_1, x_2, \ldots, x_n)=0
\end{cases}
&lt;/math&gt;

The solutions of this system are obtained by solving the first univariate equations, substitute the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from ''f''&lt;sub&gt;''i''&lt;/sub&gt; has degree ''d''&lt;sub&gt;''i''&lt;/sub&gt; and thus that this system has ''d''&lt;sub&gt;1&lt;/sub&gt; ... ''d''&lt;sub&gt;''n''&lt;/sub&gt; solutions, provided that there is no multiple root in this resolution process ([[fundamental theorem of algebra]]).

Every zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.
:&lt;math&gt;
\begin{cases}
x^2-1=0\\
(x-1)(y-1)=0\\
y^2-1=0
\end{cases}
&lt;/math&gt;

There are several algorithms for computing a [[triangular decomposition]] of an arbitrary polynomial system (not necessarily zero-dimensional)&lt;ref&gt;P. Aubry, M. Moreno Maza, ''Triangular Sets for Solving Polynomial Systems: a Comparative Implementation of Four Methods''. J. Symb. Comput. '''28''', 1999&lt;/ref&gt; into [[regular chain]]s (or [[regular semi-algebraic system]]s).

There is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the [[Gröbner basis]] for the [[monomial order|graded reverse lexicographic order (grevlex)]], then deducing the Gröbner basis by FGLM algorithm&lt;ref&gt;Faugère, J.C., Gianni, P., Lazard, D. and Mora, T., ''Efficient Computation of Zero-Dimensional Gröbner Basis by Change of Ordering''. Journal of Symbolic Computation, '''16''', 1993&lt;/ref&gt; and finally applying the Lextriangular algorithm.&lt;ref&gt;D. Lazard, ''Solving zero-dimensional algebraic systems''. Journal of Symbolic Computation '''13''', 1992&lt;/ref&gt; 

This representation of the solutions and the algorithms to compute it are presently, in practice, a very efficient way for solving zero-dimensional polynomial systems with coefficients in a finite field. 

For rational coefficients, the Lextriangular algorithm has two drawbacks:
* The output uses to involve huge integers which may make the computation and the use of the result problematic.
* To deduce the numeric values of the solutions from the output, one has to solve univariate polynomials with approximate coefficients, which is a highly unstable problem.

Most algorithms computing [[triangular decomposition]]s directly (that is, without precomputing a Gröbner Basis) share above drawbacks, but the most recent ones do not suffer from the one related to output size, as shown by the experimental results reported by Changbo Chen and M. Moreno-Maza.&lt;ref&gt;Changbo Chen and Marc Moreno-Maza. ''Algorithms for Computing Triangular Decomposition of Polynomial Systems''.In proc. ISSAC'2011, pages 83-90, ACM Press, 2011 and Journal of Symbolic Computation (to appear)&lt;/ref&gt; Actually, this observation is predicted by a theoretical argument (which does not give rise to a practical algorithm, though): For a given polynomial system &lt;math&gt;S&lt;/math&gt; whose solutions can be described by a single regular chain, there exists one regular chain representing  &lt;math&gt;S&lt;/math&gt; in a nearly optimal way in term of size.&lt;ref&gt;Xavier Dahan and Eric Schost. ''Sharp Estimates for Triangular Sets''. In proc. ISSAC'04, pages 103--110, ACM Press, 2004&lt;/ref&gt;

In order to address both drawbacks, one can take advantage of the rational univariate representation, which follows. Its output is a single regular chain whose coefficient size is also nearly optimal. However, if the set of solutions has several components of various multiplicities, an output of smaller size may be obtained by decomposing it first with a triangular decomposition algorithm.

=== Rational Univariate Representation ===
The ''rational univariate representation'' or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by  F. Rouillier &lt;ref&gt;
{{cite journal
| first=Fabrice
| last=Rouillier
| title=Solving Zero-Dimensional Systems Through the Rational Univariate Representation
|journal=Appl. Algebra Eng. Commun. Comput.
| number=9
| year=1999
}}&lt;/ref&gt; for remedying to the above drawbacks of the regular chain representation.

A RUR of a zero-dimensional system consists in a linear combination ''x''&lt;sub&gt;0&lt;/sub&gt; of the variables, called ''separating variable'', and a system of equations&lt;ref&gt;{{cite book
 | author = Saugata Basu
 | coauthors = Richard Pollack, Marie-Françoise Roy
 | year = 2006
 | title = Algorithms in real algebraic geometry, chapter 12.4
 | publisher = [[Springer Science+Business Media|Springer-Verlag]]
 | url = http://perso.univ-rennes1.fr/marie-francoise.roy/bpr-ed2-posted1.html
}}&lt;/ref&gt;
:&lt;math&gt;
\begin{cases}
h(x_0)=0\\
x_1=g_1(x_0)/g_0(x_0)\\
\cdots\\
x_n=g_n(x_0)/g_0(x_0)
\end{cases}
&lt;/math&gt;
where ''h'' is a univariate polynomial in ''x''&lt;sub&gt;0&lt;/sub&gt; of degree ''D'' and ''g''&lt;sub&gt;0&lt;/sub&gt;, ..., ''g''&lt;sub&gt;n&lt;/sub&gt; are univariate polynomials in ''x''&lt;sub&gt;0&lt;/sub&gt; of degree less than ''D''. 

Given a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.

* All but a finite number linear combinations of the variables are separating variables.
* When the separating variable is chosen, the RUR exists and is unique. In particular ''h'' and the ''g''&lt;sub&gt;''i''&lt;/sub&gt; are defined independently of any algorithm to compute them.
* The solutions of the system are in one to one correspondence with the roots of ''h'' and the [[Multiplicity (mathematics)|multiplicity]] of each root of ''h'' equals the multiplicity of the corresponding solution.
* The solutions of the system are obtained by substituting the roots of ''h'' in the other equations.
* If ''h'' does not have any multiple root then ''g''&lt;sub&gt;0&lt;/sub&gt; is the [[Techniques for differentiation|derivative]] of ''h''.

For example, for above system, every linear combination of the variable, except the multiples of ''x'', ''y'' and ''x''&amp;nbsp;+&amp;nbsp;''y'', is a separating variable. If one choose ''t'' = (''x''&amp;nbsp;&amp;minus;&amp;nbsp;''y'')/2 as separating variable, then the RUR is 
:&lt;math&gt;
\begin{cases}
t^3-t=0\\
x=\frac{t^2+2t-1}{3t^2-1}\\
y=\frac{t^2-2t-1}{3t^2-1}\\
\end{cases}
&lt;/math&gt;

The RUR is uniquely defined for a given separating element, independently of any algorithm and it preserves the information on the multiplicities of the roots. Basically, a triangular decomposition of a zero-dimensional system does not preserve the multiplicities and is not uniquely defined, but, among all triangular decompositions of a given zero-dimensional  system &lt;math&gt;S&lt;/math&gt;, the equiprojectable decomposition depends only on a coordinate choice&lt;ref&gt;. 
{{cite book
| title= Proceedings of ISAAC 2005
| year = 2005
| first1=Xavier | last1=Dahan
| first2=Marc  | last2=Moreno Maza
| first3=Eric | last3=Schost
| first4=Wenyuan  | last4=Wu
| first5=Yuzhen  | last5=Xie
| chapter=Lifting techniques for triangular decompositions
| pages=108-105
| publisher=ACM Press
}}&lt;/ref&gt; of &lt;math&gt;S&lt;/math&gt;. For this latter, as for the RUR, sharp bounds are available for the coefficients. Consequently, efficient algorithms, based on so-called modular methods, exist for computing the equiprojectable decomposition and the RUR.

These bounds can trivially been obtained for complete intersection systems for the RUR by simply deriving the u-resultant associated with the system, which gives a quite direct way to bound those of an equiprojectable decomposition which are more or less equivalent. 

On the computational point of view, there is one main difference between the equiprojectable decomposition and the RUR. The latter has the conceptual advantage of reducing the numeric computation of the solutions to computing the roots of a single univariate polynomial and substituting in some rational functions. One can easily show that the required computation time is then dominated by the isolation of the roots of the univariate polynomial and their refinement up to a sufficient precision.

Moreover, the RUR can trivially been decomposed to get a primary decomposition of the system and, in practice, to get much smaller coefficients than the non decomposed form, especially in the case of systems with high multiplicities. In short one can provide instantaneously a RUR of each primary component through a squarefree decomposition of the first polynomial.

On the other hand, one has to retain that triangular decomposition can be performed in positive dimension, which is not the case of the RUR.

==Algorithms for numerically solving==

===General solving algorithms===
The general numerical algorithms which are designed for any system of [[simultaneous equations]] work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow to find ''all'' solutions. Especially, when a general method does not find any solution, this is usually not an indication that there is no solution.

Nevertheless two methods deserve to be mentioned here.

*[[Newton's method]] may be used if the number of equations is equal to the number of variables. It does not allow to find all the solutions not to prove that there is no solution. But it is very fast when starting from a point which is close to a solution. Therefore it is a basic tool for Homotopy Continuation method described below.

*[[Optimization (mathematics)|Optimization]] is rarely used for solving polynomial systems, but it succeeded, around 1970, to show that a system of 81 quadratic equations in 56 variables is not inconsistent.&lt;ref&gt;Daniel Lazard, ''Thirty years of Polynomial System Solving, and now?'' J. Symb. Comput. '''44''' (2009)&lt;/ref&gt; With the other known methods this system remains beyond the possibilities of modern technology. This method consists simply in minimizing the sum of the squares of the equations. If zero is found as a local minimum, then it is attained at a solution. This method works for overdetermined systems, but outputs an empty information if all local minimums which are found are positive.

=== Homotopy continuation method ===

This is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades by J. Verschelde and his associates.&lt;ref name=&quot;Vers99&quot;&gt;{{cite journal
| first=Jan | last=Verschelde
| title=Algorithm 795: PHCpack: A general-purpose solver for polynomial systems by homotopy continuation
| journal=ACM Transactions on Mathematical Software
| volume=25
| year=1999
}}&lt;/ref&gt;

This method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore it is computed by, at least, four different methods and the best value, say ''N'', is kept.

In the second step, a system &lt;math&gt;g_1=0, \ldots, g_n=0&lt;/math&gt; of polynomial equations is generated which has exactly ''N'' solutions that are easy to compute. This new system has the same number ''n'' of variables and the same number ''n'' of equations and the same general structure as the system to solve, &lt;math&gt;f_1=0, \ldots, f_n=0&lt;/math&gt;.

Then a [[homotopy]] between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the system 

:&lt;math&gt;(1-t)g_1+t f_1=0, \ldots, (1-t)g_n+t f_n=0&lt;/math&gt;.

The homotopy continuation consists in deforming the parameter ''t'' from 0 to 1 and ''following'' the ''N'' solutions during this deformation. This gives the desired solutions for ''t'' = 1. ''Following'' means that, if &lt;math&gt;t_1&lt;t_2&lt;/math&gt;, the solutions for &lt;math&gt;t=t_2&lt;/math&gt; are deduced from the solutions for &lt;math&gt;t=t_1&lt;/math&gt; by Newton's method. The difficulty here is to well choose the value of &lt;math&gt;t_2-t_1&lt;/math&gt;: Too large, Newton's convergence may be slow and may even jump from a solution path to another one. Too small, and the number of steps slows down the method.

===Numerically solving from the Rational Univariate Representation===

To deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.

The roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.

* [[Aberth method]], implemented in [[MPSolve]] computes all the complex roots to any precision.
* Uspensky's algorithm of Collins and Akritas,&lt;ref&gt;George E. Collins and Alkiviadis G. Akritas, ''Polynomial Real Root Isolation Using Descarte's Rule of Signs''. Proceedings of the 1976 ACM Symposium on Symbolic and Algebraic Computation&lt;/ref&gt; improved by Rouillier and Zimmermann &lt;ref&gt;F. Rouillier and P. Zimmerman,  ''Efficient isolation of polynomial's real roots''. Journal of Computational and Applied Mathematics '''162''' (2004)&lt;/ref&gt; and based on [[Descartes' rule of signs]]. This algorithms computes the real roots, isolated in intervals of arbitrary small width. It is implemented in [[Maple (software)|Maple]] (functions ''fsolve'' and ''RootFinding[Isolate]'').

==Software packages==
There are at least three software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.

The [[Maple (software)|Maple]] function ''RootFinding[Isolate]'' takes as input any polynomial system over the rational numbers (if some coefficients are [[floating point]] numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error. 

Internally, this solver, designed by F. Rouillier computes first a Gröbner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.

The rational univariate representation may be computed with [[Maple (software)|Maple]] function ''Groebner[RationalUnivariateRepresentation]''.

To extract all the complex solutions from a rational univariate representation, one may use [[MPSolve]], which computes the complex roots of univariate polynomials to any precision. It is recommended to run [[MPSolve]] several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.

The second solver is PHCpack,&lt;ref name=&quot;Vers99&quot; /&gt;&lt;ref&gt;[http://www.math.uic.edu/~jan/download.html Release 2.3.86 of PHCpack]&lt;/ref&gt; written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method.

This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.

The third solver is the [[Maple (software)|Maple]] command ''RegularChains[RealTriangularize]''. For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.  

The command ''RegularChains[RealTriangularize]''  is part of the [[Maple (software)|Maple]] library [[RegularChains]], written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command ''RegularChains[Triangularize]'' for computing the solutions whose coordinates are in the algebraic closure of the coefficient field.  The underlying algorithms are based on the notion of a [[regular chain]].

While the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm&lt;ref&gt;Changbo Chen, James H. Davenport, John P. May, Marc Moreno-Maza, Bican Xia, Rong Xiao. Triangular decomposition of semi-algebraic systems.  Proceedings of 2010 International Symposium on Symbolic and Algebraic Computation (ISSAC 2010), ACM Press, pp. 187--194, 2010.&lt;/ref&gt; is based on the concept of a [[regular semi-algebraic system]].

==See also==
*[[Triangular decomposition]]
*[[Wu's method of characteristic set]]

== References ==
{{Reflist}}
{{Refbegin}}
*{{cite book|last=Cox|first=David|coauthors=John Little, Donal O'Shea |title=Ideals, varieties, and algorithms : an introduction to computational algebraic geometry and commutative algebra|year=1997|publisher=Springer|location=New York|isbn=978-0387946801|edition=2nd}}
*{{cite book|last=Sturmfels|first=Bernd|title=Solving systems of polynomial equations|year=2002|publisher=American Mathematical Soc.|location=Providence, RI|isbn=0821832514}}
{{Refend}}

{{DEFAULTSORT:Systems Of Polynomial Equations}}
[[Category:Equations]]
[[Category:Algebra]]
[[Category:Computer algebra]]
[[Category:Polynomials]]
[[Category:Algebraic geometry]]</text>
      <sha1>3dl0ihvo2ryyx5m6z8wp7vzk2uhr2eg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Nested radical</title>
    <ns>0</ns>
    <id>2503693</id>
    <revision>
      <id>606329773</id>
      <parentid>606329332</parentid>
      <timestamp>2014-04-29T13:46:29Z</timestamp>
      <contributor>
        <ip>129.104.29.1</ip>
      </contributor>
      <text xml:space="preserve" bytes="7775">In [[algebra]], a '''nested radical''' is a [[radical expression]] that contains another radical expression. Examples include:

:&lt;math&gt;\sqrt{5-2\sqrt{5}\ }&lt;/math&gt;

which arises in discussing the [[pentagon|regular pentagon]];

:&lt;math&gt;\sqrt{5+2\sqrt{6}\ },&lt;/math&gt;

or more complicated ones such as:

:&lt;math&gt;\sqrt[3]{2+\sqrt{3}+\sqrt[3]{4}\ }.&lt;/math&gt;

== Denesting nested radicals ==

Some nested radicals can be rewritten in a form that is not nested. For example,

:&lt;math&gt;\sqrt{3+2\sqrt{2}} = 1+\sqrt{2}\,,&lt;/math&gt;

:&lt;math&gt;\sqrt[3]{\sqrt[3]{2} - 1} = \frac{1 - \sqrt[3]{2} + \sqrt[3]{4}}{\sqrt[3]{9}} \,.&lt;/math&gt;

Rewriting a nested radical in this way is called '''denesting'''. This process is generally considered a difficult problem, although a special class of nested radical can be denested by assuming it denests into a sum of two [[Nth root#Working with surds|surd]]s:

:&lt;math&gt;\sqrt{a+b \sqrt{c}\ } = \sqrt{d}+\sqrt{e}.&lt;/math&gt;

Squaring both sides of this equation yields:

:&lt;math&gt;a+b \sqrt{c} = d + e + 2 \sqrt{de}.&lt;/math&gt;

This can be solved by using the [[quadratic formula]] and setting rational and irrational parts on both sides of the equation equal to each other.  The solutions for ''e'' and ''d'' can be obtained by first equating the rational parts:

:&lt;math&gt;a = d + e,&lt;/math&gt;

which gives

:&lt;math&gt;d = a - e,&lt;/math&gt;
:&lt;math&gt;e = a - d.&lt;/math&gt;

For the irrational parts note that

:&lt;math&gt; b\sqrt{c} = 2\sqrt{de},&lt;/math&gt;

and squaring both sides yields

:&lt;math&gt; b^2 c = 4de.&lt;/math&gt;

By plugging in ''a'' −  ''e'' for ''d'' one obtains 

:&lt;math&gt; b^2 c = 4(a-e)e = 4ae - 4e^2.&lt;/math&gt;

Rearranging terms will give an quadratic equation which can be solved for ''e'':

:&lt;math&gt; 4e^2 - 4ae + b^2 c = 0, &lt;/math&gt;

:&lt;math&gt;e=\frac{a \pm \sqrt {a^2-b^2c}}{2}.&lt;/math&gt;

The solution ''d'' is the algebraic conjugate of ''e''.  If

:&lt;math&gt;e=\frac{a \pm \sqrt {a^2-b^2c}}{2},&lt;/math&gt;

then

:&lt;math&gt;d=\frac{a \mp \sqrt {a^2-b^2c}}{2}.&lt;/math&gt;

However, this approach works for nested radicals of the form 
&lt;math&gt;\sqrt{a+b \sqrt{c}\ }&lt;/math&gt;
if and only if
&lt;math&gt; \sqrt{a^2 - b^2c} &lt;/math&gt;
is an [[integer]], in which case the nested radical can be denested into a sum of surds.

In some cases, higher-power radicals may be needed to denest the nested radical.

=== Some identities of Ramanujan ===

[[Srinivasa Ramanujan]] demonstrated a number of curious identities involving denesting of radicals.  Among them are the following:&lt;ref&gt;&quot;A note on 'Zippel Denesting'&quot;, Susan Landau, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.35.5512&amp;rep=rep1&amp;type=pdf&lt;/ref&gt;

: &lt;math&gt; \sqrt[4]{\frac{3 + 2 \sqrt[4]{5}}{3 - 2 \sqrt[4]{5}}} = \frac{ \sqrt[4]{5} + 1}{\sqrt[4]{5} - 1}=\tfrac12\left(3+\sqrt[4]5+\sqrt5+\sqrt[4]{125}\right),&lt;/math&gt;

: &lt;math&gt; \sqrt{ \sqrt[3]{28} - \sqrt[3]{27}} = \tfrac13\left(\sqrt[3]{98} - \sqrt[3]{28} -1\right), &lt;/math&gt;

: &lt;math&gt; \sqrt[3]{ \sqrt[5]{\frac{32}{5}} - \sqrt[5]{\frac{27}{5}} } = \sqrt[5]{\frac{1}{25}} + \sqrt[5]{\frac{3}{25}} - \sqrt[5]{\frac{9}{25}}, &lt;/math&gt;

: &lt;math&gt;\sqrt[3]{\ \sqrt[3]{2}\ - 1}= \sqrt[3]{\frac{1}{9}} - \sqrt[3]{\frac{2}{9}} + \sqrt[3]{\frac{4}{9}}. &lt;/math&gt; &lt;ref&gt;&quot;RADICALS AND UNITS IN RAMANUJAN’S WORK&quot;, Susan Landau, http://www.math.uiuc.edu/~berndt/articles/radicals.ps&lt;/ref&gt;

Other odd-looking radicals inspired by Ramanujan:

: &lt;math&gt; \sqrt[4]{49 + 20\sqrt{6}} + \sqrt[4]{49 - 20\sqrt{6}} = 2\sqrt{3},&lt;/math&gt;

: &lt;math&gt;\sqrt[3]{\left(\sqrt{2}+ \sqrt{3}\right)\left(5 - \sqrt{6}\right) + 3\left(2\sqrt{3} + 3\sqrt{2}\right)} = \sqrt{10 - \frac{13 - 5\sqrt{6}}{5 + \sqrt{6}}}. &lt;/math&gt;

== Landau's algorithm ==

{{main|Landau's algorithm}}

In 1989 [[Susan Landau]] introduced the first [[algorithm]] for deciding which nested radicals can be denested.&lt;ref&gt;{{cite journal | id = {{citeseerx|10.1.1.34.2003}} | title = Simplification of Nested Radicals | authorlink = Susan Landau | first = Susan | last = Landau | publisher = [[Society for Industrial and Applied Mathematics|SIAM]] | journal = Journal of Computation | volume = 21 | year = 1992 | pages = 85–110 | doi = 10.1109/SFCS.1989.63496 }}&lt;/ref&gt;  Earlier algorithms worked in some cases but not others.

== Infinitely nested radicals ==
=== Square roots ===

Under certain conditions infinitely nested square roots such as

:&lt;math&gt; x = \sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\cdots}}}} &lt;/math&gt;

represent rational numbers.  This rational number can be found by realizing that ''x'' also appears under the radical sign, which gives the equation

:&lt;math&gt; x = \sqrt{2+x}. &lt;/math&gt;

If we solve this equation, we find that ''x'' = 2 (the second solution ''x'' =&amp;nbsp;&amp;minus;1 doesn't apply, under the convention that the positive square root is meant). This approach can also be used to show that generally, if ''n''&amp;nbsp;&gt;&amp;nbsp;0, then:

:&lt;math&gt; \sqrt{n+\sqrt{n+\sqrt{n+\sqrt{n+\cdots}}}} = \tfrac12\left(1 +
\sqrt {1+4n}\right). &lt;/math&gt;

The same procedure also works to get 

:&lt;math&gt; \sqrt{n-\sqrt{n-\sqrt{n-\sqrt{n-\cdots}}}} = \tfrac12\left(-1 + \sqrt {1+4n}\right). &lt;/math&gt;

This method will give a rational ''x'' value for all values of ''n'' such that

:&lt;math&gt; n = x^2 + x. \,&lt;/math&gt;

Ramanujan posed this problem to the 'Journal of Indian Mathematical Society':

: &lt;math&gt;? = \sqrt{1+2\sqrt{1+3 \sqrt{1+\cdots}}}. \, &lt;/math&gt;

This can be solved by noting a more general formulation:

: &lt;math&gt;? = \sqrt{ax+(n+a)^2 +x\sqrt{a(x+n)+(n+a)^2+(x+n) \sqrt{\mathrm{\cdots}}}} \, &lt;/math&gt;

Setting this to ''F''(''x'') and squaring both sides gives us:

: &lt;math&gt;F(x)^2 = ax+(n+a)^2 +x\sqrt{a(x+n)+(n+a)^2+(x+n) \sqrt{\mathrm{\cdots}}} \, &lt;/math&gt;

Which can be simplified to:

: &lt;math&gt;F(x)^2 = ax+(n+a)^2 +xF(x+n) \, &lt;/math&gt;

It can then be shown that:

: &lt;math&gt;F(x) = {x + n + a} \, &lt;/math&gt;

So, setting ''a''&amp;nbsp;=0, ''n''&amp;nbsp;=&amp;nbsp;1, and&amp;nbsp;''x''&amp;nbsp;=&amp;nbsp;2:

: &lt;math&gt;3 = \sqrt{1+2\sqrt{1+3 \sqrt{1+\cdots}}}. \, &lt;/math&gt;
Ramanujan stated this radical in his lost notebook
:&lt;math&gt;\sqrt{5+\sqrt{5+\sqrt{5-\sqrt{5+\sqrt{5+\sqrt{5+\sqrt{5-\cdots}}}}}}}=\frac{2+\sqrt{5}+\sqrt{15-6\sqrt{5}}}{2}&lt;/math&gt;

=== Cube roots ===

In certain cases, infinitely nested cube roots such as

:&lt;math&gt; x = \sqrt[3]{6+\sqrt[3]{6+\sqrt[3]{6+\sqrt[3]{6+\cdots}}}} &lt;/math&gt;

can represent rational numbers as well.  Again, by realizing that the whole expression appears inside itself, we are left with the equation

:&lt;math&gt; x = \sqrt[3]{6+x}. &lt;/math&gt;

If we solve this equation, we find that&amp;nbsp;''x''&amp;nbsp;=&amp;nbsp;2. More generally, we find that

:&lt;math&gt; \sqrt[3]{n+\sqrt[3]{n+\sqrt[3]{n+\sqrt[3]{n+\cdots}}}}&lt;/math&gt;

is the real root of the equation ''x''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''x''&amp;nbsp;&amp;minus;&amp;nbsp;''n''&amp;nbsp;=&amp;nbsp;0 for all&amp;nbsp;''n''&amp;nbsp;&gt;&amp;nbsp;0. For ''n'' = 1, this root is the [[plastic number]] ''ρ'', approximately equal to 1.3247.

The same procedure also works to get 

:&lt;math&gt; \sqrt[3]{n-\sqrt[3]{n-\sqrt[3]{n-\sqrt[3]{n-\cdots}}}} &lt;/math&gt;

as the real root of the equation ''x''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''x''&amp;nbsp;&amp;minus;&amp;nbsp;''n''&amp;nbsp;=&amp;nbsp;0 for all ''n'' and ''x'' where ''n''&amp;nbsp;&gt;&amp;nbsp;0 and&amp;nbsp;|''x''|&amp;nbsp;≥&amp;nbsp;1.

==See also==
*[[Sum of radicals]]
*[[Spiral of Theodorus]]
*[[Viète's formula]]

== References ==

{{reflist}}

===Further reading ===
* {{cite journal | authorlink = Susan Landau | first = Susan | last = Landau | title = How to Tangle with a Nested Radical | journal = [[Mathematical Intelligencer]] | volume = 16 | pages = 49–55 | year = 1994 | doi=10.1007/bf03024284}}
*
* [http://www.almaden.ibm.com/cs/people/fagin/symb85.pdf Decreasing the Nesting Depth of Expressions Involving Square Roots]
* [http://www.cybertester.com/data/denest.pdf Simplifying Square Roots of Square Roots]
* {{mathworld|urlname=SquareRoot|title=Square Root}}
* {{mathworld|urlname=NestedRadical|title=Nested Radical}}

[[Category:Algebra]]</text>
      <sha1>4rbq50fwqg0ilt7ur5rnb4c3ejhjt99</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Series expansion</title>
    <ns>0</ns>
    <id>1575813</id>
    <revision>
      <id>610448183</id>
      <parentid>591566423</parentid>
      <timestamp>2014-05-28T04:59:44Z</timestamp>
      <contributor>
        <username>Gorthian</username>
        <id>15179295</id>
      </contributor>
      <minor/>
      <comment>Disambiguated: [[superposition]] → [[superposition principle]]</comment>
      <text xml:space="preserve" bytes="1989">In [[mathematics]], a '''series expansion''' is a method for calculating a [[Function (mathematics)|function]] that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division).

The resulting so-called ''[[Series (mathematics)|series]]'' often can be limited to a finite number of terms, thus yielding an [[approximation]] of the function. The  fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the [[partial sum]] of the omitted terms) can be described by an equation involving [[Big O notation]] (see also [[asymptotic expansion]]).

There are several kinds of series expansions, such as:

* [[Taylor series]]: A [[power series]] based on a function’s [[derivative]]s at a single point.
* [[Maclaurin series]]: A special case of a Taylor series, centred at zero.
* [[Laurent series]]: An extension of the Taylor series, allowing negative exponent values.
* [[Dirichlet series]]: Used in [[number theory]].
* [[Fourier series]]: Describes periodical functions as a series of [[Trigonometric functions#Sine, cosine, and tangent|sine]] and [[Trigonometric functions#Sine, cosine, and tangent|cosine]] functions. In [[acoustics]], e.g., the [[Fundamental frequency|fundamental tone]] and the [[overtone]]s together form an example of a Fourier series.
* [[Table of Newtonian series|Newtonian series]]
* [[Legendre polynomials]]: Used in [[physics]] to describe an arbitrary electrical field as a [[superposition principle|superposition]] of a [[dipole]] field, a [[quadrupole]] field, an [[Multipole expansion|octupole]] field, etc.
* [[Zernike polynomials]]: Used in [[optics]] to calculate [[Optical aberration|aberration]]s of optical systems. Each term in the series describes a particular type of aberration.

For more details, refer to the articles mentioned.

[[Category:Algebra]]
[[Category:Polynomials]]
[[Category:Mathematical analysis]]
[[Category:Mathematical series]]</text>
      <sha1>5dyy07qae4uyz96xq0yzprcr9okvpy5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Shuffle algebra</title>
    <ns>0</ns>
    <id>32234404</id>
    <revision>
      <id>604615132</id>
      <parentid>603653775</parentid>
      <timestamp>2014-04-17T16:47:58Z</timestamp>
      <contributor>
        <username>Deltahedron</username>
        <id>16759019</id>
      </contributor>
      <comment>/* See also */ [[Zinbiel algebra]]</comment>
      <text xml:space="preserve" bytes="4753">In mathematics, a '''shuffle algebra''' is a [[Hopf algebra]] with a basis corresponding to words on some set, whose product is given by the '''shuffle product''' ''X''ш''Y'' of two words ''X'', ''Y'': the sum of all ways of interlacing them.

The shuffle algebra on a finite set is the graded dual of the [[universal enveloping algebra]] of the [[free Lie algebra]] on the set.

Over the rational numbers, the shuffle algebra is isomorphic to the polynomial algebra in the [[Lyndon word]]s.

==Shuffle product==
The shuffle product of words of lengths ''m'' and ''n'' is a sum over the (''m''+''n'')!/''m''!''n''! ways of interleaving the two words, as shown in the following examples:
:''ab'' ш ''xy'' = ''abxy'' + ''axby'' + ''xaby'' + ''axyb'' + ''xayb'' + ''xyab'' ;
:''aaa'' ш ''aa'' = 10''aaaaa'' .

It may be defined inductively by&lt;ref&gt;Lothaire (1997) pp.101,126&lt;/ref&gt;
:''ua'' ш ''vb'' = (''u'' ш ''vb'')''a'' + (''ua'' ш ''v'')''b'' .

The shuffle product was introduced by {{harvtxt|Eilenberg|Mac Lane|1953}}. The name &quot;shuffle product&quot; refers to the fact that the product can be thought of as a sum over all ways of [[riffle shuffle|riffle shuffling]] two words together.  The product is [[Commutative property|commutative]] and [[Associative property|associative]].&lt;ref name=L126&gt;Lothaire (1997) p.126&lt;/ref&gt;

The shuffle product of two words in some alphabet is often denoted by the '''shuffle product symbol''' ш (a [[cyrillic]] [[sha]], or the [[unicode]] character SHUFFLE PRODUCT (U+29E2)).

==Infiltration product==
The closely related '''infiltration product''' was introduced by {{harvtxt|Chen|Fox|Lyndon|1958}}.  It is defined inductively on words over an alphabet ''A'' by

:''fa'' ↑ ''ga'' = (''f''↑''ga'')''a'' + (''fa''↑''g'')''a'' + (''f''↑''g'')''a'' ;
:''fa'' ↑ ''gb'' = (''f''↑''gb'')''a'' + (''fa''↑''g'')''b'' .

For example:
:''ab'' ↑ ''ab'' = ''abb'' + 2''aab'' + 2''abb'' + 4 ''aabb'' + 2''abab'' ;
:''ab'' ↑ ''ba'' = ''aba'' + ''bab'' + ''abab'' + 2''abba'' + 2''baab'' + ''baba'' .

The infiltration product is also commutative and associative.&lt;ref name=L128&gt;Lothaire (1997) p.128&lt;/ref&gt;

==See also==
* [[Hopf algebra of permutations]]
* [[Zinbiel algebra]]

==References==
{{reflist}}
*{{Citation | last1=Chen | first1=Kuo-Tsai | last2=Fox | first2=Ralph H. | author2-link=Ralph Fox | last3=Lyndon | first3=Roger C. | author3-link=Roger Lyndon | title=Free differential calculus. IV. The quotient groups of the lower central series | jstor=1970044 | mr=0102539 | zbl=0142.22304  |year=1958 | journal=[[Annals of Mathematics|Annals of Mathematics. Second Series]] | issn=0003-486X | volume=68 | pages=81–95 | issue=1 | doi=10.2307/1970044}}
*{{Citation | last1=Eilenberg | first1=Samuel | author1-link=Samuel Eilenberg | last2=Mac Lane | first2=Saunders | author2-link=Saunders Mac Lane | title=On the groups of H(Π,n). I | jstor=1969820 | mr=0056295 | zbl=0050.39304  | year=1953 | journal=[[Annals of Mathematics|Annals of Mathematics. Second Series]] | issn=0003-486X | volume=58 | pages=55–106}}
*{{Citation | last1=Green | first1=J. A. | title=Shuffle algebras, Lie algebras and quantum groups | url=http://books.google.com/books?id=jxvvAAAAMAAJ | publisher=Universidade de Coimbra Departamento de Matemática | location=Coimbra | series=Textos de Matemática. Série B  | mr=1399082 | year=1995 | volume=9}}
*{{eom|id=S/s110110|first=M. |last=Hazewinkel}}
*{{citation | mr=2724822 | zbl=1211.16023
|last=Hazewinkel|first= Michiel|last2= Gubareni|first2= Nadiya|last3= Kirichenko|first3= V. V.
|title=Algebras, rings and modules. 
Lie algebras and Hopf algebras|series= Mathematical Surveys and Monographs|volume= 168|publisher= American Mathematical Society|place= Providence, RI|year= 2010|ISBN= 978-0-8218-5262-0 }}
*{{Citation | last=Lothaire | first=M. | authorlink=M. Lothaire | others=Perrin, D.; Reutenauer, C.; Berstel, J.; Pin, J. E.; Pirillo, G.; Foata, D.; Sakarovitch, J.; Simon, I.; Schützenberger, M. P.; Choffrut, C.; Cori, R.; Lyndon, Roger; Rota, Gian-Carlo. Foreword by Roger Lyndon | title=Combinatorics on words | edition=2nd | series=Encyclopedia of Mathematics and Its Applications | volume=17 | publisher=[[Cambridge University Press]] | year=1997 | isbn=0-521-59924-5 | zbl=0874.20040 }}
*{{Citation | last1=Reutenauer | first1=Christophe | title=Free Lie algebras | url=http://books.google.com/books?id=cBvvAAAAMAAJ | publisher=The Clarendon Press Oxford University Press | series=London Mathematical Society Monographs. New Series | isbn=978-0-19-853679-6 | mr=1231799 | zbl=0798.17001 | year=1993 | volume=7}}

==External links==
* [http://mirror.ctan.org/fonts/shuffle/shuffle.pdf Shuffle product symbol]

[[Category:Combinatorics]]
[[Category:Algebra]]</text>
      <sha1>ct1mt7h56l0m672rb5h12j92zb5tkdl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Comeasuring</title>
    <ns>0</ns>
    <id>32291149</id>
    <revision>
      <id>523536585</id>
      <parentid>491841309</parentid>
      <timestamp>2012-11-17T19:06:26Z</timestamp>
      <contributor>
        <username>DoctorKubla</username>
        <id>16383496</id>
      </contributor>
      <comment>rm deprecated wikify tag</comment>
      <text xml:space="preserve" bytes="636">{{Multiple issues|context = September 2011|orphan = September 2011|
{{Expert-subject|mathematics|date=September 2011}}
}}

Let ''A'' be an [[Algebra over a field|algebra]]. A '''comeasuring''' of ''A'' is a pair (''B'',&amp;nbsp;''β'') where:
# ''B'' is an algebra.
# ''β: A → A ⊗ B'' is an [[algebra map]].

== References ==
* {{Citation| last=Majid| first=Shahn| authorlink=Shahn Majid | year=2002| title=A quantum groups primer | url=http://books.google.com/books?id=o9D8S-vZdbUC| series=London Mathematical Society Lecture Note Series | publisher=Cambridge Univ. Press| isbn = 978-0-521-01041-2|edition=1.}}.

[[Category:Algebra]]</text>
      <sha1>6k8288dyg958qxa468numfq39lxcbpl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Componendo and dividendo</title>
    <ns>0</ns>
    <id>15150150</id>
    <revision>
      <id>603887314</id>
      <parentid>603887172</parentid>
      <timestamp>2014-04-12T15:23:06Z</timestamp>
      <contributor>
        <ip>120.61.86.188</ip>
      </contributor>
      <text xml:space="preserve" bytes="2317">

In [[algebra]], '''componendo and dividendo''' (or '''componendo et dividendo''') is a method of [[simplification]] based on fractions provided that they are in proportion.  It states that&lt;ref&gt;Bhamra, ''Partial Differential Equations''. PHI Learning Pvt. Ltd.  ISBN 978-81-203-3917-0
&lt;/ref&gt;
&lt;ref&gt;
http://www.qc.edu.hk/math/Junior%20Secondary/Componendo%20et%20Dividendo.htm
&lt;/ref&gt;

:&lt;math&gt; \text{If } \frac{a}{b} = \frac{c}{d} \text{ and } a \neq b \text{, then } .&lt;/math&gt;

1. b/a = d/c

2. a/c = b/d

3. (a+b)/b = (c+d)/d

4. (a-b)/b = (c-d)/d

5.  (a+b)/(a-b) = (c+d)/(c-d)

Relationship (1) above is called INVERTENDO

Relationship (2) above is called ALTERNENDO

Relationship (3) above is called COMPONENDO

Relationship (4) above is called DIVIDENDO

Relationship (5) above is called COMPONENDO-DIVIDENDO

==Comment on the proof==
We can similarly deduce the much more general fact that the value of any fraction

:&lt;math&gt;\frac{x_0 + \cdots + x_n}{y_0 + \cdots +y_n}&lt;/math&gt;

in which &lt;math&gt;x_0&lt;/math&gt; and &lt;math&gt;y_0&lt;/math&gt; are nonzero and can be expressed in terms of the values of

:&lt;math&gt;\frac{x_1}{x_0}, \ldots, \frac{x_n}{x_0}, \frac{y_1}{y_0}, \ldots, \frac{y_n}{y_0} &lt;/math&gt;

and the value of &lt;math&gt;\frac{x_0}{y_0}&lt;/math&gt;, and so depends only on the values of those 2''n''&amp;nbsp;+&amp;nbsp;1 fractions:

:&lt;math&gt; \frac{x_0 + \cdots + x_n}{y_0 + \cdots +y_n}
= \frac{x_0}{y_0} \left(\frac{1 + \frac{x_1}{x_0} + \cdots + \frac{x_n}{x_0}}{1 + \frac{y_1}{y_0} + \cdots + \frac{y_n}{y_0}}\right)&lt;/math&gt;

The original result is essentially a special case of this fact, because

:&lt;math&gt;\frac{x+y}{x-y} = \frac{x+y}{x+(-y)}&lt;/math&gt;

can be regarded as a fraction of the above form.

==Example==
This method can be used in various situations.

For instance :

:&lt;math&gt;\frac{\sqrt{3} + x}{\sqrt{3} - x} = 2&lt;/math&gt;

Find the value of ''x''.

Solution :

Applying C and D

: &lt;math&gt;\frac{(\sqrt{3} + x) + (\sqrt{3} - x)}{(\sqrt{3} + x) - (\sqrt{3} - x)} = \frac{2 + 1}{2 - 1}&lt;/math&gt;
: &lt;math&gt;=&gt; \frac{2 \sqrt{3}}{2 x} = \frac{3}{1}&lt;/math&gt;
: &lt;math&gt;=&gt; \frac{\sqrt{3}}{x} = 3&lt;/math&gt;
: &lt;math&gt;=&gt; x = \frac{1}{\sqrt{3}}&lt;/math&gt;

==References==
&lt;references/&gt;

==See also==
* [[Reduction (mathematics)]]
* [[Fraction (mathematics)]]

{{DEFAULTSORT:Componendo And Dividendo}}
[[Category:Fractions]]
[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>g9fssp4y8tessp302nrub0h2h4ac6lz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Series multisection</title>
    <ns>0</ns>
    <id>32592462</id>
    <revision>
      <id>601670079</id>
      <parentid>577464395</parentid>
      <timestamp>2014-03-28T16:03:18Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>per [[WP:HYPHEN]], sub-subsection 3, points 3,4,5, replaced: equally- → equally using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="1812">In mathematics, a '''multisection''' of a power series is a new [[power series]] composed of equally spaced terms extracted unaltered from the original series. Formally, if one is given a power series

: &lt;math&gt;\sum_{n=-\infty}^\infty a_n\cdot x^n&lt;/math&gt;

then its multisection is a power series of the form

: &lt;math&gt;\sum_{m=-\infty}^\infty a_{cm+d}\cdot x^{cm+d}&lt;/math&gt;

where ''c'', ''d'' are integers, with 0 ≤ ''d'' &lt; ''c''.

== Multisection of analytic functions ==
A multisection of the series of an [[analytic function]]

: &lt;math&gt;F(x) = \sum_{n=-\infty}^\infty a_n\cdot x^n&lt;/math&gt;

has a [[closed-form expression]] in terms of the function &lt;math&gt;F(x)&lt;/math&gt;:

: &lt;math&gt;\sum_{m=-\infty}^\infty a_{cm+d}\cdot x^{cm+d} = \tfrac{1}{c}\cdot \sum_{k=0}^{c-1} w^{-kd}\cdot F(w^k\cdot x),&lt;/math&gt;

where &lt;math&gt;w = e^{\frac{2\pi i}{c}}&lt;/math&gt; is a [[Primitive nth root of unity|primitive ''c''-th root of unity]].

=== Example ===
Multisection of a binomial 

: &lt;math&gt;(1+x)^q = {q\choose 0} x^0 + {q\choose 1} x + {q\choose 2} x^2 + \cdots&lt;/math&gt;

at ''x'' = 1 gives the following identity for the sum of [[binomial coefficients]] with step ''c'':

: &lt;math&gt;{q\choose d} + {q\choose d+c} + {q\choose d+2c} + \cdots = \frac{1}{c}\cdot \sum_{k=0}^{c-1} \left(2 \cos\frac{\pi k}{c}\right )^q\cdot \cos \frac{\pi(q-2d)k}{c}.&lt;/math&gt;

== References == 
*{{MathWorld|urlname=SeriesMultisection|title=Series Multisection}}
*Somos, M. [http://cis.csuohio.edu/~somos/multiq.html A Multisection of q-Series], 2006.
*{{cite book |author=John Riordan |title=Combinatorial identities |authorlink=John Riordan (mathematician)|publisher=John Wiley and Sons |place=New York |year=1968}}

[[Category:Algebra]]
[[Category:Combinatorics]]
[[Category:Mathematical analysis]]
[[Category:Complex analysis]]
[[Category:Mathematical series]]</text>
      <sha1>1duzc0fgujpkt4i3wkktr44idneyb25</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Spherically complete field</title>
    <ns>0</ns>
    <id>33175921</id>
    <revision>
      <id>522127910</id>
      <parentid>509048339</parentid>
      <timestamp>2012-11-09T05:13:59Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <text xml:space="preserve" bytes="1640">In mathematics, a [[Field (mathematics)|field]] ''K'' with an [[Absolute_value#Fields|absolute value]] is called '''spherically complete''' if the [[Intersection (set theory)|intersection]] of every [[decreasing sequence]] of [[ball]]s (in the sense of the metric induced by the absolute value) is nonempty:
:&lt;math&gt;B_1\supseteq B_2\supseteq \cdots \Rightarrow\bigcap_{n\in {\mathbf N}} B_n\neq \empty.&lt;/math&gt;
The definition can be adapted also to a field ''K'' with a [[Valuation (algebra)|valuation]] ''v'' taking values in an arbitrary ordered abelian group: (''K'',''v'') is spherically complete if every collection of balls that is totally ordered by inclusion has a nonempty intersection.

Spherically complete fields are important in [[archimedean property|nonarchimedean]] [[functional analysis]], since many results analogous to theorems of classical functional analysis require the base field to be spherically complete.

==Examples==
*Any [[locally compact]] field is spherically complete. This includes, in particular, the fields '''Q'''&lt;sub&gt;''p''&lt;/sub&gt; of [[p-adic number]]s, and any of their finite extensions.
*On the other hand, '''C'''&lt;sub&gt;''p''&lt;/sub&gt;, the [[complete metric space|completion]] of the [[algebraic closure]] of '''Q'''&lt;sub&gt;''p''&lt;/sub&gt;, is not spherically complete.
*Any field of [[Hahn series]] is spherically complete.

==References==
{{cite book |title=Nonarchimedean Functional Analysis
            |last=Schneider
            |first=Peter
            |year=2001
            |publisher=Springer
            |isbn=3-540-42533-0}}

[[Category:Algebra]]
[[Category:Functional analysis]]

{{mathanalysis-stub}}</text>
      <sha1>lfmxxr8q2b0lrvdot1llbxzh0dqclj7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Linearly disjoint</title>
    <ns>0</ns>
    <id>3140923</id>
    <revision>
      <id>544194894</id>
      <parentid>517658100</parentid>
      <timestamp>2013-03-14T21:13:35Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q3062530]]</comment>
      <text xml:space="preserve" bytes="1503">In mathematics, [[algebra over a field|algebra]]s ''A'', ''B'' over a field ''k'' inside some field extension &lt;math&gt;\Omega&lt;/math&gt; of ''k'' (e.g., [[universal field]]) are said to be '''linearly disjoint over ''k''''' if the following equivalent conditions are met:
*(i) The map &lt;math&gt;A \otimes_k B \to AB&lt;/math&gt; induced by &lt;math&gt;(x, y) \mapsto xy&lt;/math&gt; is injective.
*(ii) Any ''k''-basis of ''A'' remains linearly independent over ''B''.
*(iii) If &lt;math&gt;u_i, v_j&lt;/math&gt; are ''k''-bases for ''A'', ''B'', then the products &lt;math&gt;u_i v_j&lt;/math&gt; are linearly independent over ''k''.

Note that, since every subalgebra of &lt;math&gt;\Omega&lt;/math&gt; is a domain, (i) implies &lt;math&gt;A \otimes_k B&lt;/math&gt; is a domain (in particular [[reduced ring|reduced]]).

One also has: ''A'', ''B'' are linearly disjoint over ''k'' if and only if subfields of &lt;math&gt;\Omega&lt;/math&gt; generated by &lt;math&gt;A, B&lt;/math&gt;, resp. are linearly disjoint over ''k''. (cf. [[tensor product of fields]])

Suppose ''A'', ''B'' are linearly disjoint over ''k''. If &lt;math&gt;A' \subset A&lt;/math&gt;, &lt;math&gt;B' \subset B&lt;/math&gt; are subalgebras, then &lt;math&gt;A'&lt;/math&gt; and &lt;math&gt;B'&lt;/math&gt; are linearly disjoint over ''k''. Conversely, if any finitely generated subalgebras of algebras ''A'', ''B'' are linearly disjoint, then ''A'', ''B'' are linearly disjoint (since the condition involves only finite sets of elements.)

== See also ==
*[[Tensor product of fields]]

== References ==
* P.M. Cohn (2003). Basic algebra


{{algebra-stub}}

[[Category:Algebra]]</text>
      <sha1>pmppu0l7rjec7uxyj4zn9fmo60em00g</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Determinant</title>
    <ns>0</ns>
    <id>8468</id>
    <revision>
      <id>610721122</id>
      <parentid>610664677</parentid>
      <timestamp>2014-05-30T00:30:33Z</timestamp>
      <contributor>
        <username>Quondum</username>
        <id>12331483</id>
      </contributor>
      <comment>/* 3&amp;nbsp;×&amp;nbsp;3 matrices */ 3x3 → 3 × 3</comment>
      <text xml:space="preserve" bytes="67142">{{about|determinants in mathematics|determinants in [[epidemiology]]|Risk factor}}

{{merge|Matrix determinant lemma|discuss=Talk:THIS PAGE#Merger proposal|date=November 2013}}

In [[linear algebra]], the '''determinant''' is a value associated with a [[Square matrix#Square matrices|square matrix]]. It can be computed from the entries of the matrix by a specific arithmetic expression, while other ways to determine its value exist as well. The determinant provides important information about a matrix of [[coefficient]]s of a [[system of linear equations]], or about a matrix that corresponds to a [[linear transformation]] of a vector space.  In the first case the system has a unique solution exactly when the determinant is nonzero; when the determinant is zero there are either no solutions or many solutions. In the second case the transformation has an [[inverse operation]] exactly when the determinant is nonzero. A geometric interpretation can be given to the value of the determinant of a square matrix with [[real number|real]] entries: the [[absolute value]] of the determinant gives the [[scale factor]] by which area or volume (or a higher-dimensional analogue) is multiplied under the associated linear transformation, while its sign indicates whether the transformation preserves [[orientation (vector space)|orientation]]. Thus a 2&amp;nbsp;&amp;times;&amp;nbsp;2 matrix with determinant −2, when applied to a region of the plane with finite area, will transform that region into one with twice the area, while reversing its orientation.

Determinants occur throughout mathematics. The use of determinants in [[calculus]] includes the [[Jacobian matrix and determinant|Jacobian determinant]] in the [[substitution rule]] for [[integral]]s of functions of several variables. They are used to define the [[characteristic polynomial]] of a matrix that is an essential tool in [[eigenvalue]] problems in linear algebra. In some cases they are used just as a compact notation for expressions that would otherwise be unwieldy to write down.

The determinant of a matrix '''A''' is denoted det('''A'''), det&amp;nbsp;'''A''', or |'''A'''|.&lt;ref&gt;{{Citation |title=Linear Algebra: A Modern Introduction | first=David |last=Poole |publisher=Thomson Brooks/Cole |year=2006 |isbn=0-534-99845-3 |page=262}}&lt;/ref&gt; In the case where the matrix entries are written out in full, the determinant is denoted by surrounding the matrix entries by vertical bars instead of the brackets or parentheses of the matrix. For instance, the determinant of the matrix 
:&lt;math&gt; \begin{bmatrix}a&amp;b&amp;c\\d&amp;e&amp;f\\g&amp;h&amp;i\end{bmatrix}&lt;/math&gt; 
is written 
:&lt;math&gt;\begin{vmatrix} a &amp; b &amp; c\\d &amp; e &amp; f\\g &amp; h &amp; i \end{vmatrix}&lt;/math&gt; 
and has the value 
:&lt;math&gt;(aei+bfg+cdh)-(ceg+bdi+afh).\,&lt;/math&gt;

Although most often used for matrices whose entries are [[Real number|real]] or [[complex number]]s, the definition of the determinant only involves addition, subtraction and multiplication, and so it can be defined for square matrices with entries taken from any [[commutative ring]]. Thus for instance the determinant of a matrix with [[integer]] coefficients will be an integer, and the matrix has an inverse with integer coefficients if and only if this determinant is 1 or −1 (these being the only [[Unit (ring theory)|invertible]] elements of the integers). For square matrices with entries in a non-commutative ring, for instance the [[quaternion]]s, there is no unique definition for the determinant, and no definition that has all the usual properties of determinants over commutative rings.

== Definition ==
There are various ways to define the determinant of a [[square matrix]] ''A'', i.e. one with the same number of rows and columns.  Perhaps the most natural way is expressed in terms of the columns of the matrix.  If we write an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix in terms of its column vectors

: &lt;math&gt;A = \begin{bmatrix} a_1, &amp; a_2, &amp; \ldots, &amp; a_n \end{bmatrix}&lt;/math&gt;

where the &lt;math&gt;a_j&lt;/math&gt; are vectors of size ''n'', then the determinant of ''A'' is defined so that

:&lt;math&gt;
\begin{align}
&amp; \det \begin{bmatrix} a_1, &amp; \ldots, &amp; b a_j + c v, &amp; \ldots, a_n \end{bmatrix} = b \det(A) + c \det \begin{bmatrix} a_1, &amp; \ldots, &amp; v, &amp; \ldots, a_n \end{bmatrix} \\ 
&amp; \det \begin{bmatrix} a_1, &amp; \ldots, &amp; a_j, &amp; a_{j+1}, &amp; \ldots, a_n \end{bmatrix} = - \det \begin{bmatrix} a_1, &amp; \ldots, &amp; a_{j+1}, &amp; a_j, &amp; \ldots, a_n \end{bmatrix} \\ 
&amp; \det(I) = 1
\end{align}
&lt;/math&gt;

where ''b'' and ''c'' are scalars, ''v'' is any vector of size ''n'' and ''I'' is the [[identity matrix]] of size ''n''.  These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar.  These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.&lt;ref&gt;Serge Lang, ''Linear Algebra'', 2nd Edition, Addison-Wesley, 1971, pp 173, 191.&lt;/ref&gt;

Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has ''n'' terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a [[polynomial expression]] of the matrix entries. This expression grows rapidly with the size of the matrix (an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix contributes [[Factorial|''n''!]] terms), so it will first be given explicitly for the case of 2×2 matrices and 3&amp;nbsp;×&amp;nbsp;3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.

Assume ''A'' is a square matrix with ''n'' rows and ''n'' columns, so that it can be written as
:&lt;math&gt;
A = \begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; \dots &amp; a_{1,n} \\
a_{2,1} &amp; a_{2,2} &amp; \dots &amp; a_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n,1} &amp; a_{n,2} &amp; \dots &amp; a_{n,n} \end{bmatrix}.\,&lt;/math&gt;
The entries can be numbers or expressions (as happens when the determinant is used to define a [[characteristic polynomial]]); the definition of the determinant depends only on the fact that they can be added and multiplied together in a [[Commutativity|commutative]] manner.

The determinant of ''A'' is denoted as det(''A''), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:
:&lt;math&gt;\begin{vmatrix}  a_{1,1} &amp; a_{1,2} &amp; \dots &amp; a_{1,n} \\
a_{2,1} &amp; a_{2,2} &amp; \dots &amp; a_{2,n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n,1} &amp; a_{n,2} &amp; \dots &amp; a_{n,n} \end{vmatrix}.\,&lt;/math&gt;

===2&amp;nbsp;×&amp;nbsp;2 matrices===
[[Image:Area parallellogram as determinant.svg|thumb|right|The area of the parallelogram is the absolute value of the determinant of the matrix formed by the vectors representing the parallelogram's sides.]]
The determinant of a {{nowrap|2 × 2}} matrix  is defined by
:&lt;math&gt;\begin{vmatrix} a &amp; b\\c &amp; d \end{vmatrix}=ad - bc .&lt;/math&gt;

If the matrix entries are real numbers, the matrix ''A'' can be used to represent two [[linear map]]s:  one that maps the standard basis vectors to the rows of ''A'', and one that maps them to the columns of ''A''.  In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping.  The parallelogram defined by the rows of the above matrix is the one with vertices at {{nowrap|(0, 0)}}, {{nowrap|(''a'', ''b'')}}, {{nowrap|(''a'' + ''c'', ''b'' + ''d'')}}, and {{nowrap|(''c'', ''d'')}}, as shown in the accompanying diagram.  The absolute value of ''ad'' − ''bc'' is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by ''A''.  (The parallelogram formed by the columns of ''A'' is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)

The absolute value of the determinant together with the sign becomes the ''oriented area'' of the parallelogram.  The oriented area is the same as the usual [[area (geometry)|area]], except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the [[identity matrix]]).

Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by ''A''.  When the determinant is equal to one, the linear mapping defined by the matrix is [[2×2 real matrices#Equi-areal mapping|equi-areal]] and orientation-preserving.

The object known as the ''[[bivector]]'' is related to these ideas. In 2D, it can be interpreted as an ''oriented plane segment'' formed by imagining two vectors each with origin {{nowrap|(0, 0)}}, and coordinates {{nowrap|(''a'', ''b'')}} and {{nowrap|(''c'', ''d'')}}. The bivector magnitude (denoted {{nowrap|(''a'', ''b'') ∧ (''c'', ''d'')}}) is the ''signed area'', which is also the determinant {{nowrap|''ad'' − ''bc''}}.&lt;ref&gt;[http://www.youtube.com/watch?v=6XghF70fqkY WildLinAlg episode 4], Norman J Wildberger, Univ. of New South Wales, 2010, lecture via youtube&lt;/ref&gt;

===3&amp;nbsp;×&amp;nbsp;3 matrices===
[[Image:Determinant parallelepiped.svg|300px|right|thumb|The volume of this [[parallelepiped]] is the absolute value of the determinant of the matrix formed by the rows constructed from the vectors r1, r2, and r3.]]

The determinant of a 3&amp;nbsp;×&amp;nbsp;3 matrix is defined by

:&lt;math&gt;\begin{align}\begin{vmatrix}a&amp;b&amp;c\\d&amp;e&amp;f\\g&amp;h&amp;i\end{vmatrix} &amp; = a\begin{vmatrix}e&amp;f\\h&amp;i\end{vmatrix}-b\begin{vmatrix}d&amp;f\\g&amp;i\end{vmatrix}+c\begin{vmatrix}d&amp;e\\g&amp;h\end{vmatrix} \\
&amp; = a(ei-fh)-b(di-fg)+c(dh-eg) \\
&amp; = aei+bfg+cdh-ceg-bdi-afh.
\end{align} &lt;/math&gt;

The [[rule of Sarrus]] is a mnemonic for the 3&amp;nbsp;×&amp;nbsp;3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3&amp;nbsp;×&amp;nbsp;3 matrix does not carry over into higher dimensions.

===''n''&amp;nbsp;×&amp;nbsp;''n'' matrices===
The determinant of a matrix of arbitrary size can be defined by the [[Leibniz formula for determinants|Leibniz formula]] or the [[Laplace expansion|Laplace formula]].

The Leibniz formula for the determinant of an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix '''A''' is

:&lt;math&gt;\det(A) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{i,\sigma_i}.\ &lt;/math&gt;

Here the sum is computed over all [[permutation]]s σ of the set {{nowrap|{1, 2, ..., ''n''}.}} A permutation is a function that reorders this set of integers. The value in the ''i''th position after the reordering σ is denoted σ&lt;sub&gt;''i''&lt;/sub&gt;. For example, for ''n'' = 3, the original sequence 1, 2, 3 might be reordered to σ = [2, 3, 1], with σ&lt;sub&gt;1&lt;/sub&gt; = 2, σ&lt;sub&gt;2&lt;/sub&gt; = 3, and σ&lt;sub&gt;3&lt;/sub&gt; = 1.  The set of all such permutations (also known as the [[symmetric group]] on ''n'' elements) is denoted ''S''&lt;sub&gt;''n''&lt;/sub&gt;. For each permutation σ, sgn(σ) denotes the [[signature (permutation)|signature]] of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.

In any of the &lt;math&gt;n!&lt;/math&gt; summands, the term

:&lt;math&gt;\prod_{i=1}^n A_{i, \sigma_i}\ &lt;/math&gt;

is notation for the product of the entries at positions (''i'', σ&lt;sub&gt;''i''&lt;/sub&gt;), where ''i'' ranges from 1 to ''n'':

:&lt;math&gt;A_{1, \sigma_1} \cdot A_{2, \sigma_2} \cdots  A_{n, \sigma_n}.\ &lt;/math&gt;

For example, the determinant of a 3&amp;nbsp;×&amp;nbsp;3 matrix ''A'' (''n'' = 3) is

:&lt;math&gt;\begin{align}

\sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n A_{i,\sigma_i}

&amp;=\sgn([1,2,3]) \prod_{i=1}^n A_{i,[1,2,3]_i} + \sgn([1,3,2]) \prod_{i=1}^n A_{i,[1,3,2]_i} + \sgn([2,1,3]) \prod_{i=1}^n A_{i,[2,1,3]_i} \\ &amp;+ \sgn([2,3,1]) \prod_{i=1}^n A_{i,[2,3,1]_i} + \sgn([3,1,2]) \prod_{i=1}^n A_{i,[3,1,2]_i} + \sgn([3,2,1]) \prod_{i=1}^n A_{i,[3,2,1]_i}

\\

&amp;=\prod_{i=1}^n A_{i,[1,2,3]_i} - \prod_{i=1}^n A_{i,[1,3,2]_i} - \prod_{i=1}^n A_{i,[2,1,3]_i} + \prod_{i=1}^n A_{i,[2,3,1]_i} + \prod_{i=1}^n A_{i,[3,1,2]_i} - \prod_{i=1}^n A_{i,[3,2,1]_i}

\\

&amp;=A_{1,1}A_{2,2}A_{3,3}-A_{1,1}A_{2,3}A_{3,2}-A_{1,2}A_{2,1}A_{3,3}+A_{1,2}A_{2,3}A_{3,1} \\
&amp; \qquad +A_{1,3}A_{2,1}A_{3,2}-A_{1,3}A_{2,2}A_{3,1}.

\end{align}&lt;/math&gt;

==== Levi-Civita symbol ====

It is sometimes useful to extend the Leibniz formula to a summation in which not only permutations, but all sequences of ''n'' indices in the  range 1,...,''n'' occur, ensuring that the contribution of a sequence will be zero unless it denotes a permutation. Thus the totally antisymmetric [[Levi-Civita symbol]] &lt;math&gt;\varepsilon_{i_1,\cdots,i_n}&lt;/math&gt; extends the signature of a permutation, by setting &lt;math&gt;\varepsilon_{\sigma(1),\cdots,\sigma(n)}=\operatorname{sgn}(\sigma)&lt;/math&gt; for any permutation σ of ''n'', and &lt;math&gt;\varepsilon_{i_1,\cdots,i_n}=0&lt;/math&gt; when no permutation σ exists such that &lt;math&gt;\sigma(j)=i_j&lt;/math&gt; for &lt;math&gt;j=1,\ldots,n&lt;/math&gt; (or equivalently, whenever some pair of indices are equal). The determinant for an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix can then be expressed using an ''n''-fold summation as
::&lt;math&gt; \det A = \sum_{i_1,i_2,\ldots,i_n=1}^n \varepsilon_{i_1\cdots i_n}  a_{1,i_1} \cdots a_{n,i_n}.&lt;/math&gt;

== Properties of the determinant ==
The determinant has many properties. Some basic properties of determinants are:

#&lt;math&gt;\det(I_n) = 1&lt;/math&gt; where ''I''&lt;sub&gt;''n''&lt;/sub&gt; is the ''n''&amp;nbsp;×&amp;nbsp;''n'' [[identity matrix]].
#&lt;math&gt;\det(A^{\rm T}) = \det(A).&lt;/math&gt;
#&lt;math&gt;\det(A^{-1}) = \frac{1}{\det(A)}=\det(A)^{-1}.&lt;/math&gt;
#For square matrices ''A'' and ''B'' of equal size,
:::&lt;math&gt;\det(AB) = \det(A)\det(B).&lt;/math&gt;
#&lt;li value=&quot;5&quot;&gt;&lt;math&gt;\det(cA) = c^n\det(A)&lt;/math&gt; for an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix.
#If ''A'' is a [[triangular matrix]], i.e. ''a''&lt;sub&gt;''i'',''j''&lt;/sub&gt; = 0 whenever ''i'' &gt; ''j'' or, alternatively, whenever ''i'' &lt; ''j'', then its determinant equals the product of the diagonal entries:
:::&lt;math&gt;\det(A) =  a_{1,1} a_{2,2} \cdots a_{n,n} = \prod_{i=1}^n a_{i,i}.&lt;/math&gt;
This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.

A number of additional properties relate to the effects on the determinant of changing particular rows or columns:
#&lt;li value=&quot;7&quot;&gt;Viewing an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix as being composed of ''n'' columns, the determinant is an [[Multilinear map|''n''-linear function]]. This means that if one column of a matrix ''A'' is written as a sum ''v'' + ''w'' of two [[column vector]]s, and all other columns are left unchanged, then the determinant of ''A'' is the sum of the determinants of the matrices obtained from ''A'' by replacing the column by ''v'' and then by ''w'' (and a similar relation holds when writing a column as a scalar multiple of a column vector).
#This ''n''-linear function is an [[alternating form]]. This means that whenever two columns of a matrix are identical, or more generally some column can be expressed as a linear combination of the other columns (i.e. the columns of the matrix form a [[Linearly independent|linearly dependent]] set), its determinant is 0.

Properties 1, 7 and 8 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from ''n''&amp;nbsp;×&amp;nbsp;''n'' matrices to scalars that is ''n''-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given [[commutative ring]]). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a [[standard basis]] vector. These determinants are either 0 (by property&amp;nbsp;8) or else ±1 (by properties 1 and&amp;nbsp;11 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 7 and 8 are incompatible for ''n'' ≥ 2,&lt;ref&gt;In a non-commutative setting left-linearity (compatibility with left-multiplication by scalars) should be distinguished from right-linearity. Assuming linearity in the columns is taken to be left-linearity, one would have, for non-commuting scalars ''a'', ''b'':
:&lt;math&gt;ab = ab \left|\begin{matrix}1&amp;0\\0&amp;1\end{matrix} \right| = a \left|\begin{matrix}1&amp;0\\0&amp;b\end{matrix} \right| = \left|\begin{matrix}a&amp;0\\0&amp;b\end{matrix} \right| = b \left|\begin{matrix}a&amp;0\\0&amp;1\end{matrix} \right| = ba \left|\begin{matrix}1&amp;0\\0&amp;1\end{matrix} \right|= ba,&lt;/math&gt;
a contradiction. There is no useful notion of multi-linear functions over a non-commutative ring.&lt;/ref&gt; so there is no good definition of the determinant in this setting.

Property 2 above implies that properties for columns have their counterparts in terms of rows:
#&lt;li value=&quot;9&quot;&gt;Viewing an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix as being composed of ''n'' rows, the determinant is an ''n''-linear function.
#This ''n''-linear function is an alternating form: whenever two rows of a matrix are identical, its determinant is 0.
#Interchanging two columns of a matrix multiplies its determinant by&amp;nbsp;−1. This follows from properties 7 and 8 (it is a general property of multilinear alternating maps). Iterating gives that more generally a permutation of the columns multiplies the determinant by the [[parity of a permutation|sign]] of the permutation. Similarly a permutation of the rows multiplies the determinant by the sign of the permutation.
#Adding a scalar multiple of one column to ''another'' column does not change the value of the determinant. This is a consequence of properties 7 and 8: by property&amp;nbsp;7 the determinant changes by a multiple of the determinant of a matrix with two equal columns, which determinant is 0 by property&amp;nbsp;8. Similarly, adding a scalar multiple of one row to another row leaves the determinant unchanged.
&lt;/li&gt;

These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a [[field (mathematics)|field]], properties 11 and 12 can be used to transform any matrix into a triangular matrix, whose determinant is given by property&amp;nbsp;6; this is essentially the method of [[Gaussian elimination]].

For example, the determinant of

:&lt;math&gt;A = \begin{bmatrix}-2&amp;2&amp;-3\\
-1&amp; 1&amp; 3\\
2 &amp;0 &amp;-1\end{bmatrix} &lt;/math&gt;

can be computed using the following matrices:

:&lt;math&gt;B = \begin{bmatrix}-2&amp;2&amp;-3\\
0 &amp; 0 &amp; 4.5\\
2 &amp;0 &amp;-1\end{bmatrix},
\quad
C = \begin{bmatrix}-2&amp;2&amp;-3\\
0 &amp; 0 &amp; 4.5\\
0 &amp; 2 &amp;-4\end{bmatrix},
\quad
D = \begin{bmatrix}-2&amp;2&amp;-3\\
0 &amp; 2 &amp;-4\\
0 &amp; 0 &amp; 4.5
\end{bmatrix}.
&lt;/math&gt;

Here, ''B'' is obtained from ''A'' by adding −1/2×the first row to the second, so that det(''A'') = det(''B''). ''C'' is obtained from ''B'' by adding the first to the third row, so that det(''C'') = det(''B''). Finally, ''D'' is obtained from ''C'' by exchanging the second and third row, so that det(''D'') = −det(''C''). The determinant of the (upper) triangular matrix ''D'' is the product of its entries on the [[main diagonal]]: (−2) · 2 · 4.5 = −18. Therefore det(''A'') = −det(''D'') = +18.

===Multiplicativity and matrix groups===
The determinant of a [[matrix product]] of square matrices equals the product of their determinants:

:&lt;math&gt;\det(AB) = \det (A) \det (B).\ &lt;/math&gt;

Thus the determinant is a ''multiplicative map''. This property is a consequence of the characterization given above of the determinant as the unique ''n''-linear alternating function of the columns with value&amp;nbsp;1 on the identity matrix, since the function ''M''&lt;sub&gt;''n''&lt;/sub&gt;(''K'') → ''K'' that maps ''M'' ↦ det(''AM'') can easily be seen to be ''n''-linear and alternating in the columns of ''M'', and takes the value det(''A'') at the identity. The formula can be generalized  to (square) products of rectangular matrices, giving the [[Cauchy–Binet formula]], which also provides an independent proof of the multiplicative property.

The determinant det(''A'') of a matrix ''A'' is non-zero if and only if ''A'' is invertible or, yet another equivalent statement, if its [[rank (linear algebra)|rank]] equals the size of the matrix. If so, the determinant of the inverse matrix is given by
:&lt;math&gt;\det (A^{-1}) = \frac 1 {\det (A)}.&lt;/math&gt;

In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size ''n'') form a group known as the [[special linear group]]. More generally, the word &quot;special&quot; indicates the subgroup of another [[matrix group]] of matrices of determinant one. Examples include the [[special orthogonal group]] (which if ''n'' is 2 or 3 consists of all [[rotation matrix|rotation matrices]]), and the [[special unitary group]].

===Laplace's formula and the adjugate matrix===
[[Laplace expansion|Laplace's formula]] expresses the determinant of a matrix in terms of its [[minor (matrix)|minors]]. The minor ''M''&lt;sub&gt;''i'',''j''&lt;/sub&gt; is defined to be the determinant of the (''n''−1)&amp;nbsp;×&amp;nbsp;(''n''−1)-matrix that results from ''A'' by removing the ''i''th row and the ''j''th column. The expression (−1)&lt;sup&gt;''i''+''j''&lt;/sup&gt;''M''&lt;sub&gt;''i'',''j''&lt;/sub&gt; is known as [[cofactor (linear algebra)|cofactor]]. The determinant of ''A'' is given by

:&lt;math&gt;\det(A) = \sum_{j=1}^n (-1)^{i+j} a_{i,j} M_{i,j} = \sum_{i=1}^n (-1)^{i+j} a_{i,j} M_{i,j}.&lt;/math&gt;

Calculating det(''A'') by means of that formula is referred to as expanding the determinant along a row or column. For the example 3&amp;nbsp;×&amp;nbsp;3 matrix 
:&lt;math&gt;A = \begin{bmatrix}-2&amp;2&amp;-3\\
-1&amp; 1&amp; 3\\
2 &amp;0 &amp;-1\end{bmatrix} \,,&lt;/math&gt;
Laplace expansion along the second column (''j'' = 2, the sum runs over ''i'') yields:
{| border=&quot;0&quot;
|-
|&lt;math&gt;\det(A)\,&lt;/math&gt;
|&lt;math&gt;=\,&lt;/math&gt;
|&lt;math&gt;(-1)^{1+2}\cdot 2 \cdot \det \begin{bmatrix}-1&amp;3\\ 2 &amp;-1\end{bmatrix} + (-1)^{2+2}\cdot 1 \cdot \det \begin{bmatrix}-2&amp;-3\\ 2&amp;-1\end{bmatrix} + (-1)^{3+2}\cdot 0 \cdot \det \begin{bmatrix}-2&amp;-3\\ -1&amp;3\end{bmatrix} &lt;/math&gt;
|-
|
|&lt;math&gt;=\,&lt;/math&gt;
|&lt;math&gt;(-2)\cdot((-1)\cdot(-1)-2\cdot3)+1\cdot((-2)\cdot(-1)-2\cdot(-3))&lt;/math&gt;
|-
|
|&lt;math&gt;=\,&lt;/math&gt;
|&lt;math&gt;(-2)\cdot(-5)+8 = 18.\,&lt;/math&gt;
|-
|
|}
However, Laplace expansion is efficient for small matrices only.

The [[adjugate matrix]] adj(''A'') is the transpose of the matrix consisting of the cofactors, i.e.,
:&lt;math&gt;(\operatorname{adj}(A))_{i,j} = (-1)^{i+j} M_{j,i}.\, &lt;/math&gt;

===Sylvester's determinant theorem===
[[Sylvester's determinant theorem]] states that for ''A'', an ''m''&amp;nbsp;×&amp;nbsp;''n'' matrix, and ''B'', an ''n''&amp;nbsp;×&amp;nbsp;''m'' matrix (so that ''A'' and ''B'' have dimensions allowing them to be multiplied in either order):

:&lt;math&gt;\det(I_\mathit{m} + AB) = \det (I_\mathit{n} + BA)&lt;/math&gt;,

where ''I''&lt;sub&gt;''m''&lt;/sub&gt; and ''I''&lt;sub&gt;''n''&lt;/sub&gt; are the ''m''&amp;nbsp;×&amp;nbsp;''m'' and ''n''&amp;nbsp;×&amp;nbsp;''n'' identity matrices, respectively.

From this general result several consequences follow.

:(a) For the case of column vector ''c'' and row vector ''r'', each with ''m'' components, the formula allows quick calculation of the determinant of a matrix that differs from the identity matrix by a matrix of rank 1:

::&lt;math&gt;\det(I_\mathit{m} + cr) = 1 + rc&lt;/math&gt;.

:(b) More generally,&lt;ref&gt;Proofs can be found in http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/proof003.html&lt;/ref&gt; for any invertible ''m''&amp;nbsp;×&amp;nbsp;''m'' matrix ''X'',

::&lt;math&gt;\det(X + AB) = \det(X) \det(I_\mathit{n} + BX^{-1}A)&lt;/math&gt;,

:(c) For a column and row vector as above, &lt;math&gt;\det(X + cr) = \det(X) (1 + rX^{-1}c)&lt;/math&gt;.

== Properties of the determinant in relation to other notions ==

===Relation to eigenvalues and trace===
{{Main|Eigenvalues and eigenvectors}}

Determinants can be used to find the [[eigenvalue]]s of the matrix ''A'': they are the solutions of the [[characteristic polynomial|characteristic equation]]
:&lt;math&gt;\det(A - xI) = 0, \,&lt;/math&gt;

where ''I'' is the [[identity matrix]] of the same dimension as ''A''. Conversely, det(''A'') is the product of the [[eigenvectors|eigenvalues]] of ''A'', counted with their [[algebraic multiplicity|algebraic multiplicities]]. The product of all non-zero eigenvalues is referred to as [[pseudo-determinant]].

An [[Hermitian matrix]] is [[positive definite matrix|positive definite]] if all its eigenvalues are positive. [[Sylvester's criterion]] asserts that this is equivalent to the determinants of the submatrices
:&lt;math&gt;A_k := \begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; \dots &amp; a_{1,k} \\
a_{2,1} &amp; a_{2,2} &amp; \dots &amp; a_{2,k} \\
\vdots &amp;  \vdots &amp; \ddots &amp; \vdots \\
a_{k,1} &amp; a_{k,2} &amp; \dots &amp; a_{k,k} \end{bmatrix} &lt;/math&gt;
being positive, for all ''k'' between 1 and ''n''.

The [[Trace (linear algebra)|trace]] tr(''A'') is by definition the sum of the diagonal entries of ''A'' and also equals the sum of the eigenvalues. Thus, for complex matrices ''A'',
:&lt;math&gt;\det(\exp(A)) = \exp(\mathrm{tr}(A))\, &lt;/math&gt;
or, for real matrices ''A'',
:&lt;math&gt;\mathrm{tr}(A) = \log(\det(\exp(A))). \,&lt;/math&gt;
Here exp(''A'') denotes the [[matrix exponential]] of ''A'', because every eigenvalue λ of ''A'' corresponds to the eigenvalue exp(λ) of exp(''A''). In particular, given any [[matrix logarithm|logarithm]] of ''A'', that is, any matrix ''L'' satisfying
:&lt;math&gt;\exp(L) = A\,&lt;/math&gt;
the determinant of ''A'' is given by
:&lt;math&gt;\det(A) = \exp(\mathrm{tr}(L)). \,&lt;/math&gt;

For example, for ''n'' = 2, ''n'' = 3,  and ''n'' = 4, respectively,
:&lt;math&gt;\det(A) = \bigl( (\mathrm{tr}A)^2 - \mathrm{tr}(A^2)\bigr )/2, \, &lt;/math&gt;
:&lt;math&gt;\det(A) = \Bigl((\mathrm{tr}A)^3 - 3 \mathrm{tr}A ~  \mathrm{tr}(A^2) + 2 \mathrm{tr}(A^3)\Bigr)/6, \,&lt;/math&gt;
:&lt;math&gt;\det(A)= \Bigl( (\mathrm{tr}A)^4 - 6   \mathrm{tr}(A^2)(\mbox{tr}A)^2+3(\mbox{tr}(A^2))^2     +8\mbox{tr}(A^3)~\mbox{tr}A -6\mbox{tr}(A^4)\Bigr)/24~.&lt;/math&gt;
cf. [[Cayley-Hamilton theorem#Illustration for specific dimensions and practical applications|Cayley-Hamilton theorem]]. Such expressions are deducible from  [[Newton's identities#Computing coefficients|Newton's identities]].

In the general case,&lt;ref&gt;A proof can be found in the Appendix B of L. A. Kondratyuk, M. I. Krivoruchenko (1992), ''Zeitschrift für Physik A'' '''344''', 99-115. {{doi|10.1007/BF01291027}}&lt;/ref&gt;
:&lt;math&gt;
\det (A)  = \sum_{k_{1},k_{2},\ldots,k_{n}}\prod_{l=1}^{n} \frac{(-1)^{k_{l}+1}}{l^{k_{l}}k_{l}!} \mathrm{tr}(A^{l})^{k_{l}},
&lt;/math&gt;
where the sum is taken over the set of all integers ''k&lt;sub&gt;l&lt;/sub&gt;'' ≥ 0 satisfying the equation
:&lt;math&gt;
\sum_{l=1}^{n}lk_{l} = n.
&lt;/math&gt;
This formula can also be used to find the determinant of a matrix ''A&lt;sup&gt;I&lt;/sup&gt;&lt;sub&gt;J&lt;/sub&gt;'' with multidimensional indices ''I'' = (i&lt;sub&gt;1&lt;/sub&gt;,i&lt;sub&gt;2&lt;/sub&gt;,...,i&lt;sub&gt;r&lt;/sub&gt;) and ''J'' = (j&lt;sub&gt;1&lt;/sub&gt;,j&lt;sub&gt;2&lt;/sub&gt;,...,j&lt;sub&gt;r&lt;/sub&gt;). The product and trace of such matrices are defined in a natural way as 
:&lt;math&gt; (AB)^I_J = \sum_{K}A^I_K B^K_J,    \mathrm{tr}(A)    = \sum_{I}A^I_I. &lt;/math&gt;

An arbitrary  dimension ''n''  identity can be obtained from the  [[Mercator series]] expansion of the logarithm,
:&lt;math&gt;\begin{align}
\det(I + A) = \sum_{k=0}^{\infty} \frac{1}{k!} \left( - \sum_{j=1}^{\infty} \frac{(-1)^j}{j}\mathrm{tr}(A^j) \right) ^k\, ,
\end{align}
&lt;/math&gt;

where ''I'' is the identity matrix. The sum and the expansion of the exponential  only need to go up to ''n'' instead of ∞, since the determinant cannot exceed ''O(A&lt;sup&gt;n&lt;/sup&gt;)''.

===Cramer's rule===
For a matrix equation
:&lt;math&gt; Ax = b\,&lt;/math&gt;

the solution is given by [[Cramer's rule]]:
:&lt;math&gt; x_i = \frac{\det(A_i)}{\det(A)} \qquad i = 1, \ldots, n \, &lt;/math&gt;
where ''A''&lt;sub&gt;''i''&lt;/sub&gt; is the matrix formed by replacing the ''i''th column of ''A'' by the column vector ''b''. This follows immediately by column expansion of the determinant, i.e.
:&lt;math&gt; \det(A_i) = \det\begin{bmatrix}a_1, &amp; \ldots, &amp; b, &amp; \ldots, &amp; a_n\end{bmatrix} = \sum_{j=1}^n x_j\det\begin{bmatrix}a_1, &amp; \ldots, a_{i-1}, &amp; a_j, &amp; a_{i+1}, &amp; \ldots, &amp; a_n \end{bmatrix} = x_i \det(A)&lt;/math&gt;
where the vectors &lt;math&gt;a_j&lt;/math&gt; are the columns of ''A''.  The rule is also implied by the identity

:&lt;math&gt;A\, \mathrm{adj}(A) = \mathrm{adj}(A)\, A = \det(A)\, I_n.&lt;/math&gt;

It has recently been shown that Cramer's rule can be implemented in O(''n''&lt;sup&gt;3&lt;/sup&gt;) time,&lt;ref&gt;Ken Habgood, Itamar Arel, ''A condensation-based application of Cramerʼs rule for solving large-scale linear systems'', Journal of Discrete Algorithms, 10 (2012), pp. 98–109.  Available online 1 July 2011, ISSN 1570–8667, 10.1016/j.jda.2011.06.007.
&lt;/ref&gt; which is comparable to more common methods of solving systems of linear equations, such as [[LU decomposition|LU]], [[QR decomposition|QR]], or [[singular value decomposition]].

===Block matrices===
Suppose ''A'', ''B'', ''C'', and ''D'' are matrices of dimension (''n''&amp;nbsp;×&amp;nbsp;''n''), (''n''&amp;nbsp;×&amp;nbsp;''m''), (''m''&amp;nbsp;×&amp;nbsp;''n''), and (''m''&amp;nbsp;×&amp;nbsp;''m''), respectively. Then

:&lt;math&gt;\det\begin{pmatrix}A&amp; 0\\ C&amp; D\end{pmatrix} = \det\begin{pmatrix}A&amp; B\\ 0&amp; D\end{pmatrix} = \det(A) \det(D) .&lt;/math&gt;

This can be seen from the [[Leibniz formula for determinants|Leibniz formula]] or by induction on ''n''. When ''A'' is [[Invertible matrix|invertible]], employing the following identity

:&lt;math&gt;\begin{pmatrix}A&amp; B\\ C&amp; D\end{pmatrix} = \begin{pmatrix}A&amp; 0\\ C&amp; I\end{pmatrix} \begin{pmatrix}I&amp; A^{-1} B\\ 0&amp; D - C A^{-1} B\end{pmatrix}&lt;/math&gt;

leads to

:&lt;math&gt;\det\begin{pmatrix}A&amp; B\\ C&amp; D\end{pmatrix} = \det(A) \det(D - C A^{-1} B) .&lt;/math&gt;

When ''D'' is invertible, a similar identity with &lt;math&gt;\det(D)&lt;/math&gt; factored out can be derived analogously,&lt;ref&gt;These identities were taken from http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/proof003.html&lt;/ref&gt; that is,

:&lt;math&gt;\det\begin{pmatrix}A&amp; B\\ C&amp; D\end{pmatrix} = \det(D) \det(A - B D^{-1} C) .&lt;/math&gt;

When the blocks are square matrices of the same order further formulas hold. For example, if ''C'' and ''D'' commute (i.e., ''CD'' = ''DC''), then the following formula comparable to the determinant of a 2×2 matrix holds:&lt;ref&gt;Proofs are given in J.R. Silvester, Determinants of Block Matrices, Math. Gazette, 84 (2000), pp. 460–467, available at  http://www.jstor.org/stable/3620776&lt;/ref&gt;

:&lt;math&gt;\det\begin{pmatrix}A&amp; B\\ C&amp; D\end{pmatrix} = \det(AD - BC).&lt;/math&gt;

When ''A'' = ''D'' and ''B'' = ''C'', the blocks are square matrices of the same order and the following formula holds (even if ''A'' and ''B'' do not commute)
:&lt;math&gt;\det\begin{pmatrix}A&amp; B\\ B&amp; A\end{pmatrix} = \det(A-B) \det(A+B).&lt;/math&gt;

When ''D'' is a 1×1 matrix, ''B'' is a column vector, and ''C'' is a row vector then
:&lt;math&gt;\det\begin{pmatrix}A&amp; B\\ C&amp; D\end{pmatrix} = (D-1)\det(A) + \det(A-BC) = (D+1)\det{A} - \det(A+BC)\,.&lt;/math&gt;

===Derivative===
By definition, e.g., using the [[Leibniz formula for determinants|Leibniz formula]], the determinant of real (or analogously for complex) square matrices is a [[polynomial|polynomial function]] from '''R'''&lt;sup&gt;''n''&amp;nbsp;×&amp;nbsp;''n''&lt;/sup&gt; to '''R'''. As such it is everywhere [[derivative|differentiable]]. Its derivative can be expressed using [[Jacobi's formula]]:

:&lt;math&gt;\frac{\mathrm{d} \det(A)}{\mathrm{d} \alpha} = \operatorname{tr}\left(\operatorname{adj}(A) \frac{\mathrm{d} A}{\mathrm{d} \alpha}\right).&lt;/math&gt;

where adj(''A'') denotes the [[adjugate]] of ''A''. In particular, if ''A'' is invertible, we have

:&lt;math&gt;\frac{\mathrm{d} \det(A)}{\mathrm{d} \alpha} =  \det(A) \operatorname{tr}\left(A^{-1} \frac{\mathrm{d} A}{\mathrm{d} \alpha}\right).&lt;/math&gt;

Expressed in terms of the entries of ''A'', these are

: &lt;math&gt; \frac{\partial \det(A)}{\partial A_{ij}}= \operatorname{adj}(A)_{ji}= \det(A)(A^{-1})_{ji}.&lt;/math&gt;

Yet another equivalent formulation is

:&lt;math&gt;\det(A + \epsilon X) - \det(A) = \operatorname{tr}(\operatorname{adj}(A) X) \epsilon + O(\epsilon^2) = \det(A) \operatorname{tr}(A^{-1} X) \epsilon + O(\epsilon^2)&lt;/math&gt;,

using [[big O notation]]. The special case where &lt;math&gt;A = I&lt;/math&gt;, the identity matrix, yields

:&lt;math&gt;\det(I + \epsilon X) = 1 + \operatorname{tr}(X) \epsilon + O(\epsilon^2).&lt;/math&gt;

This identity is used in describing the [[tangent space]] of certain matrix [[Lie groups]].

If the matrix A is written as &lt;math&gt;A = \begin{bmatrix}\mathbf{a} &amp; \mathbf{b} &amp; \mathbf{c}\end{bmatrix}&lt;/math&gt; where '''a''', '''b''', '''c''' are vectors, then the gradient over one of the three vectors may be written as the [[cross product]] of the other two:

: &lt;math&gt; \begin{align}

\nabla_\mathbf{a}\det(A) &amp;= \mathbf{b} \times \mathbf{c} \\

\nabla_\mathbf{b}\det(A) &amp;= \mathbf{c} \times \mathbf{a} \\

\nabla_\mathbf{c}\det(A) &amp;= \mathbf{a} \times \mathbf{b}.

\end{align} &lt;/math&gt;

== Abstract algebraic aspects {{anchor|Abstract formulation}}==

=== Determinant of an endomorphism ===
The above identities concerning the determinant of products and inverses of matrices imply that [[matrix similarity|similar matrices]] have the same determinant: two matrices ''A'' and ''B'' are similar, if there exists an invertible matrix ''X'' such that ''A'' = ''X''&lt;sup&gt;−1&lt;/sup&gt;''BX''. Indeed, repeatedly applying the above identities yields

:&lt;math&gt;\det(A) = \det(X)^{-1} \det(BX) = \det(X)^{-1} \det(B)\det(X) = \det(B) \det(X)^{-1} \det(X) = \det(B).\ &lt;/math&gt;

The determinant is therefore also called a [[similarity invariance|similarity invariant]]. The determinant of a [[linear transformation]]
:&lt;math&gt;T : V \rightarrow V\,&lt;/math&gt;
for some finite-dimensional [[vector space]] ''V'' is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of [[basis (linear algebra)|basis]] in ''V''. By the similarity invariance, this determinant is independent of the choice of the basis for ''V'' and therefore only depends on the endomorphism ''T''.

=== Transformation on alternating multilinear ''n''-forms ===
The vector space ''W'' of all alternating multilinear ''n''-forms on an ''n''-dimensional vector space ''V'' has dimension one.  To each linear transformation ''T'' on ''V'' we associate a linear transformation ''T''′ on ''W'', where for each ''w'' in ''W'' we define {{nowrap|1=(''T''′''w'')(''x''&lt;sub&gt;1&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''&lt;/sub&gt;) = ''w''(''Tx''&lt;sub&gt;1&lt;/sub&gt;,...,''Tx''&lt;sub&gt;''n''&lt;/sub&gt;)}}.  As a linear transformation on a one-dimensional space, ''T''′ is equivalent to a scalar multiple.  We call this scalar the determinant of ''T''.

=== Exterior algebra ===
The determinant can also be characterized as the unique function
:&lt;math&gt;D: M_n(K) \to K\, &lt;/math&gt;
from the set of all ''n''&amp;nbsp;×&amp;nbsp;''n'' matrices with entries in a field ''K'' to this field satisfying the following three properties: first, ''D'' is an [[Multilinear map|''n''-linear]] function: considering all but one column of ''A'' fixed, the determinant is linear in the remaining column, that is
:&lt;math&gt;D (v_1, \dots, v_{i-1}, a v_i + b w, v_{i+1}, \dots, v_n) = a D (v_1, \dots, v_{i-1}, v_i, v_{i+1}, \dots, v_n) + b D (v_1, \dots, v_{i-1}, w, v_{i+1}, \dots, v_n)\,&lt;/math&gt;
for any column vectors ''v''&lt;sub&gt;1&lt;/sub&gt;, ..., ''v''&lt;sub&gt;''n''&lt;/sub&gt;, and ''w'' and any scalars (elements of ''K'') ''a'' and ''b''. Second, ''D'' is an [[alternating form|alternating]] function: for any matrix ''A'' with two identical columns {{nowrap|''D''(''A'') {{=}} 0}}. Finally, ''D''(''I''&lt;sub&gt;''n''&lt;/sub&gt;) = 1. Here ''I''&lt;sub&gt;''n''&lt;/sub&gt; is the identity matrix.

This fact also implies that every other ''n''-linear alternating function {{nowrap|''F'': ''M''&lt;sub&gt;''n''&lt;/sub&gt;(''K'') → ''K''}} satisfies
:&lt;math&gt;F(M)=F(I)D(M).\ &lt;/math&gt;
The last part in fact follows from the preceding statement: one easily sees that if ''F'' is nonzero it satisfies ''F''(''I'') ≠ 0, and function that associates ''F''(''M'')/''F''(''I'') to ''M'' satisfies all conditions of the theorem. The importance of stating this part is mainly that it remains valid&lt;ref&gt;[[Roger Godement]], ''Cours d'Algèbre'', seconde édition, Hermann (1966), §23, Théorème 5, p.&amp;nbsp;303&lt;/ref&gt; if ''K'' is any [[commutative ring]] rather than a field, in which case the given argument does not apply.

The determinant of a linear transformation ''A'' : ''V'' → ''V'' of an ''n''-dimensional vector space ''V'' can be formulated in a coordinate-free manner by considering the ''n''th [[exterior algebra|exterior power]] Λ&lt;sup&gt;''n''&lt;/sup&gt;''V'' of ''V''. ''A'' induces a linear map
:&lt;math&gt;\Lambda^n A: \Lambda^n V \rightarrow \Lambda^n V&lt;/math&gt;
:&lt;math&gt;v_1 \wedge v_2 \wedge \dots \wedge v_n \mapsto A v_1 \wedge A v_2 \wedge \dots \wedge A v_n.&lt;/math&gt;

As Λ&lt;sup&gt;''n''&lt;/sup&gt;''V'' is one-dimensional, the map Λ&lt;sup&gt;''n''&lt;/sup&gt;A is given by multiplying with some scalar. This scalar coincides with the determinant of ''A'', that is to say
:&lt;math&gt;(\Lambda^n A)(v_1 \wedge \dots \wedge v_n) = \det(A) \cdot v_1 \wedge \dots \wedge v_n.&lt;/math&gt;
This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the parity of the determinant; likewise, permuting the vectors in the exterior product ''v''&lt;sub&gt;1&lt;/sub&gt; ∧ ''v''&lt;sub&gt;2&lt;/sub&gt;  ∧ ...  ∧ ''v''&lt;sub&gt;''n''&lt;/sub&gt; to ''v''&lt;sub&gt;2&lt;/sub&gt; ∧ ''v''&lt;sub&gt;1&lt;/sub&gt;  ∧ ''v''&lt;sub&gt;3&lt;/sub&gt; ∧ ...  ∧ ''v''&lt;sub&gt;''n''&lt;/sub&gt;, say, also alters the parity.

For this reason, the highest non-zero exterior power Λ&lt;sup&gt;''n''&lt;/sup&gt;(''V'') is sometimes also called the determinant of ''V'' and similarly for more involved objects such as [[vector bundle]]s or [[chain complex]]es of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms Λ&lt;sup&gt;''k''&lt;/sup&gt;''V'' with ''k'' &lt; n.

=== Square matrices over commutative rings and abstract properties ===
The determinant of a matrix can be defined, for example using the Leibniz formula, for matrices with entries in any [[commutative ring]]. Briefly, a ring is a structure where addition, subtraction, and multiplication are defined. The commutativity requirement means that the product does not depend on the order of the two factors, i.e.,
:&lt;math&gt;r \cdot s = s \cdot r&lt;/math&gt;
is supposed to hold for all elements ''r'' and ''s'' of the ring. For example, the [[integer]]s form a commutative ring.

Many{{Clarify|date=April 2011}} of the above statements and notions carry over mutatis mutandis to determinants of these more general matrices: the determinant is multiplicative in this more general situation, and Cramer's rule also holds. A square matrix over a [[commutative ring]] ''R'' is invertible if and only if its determinant is a [[Unit (ring theory)|unit]] in ''R'', that is, an element having a (multiplicative) [[inverse element|inverse]]. (If ''R'' is a field, this latter condition is equivalent to the determinant being nonzero, thus giving back the above characterization.) For example, a matrix ''A'' with entries in '''Z''', the integers, is invertible (in the sense that the inverse matrix has again integer entries) if the determinant is +1 or &amp;minus;1. Such a matrix is called [[unimodular matrix|unimodular]].

The determinant defines a mapping 
:&lt;math&gt;\mathrm{GL}_n(R) \rightarrow R^\times, \,&lt;/math&gt;
between the group of invertible ''n''&amp;nbsp;×&amp;nbsp;''n'' matrices with entries in ''R'' and the [[multiplicative group]] of units in ''R''. Since it respects the multiplication in both groups, this map is a [[group homomorphism]]. Secondly, given a [[ring homomorphism]] ''f'': ''R'' &amp;rarr; ''S'', there is a map GL&lt;sub&gt;''n''&lt;/sub&gt;(''R'') → GL&lt;sub&gt;''n''&lt;/sub&gt;(''S'') given by replacing all entries in ''R'' by their images under ''f''. The determinant respects these maps, i.e., given a matrix ''A'' {{=}} (''a''&lt;sub&gt;''i'',''j''&lt;/sub&gt;) with entries in ''R'', the identity
:&lt;math&gt;f(\det((a_{i,j}))) = \det ((f(a_{i,j})))\,&lt;/math&gt;
holds. For example, the determinant of the [[complex conjugate]] of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo&amp;nbsp;''m'' of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo&amp;nbsp;''m'' (the latter determinant being computed using [[modular arithmetic]]). In the more high-brow parlance of [[category theory]], the determinant is a [[natural transformation]] between the two functors GL&lt;sub&gt;''n''&lt;/sub&gt; and (⋅)&lt;sup&gt;&amp;times;&lt;/sup&gt;.&lt;ref&gt;{{Citation | first = Saunders | last = Mac Lane | authorlink = Saunders Mac Lane | year = 1998 | title = [[Categories for the Working Mathematician]] | series = Graduate Texts in Mathematics '''5''' | edition = (2nd ed.) | publisher = Springer-Verlag | isbn = 0-387-98403-8}}&lt;/ref&gt; Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of [[algebraic group]]s, from the general linear group to the [[multiplicative group]],
:&lt;math&gt;\det: \mathrm{GL}_n \rightarrow \mathbb G_m.\,&lt;/math&gt;

==Generalizations and related notions==

=== Infinite matrices ===
For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in Leibniz' formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. [[Functional analysis]] provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.

The [[Fredholm determinant]] defines the determinant for operators known as [[trace class operator]]s by an appropriate generalization of the formula
:&lt;math&gt;\det(I+A) = \exp(\mathrm{tr}(\log(I+A))). \,&lt;/math&gt;

Another infinite-dimensional notion of determinant is the [[functional determinant]].

=== Notions of determinant over non-commutative rings ===
For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants in a manner analogous to that for commutative rings. A meaning can be given to the Leibniz formula provided the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (if a bilinear form exists with a [[regular element]] of ''R'' as value on some pair of arguments, it can be used to show that all elements of ''R'' commute). Nevertheless various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably [[quasideterminant]]s and the [[Dieudonné determinant]]. It should also
be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems which are very similar to their commutative analogs. Examples include: quantum groups and ''q''-determinant; Capelli matrix and [[Capelli determinant]]; super-matrices and [[Berezinian]]; [[Manin matrices]] is the class of matrices which is most close to matrices with commutative elements.

=== Further variants ===
Determinants of matrices in [[superring]]s (that is, '''Z'''&lt;sub&gt;2&lt;/sub&gt;-[[graded ring]]s) are known as [[Berezinian]]s or superdeterminants.&lt;ref&gt;{{Citation | url = http://books.google.com/?id=sZ1-G4hQgIIC&amp;pg=PA116&amp;dq=Berezinian#v=onepage&amp;q=Berezinian&amp;f=false | title = Supersymmetry for mathematicians: An introduction | isbn = 978-0-8218-3574-6 | author1 = Varadarajan | first1 = V. S | year = 2004 | postscript = .}}&lt;/ref&gt;

The [[permanent]] of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz' rule are omitted. The [[immanant of a matrix|immanant]] generalizes both by introducing a [[character theory|character]] of the [[symmetric group]] S&lt;sub&gt;''n''&lt;/sub&gt; in Leibniz' rule.

== Calculation ==
Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in [[numerical linear algebra]], where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.&lt;ref name=Trefethen&gt;L. N. Trefethen and D. Bau, ''Numerical Linear Algebra'' (SIAM, 1997). e.g. in Lecture 1: &quot;... we mention that the determinant, though a convenient notion theoretically, rarely finds a useful role in numerical algorithms.&quot;&lt;/ref&gt; Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.

Naive methods of implementing an algorithm to compute the determinant include using [[Leibniz formula for determinants|Leibniz' formula]] or [[Laplace expansion|Laplace's formula]]. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is [[Big O notation|of order]] ''n''! (''n'' [[factorial]]) for an ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix ''M''. For example, Leibniz' formula requires to calculate ''n''! products. Therefore, more involved techniques have been developed for calculating determinants.

===Decomposition methods===
Given a matrix ''A'', some methods compute its determinant by writing ''A'' as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as [[decomposition method]]s. Examples include the [[LU decomposition]], the [[QR decomposition]] or the [[Cholesky decomposition]] (for [[Positive definite matrix|positive definite matrices]]). These methods are of order O(''n''&lt;sup&gt;3&lt;/sup&gt;), which is a significant improvement over O(''n''!)

The LU decomposition expresses ''A'' in terms of a lower triangular matrix ''L'', an upper triangular matrix ''U'' and a [[permutation matrix]] ''P'':
:&lt;math&gt; A = PLU. \,&lt;/math&gt;
The determinants of ''L'' and ''U'' can be quickly calculated, since they are the products of the respective diagonal entries.  The determinant of ''P'' is just the sign &lt;math&gt;\varepsilon&lt;/math&gt; of the corresponding permutation (which is +1 for an even number of permutations and is −1 for an uneven number of permutations). The determinant of ''A'' is then

:&lt;math&gt; \det(A) = \varepsilon \det(L)\cdot\det(U), \, &lt;/math&gt;

Moreover, the decomposition can be chosen such that ''L'' is a [[unitriangular matrix]] and therefore has determinant&amp;nbsp;1, in which case the formula further simplifies to

:&lt;math&gt; \det(A) = \varepsilon\det(U).&lt;/math&gt;

===Further methods===
If the determinant of ''A'' and the inverse of ''A'' have already been computed, the [[matrix determinant lemma]] allows to quickly calculate the determinant of {{nowrap|''A'' + ''uv''&lt;sup&gt;T&lt;/sup&gt;}}, where ''u'' and ''v'' are column vectors.

Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed algorithms with run-time proportional to ''n''&lt;sup&gt;4&lt;/sup&gt; exist. An algorithm of Mahajan and Vinay, and Berkowitz&lt;ref&gt;http://page.inf.fu-berlin.de/~rote/Papers/pdf/Division-free+algorithms.pdf&lt;/ref&gt; is based on [[closed ordered walk]]s (short ''clow''). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.

If two matrices of order ''n'' can be multiplied in time ''M''(''n''), where ''M''(''n'')&amp;nbsp;≥&amp;nbsp;''n''&lt;sup&gt;''a''&lt;/sup&gt; for some ''a''&amp;nbsp;&gt;&amp;nbsp;2, then the determinant can be computed in time O(''M''(''n'')).&lt;ref&gt;J.R. Bunch and J.E. Hopcroft, Triangular factorization and inversion by fast matrix multiplication, ''Mathematics of Computation'', 28 (1974) 231–236.&lt;/ref&gt; This means, for example, that an O(''n''&lt;sup&gt;2.376&lt;/sup&gt;) algorithm exists based on the [[Coppersmith–Winograd algorithm]].

Algorithms can also be assessed according to their [[bit complexity]], i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the [[Gaussian elimination]] (or LU decomposition) methods is of order O(''n''&lt;sup&gt;3&lt;/sup&gt;), but the bit length of intermediate values can become exponentially long.&lt;ref&gt;{{Cite conference
  | first1 = Xin Gui
  | last1 = Fang
  | first2 = George
  | last2 = Havas
  | title = On the worst-case complexity of integer Gaussian elimination
  | booktitle = Proceedings of the 1997 international symposium on Symbolic and algebraic computation
  | conference = ISSAC '97
  | pages = 28–31
  | publisher = ACM
  | year = 1997
  | location = Kihei, Maui, Hawaii, United States
  | url = http://perso.ens-lyon.fr/gilles.villard/BIBLIOGRAPHIE/PDF/ft_gateway.cfm.pdf
  | doi = 10.1145/258726.258740
  | isbn = 0-89791-875-4}}&lt;/ref&gt; The [[Bareiss Algorithm]], on the other hand, is an exact-division method based on [[Sylvester's determinant theorem|Sylvester's identity]] is also of order ''n''&lt;sup&gt;3&lt;/sup&gt;, but the bit complexity is roughly the bit size of the original entries in the matrix times ''n''.&lt;ref&gt;{{citation|first=Erwin|last=Bareiss|title= Sylvester's Identity and Multistep Integer-Preserving Gaussian Elimination|pages=565–578|url=http://www.ams.org/journals/mcom/1968-22-103/S0025-5718-1968-0226829-0/S0025-5718-1968-0226829-0.pdf|journal=Mathematics of computation|year=1968|volume=22|issue=102}}&lt;/ref&gt;

==History==
Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a [[system of linear equations]]. The determinant &quot;determines&quot; whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook ''[[The Nine Chapters on the Mathematical Art]]'' (九章算術, Chinese scholars, around the 3rd century BC). In Europe, 2 × 2 determinants were considered by [[Gerolamo Cardano|Cardano]] at the end of the 16th century and larger ones by [[Gottfried Leibniz|Leibniz]].&lt;ref name = &quot;Campbell&quot;/&gt;&lt;ref name = &quot;Eves&quot;&gt;Eves, H: &quot;An Introduction to the History of Mathematics&quot;, pages 405, 493&amp;ndash;494, Saunders College Publishing, 1990.&lt;/ref&gt;&lt;ref&gt;A Brief History of Linear Algebra and Matrix Theory : http://darkwing.uoregon.edu/~vitulli/441.sp04/LinAlgHistory.html&lt;/ref&gt;&lt;ref&gt;Cajori, F. [http://books.google.com/books?id=bBoPAAAAIAAJ&amp;pg=PA80#v=onepage&amp;f=false ''A History of Mathematics'' p. 80]&lt;/ref&gt;

In Europe, [[Gabriel Cramer|Cramer]] (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by [[Bézout]] (1764).

It was [[Vandermonde]] (1771) who first recognized determinants as independent functions.&lt;ref name = &quot;Campbell&quot;&gt;Campbell, H: &quot;Linear Algebra With Applications&quot;, pages 111–112. Appleton Century Crofts, 1971&lt;/ref&gt; [[Laplace]] (1772) &lt;ref&gt;Expansion of determinants in terms of minors: Laplace, Pierre-Simon (de) &quot;Researches sur le calcul intégral et sur le systéme du monde,&quot; ''Histoire de l'Académie Royale des Sciences'' (Paris), seconde partie, pages 267–376 (1772).&lt;/ref&gt;&lt;ref&gt;Muir, Sir Thomas, ''The Theory of Determinants in the historical Order of Development'' [London, England: Macmillan and Co., Ltd., 1906]. {{JFM|37.0181.02}}&lt;/ref&gt; gave the general method of expanding a determinant in terms of its complementary [[minor (matrix)|minors]]: Vandermonde had already given a special case. Immediately following, [[Joseph Louis Lagrange|Lagrange]] (1773) treated determinants of the second and third order. Lagrange was the first to apply determinants to questions of [[elimination theory]]; he proved many special cases of general identities.

[[Carl Friedrich Gauss|Gauss]] (1801) made the next advance. Like Lagrange, he made much use of determinants in the [[theory of numbers]]. He introduced the word '''''determinant''''' (Laplace had used ''resultant''), though not in the present signification, but rather as applied to the [[discriminant]] of a [[algebraic form|quantic]]. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.

The next contributor of importance is [[Jacques Philippe Marie Binet|Binet]] (1811, 1812), who formally stated the theorem relating to the product of two matrices of ''m'' columns and ''n'' rows, which for the special case of ''m'' = ''n'' reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, [[Cauchy]] also presented one on the subject. (See [[Cauchy–Binet formula]].) In this he used the word '''''determinant''''' in its present sense,&lt;ref&gt;The first use of the word &quot;determinant&quot; in the modern sense appeared in: Cauchy, Augustin-Louis “Memoire sur les fonctions qui ne peuvent obtenir que deux valeurs égales et des signes contraires par suite des transpositions operées entre les variables qu'elles renferment,&quot; which was first read at the Institute de France in Paris on November 30, 1812, and which was subsequently published in the ''Journal de l'Ecole Polytechnique'', Cahier 17, Tome 10, pages 29–112 (1815).&lt;/ref&gt;&lt;ref&gt;Origins of mathematical terms: http://jeff560.tripod.com/d.html&lt;/ref&gt; summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.&lt;ref name = &quot;Campbell&quot;/&gt;&lt;ref&gt;History of matrices and determinants: http://www-history.mcs.st-and.ac.uk/history/HistTopics/Matrices_and_determinants.html&lt;/ref&gt; With him begins the theory in its generality.

The next important figure was [[Carl Gustav Jakob Jacobi|Jacobi]]&lt;ref name = &quot;Eves&quot;/&gt; (from 1827). He early used the functional determinant which Sylvester later called the [[Jacobian matrix and determinant|Jacobian]], and in his memoirs in ''[[Crelle]]'' for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called ''alternants''. About the time of Jacobi's last memoirs, [[James Joseph Sylvester|Sylvester]] (1839) and [[Arthur Cayley|Cayley]] began their work.&lt;ref&gt;The first use of vertical lines to denote a determinant appeared in: Cayley, Arthur &quot;On a theorem in the geometry of position,&quot; ''Cambridge Mathematical Journal'', vol. 2, pages 267–271 (1841).&lt;/ref&gt;&lt;ref&gt;History of matrix notation: http://jeff560.tripod.com/matrices.html&lt;/ref&gt;

The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by [[Lebesgue]], [[Otto Hesse|Hesse]], and Sylvester; [[persymmetric]] determinants by Sylvester and [[Hermann Hankel|Hankel]]; [[circulant]]s by [[Eugène Charles Catalan|Catalan]], [[William Spottiswoode|Spottiswoode]], [[James Whitbread Lee Glaisher|Glaisher]], and Scott; skew determinants and [[Pfaffian]]s, in connection with the theory of [[orthogonal transformation]], by Cayley; continuants by Sylvester; [[Wronskian]]s (so called by [[Thomas Muir (mathematician)|Muir]]) by [[Elwin Bruno Christoffel|Christoffel]] and [[Ferdinand Georg Frobenius|Frobenius]]; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and [[Hessian matrix|Hessians]] by Sylvester; and symmetric gauche determinants by [[Trudi]]. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.

==Applications==

===Linear independence===
As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors ''v''&lt;sub&gt;1&lt;/sub&gt;, ''v''&lt;sub&gt;2&lt;/sub&gt; in '''R'''&lt;sup&gt;3&lt;/sup&gt;, a third vector ''v''&lt;sub&gt;3&lt;/sub&gt; lies in the [[Plane (geometry)|plane]] [[Linear span|spanned]] by the former two vectors exactly if the determinant of the 3&amp;nbsp;×&amp;nbsp;3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of [[differential equation]]s: given ''n'' functions ''f''&lt;sub&gt;1&lt;/sub&gt;(''x''), ..., ''f''&lt;sub&gt;''n''&lt;/sub&gt;(''x'') (supposed to be ''n''&amp;minus;1 times differentiable), the [[Wronskian]] is defined to be
:&lt;math&gt;
W(f_1, \ldots, f_n) (x)=
\begin{vmatrix}
f_1(x) &amp; f_2(x) &amp; \cdots &amp; f_n(x) \\
f_1'(x) &amp; f_2'(x) &amp; \cdots &amp; f_n' (x)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
f_1^{(n-1)}(x)&amp; f_2^{(n-1)}(x) &amp; \cdots &amp; f_n^{(n-1)}(x)
\end{vmatrix}.
&lt;/math&gt;
It is non-zero (for some ''x'') in a specified interval if and only if the given functions and all their derivatives up to order ''n''−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of [[analytic function]]s, this implies the given functions are linearly dependent. See [[Wronskian#The Wronskian and linear independence|the Wronskian and linear independence]].

===Orientation of a basis===
{{Main|Orientation (vector space)}}
The determinant can be thought of as assigning a number to every [[sequence]] of ''n'' vectors in '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, by using the square matrix whose columns are the given vectors. For instance, an [[orthogonal matrix]] with entries in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; represents an [[orthonormal basis]] in [[Euclidean space]]. The determinant of such a matrix determines whether the [[orientation (mathematics)|orientation]] of the basis is consistent with or opposite to the orientation of the [[standard basis]]. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.

More generally, if the determinant of ''A'' is positive, ''A'' represents an orientation-preserving [[linear transformation]] (if ''A'' is an orthogonal 2×2 or 3&amp;nbsp;×&amp;nbsp;3 matrix, this is a [[rotation (mathematics)|rotation]]), while if it is negative, ''A'' switches the orientation of the basis.

===Volume and Jacobian determinant===
As pointed out above, the [[absolute value]] of the determinant of real vectors is equal to the volume of the [[parallelepiped]] spanned by those vectors. As a consequence, if {{nowrap|''f'': '''R'''&lt;sup&gt;''n''&lt;/sup&gt; → '''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} is the linear map represented by the matrix ''A'', and ''S'' is any [[Lebesgue measure|measurable]] [[subset]] of '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, then the volume of ''f''(''S'') is given by |det(''A'')| times the volume of ''S''. More generally, if the linear map {{nowrap|''f'': '''R'''&lt;sup&gt;''n''&lt;/sup&gt; → '''R'''&lt;sup&gt;''m''&lt;/sup&gt;}} is represented by the ''m''&amp;nbsp;×&amp;nbsp;''n'' matrix ''A'', then the ''n''-[[dimension]]al volume of ''f''(''S'') is given by:
:&lt;math&gt;\operatorname {volume} (f(S)) = \sqrt{\det(A^\mathrm{T} A)} \times \operatorname{volume}(S).&lt;/math&gt;

By calculating the volume of the [[tetrahedron]] bounded by four points, they can be used to identify [[skew line]]s. The volume of any tetrahedron, given its vertices '''a''', '''b''', '''c''', and '''d''', is (1/6)·|det('''a'''&amp;nbsp;−&amp;nbsp;'''b''',&amp;nbsp;'''b'''&amp;nbsp;−&amp;nbsp;'''c''', '''c'''&amp;nbsp;−&amp;nbsp;'''d''')|, or any other combination of pairs of vertices that would form a [[spanning tree]] over the vertices.

For a general [[differentiable function]], much of the above carries over by considering the [[Jacobian matrix]] of ''f''. For
:&lt;math&gt;f: \mathbf R^n \rightarrow \mathbf R^n,&lt;/math&gt;
the Jacobian is the ''n''&amp;nbsp;×&amp;nbsp;''n'' matrix whose entries are given by
:&lt;math&gt;D(f) = \left (\frac {\partial f_i}{\partial x_j} \right )_{1 \leq i, j \leq n}. \,&lt;/math&gt;
Its determinant, the [[Jacobian determinant]] appears in the higher-dimensional version of [[integration by substitution]]: for suitable functions ''f'' and an [[open subset]] ''U'' of '''R''''&lt;sup&gt;''n''&lt;/sup&gt; (the domain of ''f''), the integral over ''f''(''U'') of some other function {{nowrap|φ: '''R'''&lt;sup&gt;''n''&lt;/sup&gt; → '''R'''&lt;sup&gt;''m''&lt;/sup&gt;}} is given by
:&lt;math&gt; \int_{f(U)} \phi(\mathbf{v})\, d \mathbf{v} = \int_U \phi(f(\mathbf{u})) \left|\det(\operatorname{D}f)(\mathbf{u})\right| \,d \mathbf{u}.&lt;/math&gt;
The Jacobian also occurs in the [[inverse function theorem]].

===Vandermonde determinant (alternant)===
{{Main|Vandermonde matrix}}
Third order
:&lt;math&gt;\left|
\begin{array}{ccc}
 1 &amp; 1 &amp; 1 \\
 x_1 &amp; x_2 &amp; x_3 \\
 x_1^2 &amp; x_2^2 &amp; x_3^2
\end{array}
\right|=\left(x_3-x_2\right)\left(x_3-x_1\right)\left(x_2-x_1\right).&lt;/math&gt;
In general, the ''n''th-order Vandermonde determinant is &lt;ref name = &quot;Gradshteyn&quot;&gt;Gradshteyn, I. S., I. M. Ryzhik: &quot;Table of Integrals, Series, and Products&quot;, 14.31, Elsevier, 2007.&lt;/ref&gt;
:&lt;math&gt;\left|
\begin{array}{ccccc}
 1 &amp; 1 &amp; 1 &amp; \cdots  &amp; 1 \\
 x_1 &amp; x_2 &amp; x_3 &amp; \cdots  &amp; x_n \\
 x_1^2 &amp; x_2^2 &amp; x_3^2 &amp; \cdots  &amp; x_n^2 \\
 \vdots  &amp; \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
 x_1^{n-1} &amp; x_2^{n-1} &amp; x_3^{n-1} &amp; \cdots  &amp; x_n^{n-1}
\end{array}
\right|=\prod _{1\leq i&lt;j\leq n} \left(x_j-x_i\right),&lt;/math&gt;
where the right-hand side is the continued product of all the differences that can be formed from the ''n''(''n''−1)/2 pairs of numbers taken from ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;, with the order of the differences taken in the reversed order of the suffixes that are involved.

===Circulants===
{{Main|Circulant matrix}}
Second order
:&lt;math&gt;\left|
\begin{array}{cc}
 x_1 &amp; x_2 \\
 x_2 &amp; x_1
\end{array}
\right|=\left(x_1+x_2\right)\left(x_1-x_2\right).&lt;/math&gt;
Third order
:&lt;math&gt;\left|
\begin{array}{ccc}
 x_1 &amp; x_2 &amp; x_3 \\
 x_3 &amp; x_1 &amp; x_2 \\
 x_2 &amp; x_3 &amp; x_1
\end{array}
\right|=\left(x_1+x_2+x_3\right)\left(x_1+\omega  x_2+\omega ^2x_3\right)\left(x_1+\omega ^2x_2+\omega  x_3\right),&lt;/math&gt;
where ω and ω&lt;sup&gt;2&lt;/sup&gt; are the complex cube roots of 1. In general, the ''n''th-order circulant determinant is&lt;ref name = &quot;Gradshteyn&quot;/&gt;
:&lt;math&gt;\left|
\begin{array}{ccccc}
 x_1 &amp; x_2 &amp; x_3 &amp; \cdots  &amp; x_n \\
 x_n &amp; x_1 &amp; x_2 &amp; \cdots  &amp; x_{n-1} \\
 x_{n-1} &amp; x_n &amp; x_1 &amp; \cdots  &amp; x_{n-2} \\
 \vdots  &amp; \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
 x_2 &amp; x_3 &amp; x_4 &amp; \cdots  &amp; x_1
\end{array}
\right|=\prod _{j=1}^n \left(x_1+x_2\omega _j+x_3\omega _j^2+\ldots +x_n\omega _j^{n-1}\right),&lt;/math&gt;
where ω&lt;sub&gt;''j''&lt;/sub&gt; is an ''n''th root of 1.

==See also==
*[[Dieudonné determinant]]
*[[Functional determinant]]
*[[Immanant of a matrix|Immanant]]
*[[Matrix determinant lemma]]
*[[Permanent]]
*[[Pfaffian]]
*[[Slater determinant]]

==Notes==
{{Reflist|group=nb}}
{{Reflist}}

==References==
{{see also|Linear algebra#Further reading}}
* {{Citation | last = Axler | first = Sheldon Jay | authorlink=Sheldon Axler | year = 1997 | title = Linear Algebra Done Right | publisher = Springer-Verlag | edition = 2nd | isbn = 0-387-98259-0 }}
* {{Citation | last1=de Boor | first1=Carl | author1-link=Carl R. de Boor | title=An empty exercise | url=http://ftp.cs.wisc.edu/Approx/empty.pdf | doi=10.1145/122272.122273 |year=1990 | journal=ACM SIGNUM Newsletter | volume=25 | issue=2 | pages=3–7}}.
* {{Citation
 | last = Lay
 | first = David C.
 | date = August 22, 2005
 | title = Linear Algebra and Its Applications
 | publisher = Addison Wesley
 | edition = 3rd
 | isbn = 978-0-321-28713-7
}}
* {{Citation
 | last = Meyer
 | first = Carl D.
 | date = February 15, 2001
 | title = Matrix Analysis and Applied Linear Algebra
 | publisher = Society for Industrial and Applied Mathematics (SIAM)
 | isbn = 978-0-89871-454-8
 | url = http://www.matrixanalysis.com/DownloadChapters.html
}}
* {{citation | last=Muir | first=Thomas | authorlink=Thomas Muir (mathematician) | title=A treatise on the theory of determinants | others=Revised and enlarged by William H. Metzler | origyear=1933 | year=1960 | publisher=Dover | location=New York, NY }}
* {{Citation
 | last = Poole
 | first = David
 | year = 2006
 | title = Linear Algebra: A Modern Introduction
 | publisher = Brooks/Cole
 | edition = 2nd
 | isbn = 0-534-99845-3
}}
* {{Citation
 | last = Anton
 | first = Howard
 | year = 2005
 | title = Elementary Linear Algebra (Applications Version)
 | publisher = Wiley International
 | edition = 9th
}}
* {{Citation
 | last = Leon
 | first = Steven J.
 | year = 2006
 | title = Linear Algebra With Applications
 | publisher = Pearson Prentice Hall
 | edition = 7th
}}

==External links==
{{wikibooks
 |1= Linear Algebra
 |2= Linear Algebra#Determinants
 |3= Determinants
}}
*{{SpringerEOM|title=Determinant|id=Determinant&amp;oldid=12692|Suprunenko=|first=D.A.}}
*{{MathWorld|title=Determinant|urlname=Determinant}}
*{{MacTutor|class=HistTopics||id=Matrices_and_determinants|title=Matrices and determinants}}
*[http://sole.ooz.ie/en WebApp to calculate determinants and descriptively solve systems of linear equations]
*[http://people.revoledu.com/kardi/tutorial/LinearAlgebra/MatrixDeterminant.html Determinant Interactive Program and Tutorial]
*[http://matrixcalc.org/en.index.html Online Matrix Calculator]
*[http://www.umat.feec.vutbr.cz/~novakm/determinanty/en/ Linear algebra: determinants.] Compute determinants of matrices up to order 6 using Laplace expansion you choose.
*[http://www.economics.soton.ac.uk/staff/aldrich/matrices.htm Matrices and Linear Algebra on the Earliest Uses Pages]
*[http://algebra.math.ust.hk/course/content.shtml Determinants explained in an easy fashion in the 4th chapter as a part of a Linear Algebra course.]
*[http://khanexercises.appspot.com/video?v=H9BWRYJNIv4 Instructional Video on taking the determinant of an nxn matrix (Khan Academy)]
*[http://www.elektro-energetika.cz/calculations/matreg.php?language=english Online matrix calculator (determinant, track, inverse, adjoint, transpose)] Compute determinant of matrix up to order 8
*[http://www.amarketplaceofideas.com/math-derivation-of-matrix-determinant.htm Derivation of Determinant of a Matrix]

[[Category:Determinants| ]]
[[Category:Matrix theory]]
[[Category:Linear algebra]]
[[Category:Homogeneous polynomials]]
[[Category:Algebra]]

{{Link FA|ca}}
{{Link FA|fr}}
{{Link FA|zh}}</text>
      <sha1>t6h8hsdj5g5tp5snjdqzpgzke8y3xwm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>New algebra</title>
    <ns>0</ns>
    <id>23304169</id>
    <revision>
      <id>600058804</id>
      <parentid>600058738</parentid>
      <timestamp>2014-03-17T20:15:30Z</timestamp>
      <contributor>
        <ip>146.187.25.32</ip>
      </contributor>
      <comment>/* Chapter III: Des lege homogeneorum */</comment>
      <text xml:space="preserve" bytes="9300">{{Notice|This article has been translated from the French at http://fr.wikipedia.org/wiki/Algèbre_nouvelle. The original translation, while substantially good, needed some cleanup. Some work has been done to improve the translation, but please make further improvements if you have the necessary knowledge.}}

The '''new algebra''' or '''symbolic analysis''' is a formalization of [[algebra]] promoted by [[François Viète]] in 1591 and by his successors (after 1603). It marks the beginning of the algebraic formalization (late sixteenth – the early seventeenth centuries).

==The Isagoge==
''In artem analyticem Isagoge'' (1591) is the program of this large axiomatic project.

This work is available via gallica,&lt;ref&gt;{{fr}} François Viète [http://gallica.bnf.fr/ark:/12148/bpt6k108865t.r=viète+1591.langFR In artem analyticem Isagoge, Meteyer publisher, in Tours, (1591) ]&lt;/ref&gt; written in Latin, and announcing that it will be the first volume of a work divided into ten parts:

* In artem Analyticem Isagoge
* Ad Logiticem speciosam Nota priores
* Zeteticorum libri quinque
* De numerosa potestatum ad exegesim resolution
* De recognitione Aequationum
* Ad logiticem speciosam nota posteriores
* Effectionum geometricarum Canonica recensio
* Supllementum Geometria
* Analytica angularium sectionum in tres partes
* Varorium de rebus Mathematicis responsorum

It provides a new approach to writing algebra and begins with the famous dedication to the ''[[Melusine|Melusinide]]'' princess  [[Catherine de Parthenay]].

===Chapter I: Introduction===
In the first part of his ''Isagoge'', Viète provides definitions of his symbolic analysis, and gives, in a rhythmic movement, the definitions of Zetetic, Poristic, and Exegetic, for the purpose of writing the science of inventing Mathematics. He gives, concurrently, an axiomatic for calculation on the quantities (known and unknown) and a program, which provides heuristic rules.

* The Zetetic is the art of translating a problem into an equation and the art of handling this equation to put it in a canonical form which gives rise to an interpretation in terms of proportions.
* The Poristic is the examination of the truth (by the means of ordinary theorems).
* The Exegetic, is the determination (the exhibition, says Antoine Vasset), of geometric solution or numerical solution, obtained from the general propositions of the Poristic.

In this introduction, Viète requires three steps to solve algebraic or geometrical problems: formalization, general resolution, special resolution. He adds that, contrary to the former analysts, his method will act on the resolution of symbols (non iam in numeris sed sub specie)... which is the major input. He also predicts that after his works, training in Zetetic will be done through the analysis of symbols and not by the numbers.

===Chapter II About symbols, equalities, and proportions===
Viète continues, in this second part, to describe the symbols and gives axiomatic rules:

* paragraphs 1 to 6 cover the properties of equality:
Transitivity of equality, conservation sum, subtraction, product, and division
* paragraphs 7 to 11 cover the properties of laws (addition, product, etc. with fractions).
* paragraphs 15 and 16 explain when fractions are equal or not.

===Chapter III: Des lege homogeneorum===
Viète, then, continues to give the [[law of homogeneity]], and distinguishes the symbols according to their powers, where 1 is the side (or root), 2 square, cube 3, and so on. Factors and powers are of complementary homogeneity; he notes them:

1. Length, 2 Plane, Solid 3, and 4 Plane / Plane 5 Plane / Solid 6 Solid / Solid, etc. as if he had the intuition that a geometry can be deployed beyond the ordinary dimension 3.

===Chapter IV De praeceptis logistices speciosae===
In this fourth chapter, Viète gives the rules of a calculus of symbols, i.e. the axioms of addition, product, etc. Symbols designate types of comparable dimension.

Firstly, his attention is focused on addition and subtraction of quantities of the same order, with rules such as ''A''&amp;nbsp;&amp;minus;&amp;nbsp;(''B''&amp;nbsp;+&amp;nbsp;''D'') =&amp;nbsp;''A''&amp;nbsp;&amp;minus;&amp;nbsp;''B''&amp;nbsp;&amp;minus;&amp;nbsp;''D'' or ''A''&amp;nbsp;&amp;minus;&amp;nbsp;(''B''&amp;nbsp;&amp;minus;&amp;nbsp;''D'') = ''A''&amp;nbsp;&amp;minus;&amp;nbsp;''B''&amp;nbsp;+&amp;nbsp;''D''

Then, secondly, he defines products and quotients of homogeneous quantities. He then notes

: &lt;math&gt;\frac{A \text{ plano}}{B} \text{ subducere } \frac {Z \text{ quadratum}}{G}\text{ residua erit }\frac {A \text{ planum in } G - Z \text{ quadrato in } B}{B\text{ in }G}&lt;/math&gt;

what we note now

: &lt;math&gt;\frac{A }{B} - \frac {Z }{G} =  \frac {A G - Z  B}{B  G}&lt;/math&gt;

without attempting to mark the homogeneity factor.

===Chapter V: Laws of the Zététic===
In this chapter we have the foundations of the formulation of equations and particularly in paragraph 5 of this chapter, the idea that '''some letters should be reserved for known quantities (data) and other letters to unknown quantities (incertitus)'''. Viète designates the first quantities by consonants and the others by vowels.
Then, after a few propositions, the book ends with two short chapters that describe how, in practice, it is necessary to conduct the analysis of a problem, its resolution and geometrical checking.

===Chapter VI: The theorems of Poristic===
{{Empty section|date=January 2011}}

===Chapter VII: About Rhétic (or Exegetic)===
{{Empty section|date=January 2011}}
In chapter 7, on the function of the rhetic art, Viete treats the third kind of analysis (rhetic or exegetic), which is applied to numbers if the search is for a magnitude expressible in a number, as well as to lengths, planes, or solids if the thing itself must be shown, starting from canonically ordered equations.

===Chapter VIII: Epilogue===
In this final part Viète defines some notations, including the first and second roots (in other words square and cube roots).

===Variations of 1631===
The manuscript published by Vasset doesn't contain the definition of Poristic and Exegetic (ch VI and VII), but some results on the development of the binomial (to level 6) and general theorems of Poristic: which way to insert a medium proportional you want between two lengths.

This means, for instance, that the sequence

: &lt;math&gt;A^6,A^5B,A^4B^2,A^3B^3,A^2B^4,AB^5,B^6\,&lt;/math&gt;

is geometric.

Reflecting Viète, Vasset writes also:

{{quote|A − B cubus cubus aequabitur A cubo-cubus − 6 A quadrato-cubus in B + 15 A quad.quad. in B quad. − 20  A cubus in B cubum + 15A quadratum in B quad.-quad − 6 A  B quad.-cub. + B cubus-cubus}}

which we denote now by:

: &lt;math&gt;(A-B)^6 = A^6-6A^5B+15A^4B^2-20A^3B^3+15A^2B^4-6AB^5+B^6.\,&lt;/math&gt;

He then gave the rule for forming binomial coefficients (already known by Stiffel and Tartaglia), noting that he obtains the coefficients of the development, by addition in the development of the previous power, of the first and second coefficient, of the second and third, and so on.

{{Infobox scientist
|name              = Francois Viete
|image             = Francois Viete.jpg
|image_size       =
|caption           = Francois Viete, French mathematician
|birth_date        = 1540
|birth_place       = [[Fontenay-le-Comte]], [[Poitou]]
|death_date        = 23 December 1603
|death_place       = [[Paris, France|Paris]], France
|residence         =
|citizenship       =
|nationality       =
|ethnicity         =
|field             = [[algebra]]
|work_institutions =
|alma_mater        =
|doctoral_advisor  =
|doctoral_students =
|known_for         = first notation of new algebra
|author_abbrev_bot =
|author_abbrev_zoo =
|influences        = [[Peter Ramus|Ramus]]
|influenced        = [[Pierre de Fermat]]
|prizes            =
|religion          = unknown
|footnotes         =
|signature         =
}}

==Zététic, Poristic and Exegetic==

{{Empty section|date=March 2013}}

==The contributions of the new algebra==

===Historical critics===
Viète made good Algebra with excellent geometry. However, in his desire to spend his new label under the Diophantine's aegis, Viète was taken to preserve the language of the elders. Moreover, he  has no symbol to rate the multiplication, roots or equality.
For example

: &lt;math&gt;\frac{S\text{ in }A\text{ planum }+\text{ Rbis in }A\text{ planum}}{R} \text{ aequabitur }B\text{ plano}&lt;/math&gt;

written today

: &lt;math&gt; \frac{S A+ 2RA}{R} = B. &lt;/math&gt;

Although effective, this new algebra maintained a requirement of homogeneity which is very heavy and condemns the constant reference to the meaning of geometrical parameters involved. A second Algebraic revolution will be done in the next generation with [[William Oughtred]], [[Thomas Harriot]], [[Pierre de Fermat]], and finally [[René Descartes]]. Nevertheless, Viète gave us for the first time the ability to work efficiently on letters. And, for that,  he must be honored as one of the fathers of Algebra.

==See also==
*[[Johannes Hispalensis]]
*[[Jordanus Nemorarius]]
*[[Regiomontanus]]
*[[Michael Stifel]]
*[[Parthenay]]
*[[Marino Ghetaldi]]
*[[Alexander Anderson (mathematician)|Alexander Anderson]]
*[[Jean de Beaugrand]]
*[[François Viète]]
*[[Simon Stevin]]
*[[Thomas Harriot]]
*[[Pierre de Fermat]]

==Sources==

===References===
&lt;references/&gt;

[[Category:Algebra]]

{{Link GA|fr}}</text>
      <sha1>lfseszurdkihuwexf9m536aa7jw29ji</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Horner's method</title>
    <ns>0</ns>
    <id>14263</id>
    <revision>
      <id>610719440</id>
      <parentid>609846325</parentid>
      <timestamp>2014-05-30T00:11:35Z</timestamp>
      <contributor>
        <username>Hgrosser</username>
        <id>590283</id>
      </contributor>
      <minor/>
      <comment>/* Efficiency */ Fixing style/layout errors</comment>
      <text xml:space="preserve" bytes="31923">In [[mathematics]], '''Horner's method''' (also known as '''Horner scheme''' in the UK or '''Horner's rule''' in the U.S.&lt;ref name=&quot;Cormen et al.&quot;&gt;{{cite book|last=Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein|title=Introduction to Algorithms|edition=3rd|year=2009|publisher=MIT Press|pages=41, 900, 990}}&lt;/ref&gt;&lt;ref name=&quot;HornerRule&quot;&gt;{{cite web|title=Wolfram MathWorld: Horner's Rule|url=http://mathworld.wolfram.com/HornersRule.html}}&lt;/ref&gt;) is either of two things: (i) an [[algorithm]] for calculating [[polynomial]]s, which consists of transforming the [[Monomial basis|monomial form]] into a computationally efficient form;&lt;ref name=&quot;HornerRule&quot; /&gt; or (ii) a method for approximating the roots of a polynomial.&lt;ref name=&quot;HornerMethod&quot;&gt;{{cite web|title=Wolfram MathWorld: Horner's Method|url=http://mathworld.wolfram.com/HornersMethod.html}}&lt;/ref&gt; The latter is also known as '''Ruffini–Horner's method'''.&lt;ref&gt;{{cite web|title=French Wikipedia: Méthode de Ruffini-Horner|url=http://fr.wikipedia.org/wiki/M%C3%A9thode_de_Ruffini-Horner}}&lt;/ref&gt;

These methods are named after the British mathematician [[William George Horner]], although they were known before him by [[Paolo Ruffini]]&lt;ref name=&quot;Cajori&quot;&gt;[[Florian Cajori]], [http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.bams/1183421253 Horner's method of approximation anticipated by Ruffini], Bulletin of the American Mathematical Society, Vol. 17, No. 9, pp. 409&amp;ndash;414, 1911 (read before the Southwestern Section of the American Mathematical Society on November 26, 1910).&lt;/ref&gt; and, six hundred years earlier, by the Chinese mathematician [[Qin Jiushao]].&lt;ref&gt;''It is obvious that this procedure is a Chinese invention'', Ulrich Librecht, Chinese Mathematics in the Thirteenth Century, Chapter 13, '' Equations of Higher Degree'', p178 Dover, ISBN 0-486-44619-0&lt;/ref&gt;

==Description of the algorithm==
Given the polynomial

:&lt;math&gt;p(x) = \sum_{i=0}^n a_i x^i = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots + a_n x^n,&lt;/math&gt;

where &lt;math&gt;a_0, \ldots, a_n&lt;/math&gt; are real numbers, we wish to evaluate the polynomial at a specific value of &lt;math&gt;x&lt;/math&gt;, say &lt;math&gt;x_0&lt;/math&gt;.

To accomplish this, we define a new sequence of constants as follows:

:&lt;math&gt;\begin{align}
b_n &amp; := a_n \\
b_{n-1} &amp; := a_{n-1} + b_n x_0 \\
&amp; {}\  \  \vdots \\
b_0 &amp; := a_0 + b_1 x_0.
\end{align}&lt;/math&gt;

Then &lt;math&gt;b_0&lt;/math&gt; is the value of &lt;math&gt;p(x_0)&lt;/math&gt;.

To see why this works, note that the polynomial can be written in the form

:&lt;math&gt;p(x) = a_0 + x(a_1 + x(a_2 + \cdots + x(a_{n-1} + a_n x)\cdots)). \, &lt;/math&gt;

Thus, by iteratively substituting the &lt;math&gt;b_i&lt;/math&gt; into the expression,
: &lt;math&gt;
\begin{align}
p(x_0) &amp; = a_0 + x_0(a_1 + x_0(a_2 + \cdots + x_0(a_{n-1} + b_n x_0)\cdots)) \\
&amp; = a_0 + x_0(a_1 + x_0(a_2 + \cdots + x_0(b_{n-1})\cdots)) \\
&amp; {} \ \  \vdots \\
&amp; = a_0 + x_0(b_1) \\
&amp; = b_0.
\end{align}
&lt;/math&gt;

==Examples==
Evaluate 
:&lt;math&gt;f(x)=2x^3-6x^2+2x-1\,&lt;/math&gt; for &lt;math&gt;x=3.\;&lt;/math&gt;

We use  [[synthetic division]] as follows:

  x₀│   x³    x²    x¹    x⁰
  3 │   2    −6     2    −1
    │         6     0     6    
    └────────────────────────
        2     0     2     5

The entries in the third row are the sum of those in the first two. Each entry in the second row is the product of the ''x''-value (3 in this example) with the third-row entry immediately to the left. The entries in the first row are the coefficients of the polynomial to be evaluated. Then the remainder of &lt;math&gt;f(x)&lt;/math&gt; on division by &lt;math&gt;x-3&lt;/math&gt; is 5.

But by the polynomial remainder theorem, we know that the remainder is &lt;math&gt;f(3) &lt;/math&gt;. Thus &lt;math&gt;f(3) = 5&lt;/math&gt;

In this example, if &lt;math&gt;a_3 = 2, a_2 = -6, a_1 = 2, a_0 = -1&lt;/math&gt; we can see that &lt;math&gt;b_3 = 2, b_2 = 0, b_1 = 2, b_0 = 5 &lt;/math&gt;, the entries in the third row. So, synthetic division is based on Horner's method.

As a consequence of the [[polynomial remainder theorem]], the entries in the third row are the coefficients of the second-degree polynomial, the quotient of &lt;math&gt;f(x)&lt;/math&gt; on division by &lt;math&gt; x-3 &lt;/math&gt;. 
The remainder is 5. This makes Horner's method useful for [[polynomial long division]].

Divide &lt;math&gt;x^3-6x^2+11x-6\,&lt;/math&gt; by &lt;math&gt;x-2\,&lt;/math&gt;:

  2 │   1    -6    11    -6
    │         2    -8     6    
    └────────────────────────
        1    -4     3     0

The quotient is &lt;math&gt;x^2-4x+3\,&lt;/math&gt;.

Let &lt;math&gt;f_1(x)=4x^4-6x^3+3x-5\,&lt;/math&gt; and &lt;math&gt;f_2(x)=2x-1\,&lt;/math&gt;. Divide &lt;math&gt;f_1(x)\,&lt;/math&gt; by &lt;math&gt;f_2\,(x)&lt;/math&gt; using Horner's method.

   2 │  4    -6    0    3   │   -5
 ────┼──────────────────────┼───────
   1 │        2   -2   -1   │    1
     │                      │  
     └──────────────────────┼───────
        2    -2    -1   1   │   -4

The third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer is

:&lt;math&gt;\frac{f_1(x)}{f_2(x)}=2x^3-2x^2-x+1-\frac{4}{2x-1}.&lt;/math&gt;

===Floating point multiplication and division===
{{main|multiplication algorithm#Shift and add}}

Horner's method is a fast, code-efficient method for multiplication and division of binary numbers on a [[microcontroller]] with no [[binary multiplier|hardware multiplier]].  One of the binary numbers to be multiplied is represented as a trivial polynomial, where, (using the above notation):  a&lt;sub&gt;i&lt;/sub&gt; = 1, and x = 2.  Then, x (or x to some power) is repeatedly factored out.  In this [[binary numeral system]] (base 2), x = 2, so powers of 2 are repeatedly factored out.

====Example====
For example, to find the product of two numbers, (0.15625) and ''m'':

:&lt;math&gt;
\begin{align}
( 0.15625) m &amp; = (0.00101_b) m = ( 2^{-3} + 2^{-5}) m = (2^{-3})m + (2^{-5})m \\[4pt]
&amp; = 2^{-3} (m + (2^{-2})m) = 2^{-3} (m + 2^{-2} (m)).
\end{align}
&lt;/math&gt;

====Method====
To find the product of two binary numbers, d and m:
*1. A register holding the intermediate result is initialized to d.
*2. Begin with the least significant (rightmost) non-zero bit in m.
**2b. Count (to the left) the number of bit positions to the next most significant non-zero bit.  If there are no more-significant bits, then take the value of the current bit position.
**2c. Using that value, perform a right-shift operation by that number of bits on the register holding the intermediate result
*3. If all the non-zero bits were counted, then the intermediate result register now holds the final result.  Otherwise, add d to the intermediate result, and continue in step #2 with the next most significant bit in m.

====Derivation====
In general, for a binary number with bit values: (&lt;math&gt; d_3 d_2 d_1 d_0 &lt;/math&gt;) the product is:
:&lt;math&gt; (d_3 2^3 + d_2 2^2 + d_1 2^1 + d_0 2^0)m = d_3 2^3 m + d_2 2^2 m + d_1 2^1 m + d_0 2^0 m &lt;/math&gt;
At this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or [[division by zero]] is not an issue, despite this implication in the factored equation:

:&lt;math&gt; = d_0(m + 2 \frac{d_1}{d_0} (m + 2 \frac{d_2}{d_1} (m + 2 \frac{d_3}{d_2} (m)))). &lt;/math&gt;

The denominators all equal one (or the term is absent), so this reduces to:
:&lt;math&gt; = d_0(m + 2 {d_1} (m + 2 {d_2} (m + 2 {d_3} (m)))),&lt;/math&gt;
or equivalently (as consistent with the &quot;method&quot; described above):
:&lt;math&gt; = d_3(m + 2^{-1} {d_2} (m + 2^{-1}{d_1} (m + {d_0} (m)))). &lt;/math&gt;

In binary (base 2) math, multiplication by a power of 2 is merely a [[arithmetic shift|register shift]] operation.  Thus, multiplying by 2 is calculated in base-2 by an [[arithmetic shift]].  The factor (2&lt;sup&gt;−1&lt;/sup&gt;) is a right [[arithmetic shift]], a (0) results in no operation (since 2&lt;sup&gt;0&lt;/sup&gt; = 1, is the multiplicative [[identity element]]), and a (2&lt;sup&gt;1&lt;/sup&gt;) results in a left arithmetic shift.
The multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.

The method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate.  Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the &quot;[[canonical signed digit]]&quot; (CSD) form is used), and uses only 20% of the code space.&lt;ref&gt;Kripasagar, March 2008, &quot;Efficient Micro Mathematics&quot;, Circuit Cellar, issue 212, p. 62.&lt;/ref&gt;

=== Polynomial root finding ===
Using Horner's method in combination with [[Newton's method]], it is possible to approximate the real roots of a polynomial. The algorithm works as follows. Given a polynomial &lt;math&gt;p_n(x)&lt;/math&gt; of degree &lt;math&gt;n&lt;/math&gt; with zeros &lt;math&gt; z_n &lt; z_{n-1} &lt; \cdots &lt; z_1 &lt;/math&gt;, make some initial guess &lt;math&gt; x_0 &lt;/math&gt; such that &lt;math&gt; x_0 &gt; z_1 &lt;/math&gt;. Now iterate the following two steps:

1. Using [[Newton's method]], find the largest zero &lt;math&gt;z_1&lt;/math&gt; of &lt;math&gt;p_n(x)&lt;/math&gt; using the guess &lt;math&gt;x_0&lt;/math&gt;.

2. Using Horner's method, divide out &lt;math&gt;(x-z_1)&lt;/math&gt; to obtain &lt;math&gt;p_{n-1}&lt;/math&gt;. Return to step 1 but use the polynomial &lt;math&gt;p_{n-1}&lt;/math&gt; and the initial guess &lt;math&gt;z_1&lt;/math&gt;.

These two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials.&lt;ref&gt;Kress, Rainer, &quot;Numerical Analysis&quot;, Springer, 1991, p.112.&lt;/ref&gt;

==== Example ====

[[File:HornerandNewton.gif|thumb|right|500px|Polynomial root finding using Horner's method]]

Consider the polynomial,

: &lt;math&gt;
p_6(x) = (x-3)(x+3)(x+5)(x+8)(x-2)(x-7)
&lt;/math&gt;

which can be expanded to

: &lt;math&gt;
p_6(x) = x^6 + 4x^5 - 72x^4 -214x^3 + 1127x^2 + 1602x -5040.
&lt;/math&gt;

From the above we know that the largest root of this polynomial is 7 so we are able to make an initial guess of 8. Using Newton's method the first zero of 7 is found as shown in black in the figure to the right. Next &lt;math&gt;p(x)&lt;/math&gt; is divided by &lt;math&gt;(x-7)&lt;/math&gt; to obtain

: &lt;math&gt;
p_5(x) = x^5 + 11x^4 + 5x^3 - 179x^2 - 126x + 720 \,
&lt;/math&gt;

which is drawn in red in the figure to the right. Newton's method is used to find the largest zero of this polynomial with an initial guess of 7. The largest zero of this polynomial which corresponds to the second largest zero of the original polynomial is found at 3 and is circled in red. The degree 5 polynomial is now divided by &lt;math&gt;(x-3)&lt;/math&gt; to obtain

: &lt;math&gt;
p_4(x) = x^4 + 14x^3 + 47x^2 - 38x - 240 \,
&lt;/math&gt;

which is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtain

: &lt;math&gt;
p_3(x) = x^3 + 16x^2 + 79x + 120 \,
&lt;/math&gt;

which is shown in green and found to have a zero at&amp;nbsp;&amp;minus;3. This polynomial is further reduced to

: &lt;math&gt;
p_2(x) = x^2 + 13x + 40 \,
&lt;/math&gt;

which is shown in blue and yields a zero of&amp;nbsp;&amp;minus;5. The final root of the original polynomial may be found by either using the final zero as an initial guess for Newton's method, or by reducing &lt;math&gt;p_2(x)&lt;/math&gt; and solving the linear equation. As can be seen, the expected roots of &amp;minus;8, &amp;minus;5, &amp;minus;3, 2, 3, and 7 were found.

==== Octave implementation ====
The following [[GNU Octave|Octave]] code was used in the example above to implement Horner's method.

&lt;source lang=&quot;octave&quot;&gt;
function [y b] = horner(a,x)
  % Input a is the polynomial coefficient vector, x the value to be evaluated at.
  % The output y is the evaluated polynomial and b the divided coefficient vector.
  b(1) = a(1);
  for i = 2:length(a)
    b(i) = a(i)+x*b(i-1);
  end
  y = b(length(a));
  b = b(1:length(b)-1);
end
&lt;/source&gt;

==== Python implementation ====

The following [[Python (programming language)|Python]] code implements Horner's method.

&lt;source lang=&quot;python&quot;&gt;
def horner(x, *polynomial):
    &quot;&quot;&quot;A function that implements the Horner Scheme for evaluating a
    polynomial of coefficients *polynomial in x.&quot;&quot;&quot;
    result = 0
    for coefficient in polynomial:
        result = result * x + coefficient
    return result
&lt;/source&gt;

==Application==
Horner's method can be used to convert between different positional [[numeral system]]s – in which case ''x'' is the base of the number system, and the ''a''&lt;sub&gt;''i''&lt;/sub&gt; coefficients are the digits of the base-''x'' representation of a given number – and can also be used if ''x'' is a [[matrix (math)|matrix]], in which case the gain in computational efficiency is even greater. In fact, when ''x'' is a matrix, further acceleration is possible which exploits the structure of [[matrix multiplication]], and only &lt;math&gt;\sqrt{n}&lt;/math&gt; instead of ''n'' multiplies are needed (at the expense of requiring more storage) using the 1973 method of Paterson and Stockmeyer.&lt;ref&gt;Higham, Nicholas. (2002). ''Accuracy and Stability of Numerical Algorithms''. Philadelphia: SIAM. ISBN 0-89871-521-0. Section 5.4.&lt;/ref&gt;

==Efficiency==
Evaluation using the monomial form of a degree-''n'' polynomial requires at most ''n'' additions and (''n''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''n'')/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually.  (This can be reduced to ''n'' additions and 2''n''&amp;nbsp;&amp;minus;&amp;nbsp;1 multiplications by evaluating the powers of ''x'' iteratively.)  If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2''n'' times the number of bits of ''x'' (the evaluated polynomial has approximate magnitude ''x''&lt;sup&gt;''n''&lt;/sup&gt;, and one must also store ''x''&lt;sup&gt;''n''&lt;/sup&gt; itself).  By contrast, Horner's method requires only ''n'' additions and ''n'' multiplications, and its storage requirements are only ''n'' times the number of bits of ''x''. Alternatively, Horner's method can be computed with ''n'' [[fused multiply–add]]s.  Horner's method can also be extended to evaluate the first ''k'' derivatives of the polynomial with ''kn'' additions and multiplications.&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?doid=364063.364089|title=Algorithm 337: calculation of a polynomial and its derivative values by Horner scheme|author=W. Pankiewicz}}&lt;/ref&gt;

Horner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. [[Alexander Ostrowski]] proved in 1954 that the number of additions required is minimal.&lt;ref&gt;Ostrowski, A. M. (1954). &quot;On two problems in abstract algebra connected with Horner's rule&quot;, Studies in Math. Mech., pp. 40-48. New York: Academic Press.&lt;/ref&gt; [[Victor Pan]] proved in 1966 that the number of multiplications is minimal.&lt;ref&gt;Pan, Y. Ja. (1966). “On means of calculating values of polynomials”, ''Russian Math. Surveys'' '''21''', pp. 105-136.&lt;/ref&gt; However, when ''x'' is a matrix, Horner's method is not optimal.

This assumes that the polynomial is evaluated in monomial form and no [[preconditioning]] of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-''n'' polynomial can be evaluated using only &lt;math&gt;{\scriptstyle{\left\lfloor n/2 \right\rfloor + 2}}&lt;/math&gt; multiplications and ''n'' additions (see [[Donald Knuth|Knuth]]: ''[[The Art of Computer Programming]]'', Vol.2).

==History==
[[File:Qingjiushaoquad1.GIF|thumb|right|200px|Qin Jiushao algorithm for solving quadratic polymial equation&lt;math&gt;-x^4+763200x^2-40642560000=0&lt;/math&gt;&lt;br&gt;result: x=840&lt;ref&gt;Urich Libbrecht Chinese Mathematics in the Thirteenth Century, p181&amp;ndash;191, Dover ISBN 0-486-44619-0&lt;/ref&gt;]]
Horner's paper entitled &quot;[http://turing.une.edu.au/~ernie/Horner/Horner1819PhilTransRoySocLon.pdf A new method of solving numerical equations of all orders, by continuous approximation]&quot;, was read before the Royal Society of London, at its meeting on July 1, 1819, with [[Davies Gilbert]], Vice-President and Treasurer, in the chair; this was the final [http://hdl.handle.net/2027/mdp.39015014105277?urlappend=%3Bseq=158 meeting] of the session before the Society adjorned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by [[James Ivory (mathematician)|James Ivory]], FRS, were also read. In 1819, it was Horner's paper that got through to publication in the &quot;Philosophical Transactions&quot;.&lt;ref name=&quot;Horner&quot;&gt;{{cite journal
|last = William George Horner
|title = A new method of solving numerical equations of all orders, by continuous approximation
|journal = Philosophical Transactions
|date=July 1819
|pages = pp.&amp;nbsp;308&amp;ndash;335
|publisher = Royal Society of London
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
|title=JSTOR archive of the Royal Society of London
|jstor=107508
}}&lt;/ref&gt; later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. But Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own [http://turing.une.edu.au/~ernie/Horner/Gilbert1823QJSLA.pdf survey] of Horner-type methods earlier in 1823.

Horner's paper in Part II of ''Philosophical Transactions of the Royal Society of London'' for 1819 was warmly and expansively welcomed by a [http://turing.une.edu.au/~ernie/Horner/Horner1820MonthlyRev91-4.pdf reviewer] in the issue of ''The Monthly Review: or, Literary Journal'' for April, 1820; in comparison, a technical paper by [[Charles Babbage]] is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been promoted by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further [http://turing.une.edu.au/~ernie/Horner/Horner1820MonthlyRev93-12.pdf review] of some of Nicholson's books in the issue of ''The Monthly Review'' for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of ''The Monthly Review'' for September, 1821, with the [http://turing.une.edu.au/~ernie/Horner/Horner1821MonthlyRev96-9.pdf reviewer] reasserting both Horner's priority and the primacy of his method, judiciously observing that had Holdred published forty years earlier, his contribution could more easily be recognized. The reviewer is exceptionally well-informed, even having sighted Horner's preparatory correspondence with [[Peter Barlow (mathematician)|Peter Barlow]] in 1818, seeking work of [[François Budan de Boislaurent|Budan]]. The Bodlean Library, Oxford has the Editor's annotated copy of ''The Monthly Review'' from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow,one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow [[Geordie]], [[Charles Hutton]], another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.

The feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of [[Louis François Antoine Arbogast|Arbogast]]. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of [[Paolo Ruffini]], except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in ''Philosophical Transactions'' speak volumes: there are none - Ruffini's [http://hdl.handle.net/2027/njp.32101013501372?urlappend=%3Bseq=695 name] only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had [[Malfatti's problem|Malfatti's Problem]] in the reformulation of [[Gergonne|Joseph Diaz Gergonne]], or had he written in French, as had [[:it:Antonio Cagnoli|Antonio Cagnoli]], a source quoted by Bonneycastle on series reversion (today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English).

Fuller&lt;ref&gt;Fuller A. T. :Horner versus Holdred: An Episode in the History of Root Computation, Historia Mathematica 26 (1999), 29–51&lt;/ref&gt; develops the thesis that Horner's method was never published by Horner until after it was published by Holdred. But this is at variance with the contemporary reception of the works of both Horner and Holdred, as indicated in the previous paragraph, besides the numerous internal flaws in Fuller's paper, flaws that are so strange as to raise doubt as to Fuller's purpose (see the [[Talk:Horner's method#Is Fuller.27s article a hoax.3F|Talk]] page). Fuller also takes aim at [[Augustus De Morgan]]. Precocious though Augustus de Morgan was, he was not the reviewer for ''The Monthly Review'', while several others - [[Thomas Stephens Davies]], J. R. Young, Stephen Fenwick, T. T. Wilkinson - wrote Horner firmly into their records, not least Horner himself, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to ''The Gentleman's Mathematical Companion'', an answer to a problem.

It is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery&lt;ref name=&quot;Cajori&quot; /&gt;&lt;ref name=&quot;St Andrews&quot; /&gt; that led to &quot;Horner's method&quot; being so called in textbooks, but it is true that those suggesting this tend themselves to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method ''qua'' method was known long before Horner. In reverse chronological order, Horner's method was already known to:

* [[Paolo Ruffini]] in 1809 (see [[Ruffini's rule]])&lt;ref name=&quot;Cajori&quot; /&gt;&lt;ref name=&quot;St Andrews&quot;&gt;{{MacTutor Biography|id=Horner}}&lt;/ref&gt;
* [[Isaac Newton]] in 1669 (but precise reference needed)
* the [[Chinese mathematics|Chinese mathematician]] [[Zhu Shijie]] in the 14th century&lt;ref name=&quot;St Andrews&quot; /&gt;
* the [[Chinese mathematics|Chinese mathematician]] [[Qin Jiushao]] in his ''[[Mathematical Treatise in Nine Sections]]'' in the 13th century
* the [[Persian people|Persian]] [[Islamic mathematics|mathematician]] [[Sharaf al-Dīn al-Tūsī]] in the 12th century&lt;ref&gt;J. L. Berggren (1990). &quot;Innovation and Tradition in Sharaf al-Din al-Tusi's Muadalat&quot;, ''Journal of the American Oriental Society'' '''110''' (2), p. 304&amp;ndash;309.&lt;/ref&gt;
* the Chinese mathematician [[Jia Xian]] in the 11th century ([[Song Dynasty]])
* ''[[The Nine Chapters on the Mathematical Art]]'', a Chinese work of the [[Han Dynasty]] (202 BC &amp;ndash; 220 AD) edited by [[Liu Hui]] (fl. 3rd century).&lt;ref&gt;Temple, Robert. (1986). ''The Genius of China: 3,000 Years of Science, Discovery, and Invention''. With a forward by Joseph Needham. New York: Simon and Schuster, Inc. ISBN 0-671-62028-2. Page 142.&lt;/ref&gt;

However, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.

[[Qin Jiushao]], in his ''Shu Shu Jiu Zhang'' (''[[Mathematical Treatise in Nine Sections]]''; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician [[Jia Xian]]; for example, one method is specifically suited to bi-qintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was [[Alexander Wylie (missionary)|Alexander Wylie]], writing in ''The North China Herald'' in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method ''Harmoniously Alternating Evolution'' (which does not agree with his Chinese, ''linglong kaifang'', not that at that date he uses [[pinyin]]), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. [[Yoshio Mikami]] in ''Development of Mathematics in China and Japan'' published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote: &quot;who can deny the fact of Horner's illustrious process being used in China at least nearly six long centuries earlier than in Europe ... We of course don't intend in any way to ascribe Horner's invention to a Chinese origin, but the lapse of time sufficiently makes it not altogether impossible that the Europeans could have known of the Chinese method in a direct or indirect way.&quot;.&lt;ref&gt;Yoshio Mikami, Chinese Mathematics in the Thirteenth Century, Chapter 11, Chin Chiu Shao, p77 Chelsea Publishing Co&lt;/ref&gt; However, as Mikami is also aware, it was ''not altogether impossible'' that a related work, ''Si Yuan Yu Jian'' (''Jade Mirror of the Four Unknowns; 1303)'' by [[Zhu Shijie]] might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, ''Suan Xue Qi Meng'', had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. [[Ulrich Libbrecht]] (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: ''It is obvious that this procedure is a Chinese invention....the method was not known in India''. He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese.&lt;ref&gt;Ulrich Libbrecht, Chinese Mathematics in the Thirteenth Century, Chapter 13, Numerial Equations of Higher Degree, p208 Dover, ISBN 0-486-44619-0&lt;/ref&gt; Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by [[Liu Hui]] in connection with Problems IV.16 and 22 in ''Jiu Zhang Suan Shu'', while [[Wang Xiaotong]] in the 7th century supposes his readers can solve cubics by an approximation method he does not specify.

==See also==
*[[Clenshaw algorithm]] to evaluate polynomials in [[Chebyshev form]]
*[[De Boor's algorithm]] to evaluate [[spline curve|splines]] in [[B-spline]] form
*[[De Casteljau's algorithm]] to evaluate polynomials in [[Bézier form]]
*[[Estrin's scheme]] to facilitate parallelization on modern computer architectures
*[[Lill's method]] to approximate roots graphically
*[[Ruffini's rule]] to divide a polynomial by a binomial of the form x − r

== References ==
=== Citations ===
{{Reflist}}

=== Bibliography ===
* {{cite journal
|last = Horner
|first = William George
|title = [http://turing.une.edu.au/~ernie/Horner/Horner1819PhilTransRoySocLon.pdf A new method of solving numerical equations of all orders, by continuous approximation]
|journal = Philosophical Transactions
|date = July 1819
|pages = pp.&amp;nbsp;308&amp;ndash;335
|publisher = Royal Society of London
}} Directly available online via the link, but also reprinted with appraisal in D.E.Smith: ''A Source Book in Mathematics'', McGraw-Hill, 1929; Dover reprint, 2 vols 1959
* {{cite book
|last = Spiegel
|first = Murray R.
|title = Schaum's Outline of Theory and Problems of College Algebra
|year = 1956
|publisher = McGraw-Hill Book Company
}}
* {{cite book
|last = Knuth
|first = Donald
|title = The Art of Computer Programming
|volume = Vol. 2: Seminumerical Algorithms
|edition = 3rd
|year = 1997
|publisher = Addison-Wesley
|isbn = 0-201-89684-2
|pages = 486&amp;ndash;488 in section 4.6.4
}}
* {{Cite journal
|last = Kripasagar 
|first =  Venkat
|title = Efficient Micro Mathematics &amp;ndash; Multiplication and Division Techniques for MCUs
|journal = Circuit Cellar magazine
|issue = 212
|page = p.&amp;nbsp;60
|date = March 2008
}}
* {{cite book
|last = Mikami
|first = Yoshio
|title = [http://archive.org/details/treatiseindynami033561mbp The Development of Mathematics in China and Japan]
|chapter = [http://archive.org/stream/treatiseindynami033561mbp#page/n77/mode/2up 11]
|edition = 1st
|year = 1913
|publisher = Chelsea Publishing Co reprint
|isbn = 
|pages = [http://archive.org/stream/treatiseindynami033561mbp#page/n89/mode/2up 74–77]
}} Yes, really! It looks as though the link is taking you to a completely different work, but you end up at Mikami's book, as you find on checking the specified pages.

* {{cite book
|last = Ulrich
|first = Librecht
|title = Chinese Mathematics in the Thirteenth Century
|chapter = 13
|edition = 2nd
|year = 2005
|publisher = Dover
|isbn = 0-486-44619-0
|pages = 175–211
}}

* {{cite book
|last = Wylie
|first = Alexander
|title = [http://archive.org/details/chineseresearche00wyliuoft Chinese Researches]
|year = 1897
|publisher = Printed in Shanghai
}}, ''[http://archive.org/stream/chineseresearche00wyliuoft#page/158/mode/2up Jottings on the Science of Chinese Arithmetic]'' (reprinted from issues of ''The North China Herald'' (1852).
* T. Holdred (1820), [http://turing.une.edu.au/~ernie/Horner/Holdred1820.pdf A New Method of Solving Equations with Ease and Expedition; by which the True Value of the Unknown Quantity is Found Without Previous Reduction. With a Supplement, Containing Two Other Methods of Solving Equations, Derived from the Same Principle] Richard Watts. Sold by Davis and Dickson, mathematical and philosophical booksellers, 17, St. Martin's-le-Grand; and by the author, 2, Denzel Street, Clare-Market, 56pp..

==External links==
* {{springer|title=Horner scheme|id=p/h048030}}
* {{MathWorld|urlname=HornersMethod|title=Horner's method}}
* [http://math.fullerton.edu/mathews/n2003/HornerMod.html Module for Horner's Method by John H. Mathews]
* Qiu Jin-Shao, [http://turing.une.edu.au/~ernie/Chinese/SSJZ.pdf Shu Shu Jiu Zhang] (Cong Shu Ji Cheng ed.)

{{DEFAULTSORT:Horner Scheme}}
[[Category:Algebra]]
[[Category:Polynomials]]
[[Category:Numerical analysis]]</text>
      <sha1>cuebqc3rqs7dal87j5bmx7hhb9rrvtr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Oriented Point Relation Algebra</title>
    <ns>0</ns>
    <id>35574895</id>
    <revision>
      <id>493819720</id>
      <parentid>493305689</parentid>
      <timestamp>2012-05-22T13:13:55Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <text xml:space="preserve" bytes="601">The '''Oriented Point Relation Algebra (OPRA)''' serves for qualitative spatial representation and [[Spatial-temporal reasoning|reasoning]]. OPRA is an [[orientation calculus]] with adjustable [[granularity]].  OPRA is based on objects which are represented as oriented points.  Oriented points are specified as pair of a point and a direction on the 2D-plane.

==References==
* Till Mossakowski, Reinhard Moratz (2012). Qualitative Reasoning about Relative Direction of Oriented Points.  Artificial Intelligence Journal, Vol. 180–181, pp. 34–45.

[[Category:Algebra]]
[[Category:Logical calculi]]</text>
      <sha1>e3giakjl19cm7pmy655juauty7m530i</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Distribution algebra</title>
    <ns>0</ns>
    <id>35987528</id>
    <revision>
      <id>601857322</id>
      <parentid>565691301</parentid>
      <timestamp>2014-03-29T19:36:35Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10067)</comment>
      <text xml:space="preserve" bytes="716">{{multiple issues|
{{Underlinked|date=December 2012}}
{{Orphan|date=December 2012}}
}}

In [[algebra]], the '''distribution algebra''' &lt;math&gt;D(G, K)&lt;/math&gt; of a [[p-adic Lie group|''p''-adic Lie group]] ''G'' is the ''K''-algebra of ''K''-valued distributions on ''G''. (See the reference for a more precise definition.)

== References ==
*{{cite journal |journal=Representation theory An electric journal of the American Mathematical Society |title=&lt;math&gt;U(\mathfrak{g})&lt;/math&gt;-finite locally analytic representations |last1= Schneider |first1=P. |first2=J. |last2=Teitelbaum |url=http://www.ams.org/journals/ert/2001-005-05/S1088-4165-01-00109-1/S1088-4165-01-00109-1.pdf}}

[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>1mg6esa3cxy4qucunc4jp6x7tg67vzg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Digital root</title>
    <ns>0</ns>
    <id>8286632</id>
    <revision>
      <id>594743227</id>
      <parentid>591211565</parentid>
      <timestamp>2014-02-09T23:28:31Z</timestamp>
      <contributor>
        <username>Bkell</username>
        <id>32452</id>
      </contributor>
      <minor/>
      <comment>fixed TeX</comment>
      <text xml:space="preserve" bytes="11880">The '''digital root''' (also '''repeated digital sum''') of a number is the (single digit) value obtained by an iterative process of [[digit sum|summing digits]], on each iteration using the result from the previous iteration to compute a digit sum. The process continues until a single-digit number is reached.

For example, the digital root of &lt;math&gt;65\,536&lt;/math&gt; is &lt;math&gt;7&lt;/math&gt;, because &lt;math&gt;6+5+5+3+6 = 25&lt;/math&gt; and &lt;math&gt;2+5 = 7.&lt;/math&gt;

Digital roots can be calculated with [[Congruence relation|congruence]]s in [[modular arithmetic]] rather than by adding up all the digits, a procedure that can save time in the case of very large numbers.

Digital roots can be used as a sort of [[checksum]]. For example, since the digital root of a sum is always equal to the digital root of the sum of the summands' digital roots. A person adding long columns of large numbers will often find it reassuring to apply [[casting out nines]] to his result—knowing that this technique will catch the majority of errors.

Digital roots are used in Western [[numerology]], but certain numbers deemed to have occult significance (such as 11 and 22) are not always completely reduced to a single digit.

The number of times the digits must be summed to reach the digital sum is called a number's additive [[Persistence of a number|persistence]]; in the above example, the additive persistence of 65,536 is 2.

== Significance and formula of the digital root ==
It helps to see the digital root of a positive integer as the position it holds with respect to the last multiple of nine less than it. For example, the digital root of 11 is 2, which means that 11 is the second number after 9. The digital root of 2035 is 1, which means that 2035&amp;nbsp;−&amp;nbsp;1 is a multiple of nine. A digital root of nine means that the number is a multiple of nine, which is equivalent to a digital root of 9.

With this in mind the digital root of a positive integer &lt;math&gt;n&lt;/math&gt; may be defined by using [[floor function]] &lt;math&gt;\lfloor x\rfloor &lt;/math&gt;, as
:&lt;math&gt;dr(n)=n-9\left\lfloor\frac{n-1}{9}\right\rfloor.&lt;/math&gt;

== Abstract multiplication of digital roots ==
The table below shows the digital roots produced by the familiar [[multiplication table]] in the decimal system.
&lt;center&gt;
{|class=&quot;wikitable&quot; style=&quot;text-align:center; width:270px; height:270px&quot; border=&quot;1&quot;
! dr !! 1 !! 2 !! 3 !! 4 !! 5 !! 6 !! 7 !! 8 !! 9
|-
! 1
|1||2||3||4||5||6||7||8||9
|-
!2
|2||4||6||8||1||3||5||7||9
|-
!3
|3||6||9||3||6||9||3||6||9
|-
!4
|4||8||3||7||2||6||1||5||9
|-
!5
|5||1||6||2||7||3||8||4||9
|-
!6
|6||3||9||6||3||9||6||3||9
|-
!7
|7||5||3||1||8||6||4||2||9
|-
!8
|8||7||6||5||4||3||2||1||9
|-
!9
|9||9||9||9||9||9||9||9||9
|-
|}
&lt;/center&gt;

The table shows a number of interesting [[patterns]] and [[symmetries]] and is known as the [[Vedic square]].

== Formal definition ==
Let &lt;math&gt;S(n)&lt;/math&gt; denote the sum of the digits of &lt;math&gt;n&lt;/math&gt; and let the composition of &lt;math&gt;S(n)&lt;/math&gt; as follows:
:&lt;math&gt;S^{1}(n)=S(n),\ \ S^{m}(n)=S(S^{m-1}(n)),\ \text{for}\ m\ge2.&lt;/math&gt;
Eventually the sequence &lt;math&gt;S^{1}(n),S^{2}(n),S^{3}(n),\dotsb&lt;/math&gt; becomes a one digit number. Let &lt;math&gt;S^{\sigma}(n)&lt;/math&gt; (the digital sum of &lt;math&gt;n&lt;/math&gt;) represent this a one digit number.

=== Example ===
Let us find the digital sum of &lt;math&gt;1853&lt;/math&gt;.

:&lt;math&gt;S(1853)=17\,&lt;/math&gt;

:&lt;math&gt;S(17)=8\,&lt;/math&gt;

Thus,

:&lt;math&gt;S^{2}(1853)=8.\,&lt;/math&gt;

For simplicity let us agree simply that

:&lt;math&gt;S^{\sigma}(1853)=dr(1853)=8.\,&lt;/math&gt;

=== Proof that a constant value exists ===
How do we know that the sequence &lt;math&gt;S^{1}(n),S^{2}(n),S^{3}(n),\dotsb&lt;/math&gt; eventually becomes a one digit number? Here's a proof:

Let &lt;math&gt;n=d_1+10d_2+\dotsb+10^{m-1}d_m&lt;/math&gt;, for all &lt;math&gt;i&lt;/math&gt;, &lt;math&gt;d_i&lt;/math&gt; is an [[integer]] greater than or equal to 0 and less than 10. Then, &lt;math&gt;S(n)=d_1+d_2+\dotsb+d_m&lt;/math&gt;. This means that &lt;math&gt;S(n)&lt;n&lt;/math&gt;, unless &lt;math&gt;d_2,d_3,\dotsb,d_m=0&lt;/math&gt;, in which case &lt;math&gt;n&lt;/math&gt; is a one digit number. Thus, repeatedly using the &lt;math&gt;S(n)&lt;/math&gt; function would cause &lt;math&gt;n&lt;/math&gt; to decrease by at least 1, until it becomes a one digit number, at which point it will stay constant, as &lt;math&gt;S(d_1)=d_1&lt;/math&gt;.

==Congruence formula==
The formula is:
:&lt;math&gt; \operatorname{dr}(n) = \begin{cases}0 &amp; \mbox{if}\ n = 0, \\ 9 &amp; \mbox{if}\ n \neq 0,\ n\ \equiv 0\pmod{9},\\ n\ {\rm mod}\ 9 &amp; \mbox{if}\ n \not\equiv 0\pmod{9}.\end{cases}&lt;/math&gt;
or,
:&lt;math&gt; \mbox{dr}(n) = 1\ +\ ((n-1)\ {\rm mod}\ 9).\ &lt;/math&gt;

To generalize the concept of digital roots to other bases ''b'', one can simply change the 9 in the formula to ''b'' - 1.

The digital root is the value modulo 9 because &lt;math&gt;10 \equiv 1\pmod{9},&lt;/math&gt; and thus &lt;math&gt;10^k \equiv 1^k \equiv 1\pmod{9},&lt;/math&gt; so regardless of position, the value mod 9 is the same – &lt;math&gt;a\cdot 100 \equiv a\cdot 10 \equiv a\pmod{9}&lt;/math&gt; – which is why digits can be meaningfully added. Concretely, for a three-digit number,
:&lt;math&gt;\mbox{dr}(abc) \equiv a\cdot 10^2 + b\cdot 10 + c \cdot 1 \equiv a\cdot 1 + b\cdot 1 + c \cdot 1 \equiv a + b + c \pmod{9}&lt;/math&gt;

To obtain the modular value with respect to other numbers ''n,'' one can take [[weighted sum]]s, where the weight on the ''k''th digit corresponds to the value of &lt;math&gt;10^k&lt;/math&gt; modulo ''n,'' or analogously for &lt;math&gt;b^k&lt;/math&gt; for different bases. This is simplest for 2, 5, and 10, where higher digits vanish (since 2 and 5 divide 10), which corresponds to the familiar fact that the divisibility of a decimal number with respect to 2, 5, and 10 can be checked by the last digit (even numbers end in 0, 2, 4, 6, or 8).

Also of note is &lt;math&gt;n=11:&lt;/math&gt; since &lt;math&gt;10 \equiv -1\pmod{11},&lt;/math&gt; and thus &lt;math&gt;10^2 \equiv (-1)^2 \equiv 1\pmod{11},&lt;/math&gt; taking the ''alternating'' sum of digits yields the value modulo 11.

== Some properties of digital roots ==
The digital root of a number is zero if and only if the number is itself zero.
:&lt;math&gt;\mathit{dr}(n)=0 \Leftrightarrow n=0.&lt;/math&gt;
The digital root of a number is a positive integer if and only if the number is itself a positive integer.
:&lt;math&gt;\mathit{dr}(n)&gt;0 \Leftrightarrow n&gt;0.&lt;/math&gt;
The digital root of ''n'' is ''n'' itself if and only if the number has exactly one digit.
:&lt;math&gt;\mathit{dr}(n)=n \Leftrightarrow n \in \{0,1,2,3,4,5,6,7,8,9\}.&lt;/math&gt;
The digital root of ''n'' is less than ''n'' if and only if the number is greater than or equal to 10.
:&lt;math&gt;\mathit{dr}(n)&lt;n \Leftrightarrow n \ge 10.&lt;/math&gt;
The digital root of ''a'' + ''b'' is congruent with the sum of the digital root of ''a'' and the digital root of ''b'' modulo 9.
:&lt;math&gt;\mathit{dr}(a+b) \equiv \mathit{dr}(a)+\mathit{dr}(b) \pmod{9}.&lt;/math&gt;
The digital root of ''a'' - ''b'' is congruent with the difference of the digital root of ''a'' and the digital root of ''b'' modulo 9.
:&lt;math&gt;\mathit{dr}(a-b) \equiv \mathit{dr}(a)-\mathit{dr}(b) \pmod{9}.&lt;/math&gt;
The digital root of ''a'' &amp;times; ''b'' is congruent with the multiple of the digital root of ''a'' and the digital root of ''b'' modulo 9.
:&lt;math&gt;\mathit{dr}(a \times b) \equiv \mathit{dr}(a)\times\mathit{dr}(b) \pmod{9}.&lt;/math&gt;
*The digital root of a nonzero number is 9 if and only if the number is itself a [[multiple (mathematics)|multiple]] of 9.
:&lt;math&gt;\mathit{dr}(n)=9 \Leftrightarrow n=9m \ \ \ \text{for}\  m=1,2,3,\cdots.&lt;/math&gt;
*The digital root of a nonzero number is a multiple of 3 if and only if the number is itself a multiple of 3.
:&lt;math&gt;\begin{align} \mathit{dr}(n) &amp;=3 \Leftrightarrow n=9m+3 &amp; \ \text{for}\  m=0,1,2,\cdots,\\ \mathit{dr}(n) &amp;=6 \Leftrightarrow n=9m+6 &amp; \ \text{for}\  m=0,1,2,\cdots,\\ \mathit{dr}(n) &amp;=9 \Leftrightarrow n=9m   &amp; \ \text{for}\  m=1,2,3,\cdots.\end{align}&lt;/math&gt;
*The digital root of a [[factorial]] ≥ 6! is 9.
:&lt;math&gt;\mathit{dr}(n!)=9 \Leftrightarrow n \ge 6.&lt;/math&gt;
*The digital root of a [[square number|square]] is 1, 4, 7, or 9. Digital roots of square numbers progress in the sequence 1, 4, 9, 7, 7, 9, 4, 1, 9.
*The digital root of a [[perfect cube]] is 1, 8 or 9, and digital roots of perfect cubes progress in that exact sequence.
*The digital root of a [[prime number]] (except 3) is 1, 2, 4, 5, 7, or 8.
*The digital root of a [[power of two|power of 2]] is 1, 2, 4, 5, 7, or 8. Digital roots of the powers of 2 progress in the sequence 1, 2, 4, 8, 7, 5. This even applies to negative powers of 2; for example, 2 to the power of 0 is 1; 2 to the power of -1 (minus one) is .5, with a digital root of 5; 2 to the power of -2 is .25, with a digital root of 7; and so on, ad infinitum in both directions. This is because negative powers of 2 share the same digits (after removing leading zeroes) as corresponding positive powers of 5, whose digital roots progress in the sequence 1, 5, 7, 8, 4, 2.
* The digital root of a power of 5 is 1, 2, 4, 5, 7 or 8. Digital roots of the powers of 5 progress in the secuence 1, 5, 7, 8, 4, 2. This even applies to negative powers of 5; for example, 5 to the power of 0 is 1; 5 to the power of -1 (minus one) is .2, with a digital root of 2; 5 to the power of -2 is .04, with a digital root of 4; and so on, ad infinitum in both directions. This is because the negative powers of 5 share the same digits (after removing leading zeroes) as corresponding positive powers of 2, whose digital roots progress in sequence 1, 2, 4, 8, 7, 5.
* The digital roots of powered numbers progress in sequence (only certain for positive powers, although in for some exceptions it also may occur for negative powers), and this is because of one of the previously shown properties. As the digital root of ''a''  ''b'' is congruent with the multiple of the digital root of ''a'' and the digital root of ''b'' modulo 9, the digital root of ''a''  ''a'' will also do it. So, for example, as shown above, powers of 2 will follows the sequence 1, 2, 4, 8, 7, 5; Powers of 29 (whose digital root is 2) will also follow this sequence. The very sequence follows this rule, and is appliable to any othe number.
:&lt;math&gt;\mathit{dr}(a^n) \equiv \mathit{dr}^n(a) \pmod{9}.&lt;/math&gt;
*The digital root of an even [[perfect number]] (except 6) is 1.
*The digital root of a [[star number]] is 1 or 4. Digital roots of star numbers progress in the sequence 1, 4, 1.
*The digital root of a [[triangular number]] is 1, 3, 6 or 9. Digital roots of triangular numbers progress in the sequence 1, 3, 6, 1, 6, 3, 1, 9, 9.
*The digital root of [[Fibonacci number]]s is a repeating pattern of 1, 1, 2, 3, 5, 8, 4, 3, 7, 1, 8, 9, 8, 8, 7, 6, 4, 1, 5, 6, 2, 8, 1, 9.
*The digital root of [[Lucas number]]s is a repeating pattern of 2, 1, 3, 4, 7, 2, 9, 2, 2, 4, 6, 1, 7, 8, 6, 5, 2, 7, 9, 7, 7, 5, 3, 8.
*The digital root of the product of [[twin prime]]s, other than 3 and 5, is 8. The digital root of the product of 3 and 5 (twin primes) is 6.

== See also ==
* [[Digit sum]]
* [[Vedic square]]

==References==
*[[F. M. Hall]]: ''An Introduction into Abstract Algebra''. 2nd edition, CUP ARchive 1980, ISBN 978-0-521-29861-2, p.&amp;nbsp;101 ({{Google books|qqs8AAAAIAAJ|online copy|page=101}})
*Bonnie Averbach, Orin Chein: ''Problem Solving Through Recreational Mathematics''. Courier Dover Publications 2000, ISBN 0-486-40917-1, pp.&amp;nbsp;125–127 ({{Google books|xRJxJ7L9sq8C|online copy|page=125}})
*Talal Ghannam: ''The Mystery of Numbers: Revealed Through Their Digital Root''. CreateSpace Publications 2012, ISBN 978-1477678411, pp.&amp;nbsp;68–73
*T. H. O'Beirne: ''Puzzles and Paradoxes''. In: ''New Scientist'', No. 230, 1961-4-13, pp.&amp;nbsp;53–54 ({{Google books|j4VdAP43V7cC|online copy|page=53}})¨

== External links ==
* [http://people.revoledu.com/kardi/tutorial/DigitSum/index.html pattern of digital root using MS Excel]
*{{MathWorld|title=Digital Root|id=DigitalRoot}}

[[Category:Algebra]]
[[Category:Number theory]]

[[de:Quersumme#Einstellige (oder iterierte) Quersumme]]</text>
      <sha1>jvi91unu447lsoc3tv4oy8pqljd33m0</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Smooth algebra</title>
    <ns>0</ns>
    <id>36088541</id>
    <revision>
      <id>611722873</id>
      <parentid>590172520</parentid>
      <timestamp>2014-06-05T19:31:43Z</timestamp>
      <contributor>
        <username>BTotaro</username>
        <id>13888337</id>
      </contributor>
      <minor/>
      <comment>Links added</comment>
      <text xml:space="preserve" bytes="2754">
In [[algebra]], a commutative ''k''-algebra ''A'' is said to be '''0-smooth''' if it satisfies the following lifting property: given a ''k''-algebra ''C'', an ideal ''N'' of ''C'' whose square is zero and a ''k''-algebra map  &lt;math&gt;u: A \to C/N&lt;/math&gt;, there exists a ''k''-algebra map &lt;math&gt;v: A \to C&lt;/math&gt; such that ''u'' is ''v'' followed by the canonical map. If there exists at most one such lifting ''v'', then ''A'' is said to be '''0-unramified''' (or '''0-neat'''). ''A'' is said to be '''0-étale''' if it is '''0-smooth''' and '''0-unramified'''.

A finitely generated ''k''-algebra ''A'' is 0-smooth over ''k'' if and only if Spec ''A'' is a [[smooth scheme]] over ''k''.

A [[separable extension|separable]] algebraic field extension ''L'' of ''k'' is 0-étale over ''k''.&lt;ref&gt;{{harvnb|Matsumura|1986|loc=Theorem 25.3}}&lt;/ref&gt; The formal power series ring &lt;math&gt;k[\![t_1, \ldots, t_n]\!]&lt;/math&gt; is 0-smooth only when &lt;math&gt;\operatorname{char}k = p &gt; 0&lt;/math&gt; and &lt;math&gt;[k: k^p] &lt; \infty&lt;/math&gt; (i.e., ''k'' has a finite [[p-basis|''p''-basis]].)&lt;ref&gt;{{harvnb|Matsumura|1986|loc=pg. 215}}&lt;/ref&gt;

== ''I''-smooth ==
Let ''B'' be an ''A''-algebra and suppose ''B'' is given the ''I''-adic topology, ''I'' an ideal of ''B''. We say ''B'' is '''''I''-smooth over ''A''''' if it satisfies the lifting property: given an ''A''-algebra ''C'', an ideal ''N'' of ''C'' whose square is zero and an ''A''-algebra map  &lt;math&gt;u: B \to C/N&lt;/math&gt; that is continuous when &lt;math&gt;C/N&lt;/math&gt; is given the discrete topology, there exists an ''A''-algebra map &lt;math&gt;v: B \to C&lt;/math&gt; such that ''u'' is ''v'' followed by the canonical map. As before, if there exists at most one such lift ''v'', then ''B'' is said to be '''''I''-unramified over ''A''''' (or '''''I''-neat'''). ''B'' is said to be '''''I''-étale''' if it is '''''I''-smooth''' and '''''I''-unramified'''. If ''I'' is the zero ideal and ''A'' is a field, these notions coincide with 0-smooth etc. as defined above.

A standard example is this: let ''A'' be a ring, &lt;math&gt;B = A[\![t_1, \ldots, t_n]\!]&lt;/math&gt; and &lt;math&gt;I = (t_1, \ldots, t_n).&lt;/math&gt; Then ''B'' is ''I''-smooth over ''A''.

Let ''A'' be a noetherian local ''k''-algebra with maximal ideal &lt;math&gt;\mathfrak{m}&lt;/math&gt;. Then ''A'' is &lt;math&gt;\mathfrak{m}&lt;/math&gt;-smooth over ''k'' if and only if &lt;math&gt;A \otimes_k k'&lt;/math&gt; is a regular ring for any finite extension field &lt;math&gt;k'&lt;/math&gt; of ''k''.&lt;ref&gt;{{harvnb|Matsumura|1986|loc=Theorem 28.7}}&lt;/ref&gt;

== See also ==
*[[étale morphism]]
*[[formally smooth morphism]]

== References ==
{{reflist}}
* H. Matsumura ''Commutative ring theory.'' Translated from the Japanese by M. Reid. Second edition. Cambridge Studies in Advanced Mathematics, 8.

[[Category:Algebra]]

{{algebra-stub}}</text>
      <sha1>oi02ho80ya4nmfin2f7axnbm7f2gnsc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Elementary algebra</title>
    <ns>0</ns>
    <id>9710</id>
    <revision>
      <id>612515709</id>
      <parentid>612515483</parentid>
      <timestamp>2014-06-11T16:15:47Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <minor/>
      <comment>/* Inconsistent systems */ typos</comment>
      <text xml:space="preserve" bytes="42308">{{multiple issues|{{cleanup-reorganize|date=August 2012}}
{{confusing|date=August 2012}}
{{overly detailed|date=August 2012}}
{{globalize|date=January 2013}}
}}

[[File:Quadratic root.svg|thumb|right|240px|The [[quadratic formula]], which is the solution to the [[quadratic equation]] &lt;math&gt;ax^2+bx+c=0&lt;/math&gt; . Here the symbols &lt;math&gt;a,b,c,x&lt;/math&gt; all are variables that represent numbers.]]
[[File:Polynomialdeg2.svg|thumb|right|200px|Two-dimensional plot (red curve) of the algebraic equation &lt;math&gt;y = x^2 - x - 2&lt;/math&gt;]]

'''Elementary algebra''' encompasses some of the basic concepts of [[algebra]], one of the main branches of [[mathematics]]. It is typically taught to [[secondary school]] students and builds on their understanding of [[arithmetic]]. Whereas arithmetic deals with specified numbers,&lt;ref&gt;H.E. Slaught and N.J. Lennes, ''Elementary algebra'', Publ. Allyn and Bacon, 1915, [http://books.google.co.uk/books?id=gLii_eO4dNsC&amp;lpg=PA1&amp;dq=%22Elementary%20algebra%22%20letters&amp;pg=PA1#v=onepage&amp;q&amp;f=false page 1] (republished by Forgotten Books)&lt;/ref&gt; algebra introduces quantities without fixed values, known as variables.&lt;ref&gt;Lewis Hirsch, Arthur Goodman, ''Understanding Elementary Algebra With Geometry: A Course for College Students'', Publisher: Cengage Learning, 2005, ISBN 0534999727, 9780534999728, 654 pages, [http://books.google.co.uk/books?id=7hdK4RSub5cC&amp;lpg=PA2&amp;dq=what%20is%20algebra%20for&amp;pg=PA2#v=onepage&amp;q=generalization&amp;f=false page 2]&lt;/ref&gt; This use of variables entails a use of algebraic notation and an understanding of the general rules of the [[Operation (mathematics)|operator]]s introduced in arithmetic.  Unlike [[abstract algebra]], elementary algebra is not concerned with [[algebraic structures]] outside the realm of [[real number|real]] and [[complex number]]s.

The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Most quantitative results in science and mathematics are expressed as algebraic [[equation]]s.

==Algebraic notation ==
{{main|Mathematical notation}}
Algebraic notation describes how algebra is written. It follows certain rules and conventions, and has its own terminology. For example, the expression &lt;math style=&quot;margin-bottom:8px&quot;&gt;3x^2 - 2xy + c&lt;/math&gt; has the following components:
&lt;center&gt;
[[File:algebraic equation notation.svg|256px]]&lt;br&gt;
1 : [[Exponent]] (power), 2 : [[Coefficient]], 3 : [[Addend|term]], 4 : [[Operation (mathematics)|operator]], 5 : [[Constant (mathematics)|constant]], &lt;math&gt;x, y&lt;/math&gt; : [[Variable (mathematics)|variable]]s
&lt;/center&gt;

A ''coefficient'' is a numerical value which multiplies a variable (the operator is omitted). A ''term'' is an [[Addend#Notation and terminology|addend or a summand]], a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators.&lt;ref&gt;Richard N. Aufmann, Joanne Lockwood, ''Introductory Algebra: An Applied Approach'', Publisher Cengage Learning, 2010, ISBN 1439046042, 9781439046043, [http://books.google.co.uk/books?id=MPIWikTHVXQC&amp;lpg=PP1&amp;ots=yG1m9DkIiH&amp;dq=coefficient%20algebra&amp;pg=PA78#v=onepage&amp;q=coefficient%20&amp;f=false page 78]&lt;/ref&gt; Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. &lt;math&gt;a, b, c&lt;/math&gt;) are typically used to represent [[Mathematical constant|constant]]s, and those toward the end of the alphabet (e.g. &lt;math&gt;x, y&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt;) are used to represent [[Variable (mathematics)|variable]]s.&lt;ref&gt;William L. Hosch (editor), ''The Britannica Guide to Algebra and Trigonometry'', Britannica Educational Publishing, The Rosen Publishing Group, 2010, ISBN 1615302190, 9781615302192, [http://books.google.co.uk/books?id=ad0P0elU1_0C&amp;lpg=PA71&amp;dq=elementary%20algebra%20letters%20alphabet%20constants%20variables&amp;pg=PA71#v=onepage&amp;q=letters&amp;f=false page 71]&lt;/ref&gt; They are usually written in italics.&lt;ref&gt;James E. Gentle, ''Numerical Linear Algebra for Applications in Statistics'', Publisher: Springer, 1998, ISBN 0387985425, 9780387985428, 221 pages, [James E. Gentle page 183]&lt;/ref&gt;

[[Algebraic operation]]s work in the same way as [[Arithmetic#Algebraic operations|arithmetic operation]]s,&lt;ref&gt;Horatio Nelson Robinson, ''New elementary algebra: containing the rudiments of science for schools and academies'', Ivison, Phinney, Blakeman, &amp; Co., 1866, [http://books.google.co.uk/books?id=dKZXAAAAYAAJ&amp;dq=Elementary%20algebra%20notation&amp;pg=PA7#v=onepage&amp;q=Elementary%20algebra%20notation&amp;f=false page 7]&lt;/ref&gt; such as [[addition]], [[subtraction]], [[multiplication]], [[Division (mathematics)|division]] and [[exponentiation]].&lt;ref&gt;Ron Larson, Robert Hostetler, Bruce H. Edwards, ''Algebra And Trigonometry: A Graphing Approach'', Publisher: Cengage Learning, 2007, ISBN 061885195X, 9780618851959, 1114 pages, [http://books.google.co.uk/books?id=5iXVZHhkjAgC&amp;lpg=PA6&amp;ots=iwrSrCrrOb&amp;dq=operations%20addition%2C%20subtraction%2C%20multiplication%2C%20division%20exponentiation.&amp;pg=PA6#v=onepage&amp;q=operations%20addition,%20subtraction,%20multiplication,%20division%20exponentiation.&amp;f=false page 6]&lt;/ref&gt; and are applied to algebraic variables and terms.  Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a [[coefficient]] is used. For example, &lt;math style=&quot;margin-bottom:8px&quot;&gt;3 \times x^2&lt;/math&gt; is written as &lt;math style=&quot;margin-bottom:8px&quot;&gt;3x^2&lt;/math&gt;, and &lt;math&gt;2 \times x \times y&lt;/math&gt; may be written &lt;math&gt;2xy&lt;/math&gt;.&lt;ref&gt;Sin Kwai Meng, Chip Wai Lung, Ng Song Beng, &quot;Algebraic notation&quot;, in ''Mathematics Matters Secondary 1 Express Textbook'', Publisher Panpac Education Pte Ltd, ISBN 9812738827, 9789812738820, [http://books.google.co.uk/books?id=nL5ObMmDvPEC&amp;lpg=PR9-IA8&amp;ots=T_h6l40AE5&amp;dq=%22Algebraic%20notation%22%20multiplication%20omitted&amp;pg=PR9-IA8#v=onepage&amp;q=%22Algebraic%20notation%22%20multiplication%20omitted&amp;f=false page 68]&lt;/ref&gt;

Usually terms with the highest power ([[Exponentiation|exponent]]), are written on the left, for example, &lt;math style=&quot;margin-bottom:8px&quot;&gt;x^2&lt;/math&gt; is written to the left of &lt;math&gt;x&lt;/math&gt;. When a coefficient is one, it is usually omitted (e.g. &lt;math style=&quot;margin-bottom:8px&quot;&gt;1x^2&lt;/math&gt; is written &lt;math style=&quot;margin-bottom:8px&quot;&gt;x^2&lt;/math&gt;).&lt;ref&gt;David Alan Herzog, ''Teach Yourself Visually Algebra'', Publisher John Wiley &amp; Sons, 2008, ISBN 0470185597, 9780470185599, 304 pages, [http://books.google.co.uk/books?id=Igs6t_clf0oC&amp;lpg=PA72&amp;ots=Excnhf1AgW&amp;dq=algebra%20coefficient%20one&amp;pg=PA72#v=onepage&amp;q=coefficient%20of%201&amp;f=false page 72]&lt;/ref&gt; Likewise when the exponent (power) is one, (e.g. &lt;math style=&quot;margin-bottom:8px&quot;&gt;3x^1&lt;/math&gt; is written &lt;math style=&quot;margin-bottom:8px&quot;&gt;3x&lt;/math&gt;).&lt;ref&gt;John C. Peterson, ''Technical Mathematics With Calculus'', Publisher Cengage Learning, 2003, ISBN 0766861899, 9780766861893, 1613 pages, [http://books.google.co.uk/books?id=PGuSDjHvircC&amp;lpg=PA31&amp;ots=NKrtZZ1KDE&amp;dq=%22when%20the%20exponent%20is%201%22&amp;pg=PA32#v=onepage&amp;q=%22when%20the%20exponent%20is%201%22&amp;f=false page 31]&lt;/ref&gt; When the exponent is zero, the result is always 1 (e.g. &lt;math style=&quot;margin-bottom:8px&quot;&gt;x^0&lt;/math&gt; is always rewritten to &lt;math&gt;1&lt;/math&gt;).&lt;ref&gt;Jerome E. Kaufmann, Karen L. Schwitters, ''Algebra for College Students'', Publisher Cengage Learning, 2010, ISBN 0538733543, 9780538733540, 803 pages, [http://books.google.co.uk/books?id=-AHtC0IYMhYC&amp;lpg=PP1&amp;ots=kL8erjajyR&amp;dq=algebra%20exponents%20zero%20one&amp;pg=PA222#v=onepage&amp;q=exponents%20&amp;f=false page 222]&lt;/ref&gt; However &lt;math&gt;0^0&lt;/math&gt;, being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.

===Alternative notation===
Other types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. &lt;math style=&quot;margin-bottom:8px&quot;&gt;x^2&lt;/math&gt;. In [[plain text]], and in the [[TeX]] mark-up language, the [[caret]] symbol &quot;^&quot; represents exponents, so &lt;math style=&quot;margin-bottom:8px&quot;&gt;x^2&lt;/math&gt; is written as &quot;x^2&quot;.&lt;ref&gt;Ramesh Bangia, ''Dictionary of Information Technology'', Publisher Laxmi Publications, Ltd., 2010, ISBN 9380298153, 9789380298153, [http://books.google.co.uk/books?id=zQa5I2sHPKEC&amp;lpg=PA212&amp;ots=s6pWav1Z_D&amp;dq=%22plain%20text%22%20math%20caret%20exponent&amp;pg=PA212#v=onepage&amp;q=exponentiation%20caret&amp;f=false page 212]&lt;/ref&gt;&lt;ref&gt;George Grätzer, ''First Steps in LaTeX'', Publisher Springer, 1999, ISBN 0817641327, 9780817641320, [http://books.google.co.uk/books?id=mLdg5ZdDKToC&amp;lpg=PP1&amp;ots=V9DFIaAAh0&amp;dq=tex%20math&amp;pg=PA17#v=onepage&amp;q=subscripts%20and%20superscripts%20caret&amp;f=false page 17]&lt;/ref&gt; In programming languages such as [[Ada (programming language)|Ada]],&lt;ref&gt;S. Tucker Taft, Robert A. Duff, Randall L. Brukardt, Erhard Ploedereder, Pascal Leroy, ''Ada 2005 Reference Manual'', Volume 4348 of Lecture Notes in Computer Science, Publisher Springer, 2007, ISBN 3540693351, 9783540693352, [http://books.google.co.uk/books?id=694P3YtXh-0C&amp;lpg=PA718&amp;ots=O_EgQ75FeB&amp;dq=ada%20%20asterisk&amp;pg=PA12#v=onepage&amp;q=double%20star%20exponentiate&amp;f=false page 13]&lt;/ref&gt; [[Fortran]],&lt;ref&gt;C. Xavier, ''Fortran 77 And Numerical Methods'', Publisher New Age International, 1994, ISBN 812240670X, 9788122406702, [http://books.google.co.uk/books?id=WYMgF9WFty0C&amp;lpg=PA20&amp;ots=BTtzs9F-NB&amp;dq=fortran%20asterisk%20exponentiation&amp;pg=PA20#v=onepage&amp;q=fortran%20asterisk%20exponentiation&amp;f=false page 20]&lt;/ref&gt; [[Perl]],&lt;ref&gt;Randal Schwartz, Brian Foy, Tom Phoenix, ''Learning Perl'', Publisher O'Reilly Media, Inc., 2011, ISBN 1449313140, 9781449313142, [http://books.google.co.uk/books?id=l2IwEuRjeNwC&amp;lpg=PA24&amp;ots=5nsYOLHxlD&amp;dq=perl%20asterisk%20exponentiation&amp;pg=PA24#v=onepage&amp;q=double%20asterisk%20exponentiation&amp;f=false page 24]&lt;/ref&gt; [[Python (programming language)|Python]] &lt;ref&gt;Matthew A. Telles, ''Python Power!: The Comprehensive Guide'', Publisher Course Technology PTR, 2008, ISBN 1598631586, 9781598631586, [http://books.google.co.uk/books?id=754knV_fyf8C&amp;lpg=PA46&amp;ots=8fEi1F-H8-&amp;dq=python%20asterisk%20exponentiation&amp;pg=PA46#v=onepage&amp;q=double%20asterisk%20exponentiation&amp;f=false page 46]&lt;/ref&gt; and [[Ruby (programming language)|Ruby]],&lt;ref&gt;Kevin C. Baird, ''Ruby by Example: Concepts and Code'', Publisher No Starch Press, 2007, ISBN 1593271484, 9781593271480, [http://books.google.co.uk/books?id=kq2dBNdAl3IC&amp;lpg=PA72&amp;ots=0UU3k-Pvh8&amp;dq=ruby%20asterisk%20exponentiation&amp;pg=PA72#v=onepage&amp;q=double%20asterisk%20exponentiation&amp;f=false page 72]&lt;/ref&gt; a double asterisk is used, so &lt;math style=&quot;margin-bottom:8px&quot;&gt;x^2&lt;/math&gt; is written as &quot;x**2&quot;. Many programming languages and calculators use a single asterisk to represent the multiplication symbol,&lt;ref&gt;William P. Berlinghoff, Fernando Q. Gouvêa, ''Math through the Ages: A Gentle History for Teachers and Others'', Publisher MAA, 2004, ISBN 0883857367, 9780883857366, [http://books.google.co.uk/books?id=JAXNVaPt7uQC&amp;lpg=PA75&amp;ots=-P78Lrz792&amp;dq=calculator%20asterisk%20multiplication&amp;pg=PA75#v=onepage&amp;q=calculator%20asterisk%20multiplication&amp;f=false page 75]&lt;/ref&gt; and it must be explicitly used, for example, &lt;math style=&quot;margin-bottom:8px&quot;&gt;3x&lt;/math&gt; is written &quot;3*x&quot;.

==Concepts==

===Variables===
[[File:Pi-equals-circumference-over-diametre.svg|thumb|right|Example of variables showing the relationship between a circle's diameter and its circumference. For any [[circle]], its [[circumference]] &lt;math&gt;c&lt;/math&gt;, divided by its [[diameter]] &lt;math&gt;d&lt;/math&gt;, is equal to the constant [[pi]], &lt;math&gt;\pi&lt;/math&gt; (approximately 3.14).]]
{{Main|Variable (mathematics)}}
Elementary algebra builds on and extends arithmetic&lt;ref&gt;Thomas Sonnabend, ''Mathematics for Teachers: An Interactive Approach for Grades K-8'', Publisher: Cengage Learning, 2009, ISBN 0495561665, 9780495561668, 759 pages, [http://books.google.co.uk/books?id=gBa2GzyXCF8C&amp;lpg=PR17&amp;ots=qee3RsTC6V&amp;dq=algebra%20%22extends%20arithmetic%22&amp;pg=PR17#v=onepage&amp;q=extends%20arithmetic&amp;f=false page xvii]&lt;/ref&gt; by introducing letters called variables to represent general (non-specified) numbers.  This is useful for several reasons.

#'''Variables may represent numbers whose values are not yet known'''. For example, if the temperature today, ''T'', is 20 degrees higher than the temperature yesterday, ''Y'', then the problem can be described algebraically as &lt;math&gt;T = Y + 20&lt;/math&gt;.&lt;ref&gt;Lewis Hirsch, Arthur Goodman, ''Understanding Elementary Algebra With Geometry: A Course for College Students'', Publisher: Cengage Learning, 2005, ISBN 0534999727, 9780534999728, 654 pages, [http://books.google.co.uk/books?id=jsT7kqZubvIC&amp;lpg=PA48&amp;ots=EI4_yaKasG&amp;dq=%22elementary%20algebra%22%20variables%20unknown&amp;pg=PA48#v=onepage&amp;q=%22elementary%20algebra%22%20variables%20unknown&amp;f=false page 48]&lt;/ref&gt;
#'''Variables allow one to describe ''general'' problems,&lt;ref&gt;Lawrence S. Leff, ''College Algebra: Barron's Ez-101 Study Keys'', Publisher: Barron's Educational Series, 2005, ISBN 0764129147, 9780764129148, 230 pages, [http://books.google.co.uk/books?id=XesryURrNKAC&amp;lpg=PA2&amp;ots=Ga44CTvNHI&amp;dq=algebra%20variables%20generalize&amp;pg=PA2#v=onepage&amp;q=algebra%20variables%20generalize&amp;f=false page 2]&lt;/ref&gt; without specifying the values of the quantities that are involved.''' For example, it can be stated specifically that 5 minutes is equivalent to &lt;math&gt;60 \times 5 = 300&lt;/math&gt; seconds. A more general (algebraic) description may state that the number of seconds, &lt;math&gt;s = 60 \times m&lt;/math&gt;, where m is  the number of minutes.
#'''Variables allow one to describe mathematical relationships between quantities that may vary.'''&lt;ref&gt;Ron Larson, Kimberly Nolting, ''Elementary Algebra'', Publisher: Cengage Learning, 2009, ISBN 0547102275, 9780547102276, 622 pages, [http://books.google.co.uk/books?id=U6v78M5nYKAC&amp;lpg=PP1&amp;ots=R0dl97lfm0&amp;dq=%22elementary%20algebra%22%20relationships&amp;pg=PA210#v=onepage&amp;q=relationships&amp;f=false page 210]&lt;/ref&gt; For example, the relationship between the circumference, ''c'', and diameter, ''d'', of a circle is described by &lt;math&gt;\pi = c /d&lt;/math&gt;.
#'''Variables allow one to describe some mathematical properties.''' For example, a basic property of addition is [[commutativity]] which states that the order of numbers being added together does not matter. Commutativity is stated algebraically as &lt;math&gt;(a + b) = (b + a)&lt;/math&gt;.&lt;ref&gt;Charles P. McKeague, ''Elementary Algebra'', Publisher: Cengage Learning, 2011, ISBN 0840064217, 9780840064219, 571 pages, [http://books.google.co.uk/books?id=etTbP0rItQ4C&amp;lpg=PA49&amp;ots=I16eebO3LV&amp;dq=%22elementary%20algebra%22%20commutative&amp;pg=PA49#v=onepage&amp;q=%22elementary%20algebra%22%20commutative&amp;f=false page 49]&lt;/ref&gt;

=== Evaluating expressions ===
{{Main|Expression (mathematics)}}
Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations ([[addition]], [[subtraction]], [[multiplication]], [[Division (mathematics)|division]] and [[exponentiation]]). For example,
*Added terms are simplified using coefficients. For example &lt;math&gt;x + x + x&lt;/math&gt; can be simplified as &lt;math&gt;3x&lt;/math&gt; (where 3 is the coefficient).
*Multiplied terms are simplified using exponents. For example &lt;math&gt;x \times x \times x&lt;/math&gt; is represented as &lt;math&gt;x^3&lt;/math&gt;
*Like terms are added together,&lt;ref&gt;Andrew Marx, ''Shortcut Algebra I: A Quick and Easy Way to Increase Your Algebra I Knowledge and Test Scores'', Publisher Kaplan Publishing, 2007, ISBN 1419552880, 9781419552885, 288 pages, [http://books.google.co.uk/books?id=o9GYQjZ7ZwUC&amp;lpg=PP1&amp;ots=pT-MpWMJty&amp;dq=algebra%20addition%20%22like%20terms%22&amp;pg=PA51#v=onepage&amp;q=like%20terms&amp;f=false page 51]&lt;/ref&gt; for example, &lt;math&gt;2x^2 + 3ab - x^2 + ab&lt;/math&gt; is written as &lt;math&gt;x^2 + 4ab&lt;/math&gt;, because the terms containing &lt;math&gt;x^2&lt;/math&gt; are added together, and, the terms containing &lt;math&gt;ab&lt;/math&gt; are added together.
*Brackets can be &quot;multiplied out&quot;, using [[Distributive property|distributivity]]. For example, &lt;math&gt;x (2x + 3)&lt;/math&gt; can be written as &lt;math&gt;(x \times 2x) + (x \times 3)&lt;/math&gt; which can be written as &lt;math&gt;2x^2 + 3x&lt;/math&gt;
*Expressions can be factored. For example, &lt;math&gt;6x^5 + 3x^2&lt;/math&gt;, by dividing both terms by &lt;math&gt;3x^2&lt;/math&gt; can be written as &lt;math&gt;3x^2 (2x^3 + 1)&lt;/math&gt;

=== Equations ===
[[File:Pythagorean theorem.gif|thumb|Animation illustrating [[Pythagorean theorem|Pythagoras' rule]] for a right-angle triangle, which shows the algebraic relationship between the triangle's hypotenuse, and the other two sides.]]
{{Main|Equation}}
An equation states that two expressions are equal using the symbol for equality, &lt;math&gt;=&lt;/math&gt; (the [[equals sign]]).&lt;ref&gt;Mark Clark, Cynthia Anfinson, ''Beginning Algebra: Connecting Concepts Through Applications'', Publisher Cengage Learning, 2011, ISBN 0534419380, 9780534419387, 793 pages, [http://books.google.co.uk/books?id=wCzuRMC5048C&amp;lpg=PA134&amp;ots=UdyGuk1ihH&amp;dq=algebra%20equation%20%22two%20expressions%22&amp;pg=PA134#v=onepage&amp;q=equation&amp;f=false page 134]&lt;/ref&gt; One of the most well-known equations describes Pythagoras' law relating the length of the sides of a [[right angle]] triangle:&lt;ref&gt;Alan S. Tussy, R. David Gustafson, ''Elementary and Intermediate Algebra'', Publisher Cengage Learning, 2012, ISBN 1111567689, 9781111567682, 1163 pages, [http://books.google.co.uk/books?id=xqio_Xn4t7oC&amp;lpg=PA493&amp;ots=pmzfzBO1KX&amp;dq=algebra%20Pythagoras%20hypotenuse&amp;pg=PA493#v=onepage&amp;q=algebra%20Pythagoras%20hypotenuse&amp;f=false page 493]&lt;/ref&gt;

:&lt;math&gt;c^2 = a^2 + b^2&lt;/math&gt;

This equation states that &lt;math&gt;c^2&lt;/math&gt;, representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;.

An equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as &lt;math&gt;a + b = b + a&lt;/math&gt;); such equations are called [[identity (mathematics)|identities]]. Conditional equations are true for only some values of the involved variables, e.g. &lt;math&gt;x^2 - 1 = 8&lt;/math&gt; is true only for &lt;math&gt;x = 3&lt;/math&gt; and &lt;math&gt;x = -3&lt;/math&gt;. The values of the variables which make the equation true are the solutions of the equation and can be found through [[equation solving]].

Another type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are: &lt;math&gt; a &gt; b &lt;/math&gt; where &lt;math&gt; &gt; &lt;/math&gt; represents 'greater than', and &lt;math&gt; a &lt; b &lt;/math&gt; where &lt;math&gt; &lt; &lt;/math&gt; represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.

==== Properties of equality ====

By definition, equality is an [[equivalence relation]], meaning it has the properties (a) [[reflexive relation|reflexive]] (i.e. &lt;math&gt;b = b&lt;/math&gt;), (b) [[symmetric relation|symmetric]] (i.e. if &lt;math&gt;a = b&lt;/math&gt; then &lt;math&gt;b = a&lt;/math&gt;) (c) [[transitive relation|transitive]] (i.e. if &lt;math&gt;a = b&lt;/math&gt; and &lt;math&gt;b = c&lt;/math&gt; then &lt;math&gt;a = c&lt;/math&gt;).&lt;ref&gt;Douglas Downing, ''Algebra the Easy Way'', Publisher Barron's Educational Series, 2003, ISBN 0764119729, 9780764119729, 392 pages, [http://books.google.co.uk/books?id=RiX-TJLiQv0C&amp;lpg=PA20&amp;ots=BjArsBq8vc&amp;dq=algebra%20equality%20%20%20reflexive%20%20symmetric%20%20transitive&amp;pg=PA20#v=onepage&amp;q=algebra%20equality%20%20%20reflexive%20%20symmetric%20%20transitive&amp;f=false page 20]&lt;/ref&gt; It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:

* if &lt;math&gt;a = b&lt;/math&gt; and &lt;math&gt;c = d&lt;/math&gt; then &lt;math&gt;a + c = b + d&lt;/math&gt; and &lt;math&gt;ac = bd&lt;/math&gt;;
* if &lt;math&gt;a = b&lt;/math&gt; then &lt;math&gt;a + c = b + c&lt;/math&gt;;
* more generally, for any function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;a=b&lt;/math&gt; implies &lt;math&gt;f(a) = f(b)&lt;/math&gt;.

==== Properties of inequality ====

The relations ''less than'' &lt;math&gt; &lt; &lt;/math&gt; and greater than &lt;math&gt; &gt; &lt;/math&gt; have the property of transitivity:&lt;ref&gt;Ron Larson, Robert Hostetler, ''Intermediate Algebra'', Publisher Cengage Learning, 2008, ISBN 0618753524, 9780618753529, 857 pages, [http://books.google.co.uk/books?id=b3vqad8tbiAC&amp;lpg=PA96&amp;ots=FF2OYYGNCV&amp;dq=algebra%20inequality%20properties&amp;pg=PA96#v=onepage&amp;q=algebra%20inequality%20properties&amp;f=false page 96]&lt;/ref&gt;
* If &amp;nbsp; &lt;math&gt;a &lt; b&lt;/math&gt; &amp;nbsp; and &amp;nbsp; &lt;math&gt;b &lt; c&lt;/math&gt; &amp;nbsp; then &amp;nbsp; &lt;math&gt;a &lt; c&lt;/math&gt;;
* If &amp;nbsp; &lt;math&gt;a &lt; b&lt;/math&gt; &amp;nbsp; and &amp;nbsp; &lt;math&gt;c &lt; d&lt;/math&gt; &amp;nbsp; then &amp;nbsp; &lt;math&gt;a + c &lt; b + d&lt;/math&gt;;
* If &amp;nbsp; &lt;math&gt;a &lt; b&lt;/math&gt; &amp;nbsp; and &amp;nbsp; &lt;math&gt;c &gt; 0&lt;/math&gt; &amp;nbsp; then &amp;nbsp; &lt;math&gt;ac &lt; bc&lt;/math&gt;;
* If &amp;nbsp; &lt;math&gt;a &lt; b&lt;/math&gt; &amp;nbsp; and &amp;nbsp; &lt;math&gt;c &lt; 0&lt;/math&gt; &amp;nbsp; then &amp;nbsp; &lt;math&gt;bc &lt; ac&lt;/math&gt;.
Note that by reversing the equation, we can swap &lt;math&gt; &lt; &lt;/math&gt; and &lt;math&gt; &gt; &lt;/math&gt;,&lt;ref&gt;Chris Carter, ''Physics: Facts and Practice for A Level'', Publisher Oxford University Press, 2001, ISBN 019914768X, 9780199147687, 144 pages, [http://books.google.co.uk/books?id=Ff9gxZPYafcC&amp;lpg=PA50&amp;ots=KQ5ufGdcHk&amp;dq=algebra%20inequality%20less%20greater%20exchange&amp;pg=PA50#v=onepage&amp;q=turned%20around&amp;f=false page 50]&lt;/ref&gt; for example:
* &lt;math&gt;a &lt; b&lt;/math&gt; is equivalent to &lt;math&gt;b &gt; a&lt;/math&gt;

=== Substitution ===
{{main|Substitution (algebra)}}
{{see also|Substitution (logic)}}

Substitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if &lt;math&gt;a^2:=a*a&lt;/math&gt;, where := means &quot;is defined to equal&quot;, substituting 3 for &lt;math&gt;a&lt;/math&gt; informs the reader of this statement that &lt;math&gt;3^2&lt;/math&gt; means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.

If ''x'' and ''y'' [[integers]], [[rationals]], or [[real numbers]], then ''xy''=0 implies ''x''=0 or ''y''=0. Suppose ''abc''=0. Then, substituting ''a'' for ''x'' and ''bc'' for ''y'', we learn ''a''=0 or ''bc''=0. Then we can substitute again, letting ''x''=''b'' and ''y''=''c'', to show that if ''bc''=0 then ''b''=0 or ''c''=0. Therefore if ''abc''=0, then ''a''=0 or (''b''=0 or ''c''=0), so ''abc''=0 implies ''a''=0 or ''b''=0 or ''c''=0.

Consider if the original fact were stated as &quot;''ab''=0 implies ''a''=0 or ''b''=0.&quot; Then when we say &quot;suppose ''abc''=0,&quot; we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if ''abc''=0 then ''a''=0 or ''b''=0 or ''c''=0 if instead of letting ''a''=''a'' and ''b''=''bc'' we substitute ''a'' for ''a'' and ''b'' for ''bc'' (and with ''bc''=0, substituting ''b'' for ''a'' and ''c'' for ''b''). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression ''a'' into the ''a'' term of the original equation, the ''a'' substituted does not refer to the ''a'' in the statement &quot;''ab''=0 implies ''a''=0 or ''b''=0.&quot;

== Solving algebraic equations ==
{{see also|Equation solving}}
[[File:Algebraproblem.jpg|thumb|A typical algebra problem.]]

The following sections lay out examples of some of the types of algebraic equations that may be encountered.

=== Linear equations with one variable ===
{{Main|Linear equation}}

Linear equations are so-called, because when they are plotted, they describe a straight line (hence ''linear''). The simplest equations to solve are [[linear equation]]s that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:

:Problem in words: If you double my son's age and add 4, the resulting answer is 12. How old is my son?

:Equivalent equation: &lt;math&gt;2x + 4 = 12&lt;/math&gt; where  &lt;math&gt;x&lt;/math&gt; represent my son's age

To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation.  Once the variable is isolated, the other side of the equation is the value of the variable.&lt;ref&gt;{{Cite book|title=All the Math You'll Ever Need|author=Slavin, Steve|publisher=[[John Wiley &amp; Sons]]|year=1989|isbn=0-471-50636-2|page=72}}&lt;/ref&gt; This problem and its solution are as follows:
{|
|-
| 1. Equation to solve:
| &lt;math&gt;2x + 4 = 12&lt;/math&gt;
|-
| 2. Subtract 4 from both sides:
| &lt;math&gt;2x + 4 - 4 = 12 - 4&lt;/math&gt;
|-
| 3. This simplifies to:
| &lt;math&gt;2x = 8&lt;/math&gt;
|-
| 4. Divide both sides by 2:
| &lt;math&gt;\frac{2x}{2} = \frac{8}{2}&lt;/math&gt;
|-
| 5. Simplifies to the solution:
| &lt;math&gt;x = 4&lt;/math&gt;
|}

The general form of a linear equation with one variable, can be written as: &lt;math&gt;ax+b=c\,&lt;/math&gt;

Following the same procedure (i.e. subtract &lt;math&gt;b&lt;/math&gt; from both sides, and then divide by &lt;math&gt;a&lt;/math&gt;), the general solution is given by &lt;math&gt;x=\frac{c-b}{a}&lt;/math&gt;

=== Linear equations with two variables ===
[[File:Linear-equations-two-unknowns.svg|thumb|right|Two linear equations|Solving two linear equations with a unique solution at the point that they intersect.]]
A linear equation with two variables has many (i.e. an infinite number of) solutions.&lt;ref&gt;Sinha, ''The Pearson Guide to Quantitative Aptitude for CAT 2/e''Publisher: Pearson Education India, 2010, ISBN 8131723666, 9788131723661, 599 pages, [http://books.google.co.uk/books?id=eOnaFSKRSR0C&amp;lpg=PA195&amp;ots=K9A5d10dUc&amp;dq=linear%20equation%20%20two%20variables%20%20many%20solutions&amp;pg=PA195#v=onepage&amp;q=many%20solutions&amp;f=false page 195]&lt;/ref&gt; For example:

:Problem in words: I am 22 years older than my son. How old are we?
:Equivalent equation: &lt;math&gt;y = x + 22&lt;/math&gt; where &lt;math&gt;y&lt;/math&gt; is my age, &lt;math&gt;x&lt;/math&gt; is my son's age.

This can not be worked out by itself. If I told you my son's age, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.

To solve a linear equation with two variables (unknowns), requires two related equations. For example, if I also revealed that:
{|
|-
| Problem in words:
| In 10 years time, I will be twice as old as my son.
|-
| Equivalent equation:
| &lt;math&gt;y + 10 = 2 \times (x + 10)&lt;/math&gt;
|-
| Subtract 10 from both sides:
| &lt;math&gt;y = 2 \times (x + 10) - 10&lt;/math&gt;
|-
| Multiple out brackets:
| &lt;math&gt;y = 2x + 20 - 10&lt;/math&gt;
|-
| Simplify:
| &lt;math&gt;y = 2x + 10&lt;/math&gt;
|}

Now there are two related linear equations, each with two unknowns, which lets us produce a linear equation with just one variable, by subtracting one from the other (called the elimination method):&lt;ref&gt;Cynthia Y. Young, ''Precalculus'', Publisher John Wiley &amp; Sons, 2010, ISBN 0471756849, 9780471756842, 1175 pages, [http://books.google.co.uk/books?id=9HRLAn326zEC&amp;lpg=PA703&amp;ots=t83cFBL8TU&amp;dq=linear%20equation%20%20two%20variables%20%20many%20solutions&amp;pg=PA699#v=onepage&amp;q=linear%20equation%20%20two%20variables%20%20many%20solutions&amp;f=false page 699]&lt;/ref&gt;
{|
|-
| Second equation
| &lt;math&gt;y = 2x + 10&lt;/math&gt;
|-
| First equation
| &lt;math&gt;y = x + 22&lt;/math&gt;
|-
| Subtract the first equation from&lt;br&gt;the second in order to remove &lt;math&gt;y&lt;/math&gt;
| &lt;math&gt;(y - y) = (2x - x) +10 - 22&lt;/math&gt;
|-
| Simplify
| &lt;math&gt;0 = x - 12&lt;/math&gt;
|-
| Add 12 to both sides
| &lt;math&gt;12 = x&lt;/math&gt;
|-
| Rearrange
| &lt;math&gt;x = 12&lt;/math&gt;
|}

In other words, my son is aged 12, and as I am 22 years older, I must be 34. In 10 years time, my son will 22, and I will be twice his age, 44. This problem is illustrated on the associated plot of the equations.

For other ways to solve this kind of equations, see below, '''[[#System of linear equations|System of linear equations]]'''.

=== Quadratic equations ===
{{Main|Quadratic equation}}
[[File:Quadratic-equation.svg|thumb|right|Quadratic equation plot of &lt;math&gt;y = x^2 + 3x - 10&lt;/math&gt; showing its roots at &lt;math&gt;x = -5&lt;/math&gt; and &lt;math&gt;x = 2&lt;/math&gt;, and that the quadratic can be rewritten as &lt;math&gt;y = (x + 5)(x - 2)&lt;/math&gt;
]]
A quadratic equation is one which includes a term with an exponent of 2, for example, &lt;math&gt;x^2&lt;/math&gt;,&lt;ref&gt;Mary Jane Sterling, ''Algebra II For Dummies'', Publisher: John Wiley &amp; Sons, 2006, ISBN 0471775819, 9780471775812, 384 pages, [http://books.google.co.uk/books?id=_0rTMuSpTY0C&amp;lpg=PA43&amp;ots=J8_1q1Vkul&amp;dq=quadratic%20equations&amp;pg=PA37#v=onepage&amp;q=quadratic%20equations&amp;f=false page 37]&lt;/ref&gt; and no term with higher exponent. The name derives from the Latin ''quadrus'', meaning square.&lt;ref&gt;John T. Irwin, ''The Mystery to a Solution: Poe, Borges, and the Analytic Detective Story'', Publisher JHU Press, 1996, ISBN 0801854660, 9780801854668, 512 pages, [http://books.google.co.uk/books?id=jsxTenuOQKgC&amp;lpg=PA372&amp;ots=T7p7Porq55&amp;dq=quadratic%20quadrus&amp;pg=PA372#v=onepage&amp;q=quadratic%20quadrus&amp;f=false page 372]&lt;/ref&gt; In general, a quadratic equation can be expressed in the form &lt;math&gt;ax^2 + bx + c = 0&lt;/math&gt;,&lt;ref&gt;Sharma/khattar, ''The Pearson Guide To Objective Mathematics For Engineering Entrance Examinations, 3/E'', Publisher Pearson Education India, 2010, ISBN 8131723631, 9788131723630, 1248 pages, [http://books.google.co.uk/books?id=2v-f9x7-FlsC&amp;lpg=RA13-PA33&amp;ots=P-Dz70fXbv&amp;dq=quadratic%20equations%20%20ax2%20%2B%20bx%20%2B%20c%20%3D%200&amp;pg=RA13-PA33#v=onepage&amp;q&amp;f=false page 621]&lt;/ref&gt; where &lt;math&gt;a&lt;/math&gt; is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term &lt;math&gt;ax^2&lt;/math&gt;, which is known as the quadratic term.  Hence &lt;math&gt;a \neq 0&lt;/math&gt;, and so we may divide by &lt;math&gt;a&lt;/math&gt; and rearrange the equation into the standard form

: &lt;math&gt;x^2 + px + q = 0 \, &lt;/math&gt;

where &lt;math&gt;p = b/a&lt;/math&gt; and &lt;math&gt;q = c/a&lt;/math&gt;. Solving this, by a process known as [[completing the square]], leads to the [[quadratic formula]]

:&lt;math&gt;x=\frac{-b \pm \sqrt {b^2-4ac}}{2a},&lt;/math&gt;

where [[plus-minus sign|the symbol &quot;±&quot;]] indicates that both

: &lt;math&gt; x=\frac{-b + \sqrt {b^2-4ac}}{2a}\quad\text{and}\quad x=\frac{-b - \sqrt {b^2-4ac}}{2a}&lt;/math&gt;

are solutions of the quadratic equation.

Quadratic equations can also be solved using [[factorization]] (the reverse process of which is [[polynomial expansion|expansion]], but for two [[linear function|linear terms]] is sometimes denoted [[FOIL rule|foiling]]).  As an example of factoring:

: &lt;math&gt;x^{2} + 3x - 10 = 0. \,&lt;/math&gt;

Which is the same thing as

: &lt;math&gt;(x + 5)(x - 2) = 0. \,&lt;/math&gt;

It follows from the [[zero-product property]] that either &lt;math&gt;x = 2&lt;/math&gt; or &lt;math&gt;x = -5&lt;/math&gt; are the solutions, since precisely one of the factors must be equal to [[zero]]. All quadratic equations will have two solutions in the [[complex number]] system, but need not have any in the [[real number]] system. For example,

: &lt;math&gt;x^{2} + 1 = 0 \,&lt;/math&gt;

has no real number solution since no real number squared equals −1.
Sometimes a quadratic equation has a root of [[multiplicity (mathematics)|multiplicity]] 2, such as:

: &lt;math&gt;(x + 1)^2 = 0. \,&lt;/math&gt;

For this equation, −1 is a root of multiplicity 2. This means −1 appears two times.

=== Exponential and logarithmic equations ===
{{Main|Logarithm}}
[[File:Binary logarithm plot with ticks.svg|right|thumb|upright=1.35|alt=Graph showing a logarithm curves, which crosses the ''x''-axis where ''x'' is 1 and extend towards minus infinity along the ''y''-axis.|The [[graph of a function|graph]] of the logarithm to base 2 crosses the [[x axis|''x'' axis]] (horizontal axis) at 1 and passes through the points with [[coordinate]]s {{nowrap|(2, 1)}}, {{nowrap|(4, 2)}}, and {{nowrap|(8, 3)}}. For example, {{nowrap|log&lt;sub&gt;2&lt;/sub&gt;(8) {{=}} 3}}, because {{nowrap|2&lt;sup&gt;3&lt;/sup&gt; {{=}} 8.}} The graph gets arbitrarily close to the ''y'' axis, but [[asymptotic|does not meet or intersect it]].]]

An exponential equation is one which has the form &lt;math&gt;a^x = b&lt;/math&gt; for &lt;math&gt;a &gt; 0&lt;/math&gt;,&lt;ref&gt;Aven Choo, ''LMAN OL Additional Maths Revision Guide 3'', Publisher Pearson Education South Asia, 2007, ISBN 9810600011, 9789810600013, [http://books.google.co.uk/books?id=NsBXDMrzcJIC&amp;lpg=RA2-PA29&amp;ots=WmZmHLaTey&amp;dq=%22%20exponential%20equation%20%22%20aX%20%3D%20b&amp;pg=RA2-PA29#v=onepage&amp;q=%22%20exponential%20equation%20%22%20aX%20=%20b&amp;f=false page 105]&lt;/ref&gt; which has solution

: &lt;math&gt;X = \log_a b = \frac{\ln b}{\ln a}&lt;/math&gt;

when &lt;math&gt;b &gt; 0&lt;/math&gt;. Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if

: &lt;math&gt;3 \cdot 2^{x - 1} + 1 = 10&lt;/math&gt;

then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain

: &lt;math&gt;2^{x - 1} = 3\,&lt;/math&gt;

whence

: &lt;math&gt;x - 1 = \log_2 3\,&lt;/math&gt;

or

: &lt;math&gt;x = \log_2 3 + 1.\,&lt;/math&gt;

A logarithmic equation is an equation of the form &lt;math&gt;log_a(x) = b&lt;/math&gt; for &lt;math&gt;a &gt; 0&lt;/math&gt;, which has solution

: &lt;math&gt;X = a^b.\,&lt;/math&gt;

For example, if

: &lt;math&gt;4\log_5(x - 3) - 2 = 6\,&lt;/math&gt;

then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get

: &lt;math&gt;\log_5(x - 3) = 2\,&lt;/math&gt;

whence

: &lt;math&gt;x - 3 = 5^2 = 25\,&lt;/math&gt;

from which we obtain

: &lt;math&gt;x = 28.\,&lt;/math&gt;

=== Radical equations ===
[[File:Radical equation equivalence.svg|thumb|right|Radical equation showing two ways to represent the same expression]]
A radical equation is one that includes a radical sign, which includes [[square root]]s, &lt;math&gt;\sqrt{x}&lt;/math&gt;, [[cube root]]s, &lt;math&gt;\sqrt[3]{x}&lt;/math&gt;, and [[nth root|''n''th roots]], &lt;math&gt;\sqrt[n]{x}&lt;/math&gt;. Recall that an ''n''th root can be rewritten in exponential format, so that &lt;math&gt;\sqrt[n]{x}&lt;/math&gt; is equivalent to &lt;math&gt;x^{\frac{1}{n}}&lt;/math&gt;. Combined with regular exponents (powers), then  &lt;math&gt;\sqrt[2]{x^3}&lt;/math&gt; (the square root of &lt;math&gt;x&lt;/math&gt; cubed), can be rewritten as &lt;math&gt;x^{\frac{3}{2}}&lt;/math&gt;.&lt;ref&gt;John C. Peterson, ''Technical Mathematics With Calculus'', Publisher Cengage Learning, 2003, ISBN 0766861899, 9780766861893, 1613 pages, [http://books.google.co.uk/books?id=PGuSDjHvircC&amp;lpg=PA525&amp;ots=NKrt__3KKE&amp;dq=%22%20radical%20equation%22&amp;pg=PA525#v=onepage&amp;q=%22%20radical%20equation%22&amp;f=false page 525]&lt;/ref&gt; So a common form of a radical equation is &lt;math&gt;a = \sqrt[n]{x^m}&lt;/math&gt; (equivalent to &lt;math&gt;a = x^\frac{m}{n}&lt;/math&gt;) where &lt;math&gt;m&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt; are [[integer]]s. It has solution:
{| class=&quot;wikitable&quot; style=&quot;text-align:center&quot;
|- style=&quot;vertical-align:top&quot;
! &lt;math&gt;m&lt;/math&gt; is odd
! &lt;math&gt;m&lt;/math&gt; is even&lt;br&gt;and &lt;math&gt;a \ge 0&lt;/math&gt;
|-
| &lt;math&gt;x = \sqrt[m]{a^n}&lt;/math&gt;

or

: &lt;math&gt;x = \left(\sqrt[m]a\right)^n&lt;/math&gt;
| &lt;math&gt;x = \pm \sqrt[m]{a^n}&lt;/math&gt;

or

: &lt;math&gt;x = \pm \left(\sqrt[m]a\right)^n&lt;/math&gt;
|}

For example, if:

: &lt;math&gt;(x + 5)^{2/3} = 4,\,&lt;/math&gt;

then

: &lt;math&gt;\begin{align}
x + 5 &amp; = \pm (\sqrt{4})^3\\
x + 5 &amp; = \pm 8\\
x &amp;  = -5 \pm 8\\
x &amp; = 3,-13
\end{align}&lt;/math&gt;.

=== System of linear equations ===
{{Main|System of linear equations}}

There are different methods to solve a system of linear equations with two variables.

==== Elimination method ====
[[File:Intersecting Lines.svg|thumb|right|The solution set for the equations &lt;math&gt;x - y = -1&lt;/math&gt; and &lt;math&gt;3x + y = 9&lt;/math&gt; is the single point (2,&amp;nbsp;3).]]

An example of solving a system of linear equations is by using the elimination method:

: &lt;math&gt;\begin{cases}4x + 2y&amp;= 14 \\
2x - y&amp;= 1.\end{cases} \,&lt;/math&gt;

Multiplying the terms in the second equation by 2:

: &lt;math&gt;4x + 2y = 14 \,&lt;/math&gt;
: &lt;math&gt;4x - 2y = 2. \,&lt;/math&gt;

Adding the two equations together to get:

: &lt;math&gt;8x = 16 \,&lt;/math&gt;

which simplifies to

: &lt;math&gt;x = 2. \,&lt;/math&gt;

Since the fact that &lt;math&gt;x = 2&lt;/math&gt; is known, it is then possible to deduce that &lt;math&gt;y = 3&lt;/math&gt;  by either of the original two equations (by using ''2'' instead of &lt;math&gt;x&lt;/math&gt; ) The full solution to this problem is then

: &lt;math&gt;\begin{cases} x = 2 \\ y = 3. \end{cases}\,&lt;/math&gt;

Note that this is not the only way to solve this specific system; &lt;math&gt;y&lt;/math&gt; could have been solved before &lt;math&gt;x&lt;/math&gt;.

==== Substitution method ====

Another way of solving the same system of linear equations is by substitution.

: &lt;math&gt;\begin{cases}4x + 2y &amp;= 14
\\ 2x - y &amp;= 1.\end{cases} \,&lt;/math&gt;

An equivalent for &lt;math&gt;y&lt;/math&gt; can be deduced by using one of the two equations. Using the second equation:

: &lt;math&gt;2x - y = 1 \,&lt;/math&gt;

Subtracting &lt;math&gt;2x&lt;/math&gt; from each side of the equation:

: &lt;math&gt;\begin{align}2x - 2x - y &amp; = 1 - 2x  \\
- y &amp; = 1 - 2x
\end{align}&lt;/math&gt;

and multiplying by −1:

: &lt;math&gt; y = 2x - 1. \,&lt;/math&gt;

Using this &lt;math&gt;y&lt;/math&gt; value in the first equation in the original system:

: &lt;math&gt;\begin{align}4x + 2(2x - 1) &amp;= 14\\
4x + 4x - 2 &amp;= 14 \\
8x - 2 &amp;= 14 \end{align}&lt;/math&gt;

Adding ''2'' on each side of the equation:

: &lt;math&gt;\begin{align}8x - 2 + 2 &amp;= 14 + 2 \\
8x &amp;= 16 \end{align}&lt;/math&gt;

which simplifies to

: &lt;math&gt;x = 2 \,&lt;/math&gt;

Using this value in one of the equations, the same solution as in the previous method is obtained.

: &lt;math&gt;\begin{cases} x = 2 \\ y = 3. \end{cases}\,&lt;/math&gt;

Note that this is not the only way to solve this specific system; in this case as well, &lt;math&gt;y&lt;/math&gt; could have been solved before &lt;math&gt;x&lt;/math&gt;.

=== Other types of systems of linear equations ===

==== Inconsistent systems ====
[[File:Parallel Lines.svg|thumb|right|The equations &lt;math&gt;3x + 2y = 6&lt;/math&gt; and &lt;math&gt;3x + 2y = 12&lt;/math&gt; are parallel and cannot intersect, and is unsolvable.]]
In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called '''inconsistent'''. An obvious example is

: &lt;math&gt;\begin{cases}\begin{align} x + y &amp;= 1 \\
0x + 0y &amp;= 2\,. \end{align} \end{cases}&lt;/math&gt;

As 0≠2, the second equation in the system has no solution. Therefore, the system has no solution.
However, not all inconsistent systems are recognized at first sight. As an example, let us consider the system 
: &lt;math&gt;\begin{cases}\begin{align}4x + 2y &amp;= 12 \\
-2x - y &amp;= -4\,. \end{align}\end{cases}&lt;/math&gt;

Multiplying by 2 both sides of the second equation, and adding it to the first one results in
: &lt;math&gt;0x+0y = 4 \,,&lt;/math&gt;
which has clearly no solution.

==== Undetermined systems ====
[[File:Quadratic-linear-equations.svg|thumb|right|Plot of a quadratic equation (red) and a linear equation (blue) that do not intersect, and consequently for which there is no common solution.]]
There are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;) For example:

: &lt;math&gt;\begin{cases}\begin{align}4x + 2y &amp; = 12 \\
-2x - y &amp; = -6 \end{align}\end{cases}\,&lt;/math&gt;

Isolating &lt;math&gt;y&lt;/math&gt; in the second equation:

: &lt;math&gt;y = -2x + 6 \,&lt;/math&gt;

And using this value in the first equation in the system:

: &lt;math&gt;\begin{align}4x + 2(-2x + 6) = 12 \\
4x - 4x + 12 = 12 \\
12 = 12 \end{align}&lt;/math&gt;

The equality is true, but it does not provide a value for &lt;math&gt;x&lt;/math&gt;. Indeed, one can easily verify (by just filling in some values of &lt;math&gt;x&lt;/math&gt;) that for any &lt;math&gt;x&lt;/math&gt; there is a solution as long as &lt;math&gt;y = -2x + 6&lt;/math&gt;. There is an infinite number of solutions for this system.

==== Over- and underdetermined systems ====

Systems with more variables than the number of linear equations do not have a unique solution. An example of such a system is

: &lt;math&gt;\begin{cases}\begin{align}x + 2y &amp; = 10\\
y - z  &amp; = 2 \end{align}\end{cases}&lt;/math&gt;

Such a system is called [[Underdetermined system|underdetermined]]; when trying to solve it, one is led to express some variables as functions of the other ones, but cannot express ''all'' solutions [[Number|numerically]].

A system with a greater number of equations than variables, in which necessarily some equations are [[linear combination]]s of the others if any solution exists, is called [[Overdetermined system|overdetermined]].

== See also ==
* [[History of elementary algebra]]
* [[Binary operation]]
* [[Gaussian elimination]]
* [[Mathematics education]]
* [[Number line]]
* [[Polynomial]]

== References ==

* [[Leonhard Euler]],  '' [[Elements of Algebra]]'', 1770.  English translation [[Tarquin Press]], 2007, ISBN 978-1-899618-79-8, also online digitized editions&lt;ref&gt;[http://web.mat.bham.ac.uk/C.J.Sangwin/euler/ Euler's Elements of Algebra&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 2006,&lt;ref&gt;[http://books.google.co.uk/books?id=X8yv0sj4_1YC&amp;dq=euler+elements&amp;psp=1 Elements of algebra – Leonhard Euler, John Hewlett, Francis Horner, Jean Bernoulli, Joseph Louis Lagrange – Google Books&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 1822.
* Charles Smith, ''[http://digital.library.cornell.edu/cgi/t/text/text-idx?c=math;idno=smit025 A Treatise on Algebra]'', in [http://historical.library.cornell.edu/math Cornell University Library Historical Math Monographs].
*Redden, John. [http://catalog.flatworldknowledge.com/bookhub/reader/128?e=fwk-redden-ch01 ''Elementary Algebra'']. Flat World Knowledge, 2011

{{reflist}}

== External links ==

{{Areas of mathematics}}

{{DEFAULTSORT:Elementary Algebra}}
[[Category:Elementary algebra| ]]
[[Category:Algebra]]</text>
      <sha1>mwhbp2oe7cbdm37wkq4mm4hk0cveszi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Map algebra</title>
    <ns>0</ns>
    <id>7640211</id>
    <revision>
      <id>543737660</id>
      <parentid>528015275</parentid>
      <timestamp>2013-03-13T01:25:34Z</timestamp>
      <contributor>
        <ip>72.33.238.52</ip>
      </contributor>
      <text xml:space="preserve" bytes="2668">'''Map algebra''' is a set-based [[algebra]] for manipulating [[geographic]] data, proposed by Dr. [[Dana Tomlin]] in the early 1980s.  It is a set of primitive operations in a [[geographic information system]] (GIS) which allows two or more [[Raster graphics|raster]] [[Layers (digital image editing)|layer]]s (&quot;maps&quot;) of similar dimensions to produce a new raster layer (map) using algebraic operations such as addition, subtraction etc.

Depending on the spatial neighborhood, GIS transformations are categorized into four classes: ''local'', ''focal'', ''global'', and ''zonal''. Local operations works on individual raster cells, or [[pixel]]s. Focal operations work on cells and their neighbors, whereas global operations work on the entire layer. Finally, zonal operations work on areas of cells that share the same value. The input and output for each operator being map, the operators can be combined into a procedure, script, to perform complex tasks.&lt;ref&gt;{{citebook|title=Geographic Information Systems and Science|author=Longley et al.|publisher=John Wiley &amp; Sons, Inc.|ISBN=978-0-470-72144-5|pages=414–7}}&lt;/ref&gt;

Here are some examples:
&lt;source lang=&quot;MapBasic&quot;&gt;
# demo for Brown's Pond data set
# Give layers
#  altitude
#  development – 0: vacant, 1: major, 2: minor, 3: houses, 4: buildings, 5 cement
#  water – 0: dry, 2: wet, 3: pond

# calculate the slope at each location based on altitude
slope = IncrementalGradient of altitude

# identify the areas that are too steep
toosteep = LocalRating of slope
  where 1 replaces 4 5 6
  where VOID replaces ...

# create layer unifying water and development
occupied = LocalRating of development
  where water replaces VOID

notbad = LocalRating of occupied and toosteep
  where 1 replaces VOID and VOID
  where VOID replaces ... and ...

roads = LocalRating of development
  where 1 replaces 1 2
  where VOID replaces ...

nearread = FocalNeighbor of roads at 0 ... 10

aspect = IncrementalAspect of altitude

southface = LocalRating of aspect
  where 1 replaces 135 ... 225
  where VOID replaces ...

sites = LocalMinimum of nearroad and southface and notbad

sitenums = FocalInsularity of sites at 0 ... 1

sitesize = ZonalSum of 1 within sitenums

bestsites = LocalRating of sitesize
  where sitesize replaces 100 ... 300
  where VOID replaces ...
&lt;/source&gt;
==External links ==
* [http://trac.osgeo.org/gdal/wiki/rfc39_ogr_layer_algebra osGeo-RFC-39 about Layer Algebra]

==References==
&lt;references/&gt;
*B. E. Davis ''GIS: A Visual Approach'' (2001 Cengage Learning) pp. 249ff.

{{cartography-stub}}

[[Category:Geographic information systems]]
[[Category:Applied mathematics]]
[[Category:Algebra]]</text>
      <sha1>smuk0e99xey40kjkbx9q96pyoghp9fw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Vector algebra</title>
    <ns>0</ns>
    <id>1262720</id>
    <revision>
      <id>546110393</id>
      <parentid>511337994</parentid>
      <timestamp>2013-03-21T22:54:32Z</timestamp>
      <contributor>
        <username>KLBot2</username>
        <id>14393296</id>
      </contributor>
      <minor/>
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikidata]] on [[:d:Q7917816]]</comment>
      <text xml:space="preserve" bytes="909">In mathematics, '''vector algebra''' may mean:
* [[Linear algebra]], specifically the basic algebraic operations of vector addition and scalar multiplication; see [[vector space]].
* The algebraic operations in [[vector calculus]], namely the specific additional structure of vectors in 3-dimensional [[Euclidean space]] &lt;math&gt;\mathbf{R}^3&lt;/math&gt; of [[dot product]] and especially [[cross product]]. In this sense, '''vector algebra''' is contrasted with [[geometric algebra]], which provides an alternative generalization to higher dimensions.
* Original vector algebras of the nineteenth century like [[quaternion]]s, [[tessarine]]s, or [[coquaternion]]s, each of which has its own [[product (mathematics)|product]]. The vector algebras [[biquaternion]]s and [[hyperbolic quaternion]]s enabled the revolution in physics called [[special relativity]] by providing mathematical models.



[[Category:Algebra]]</text>
      <sha1>1t8wngdbjjbu7z8h6p6r6izh1cz4s5u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Moderne Algebra</title>
    <ns>0</ns>
    <id>37252648</id>
    <revision>
      <id>607238059</id>
      <parentid>596023164</parentid>
      <timestamp>2014-05-05T21:14:13Z</timestamp>
      <contributor>
        <username>Rgdboer</username>
        <id>92899</id>
      </contributor>
      <minor/>
      <comment>/* References */ cat</comment>
      <text xml:space="preserve" bytes="8767">'''Moderne Algebra''' is a two-volume German textbook on graduate [[abstract algebra]] by {{harvs|txt|authorlink=Bartel Leendert van der Waerden|first=Bartel Leendert |last=van der Waerden|year1=1930|year2=1931}}, originally based on lectures given by [[Emil Artin]] in 1926 and by {{harvs|txt|authorlink=Emmy Noether|first=Emmy|last=Noether|year=1929}} from 1924 to 1928. The English translation of 1949–1950 had the title '''Modern algebra''', though a later extensively revised edition in 1970 had the title '''Algebra'''.

The book was one of the first [[textbook]]s to use an abstract axiomatic approach to [[group (mathematics)|group]]s, [[ring (mathematics)|ring]]s, and [[field (mathematics)|field]]s, and was by far the most successful, becoming the standard reference for [[graduate school|graduate]] algebra for several decades. It &quot;had a tremendous impact, and is widely considered to be the major text on algebra in the twentieth century&quot;&lt;ref&gt;Bernard Behan &amp; [[Karl Sigmund]] (2008) &quot;A Short Tale of Two Cities: Otto Schrier and the Hamburg–Vienna Connection&quot;, p 33, [[Mathematical Intelligencer]] 30(3):27&amp;ndash;35&lt;/ref&gt;

There is a detailed discussion of the sources used for writing the book in {{harvtxt|van der Waerden|1975}}.

==Publication history==

'''Moderne algebra''' has a rather confusing publication history, because it went through many different editions, several of which were extensively rewritten with chapters and major topics added, deleted, or rearranged. In addition the new editions of first and second volumes were issued almost independently and at different times, and the numbering of the English editions does not correspond to the numbering of the German editions. In 1955 the title was changed from &quot;Moderne Algebra&quot; to &quot;Algebra&quot; following a suggestion of Brandt, with the result that the two volumes of the third German edition do not even have the same title.

For volume 1, the first German edition was published in 1930, the second in 1937 (with the axiom of choice removed), the third in 1951 (with the axiom of choice reinstated, and with more on valuations), the fourth in 1955 (with the title changed to &quot;Algebra&quot;), the fifth in 1960, the sixth in 1964, the seventh in 1966, the eighth in 1971, the ninth in 1993. For volume 2, the first edition was published in 1931, the second in 1940, the third in 1955 (with the title changed to &quot;Algebra&quot;), the fourth in 1959 (extensively rewritten, with elimination theory replaced by algebraic functions of 1 variable), the fifth in 1967, and the sixth in 1993. The German editions were all published by Springer. 

The first English edition was published in 1949–1950 and was a translation of the second German edition. There was a second edition in 1953, and a third edition under the new title '''Algebra''' in 1970 translated from the 7th German edition of volume 1 and the 5th German edition of volume 2. The three English editions were originally published by Ungar, though the 3rd English edition was later reprinted by Springer.

There were also Russian editions published in 1976 and 1979, and Japanese editions published in 1959 and 1967–1971.

==References==
{{Reflist}}

*{{Citation | last1=Hahn | first1=H. | author1-link=Hans Hahn (mathematician) | last2=Taussky | first2=Olga | author2-link=Olga Taussky-Todd | title=Literaturberichte: Moderne Algebra | url=http://dx.doi.org/10.1007/BF01699101 | doi=10.1007/BF01699101 | year=1932 | journal=Monatshefte für Mathematik und Physik | issn=0026-9255 | volume=39 | issue=1 | pages=A11--A12}}
*{{Citation | last1=Hofreiter | title=Literaturberichte: Moderne Algebra | url=http://dx.doi.org/10.1007/BF01708057 | doi=10.1007/BF01708057 | year=1936 | journal=Monatshefte für Mathematik und Physik | issn=0026-9255 | volume=45 | issue=1 | pages=A25--A26}}
*{{Citation | last1=Hofreiter | title=Literaturberichte: Moderne Algebra | url=http://dx.doi.org/10.1007/BF01707351 | doi=10.1007/BF01707351 | year=1941 | journal=Monatshefte für Mathematik und Physik | issn=0026-9255 | volume=49 | issue=1 | pages=A21}}
*{{Citation | last1=Mac Lane | first1=Saunders | author1-link=Saunders Mac Lane | title=Van der Waerden's  Modern algebra | year=1997 | journal=[[Notices of the American Mathematical Society]] | issn=0002-9920 | volume=44 | issue=3 | pages=321–322|url=http://www.ams.org/notices/199703/index.html}}
*{{Citation | last1=Ore | first1=Oystein | title=Recent Publications: Reviews: Moderne Algebra | url=http://dx.doi.org/10.2307/2301771 | doi=10.2307/2301771 | year=1931 | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=38 | issue=4 | pages=226}}
*{{Citation | last1=Ore | first1=Oystein | title=Book Review: Moderne Algebra | url=http://dx.doi.org/10.1090/S0002-9904-1932-05385-X | doi=10.1090/S0002-9904-1932-05385-X | year=1932 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=38 | issue=5 | pages=327–329}}
*{{Citation | last1=Ore | first1=Oystein | title=Book Review: Moderne Algebra | url=http://dx.doi.org/10.1090/S0002-9904-1938-06737-7 | doi=10.1090/S0002-9904-1938-06737-7 | year=1938 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=44 | issue=5 | pages=320}}
*{{Citation | last1=Noether | first1=Emmy | author1-link=Emmy Noether | title=Hyperkomplexe Größen und Darstellungstheorie | publisher=Springer Berlin / Heidelberg | doi=10.1007/BF01187794 | year=1929 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=30 | pages=641–692}}
*{{Citation | last1=Schlote | first1=K.-H. | editor1-last=Grattan-Guinness | editor1-first=Ivor | title=Landmark writings in western mathematics 1640--1940 | url=http://books.google.com/books?id=UdGBy8iLpocC&amp;pg=PA901 | publisher=Elsevier B. V., Amsterdam | isbn=978-0-444-50871-3 | id={{MR|2169816}} | year=2005 | chapter=Chapter 70 B. L. van der Waerden, Moderne Algebra|pages=901–916}}
*{{Citation | last1=Taussky | first1=Olga | author1-link=Olga Taussky-Todd | title=Literaturberichte: Moderne Algebra | url=http://dx.doi.org/10.1007/BF01708891 | doi=10.1007/BF01708891 | year=1933 | journal=Monatshefte für Mathematik und Physik | issn=0026-9255 | volume=40 | issue=1 | pages=A3--A4}}
*{{Citation | last1=van der Waerden | first1=Bartel Leendert | author1-link=Bartel Leendert van der Waerden | title=Moderne Algebra. Teil I | publisher=[[Springer-Verlag]] | location=Berlin, New York | series= Die Grundlehren der mathematischen Wissenschaften | isbn=978-3-540-56799-8 | id={{MR|0009016}}{{MR|0037277}}{{MR|0069787}}{{MR|0122834}}{{MR|0177027}}{{MR|0263581}} | year=1930 | volume=33}}
*{{Citation | last1=van der Waerden | first1=Bartel Leendert | author1-link=Bartel Leendert van der Waerden | title=Moderne Algebra. Teil II | publisher= Springer-Verlag | series=Die Grundlehren der mathematischen Wissenschaften | isbn=978-3-540-56801-8 | id={{MR|0002841}}{{MR|0009016}}{{MR|0072829}}{{MR|0122835}}{{MR|0233647}} | year=1931 | volume=34}}
*{{Citation | last1=van der Waerden | first1=Bartel Leendert | author1-link=Bartel Leendert van der Waerden | title=Modern Algebra | publisher=Frederick Ungar Publishing Co. | location=New York, N. Y. | year=1949}}
*{{Citation | last1=van der Waerden | first1=Bartel Leendert | author1-link=Bartel Leendert van der Waerden | title=Algebra. Vol 1 | url=http://books.google.com/books?id=Xw8lAQAAIAAJ | publisher=Frederick Ungar Publishing Co. | location=New York, N. Y. | series=Translated by Fred Blum and John R. Schulenberger | isbn=978-0-387-40624-4 | id={{MR|0263582}} Reprinted by Springer | year=1970 }}
*{{Citation | last1=van der Waerden | first1=Bartel Leendert | author1-link=Bartel Leendert van der Waerden | title=Algebra. Vol. 2 | url=http://books.google.com/books?id=oWzoOWp_tWgC | publisher=Frederick Ungar Publishing Co. | location=New York, N. Y. | edition=3rd | series=Translated by John R. Schulenberger | id={{MR|0263583}} Reprinted by Springer | year=1970}}
*{{Citation | last1=van der Waerden | first1=Bartel Leendert | author1-link=Bartel Leendert van der Waerden | title=On the sources of my book  Moderne Algebra | doi=10.1016/0315-0860(75)90034-8 | id={{MR|0465721}} | year=1975 | journal=[[Historia Mathematica]] | issn=0315-0860 | volume=2 | pages=31–40}}
*{{Citation | last1=Zelinsky | first1=Daniel | title=Book Review: Moderne Algebra | url=http://dx.doi.org/10.1090/S0002-9904-1951-09496-3 | doi=10.1090/S0002-9904-1951-09496-3 | year=1951 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=57 | issue=3 | pages=206}}

[[Category:History of mathematics]]
[[Category:Mathematics textbooks]]
[[Category:1930 books]]
[[Category:Algebra]]
[[Category:Springer Science+Business Media books]]</text>
      <sha1>6zw0qwyjwyel1oqbvx6z47nil2vlnjl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Stereotype space</title>
    <ns>0</ns>
    <id>37534627</id>
    <revision>
      <id>611528797</id>
      <parentid>599327377</parentid>
      <timestamp>2014-06-04T13:53:29Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */Added 1 doi to a journal cite using [[Project:AWB|AWB]] (10216)</comment>
      <text xml:space="preserve" bytes="20231">In [[functional analysis]] and related areas of [[mathematics]] '''stereotype spaces''' are [[topological vector space]]s defined by a special variant of [[Reflexive space|reflexivity]] condition. They form a class of spaces with a series of remarkable properties, in particular, this class is very wide (for instance, it contains all [[Fréchet space]]s and thus, all [[Banach space]]s), it consists of spaces satisfying a natural condition of completeness, and it forms a [[closed monoidal category]] with the standard analytical tools for constructing new spaces, like taking closed subspace, quotient space, projective and injective limits, the space of operators, tensor products, etc.
[[File:Stereotype spaces.jpg|thumbnail|Mutual embeddings of the main classes of locally convex spaces]]

==Definition==

A '''stereotype space'''&lt;ref name=Akbarov-1&gt;{{harvtxt|S.S.Akbarov|2003}}.&lt;/ref&gt; is a [[topological vector space]] &lt;math&gt;X&lt;/math&gt; over the field &lt;math&gt;\mathbb{C}&lt;/math&gt; of complex numbers&lt;ref&gt;...or over the field &lt;math&gt;\mathbb{R}&lt;/math&gt; of real numbers, with the similar definition.&lt;/ref&gt; such that the natural map into the second dual space

:&lt;math&gt; i:X\to X^{\star\star},\quad i(x)(f)=f(x),\quad x\in X,\quad f\in X^\star &lt;/math&gt;

is an isomorphism of topological vector spaces (i.e. a [[Linear map|linear]] and a [[Homeomorphism|homeomorphic]] map). Here the ''dual space'' &lt;math&gt; X^\star&lt;/math&gt; is defined as the space of all linear continuous functionals &lt;math&gt;f:X\to\mathbb{C}&lt;/math&gt; endowed with the topology of uniform convergence on [[totally bounded set]]s in ''X'', and the ''second dual space'' &lt;math&gt; X^{\star\star}&lt;/math&gt; is the space dual to &lt;math&gt; X^{\star}&lt;/math&gt; in the same sense.

The following criterion holds:&lt;ref name=Akbarov-1 /&gt; a topological vector space &lt;math&gt;X&lt;/math&gt; is stereotype if and only if it is [[Locally convex space|locally convex]] and satisfies the following two conditions:

:* ''pseudocompleteness'': each [[Totally bounded set|totally bounded]] Cauchy net in &lt;math&gt;X&lt;/math&gt; converges,
:*
:* ''pesudosaturateness'': each closed convex balanced ''capacious''&lt;ref&gt;A set &lt;math&gt;D\subseteq X&lt;/math&gt; is said to be ''capacious'' if for each [[Totally bounded set|totally bounded]] set &lt;math&gt;A\subseteq X&lt;/math&gt; there is a finite set &lt;math&gt;F\subseteq X&lt;/math&gt; such that &lt;math&gt;A\subseteq D+F&lt;/math&gt;.&lt;/ref&gt; set &lt;math&gt;D&lt;/math&gt; in &lt;math&gt;X&lt;/math&gt; is a neighborhood of zero in &lt;math&gt;X&lt;/math&gt;.

The property of being pseudocomplete is a weakening of the usual notion of completeness, while the property of being pseudosaturated is a weakening of the notion of [[Barreled space|barreledness]] of a topological vector space.

==Examples==

Each pseudocomplete [[barreled space]] &lt;math&gt;X&lt;/math&gt; (in particular, each [[Banach space]] and each [[Frechet space|Fréchet space]]) is stereotype. A metrizable locally convex space &lt;math&gt;X&lt;/math&gt; is stereotype if and only if &lt;math&gt;X&lt;/math&gt; is complete. A normed space &lt;math&gt;X&lt;/math&gt; with the &lt;math&gt;X^{\star}&lt;/math&gt;-weak topology is stereotype if and only if X has finite dimension. There exist stereotype spaces which are not [[Mackey space]]s.

Some simple connections between the properties of a stereotype space &lt;math&gt;X&lt;/math&gt; and those of its dual space &lt;math&gt;X^\star&lt;/math&gt; are expressed in the following list of regularities.&lt;ref name=Akbarov-1 /&gt;&lt;ref name=Akbarov-2&gt;{{harvtxt|S.S.Akbarov|2009}}.&lt;/ref&gt; For a stereotype space &lt;math&gt;X&lt;/math&gt;

:* &lt;math&gt;X&lt;/math&gt; is a [[normed space]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt;  &lt;math&gt;X&lt;/math&gt; is a [[Banach space]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt; &lt;math&gt;X^\star&lt;/math&gt; is a [[Smith space]];

:* &lt;math&gt;X&lt;/math&gt; is metrizable &lt;math&gt;\Longleftrightarrow&lt;/math&gt;  &lt;math&gt;X&lt;/math&gt; is a [[Fréchet space]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt;  &lt;math&gt;X^\star&lt;/math&gt; is a [[Brauner space]];

:* &lt;math&gt;X&lt;/math&gt; is [[barreled space|barreled]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt;  &lt;math&gt;X^\star&lt;/math&gt; has the Heine-Borel property;

:* &lt;math&gt;X&lt;/math&gt; is quasibarreled &lt;math&gt;\Longleftrightarrow&lt;/math&gt; in &lt;math&gt;X^\star&lt;/math&gt; if a set &lt;math&gt;T&lt;/math&gt; is absorbed by each barrel, then &lt;math&gt;T&lt;/math&gt; is totally bounded;

:* &lt;math&gt;X&lt;/math&gt; is a [[Mackey space]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt; in &lt;math&gt;X^\star&lt;/math&gt; every &lt;math&gt;X&lt;/math&gt;-weakly compact set is compact;

:* &lt;math&gt;X&lt;/math&gt; is a [[Montel space]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt; &lt;math&gt;X&lt;/math&gt; is barreled and has the Heine-Borel peoperty &lt;math&gt;\Longleftrightarrow&lt;/math&gt; &lt;math&gt;X^\star&lt;/math&gt;  is a Montel space;

:* &lt;math&gt;X&lt;/math&gt; is a space with a weak topology &lt;math&gt;\Longleftrightarrow&lt;/math&gt; in &lt;math&gt;X^\star&lt;/math&gt; every compact set  &lt;math&gt;T&lt;/math&gt; is finite-dimentional;

:* &lt;math&gt;X&lt;/math&gt; is [[separable space|separable]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt; in &lt;math&gt;X^\star&lt;/math&gt; there is a sequence of closed subspaces &lt;math&gt;L_n&lt;/math&gt; of finite co-dimension with trivial intersection: &lt;math&gt;\bigcap_{n=1}^{\infty}L_n=\{0\}&lt;/math&gt;.

:* &lt;math&gt;X&lt;/math&gt; has the (classical) [[approximation property]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt; &lt;math&gt;X^\star&lt;/math&gt; has the (classical) [[approximation property]];

:* &lt;math&gt;X&lt;/math&gt; is [[Uniform space|complete]] &lt;math&gt;\Longleftrightarrow&lt;/math&gt; &lt;math&gt;X^\star&lt;/math&gt; is co-complete&lt;ref&gt;A locally convex space  &lt;math&gt;X&lt;/math&gt; is called ''co-complete'' if each linear functional &lt;math&gt;f:X\to\mathbb{C}&lt;/math&gt; which is continuous on every totally bounded set &lt;math&gt;S\subseteq X&lt;/math&gt;, is automatically continuous on the whole space &lt;math&gt;X&lt;/math&gt;.&lt;/ref&gt;  &lt;math&gt;\Longleftrightarrow&lt;/math&gt; &lt;math&gt;X^\star&lt;/math&gt; is saturated;&lt;ref&gt;A locally convex space &lt;math&gt;X&lt;/math&gt; is said to be ''saturated'' if for an absolutely convex set &lt;math&gt;B\subseteq X&lt;/math&gt; being a neighbourhood of zero in &lt;math&gt;X&lt;/math&gt; is equivalent to the following: for each totally bounded set  &lt;math&gt;S\subseteq X&lt;/math&gt; there is a closed neighbourhood of zero &lt;math&gt;U&lt;/math&gt; in &lt;math&gt;X&lt;/math&gt; such that &lt;math&gt;B\cap S=U&lt;/math&gt;.&lt;/ref&gt;

:* &lt;math&gt;X&lt;/math&gt; is a Pták space&lt;ref&gt;A locally convex space &lt;math&gt;X&lt;/math&gt; is called a ''Pták space'', or a '''fully complete space''', if in its dual space  &lt;math&gt;X^\star&lt;/math&gt; a subspace &lt;math&gt;Q\subseteq X^\star&lt;/math&gt; is &lt;math&gt;X&lt;/math&gt;-weakly closed when it has &lt;math&gt;X&lt;/math&gt;-weakly closed intersection with the polar &lt;math&gt;U^\circ&lt;/math&gt; of each neighbourhood of zero &lt;math&gt;U\subseteq X&lt;/math&gt;.&lt;/ref&gt; &lt;math&gt;\Longleftrightarrow&lt;/math&gt; in &lt;math&gt;X^\star&lt;/math&gt; a subspace &lt;math&gt;L&lt;/math&gt; is closed if it has clodes intersection  &lt;math&gt;L\cap K&lt;/math&gt; with each compact set &lt;math&gt;K\subseteq X^\star&lt;/math&gt;;

:* &lt;math&gt;X&lt;/math&gt; is hypercomplete&lt;ref&gt;A locally convex space &lt;math&gt;X&lt;/math&gt; is said to be ''hypercomplete'' if in its dual space &lt;math&gt;X^\star&lt;/math&gt; every absolutely convex space &lt;math&gt;Q\subseteq X^\star&lt;/math&gt; is &lt;math&gt;X&lt;/math&gt;-weakly closed if it  has &lt;math&gt;X&lt;/math&gt;-weakly closed intersection with the polar &lt;math&gt;U^\circ&lt;/math&gt; of each neighbourhood of zero  &lt;math&gt;U\subseteq X&lt;/math&gt;.&lt;/ref&gt; &lt;math&gt;\Longleftrightarrow&lt;/math&gt; in &lt;math&gt;X^\star&lt;/math&gt; an absolutely convex set  &lt;math&gt;B&lt;/math&gt; is closed if it has the closed intersection &lt;math&gt;B\cap K&lt;/math&gt; with each compact set &lt;math&gt;K\subseteq X^\star&lt;/math&gt;.

==History==

The first results on this type of reflexivity of topological vector spaces were obtained by M. F. Smith&lt;ref name=Smith&gt;{{harvtxt|M.F.Smith|1952}}.&lt;/ref&gt; in 1952. Further investigations were conducted by B. S. Brudovskii,
&lt;ref name=Brudovski&gt;{{harvtxt|B.S.Brudowski|1967}}.&lt;/ref&gt; W. C. Waterhouse,&lt;ref name=Waterhouse&gt;{{harvtxt|W.C.Waterhouse|1968}}.&lt;/ref&gt; K. Brauner,&lt;ref name=Brauner&gt;{{harvtxt|K.Brauner|1973}}.&lt;/ref&gt; S. S. Akbarov,&lt;ref name=Akbarov-1 /&gt;&lt;ref name=Akbarov-2 /&gt;&lt;ref name=Akbarov-3&gt;{{harvtxt|S.S.Akbarov|2013}}.&lt;/ref&gt; and E. T. Shavgulidze.&lt;ref name=Akbarov-Shavgulidze&gt;{{harvtxt|S.S.Akbarov, E.T.Shavgulidze|2003}}.&lt;/ref&gt;

==Pseudocompletion and pseudosaturation==

Each [[locally convex space]] &lt;math&gt;X&lt;/math&gt; can be transformed into a stereotype space with the help of the standard operations of pseudocompletion and pseudosaturation defined by the following two propositions.&lt;ref name=Akbarov-1 /&gt;

1. With any locally convex space &lt;math&gt;X&lt;/math&gt;, one can associate a linear continuous map &lt;math&gt;\triangledown_X: X\to X^\triangledown&lt;/math&gt; into some pseudocomplete locally convex space &lt;math&gt;X^\triangledown&lt;/math&gt;, called ''pseudocompletion'' of &lt;math&gt;X&lt;/math&gt;, in such a way that the following conditions are fulfilled:
:* &lt;math&gt;X&lt;/math&gt; is pseudocomplete if and only if &lt;math&gt;\triangledown_X: X\to X^\triangledown&lt;/math&gt; is an isomorphism;
:* for any linear continuous map &lt;math&gt;\varphi:X\to Y&lt;/math&gt; of locally convex spaces, there exists a unique linear continuous map &lt;math&gt;\varphi^\triangledown:X^\triangledown\to Y^\triangledown&lt;/math&gt; such that &lt;math&gt;\triangledown_Y\circ\varphi=\varphi^\triangledown\circ\triangledown_X&lt;/math&gt;.

One can imagine the pseudocompletion of &lt;math&gt;X&lt;/math&gt; as the &quot;nearest to &lt;math&gt;X&lt;/math&gt; from the outside&quot; pseudocomplete locally convex space, so that the operation &lt;math&gt;X\mapsto X^\triangledown&lt;/math&gt; adds to &lt;math&gt;X&lt;/math&gt; some supplementary elements, but does not change the topology of &lt;math&gt;X&lt;/math&gt; (like the usual operation of completion).

2. With any locally convex space &lt;math&gt;X&lt;/math&gt;, one can associate a linear continuous map &lt;math&gt;\vartriangle_X:X^\vartriangle\to X&lt;/math&gt; from some pseudosaturated locally convex space &lt;math&gt;X^\vartriangle&lt;/math&gt;, called ''pseudosaturation'' of &lt;math&gt;X&lt;/math&gt;, in such a way that the following conditions are fulfilled:
:* &lt;math&gt;X&lt;/math&gt; is pseudosaturated if and only if &lt;math&gt;\vartriangle_X:X^\vartriangle\to X&lt;/math&gt; is an isomorphism;
:* for any linear continuous map &lt;math&gt;\varphi:X\to Y&lt;/math&gt; of locally convex spaces, there exists a unique linear continuous map &lt;math&gt;\varphi^\vartriangle:X^\vartriangle\to Y^\vartriangle&lt;/math&gt; such that &lt;math&gt;\varphi\circ\vartriangle_X=\vartriangle_Y\circ\varphi^\vartriangle&lt;/math&gt;.

The pseudosaturation of &lt;math&gt;X&lt;/math&gt; can be imagined as the &quot;nearest to &lt;math&gt;X&lt;/math&gt; from the inside&quot; pseudosaturated locally convex space, so that the operation &lt;math&gt;X\mapsto X^\vartriangle&lt;/math&gt; strengthen the topology of &lt;math&gt;X&lt;/math&gt;, but does not change the elements of &lt;math&gt;X&lt;/math&gt;.

If &lt;math&gt;X&lt;/math&gt; is a pseudocomplete locally convex space, then its pseudosaturation &lt;math&gt;X^\vartriangle&lt;/math&gt; is stereotype. Dually, if &lt;math&gt;X&lt;/math&gt; is a pseudosaturated locally convex space, then its pseudocompletion &lt;math&gt;X^\triangledown&lt;/math&gt;  is stereotype. For arbitrary locally convex space &lt;math&gt;X&lt;/math&gt; the spaces  &lt;math&gt;X^{\vartriangle\triangledown}&lt;/math&gt; and &lt;math&gt;X^{\triangledown\vartriangle}&lt;/math&gt; are stereotype.&lt;ref&gt;It is not clear (2013) whether &lt;math&gt;X^{\vartriangle\triangledown}&lt;/math&gt; and &lt;math&gt;X^{\triangledown\vartriangle}&lt;/math&gt; coincide.&lt;/ref&gt;

==Category of stereotype spaces==

The class '''Ste''' of stereotype spaces forms a category with linear continuous maps as morphisms and has the following properties:,&lt;ref name=Akbarov-1 /&gt;&lt;ref name=Akbarov-3 /&gt;

:* '''Ste''' is [[Pre-abelian category|pre-abelian]];
:*
:* '''Ste''' is [[Complete category|complete]] and [[Complete category|co-complete]];
:*
:* '''Ste''' is autodual with respect to the functor &lt;math&gt;X\to X^\star&lt;/math&gt; of passing to the dual space;
:*
:* '''Ste''' is a category with ''nodal decomposition'': each morphism &lt;math&gt;\varphi:X\to Y&lt;/math&gt; has a decomposition &lt;math&gt;\varphi=\sigma\circ\beta\circ\pi&lt;/math&gt;, where &lt;math&gt;\pi&lt;/math&gt; is a strong epimorphism, &lt;math&gt;\beta&lt;/math&gt; a bimorphism, and &lt;math&gt;\sigma&lt;/math&gt; a strong monomorphism.

For any two stereotype spaces &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; the ''stereotype space of operators'' &lt;math&gt;\text{Hom}(X,Y)&lt;/math&gt; from &lt;math&gt;X&lt;/math&gt; into &lt;math&gt;Y&lt;/math&gt;, is defined as the pseudosaturation of the space &lt;math&gt;\text{L}(X,Y)&lt;/math&gt; of all linear continuous maps &lt;math&gt;\varphi:X\to Y&lt;/math&gt; endowed with the topology of uniform convergeance on totally bounded sets. The space &lt;math&gt;\text{Hom}(X,Y)&lt;/math&gt; is stereotype. It defines two natural tensor products

: &lt;math&gt;X\circledast Y:= \text{Hom}(X,Y^\star)^\star,&lt;/math&gt;

:&lt;math&gt; X\odot Y := \text{Hom}(X^\star,Y).&lt;/math&gt;

The following natural identities hold:&lt;ref name=Akbarov-1 /&gt;

:&lt;math&gt;\mathbb{C}\circledast X\cong X\cong X\circledast \mathbb{C},
&lt;/math&gt;

:&lt;math&gt;
\mathbb{C}\odot X\cong X\cong X\odot\mathbb{C},
&lt;/math&gt;

:&lt;math&gt; X\circledast Y\cong Y\circledast X,&lt;/math&gt;

:&lt;math&gt;
X\odot Y\cong Y\odot X,
&lt;/math&gt;

:&lt;math&gt; (X\circledast Y)\circledast Z\cong X\circledast (Y\circledast Z),&lt;/math&gt;

:&lt;math&gt;
(X\odot Y)\odot Z\cong X\odot (Y\odot Z),
&lt;/math&gt;

:&lt;math&gt; (X\circledast Y)^\star\cong Y^\star\odot X^\star,&lt;/math&gt;

:&lt;math&gt;
(X\odot Y)^\star\cong Y^\star\circledast X^\star,
&lt;/math&gt;

:&lt;math&gt; 
\text{Hom}(X\circledast Y,Z)\cong \text{Hom}(X,\text{Hom}(Y,Z)),&lt;/math&gt;

:&lt;math&gt;
\text{Hom}(X,Y\odot Z)\cong \text{Hom}(X,Y)\odot Z
&lt;/math&gt;

As a corollary,
:* '''Ste''' is a symmetric [[monoidal category]] with respect to the bifunctor &lt;math&gt;\odot&lt;/math&gt; and a symmetric [[closed monoidal category]] with respect to the bifunctor &lt;math&gt;\circledast&lt;/math&gt; and the internal hom-functor &lt;math&gt;\text{Hom}&lt;/math&gt;.

==Stereotype approximation property==

A stereotype space &lt;math&gt;X&lt;/math&gt; is said to have the ''stereotype approximation property'', if each linear continuous map &lt;math&gt;\varphi:X\to X&lt;/math&gt; can be approximated in the stereotype space of operators &lt;math&gt;\text{Hom}(X,X)&lt;/math&gt; by the linear continuous maps of finite rank. This condition is weaker than the existence of the [[Schauder basis]], but formally stronger than the classical [[approximation property]] (however, it is not clear (2013) whether the stereotype approximation property coincide with the classical one, or not). The following proposition holds:

:* If two stereotype spaces &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; have the stereotype approximation property, then the spaces &lt;math&gt;\text{Hom}(X,Y)&lt;/math&gt;, &lt;math&gt;X\circledast Y&lt;/math&gt; and &lt;math&gt;X\odot Y&lt;/math&gt; have the stereotype approximation property as well.&lt;ref name=Akbarov-1 /&gt;

In particular, if &lt;math&gt;X&lt;/math&gt; has the stereotype approximation property, then the same is true for &lt;math&gt;X^\star&lt;/math&gt; and for &lt;math&gt;\text{Hom}(X,X)&lt;/math&gt;.

==Applications==

Being a symmetric monoidal category, '''Ste''' generates the notions of a ''stereotype algebra'' (as a [[Monoid (category theory)|monoid]] in '''Ste''') and a ''stereotype module'' (as a module in '''Ste''' over such a monoid), and for each stereotype algebra &lt;math&gt;A&lt;/math&gt; the categories &lt;sub&gt;&lt;math&gt;A&lt;/math&gt;&lt;/sub&gt;'''Ste''' and '''Ste'''&lt;sub&gt;&lt;math&gt;A&lt;/math&gt;&lt;/sub&gt; of left and right stereotype modules over &lt;math&gt;A&lt;/math&gt; are [[Enriched category|enriched categories]] over '''Ste'''.&lt;ref name=Akbarov-1 /&gt; This distinguishes the category '''Ste''' from the other known categories of locally convex spaces, since up to the recent time only the category '''Ban''' of Banach spaces and the category '''Fin''' of finite-dimensional spaces had been known to possess this property. On the other hand, the category '''Ste''' is so wide, and the tools for creating new spaces in '''Ste''' are so diverse, that this suggests the idea that all the results of functional analysis can be reformulated inside the stereotype theory without essential losses. On this way one can even try to completely replace the category of locally convex spaces in functional analysis (and in related areas) by the category '''Ste''' of stereotype spaces with the view of possible simplifications – this program was announced by S. Akbarov in 2005&lt;ref name=Akbarov-4&gt;{{harvtxt|S.S.Akbarov|2005}}.&lt;/ref&gt; and the following results can be considered as evidences of its reasonableness:

:* In the theory of stereotype spaces the approximation property is inherited by the spaces of operators and by tensor products. This allows to reduce the list of counterexamples in comparison with the Banach theory, where as is known the space of operators does not inherit the approximation property.&lt;ref name=Szankowski&gt;{{harvtxt|A.Szankowski|1981}}.&lt;/ref&gt;
:*
:* The arising theory of stereotype algebras allows to simplify constructions in the duality theories for non-commutative groups. In particular, the group algebras in these theories become [[Hopf algebra]]s in the standard algebraic sense.&lt;ref name=Akbarov-2 /&gt;&lt;ref name=Kuznetsova&gt;{{harvtxt|J.Kuznetsova|2013}}.&lt;/ref&gt;

==Notes==
{{reflist}}

==References==
* {{cite book
 | last = Schaefer
 | first = Helmuth H. &lt;!-- | authorlink = Helmuth Schaefer --&gt;
 | year = 1966
 | title = Topological vector spaces
 | series=
 | volume=
 | publisher = The MacMillan Company
 | location = New York
 | isbn = 0-387-98726-6
}}

* {{cite book |last=Robertson |first=A.P. |coauthors=Robertson, W.J. |title= Topological vector spaces |series=Cambridge Tracts in Mathematics |volume=53 |year=1964 |publisher= [[Cambridge University Press]] }}

*{{cite journal|last=Smith|first=M.F.|title=[http://www.jstor.org/stable/1969798 The Pontrjagin duality theorem in linear spaces]|journal=Annals of Mathematics|year=1952|volume=56|issue=2|pages=248–253|doi=10.2307/1969798}}

*{{cite journal|last=Brudovski|first=B.S.|title=On k- and c-reflexivity of locally convex vector spaces|journal=Lithuanian Mathematical Journal|year=1967|volume=7|issue=1|pages=17–21}}

*{{cite journal|last=Waterhouse|first=W.C.|title=[http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.pjm/1102986038&amp;page=record Dual groups of vector spaces]|journal=Pac. J. Math.|year=1968|volume=26|issue=1|pages=193–196|doi=10.2140/pjm.1968.26.193}}

*{{cite journal|last=Brauner|first=K.|title=Duals of Frechet spaces and a generalization of the Banach-Dieudonne theorem|journal=Duke Math. Jour.|year=1973|volume=40|issue=4|pages=845–855|doi=10.1215/S0012-7094-73-04078-7}}

*{{cite journal|last=Akbarov|first=S.S.|title=[http://www.springerlink.com/content/k62m72960101g6q2/ Pontryagin duality in the theory of topological vector spaces and in topological algebra]|journal=Journal of Mathematical Sciences|year=2003|volume=113|issue=2|pages=179–349|doi=10.1023/A:1020929201133}}

*{{cite journal|last=Akbarov|first=S.S.|title=[http://www.springerlink.com/content/u07317731010573l/ Holomorphic functions of exponential type and duality for Stein groups with algebraic connected component of identity]{{subscription required}}|journal=Journal of Mathematical Sciences|year=2009|volume=162|issue=4|pages=459–586|doi=10.1007/s10958-009-9646-1}}

*{{cite arXiv|last=Akbarov|first=S.S.|title=Envelopes and imprints in categories|year=1970|eprint=1110.2013v7|class=math.FA}}

*{{cite journal|last=Akbarov|first=S.S.|coauthors=Shavgulidze, E.T.|title=On two classes of spaces reflexive in the sense of Pontryagin|journal=Mat. Sbornik|year=2003|volume=194|issue=10|pages=3–26}}

*{{cite journal|last=Kuznetsova|first=J.|title=A duality for Moore groups|journal=Journal of Operator Theory|year=2013|volume=69|issue=2|pages=101–130|arxiv=0907.1409|bibcode=2009arXiv0907.1409K|doi=10.7900/jot.2011mar17.1920}}

*{{cite journal|last=Akbarov|first=S.S.|title=[http://webmail.impan.gov.pl/cgi-bin/bc/pdf?bc67-0-05 Pontryagin duality and topological algebras]|journal=Banach Center Publications|year=2005|volume=67|pages=55–71}}

*{{cite journal|last=Szankowski|first=A.|title=B(H) does not have the approximation property|journal=Act. Math.|year=1981|volume=147|pages=147:89–108|doi=10.1007/bf02392870}}

&lt;!-- This will add a notice to the bottom of the page and won't blank it! The new template which says that your draft is waiting for a review will appear at the bottom; simply ignore the old (grey) drafted templates and the old (red) decline templates. A bot will update your article submission. Until then, please don't change anything in this text box and press &quot;Save page&quot;. --&gt;

&lt;!-- This will add a notice to the bottom of the page and won't blank it! The new template which says that your draft is waiting for a review will appear at the bottom; simply ignore the old (grey) drafted templates and the old (red) decline templates. A bot will update your article submission. Until then, please don't change anything in this text box and press &quot;Save page&quot;. --&gt;

{{Functional Analysis}}

[[Category:Functional analysis]]
[[Category:Algebra]]</text>
      <sha1>4inr5316i2powd6kplav582kz29k5cl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Portal:Algebra</title>
    <ns>100</ns>
    <id>16791919</id>
    <revision>
      <id>609267506</id>
      <parentid>609266764</parentid>
      <timestamp>2014-05-19T17:54:33Z</timestamp>
      <contributor>
        <ip>71.174.103.210</ip>
      </contributor>
      <comment>some one said something wrong</comment>
      <text xml:space="preserve" bytes="1687">&lt;!-- This portal was created using subst:box portal skeleton  --&gt;
{{browsebar}}__NOTOC__

&lt;div style=&quot;clear:both; width:100%&quot;&gt;

{{/box-header|&lt;big&gt;{{PAGENAME}}&lt;/big&gt;|{{FULLPAGENAME}}/Intro|}}
{{{{FULLPAGENAME}}/Intro}}
{{/box-footer|[[{{PAGENAME}}|Read more]]}}

&lt;div style=&quot;text-align:center; margin:0.25em auto 0.75em&quot;&gt;{{purge|'''View new selections below''' (purge)}}&lt;/div&gt;

&lt;div style=&quot;float:left; width:55%;&quot;&gt; &lt;!-- This width add to the the margin below to equal 99%--&gt;

{{Random portal component|max=7|header=Selected article|subpage=Selected article}}

{{/box-header|Categories|{{FULLPAGENAME}}/Categories|}}
{{/Categories}}
{{/box-footer-empty}}

{{/box-header|WikiProjects|Portal:Mathematics/Projects|}}
{{Portal:Mathematics/Projects}}
{{/box-footer-empty}}

&lt;!--
{{/box-header|Things to do|{{FULLPAGENAME}}/Opentask|}}
{{{{FULLPAGENAME}}/Opentask}}
{{/box-footer|}}--&gt;
&lt;/div&gt;

&lt;div style=&quot;width:44%; float:right;&quot;&gt;

{{Random portal component|max=2|header=Selected picture|subpage=Selected picture}}

{{/box-header|Did you know?|{{FULLPAGENAME}}/Did you know|}}
{{{{FULLPAGENAME}}/Did you know}}
{{/box-footer-empty}}
&lt;/div&gt;

&lt;div style=&quot;clear:both; width:100%&quot;&gt;

{{/box-header|''Topics in algebra''|Portal:Algebra/Topics}}
&lt;div style=&quot;margin: -.3em -1em -1em -1em;&quot;&gt;{{/Topics}}&lt;/div&gt;
{{/box-footer-empty}}

{{/box-header|Related portals|Portal:Mathematics/RelatedPortals|}}
{{Portal:Mathematics/RelatedPortals}}
{{/box-footer-empty}}

{{/box-header|Wikimedia|{{FULLPAGENAME}}/Wikimedia|}}
{{{{FULLPAGENAME}}/Wikimedia}}
{{/box-footer-empty}}

&lt;/div&gt;
{{portals}}
{{purgepage}}

__NOTOC__ __NOEDITSECTION__

[[Category:Algebra|*{{FULLPAGENAME}}]]
[[Category:Mathematics portals]]</text>
      <sha1>5vo0bfgnkr0n5gecf6vh06i0677kxrq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Quadratic-linear algebra</title>
    <ns>0</ns>
    <id>37890004</id>
    <revision>
      <id>527902529</id>
      <parentid>527901355</parentid>
      <timestamp>2012-12-13T19:43:11Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>[[presentation of a group]] may not be a perfect target for this link, but maybe there's nothing else suitable</comment>
      <text xml:space="preserve" bytes="912">In mathematics, a '''quadratic-linear algebra''' is an [[algebra over a field]] with a [[presentation of a group|presentation]] such that all relations are sums of monomials of degrees 1 or 2 in the generators. They were introduced by {{harvs|txt|last1=Polishchuk|last2=Positselski|year=2005|loc=p.101}}. An example is the [[universal enveloping algebra]] of a [[Lie algebra]], with generators a basis of the Lie algebra and relations of the form&amp;nbsp;''XY''&amp;nbsp;–&amp;nbsp;''YX''&amp;nbsp;–&amp;nbsp;[''X'',&amp;nbsp;''Y'']&amp;nbsp;=&amp;nbsp;0.

==References==

*{{Citation | last1=Polishchuk | first1=Alexander | last2=Positselski | first2=Leonid | title=Quadratic algebras | url=http://books.google.com/books?id=5_ZrCKU4NpAC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=University Lecture Series | isbn=978-0-8218-3834-1 | id={{MR|2177131}} | year=2005 | volume=37}}

[[Category:Algebra]]</text>
      <sha1>kep38ee6gx0r823k9ngtf0ebu14bbap</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Variable (mathematics)</title>
    <ns>0</ns>
    <id>3728109</id>
    <revision>
      <id>612441464</id>
      <parentid>612441453</parentid>
      <timestamp>2014-06-11T02:28:09Z</timestamp>
      <contributor>
        <username>Bollyjeff</username>
        <id>12037174</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/58.166.243.89|58.166.243.89]] ([[User talk:58.166.243.89|talk]]) ([[WP:HG|HG 3]])</comment>
      <text xml:space="preserve" bytes="17548">{{about||variables in computer science|Variable (computer science)|other uses|Variable (disambiguation)}}

In [[elementary mathematics]], a '''variable''' is an alphabetic character representing a number which is either arbitrary or not fully specified or unknown. Making [[Algebra#Algebra as a branch of mathematics|algebraic computations]] with variables as if they were explicit numbers allows one to solve a range of problems in a single computation. A typical example is the [[quadratic formula]], which allows to solve every [[quadratic equation]] by simply substituting the numeric values of the coefficients of the given equation to the variables that represent them.

The concept of '''variable''' is also fundamental in [[calculus]].
Typically, a [[function (mathematics)|function]] {{math|1=''y'' = ''f''(''x'')}} involves two variables, its [[argument of a function|argument]] {{math|''x''}} and its [[value (mathematics)|value]] {{math|''y''}}. The term &quot;variable&quot; comes from the fact that, when the argument (also called the &quot;variable of the function&quot;) ''varies'', then the value ''varies'' accordingly.&lt;ref&gt;{{cite web | url=http://cstl.syr.edu/fipse/algebra/unit1/..%5Cpart4%5Cappend1.htm | title =Appendix One Review of Constants and Variables | author =''[[Syracuse University]]'' | publisher =cstl.syr.edu}}&lt;/ref&gt;

In more advanced [[mathematics]], a '''variable''' is simply a symbol representing some data, which is commonly a number, but may also be any mathematical object such as a [[vector (mathematics)|vector]], a [[matrix (mathematics)|matrix]] or even a function. In this case, the original property of &quot;variability&quot; of a variable is not kept (except, sometimes, for informal explanations).

Similarly, in [[computer science]], a '''[[variable (computer science)|variable]]''' is a name (commonly an alphabetic character or a word) representing some value represented in computer memory. In [[mathematical logic]], a '''variable''' is either a symbol representing an unspecified [[term (logic)|term]] of the theory, or a basic object of the theory, which is manipulated without referring to its possible intuitive interpretation.

==Genesis and evolution of the concept==
[[François Viète]] introduced at the end of 16th century the idea of representing known and unknown numbers by letters, nowadays called variables, and of computing with them as if they were numbers, in order to obtain, at the end, the result by a simple replacement. [[François Viète]]'s convention was to use consonants for known values and vowels for unknowns.&lt;ref name=&quot;Fraleigh&quot;&gt;{{cite book
 | last = Fraleigh
 | first = John B.
 | authorlink =
 | title = A First Course in Abstract Algebra
 | publisher = [[Addison-Wesley]]
 | series = 
 | volume =
 | edition = 4
 | year = 1989
 | location = United States
 | pages = 276
 | language =
 | url =
 | doi =
 | id =
 | isbn = 0-201-52821-5
 | mr =
 | zbl =
 | jfm = }}&lt;/ref&gt; 

In 1637, [[René Descartes]] &quot;invented the convention of representing unknowns in equations by ''x'', ''y'', and ''z'', and knowns by ''a'', ''b'', and ''c''&quot;.&lt;ref&gt;Tom Sorell, ''Descartes: A Very Short Introduction'', (2000). New York: Oxford University Press. p. 19.&lt;/ref&gt; Contrarily to Viète's convention, Descartes' one is still commonly in use.

Starting in the 1660s, [[Isaac Newton]] and [[Gottfried Wilhelm Leibniz]] independently developed the [[infinitesimal calculus]], which essentially consists of studying how an infinitesimal variation of a ''variable quantity'' induces a corresponding variation of another quantity which is a ''[[function (mathematics)|function]]'' of the first variable (quantity). Almost a century later [[Leonhard Euler]] fixed the terminology of infinitesimal calculus and introduced the notation {{math|1=''y'' = ''f''(''x'')}} for a function {{math|''f''}}, its '''variable''' {{math|''x''}} and its value {{math|''y''}}. Until the end of the 19th century, the word ''variable'' referred almost exclusively to the [[argument of a function|arguments]] and the [[value (mathematics)|values]] of functions.

In the second half of the 19th century, it appeared that the foundation of infinitesimal calculus was not formalized enough to deal with apparent paradoxes such as a [[continuous function]] which is nowhere [[differentiable function|differentiable]]. To solve this problem, [[Karl Weierstrass]] introduced a new formalism consisting of replacing the intuitive notion of [[limit (mathematics)|limit]] by a formal definition. The older notion of limit was &quot;when the ''variable'' {{math|''x''}} varies and tends toward {{math|''a''}}, then {{math|''f''(''x'')}} tends toward {{math|''L''}}&quot;, without any accurate definition of &quot;tends&quot;. Weierstrass replaced this sentence by the formula
:&lt;math&gt;(\forall \epsilon &gt;0) (\exists \eta &gt;0) (\forall x) \;|x-a|&lt;\eta \Rightarrow |L-f(x)|&lt;\epsilon,&lt;/math&gt;
in which none of the five variables is considered as varying. 

This static formulation led to the modern notion of variable which is simply a symbol representing a [[mathematical object]] which either is unknown or may be replaced by any element of a given [[set (mathematics)|set]]; for example, the set of [[real number]]s.

==Specific kinds of variables==
It is common that many variables appear in the same mathematical formula, which play different roles. Some names or qualifiers have been introduced to distinguish them. 
For example, in the general [[cubic equation]]
:&lt;math&gt;ax^3+bx^2+cx+d=0,&lt;/math&gt;
there are five variables. Four of them, {{math|''a'', ''b'', ''c'', ''d''}} represent given numbers, and the last one, {{math|''x'',}} represents the ''unknown'' number, which is a solution of the equation. To distinguish them, the variable {{math|''x''}} is called ''a unknown'', and the other variables are called ''parameters'' or ''coefficients'', or sometimes ''constants'', although this last terminology is incorrect for an equation and should be reserved for the [[function (mathematics)|function]] defined by the left-hand side of this equation.

In the context of functions, the term ''variable'' refers commonly to the arguments of the functions. This is typically the case in sentences like &quot;[[function of a real variable]]&quot;, &quot;{{math|''x''}} is the variable of the function {{math|1=''f'': ''x'' ↦ ''f''(''x'')}}&quot;, &quot;{{math|''f''}} is a function of the variable {{math|''x''}}&quot; (meaning that the argument of the function is referred to by the variable {{math|''x''}}). 

In the same context, the variables that are independent of {{math|''x''}} define [[constant function]]s and are therefore called ''constant''. For example, a ''[[constant of integration]]'' is an arbitrary constant function that is added to a particular [[antiderivative]] to obtain the other antiderivatives. Because the strong relationship between [[polynomial]]s and [[polynomial function]], the term &quot;constant&quot; is often used to denote the coefficients of a polynomial, which are constant functions of the indeterminates.

This use of &quot;constant&quot; as an abbreviation of &quot;constant function&quot; must be distinguished from the normal meaning of the word in mathematics. A '''constant''', or '''[[mathematical constant]]''' is a well and unambiguously defined number or other mathematical object, as, for example, the numbers 0, 1, [[Pi|{{math|''π''}}]] and the [[identity element]] of a [[group (mathematics)|group]].

Here are other specific names for variables.

* A '''unknown''' is a variable in which an [[equation]] has to be solved for.
* An '''[[indeterminate (variable)|indeterminate]]''' is a symbol, commonly called variable, that appears in a [[polynomial]] or a [[formal power series]]. Formally speaking, an indeterminate is not a variable, but a [[constant (mathematics)|constant]] in the [[polynomial ring]] of the ring of formal power series. However, because of the strong relationship between polynomials or power series and the [[function (mathematics)|functions]] that they define, many authors consider indeterminates as a special kind of variables.
* A '''[[parameter]]''' is a quantity (usually a number) which is a part of the input of a problem, and remains constant during the whole solution of this problem. For example, in [[mechanics]] the mass and the size of a solid body are ''parameters'' for the study of its movement. It should be noted that in [[computer science]], ''parameter'' has a different meaning and denotes an argument of a function.
* '''[[Free variables and bound variables]]'''
* A '''[[random variable]]''' is a kind of variable that is used in [[probability theory]] and its applications.

It should be emphasized that all these denominations of variables are of [[semantics|semantic]] nature and that the way of computing with them ([[syntax (logic)|syntax]]) is the same for all.

===Dependent and independent variables===
{{main|Dependent and independent variables}}
In [[calculus]] and its application to [[physics]] and other sciences, it is rather common to consider a variable, say {{math|''y''}}, whose possible values depend of the value of another variable, say {{math|''x''}}. In mathematical terms, the ''dependent'' variable {{math|''y''}} represents the value of a [[function (mathematics)|function]] of {{math|''x''}}. To simplify formulas, it is often useful to use the same symbol for the dependent variable {{math|''y''}} and the function mapping {{math|''x''}} onto {{math|''y''}}. For example, the state of a physical system depends on measurable quantities such as the [[pressure]], the [[temperature]], the spatial position, ..., and all these quantities varies when the system evolves, that is, they are function of the time. In the formulas describing the system, these quantities are represented by variables which are dependent on the time, and thus considered implicitly as functions of the time.

Therefore, in a formula, a '''dependent variable''' is a variable that is implicitly a function of another (or several other) variables. An '''independent variable''' is a variable that is not dependent.&lt;ref&gt;Edwards Art. 5&lt;/ref&gt;

The property of a variable to be dependent or independent depends often of the point of view and is not intrinsic. For example, in the notation {{math|''f''(''x'', ''y'', ''z'')}}, the three variables may be all independent and the notation represents a function of three variables. On the other hand, if {{math|''y''}} and {{math|''z''}} depend on {{math|''x''}} (are ''dependent variables'') then the notation represent a function of the single ''independent variable'' {{math|''x''}}.&lt;ref&gt;Edwards Art. 6&lt;/ref&gt;

===Examples===
If one defines a function ''f'' from the [[real number]]s to the real numbers by

:&lt;math&gt;f(x) = x^2+\sin(x+4)\ &lt;/math&gt;

then ''x'' is a variable standing for the [[argument of a function|argument]] of the function being defined, which can be any real number. In the identity

:&lt;math&gt;\sum_{i=1}^n i = \frac{n^2+n}2\ &lt;/math&gt;

the variable ''i'' is a summation variable which designates in turn each of the integers 1, 2, ..., ''n'' (it is also called '''index''' because its variation is over a discrete set of values) while ''n'' is a parameter (it does not vary within the formula).

In the theory of [[polynomials]], a polynomial of degree 2 is generally denoted as ''ax''&lt;sup&gt;2&lt;/sup&gt; + ''bx'' + ''c'', where ''a'', ''b'' and ''c'' are called [[coefficient]]s (they are assumed to be fixed, i.e., parameters of the problem considered) while ''x'' is called a variable. When studying this polynomial for its [[polynomial function]] this ''x'' stands for the function argument. When studying the polynomial as an object in itself, ''x'' is taken to be an indeterminate, and would often be written with a capital letter instead to indicate this status.

==Notation==
In mathematics, the variables are generally denoted by a single letter. However, this letter is frequently followed by a subscript, as in {{math|''x''&lt;sub&gt;2&lt;/sub&gt;}}, and this subscript may be a number, another variable ({{math|''x''&lt;sub&gt;''i''&lt;/sub&gt;}}), a word or the abbreviation of a word ({{math|''x''&lt;sub&gt;in&lt;/sub&gt;}} and {{math|''x''&lt;sub&gt;out&lt;/sub&gt;}}), and even a [[mathematical expression]]. Under the influence of [[computer science]], one may encounter in pure mathematics some variable names consisting in several letters and digits. 

Following the 17th century French philosopher and mathematician, [[René Descartes]], letters at the beginning of the alphabet, e.g.  ''a'', ''b'', ''c'' are commonly used for known values and parameters, and letters at the end of the alphabet, e.g. ''x'', ''y'', ''z'', and ''t'' are commonly used for unknowns and variables of functions.&lt;ref name=E004&gt;Edwards Art. 4&lt;/ref&gt; In printed [[mathematics]], the norm is to set variables and constants in an [[italic type]]face.&lt;ref&gt;William L. Hosch (editor), ''The Britannica Guide to Algebra and Trigonometry'', Britannica Educational Publishing, The Rosen Publishing Group, 2010, ISBN 1615302190, 9781615302192, [http://books.google.co.uk/books?id=ad0P0elU1_0C&amp;lpg=PA71&amp;dq=elementary%20algebra%20letters%20alphabet%20constants%20variables&amp;pg=PA71#v=onepage&amp;q=letters&amp;f=false page 71]&lt;/ref&gt;

For example, a general quadratic function is conventionally written as:
:&lt;math&gt;a x^2 + b x + c\, ,&lt;/math&gt;
where ''a'', ''b'' and ''c'' are parameters (also called constants, because they are [[constant function]]s), while ''x'' is the variable of the function. A more explicit way to denote this function is
:&lt;math&gt;x\mapsto a x^2 + b x + c \, ,&lt;/math&gt;
which makes the function-argument status of ''x'' clear, and thereby implicitly the constant status of ''a'', ''b'' and ''c''. Since ''c'' occurs in a term that is a constant function of ''x'', it is called the [[constant term]].&lt;ref&gt;{{cite book | last = Foerster | first = Paul A. | title = Algebra and Trigonometry: Functions and Applications, Teacher's Edition | edition = Classics | year = 2006 | isbn = 0-13-165711-9 | publisher = [[Prentice Hall]] | location = Upper Saddle River, NJ}}&lt;/ref&gt;{{rp|18}}

Specific branches and applications of mathematics usually have specific [[naming convention]]s for variables.  Variables with similar roles or meanings are often assigned consecutive letters.  For example, the three axes in 3D [[coordinate space]] are conventionally called ''x'', ''y'', and ''z''.  In physics, the names of variables are largely determined by the [[physical quantity]] they describe, but various naming conventions exist.
A convention often followed in [[probability]] and [[statistics]] is to use ''X'', ''Y'', ''Z'' for the names of [[random variable]]s, keeping ''x'', ''y'', ''z'' for variables representing corresponding actual values.

There are many other notational usages. Usually, variables that play a similar role are represented by consecutive letters or by the same letter with different [[subscript]]. Below are some of the most common usages.

* ''a'', ''b'', ''c'', and ''d'' (sometimes extended to ''e'' and ''f'') often represent parameters or [[coefficient]]s.
* ''a''&lt;sub&gt;0&lt;/sub&gt;, ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ... play a similar role, when otherwise too many different letters would be needed.
* ''a&lt;sub&gt;i&lt;/sub&gt;'' or ''u&lt;sub&gt;i&lt;/sub&gt;'' is often used to denote the ''i''-th term of a [[sequence]] or the ''i''-th coefficient of a [[series (mathematics)|series]].
* ''f'' and ''g'' (sometimes ''h'') commonly denote [[Function (mathematics)|functions]].
* ''i'', ''j'', and ''k'' (sometimes ''l'' or ''h'') are often used to denote varying [[integer]]s or indices in an [[indexed family]].
* ''l'' and ''w'' are often used to represent the length and width of a figure.
* ''n'' usually denotes a fixed integer, such as a count of objects or the degree of an [[equation]].
** When two integers are needed, for example for the dimensions of a [[matrix (mathematics)|matrix]], one uses commonly ''m'' and ''n''.
* ''p'' often denotes a [[prime number]]s or a [[probability]].
* ''q'' often denotes a [[prime power]] or a [[quotient]]
* ''r'' often denotes a [[remainder]].
* ''x'', ''y'' and ''z'' usually denote the three [[Cartesian coordinates]] of a point in [[Euclidean geometry]]. By extension, they are used to name the corresponding [[axis (mathematics)|axes]].
* ''z'' typically denotes a [[complex number]], or, in statistics, a [[normal distribution|normal random variable]].
* ''α'', ''β'', ''γ'', ''θ'' and ''φ'' commonly denote [[angle]] measures.
* ''ε'' usually represents an arbitrarily small positive number.
** ''ε'' and ''δ'' commonly denote two small positives.
* ''λ'' is used for [[eigenvalues]].
* ''σ'' often denotes a sum, or, in statistics, the [[standard deviation]].
&lt;!-- mu mean, pi probability, others? --&gt;

==See also==
*[[Free variables and bound variables]] (Bound variables are also known as dummy variables)
*[[Variable (programming)]]
*[[Expression (mathematics)|Mathematical expression]]
*[[Physical constant]]
*[[Coefficient]]
*[[Constant of integration]]
*[[Constant term|Constant term of a polynomial]]
*[[Indeterminate (variable)]]

==References==
{{reflist}}
*{{Cite book| author=J. Edwards | title=Differential Calculus
| publisher= MacMillan and Co.| location=London | pages=1 ff.| year=1892
|url=http://books.google.com/books?id=unltAAAAMAAJ&amp;pg=PA1#v=onepage&amp;q&amp;f=false}}

{{DEFAULTSORT:Variable (Mathematics)}}
[[Category:Variables]]
[[Category:Algebra]]
[[Category:Calculus]]
[[Category:Elementary mathematics]]
[[Category:Statistical terminology]]
[[Category:Syntax (logic)]]

{{Link GA|uz}}
[[th:ตัวแปร (คณิตศาสตร์)]]</text>
      <sha1>muxtbrkijkd5zqs033khrdmb4cuy57c</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Primordial element (algebra)</title>
    <ns>0</ns>
    <id>38150904</id>
    <revision>
      <id>584431249</id>
      <parentid>584430245</parentid>
      <timestamp>2013-12-03T21:58:01Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>cleanup link rot, underlinked</comment>
      <text xml:space="preserve" bytes="1105">{{refimprove|date=January 2013}}

In [[algebra]], a '''primordial element''' is a particular kind of a [[Vector (mathematics and physics)|vector]] in a [[vector space]]. Let ''V'' be a vector space over a [[field (mathematics)|field]] ''k'' and fix a [[Basis (linear algebra)|basis]] for ''V'' of vectors &lt;math&gt;e_i&lt;/math&gt; for &lt;math&gt;i \in I&lt;/math&gt;. By the definition of a basis, every vector ''v'' in ''V'' can be expressed uniquely as
:&lt;math&gt;v = \sum_{i \in I} a_i(v) e_i.&lt;/math&gt;
Define &lt;math&gt;I(v) = \{ i \in I \mid a_i(v) \ne 0 \}&lt;/math&gt;, the set of indices for which the expression of ''v'' has a nonzero coefficient. Given a subspace ''W'' of ''V'', a nonzero vector ''w'' in ''W'' is said to be &quot;primordial&quot; if it has the following two properties:&lt;ref&gt;Milne, J., [http://www.jmilne.org/math/CourseNotes/cft.html Class field theory course notes], updated March 23, 2013, Ch IV, §2.&lt;/ref&gt;
#&lt;math&gt;I(w)&lt;/math&gt; is minimal among the sets &lt;math&gt;I(w')&lt;/math&gt;, &lt;math&gt;0 \ne w' \in W&lt;/math&gt; and
#&lt;math&gt;a_i(w) = 1&lt;/math&gt; for some&amp;nbsp;''i''.

== References ==
{{reflist}}

[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>42q5ks5fm2a7t6w7jupbb7wivxj3y0t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Brahmagupta–Fibonacci identity</title>
    <ns>0</ns>
    <id>326483</id>
    <revision>
      <id>608662616</id>
      <parentid>608117289</parentid>
      <timestamp>2014-05-15T08:51:02Z</timestamp>
      <contributor>
        <ip>81.95.123.216</ip>
      </contributor>
      <comment>/* History */</comment>
      <text xml:space="preserve" bytes="6307">In [[algebra]], the '''Brahmagupta–Fibonacci identity''' or simply '''Fibonacci's identity''' (and in fact due to [[Diophantus|Diophantus of Alexandria]]) says that the product of two sums each of two squares is itself a sum of two squares. In other words, the set of all sums of two squares is [[closure (mathematics)|closed]] under multiplication. Specifically:
:&lt;math&gt;\begin{align}
\left(a^2 + b^2\right)\left(c^2 + d^2\right) &amp; {}= \left(ac-bd\right)^2 + \left(ad+bc\right)^2 &amp; &amp; &amp; (1) \\
                                             &amp; {}= \left(ac+bd\right)^2 + \left(ad-bc\right)^2. &amp; &amp; &amp; (2)
\end{align}&lt;/math&gt;
For example,
:&lt;math&gt;(1^2 + 4^2)(2^2 + 7^2) = 26^2 + 15^2 = 30^2 + 1^2.\,&lt;/math&gt;
The identity is a special case (''n''&amp;nbsp;=&amp;nbsp;2) of [[Lagrange's identity]], and is first found in [[Diophantus]]. [[Brahmagupta]] proved and used a more general identity (the Brahmagupta identity), equivalent to
:&lt;math&gt;\begin{align}
\left(a^2 + nb^2\right)\left(c^2 + nd^2\right) &amp; {}= \left(ac-nbd\right)^2 + n\left(ad+bc\right)^2 &amp; &amp; &amp; (3) \\
                                               &amp; {}= \left(ac+nbd\right)^2 + n\left(ad-bc\right)^2, &amp; &amp; &amp; (4)
\end{align}&lt;/math&gt;
showing that the set of all numbers of the form &lt;math&gt;x^2 + ny^2&lt;/math&gt; is closed under multiplication.

Both (1) and (2) can be verified by [[polynomial expansion|expanding]] each side of the equation.  Also, (2) can be obtained from (1), or (1) from (2), by changing ''b'' to&amp;nbsp;&amp;minus;''b''.

This identity holds in both the [[integer|ring of integers]] and the [[rational number|ring of rational numbers]], and more generally in any [[commutative ring]].

In the [[integer]] case this identity finds applications in [[number theory]] for example when used in conjunction with one of [[Fermat's theorem on sums of two squares|Fermat's theorems]] it proves that the product of a square and any number of primes of the form 4''n''&amp;nbsp;+&amp;nbsp;1 is also a sum of two squares.

==History==
The identity is actually first found in [[Diophantus]]' ''[[Arithmetica]]'' (III, 19), of the third century B.C.
It was rediscovered by Brahmagupta (598&amp;ndash;668), an [[Indian mathematicians|Indian mathematician]] and [[Indian astronomy|astronomer]], who generalized it (to the Brahmagupta identity) and used it in his study of what is now called [[Pell's equation]]. His ''[[Brahmasphutasiddhanta]]'' was translated from [[Sanskrit]] into [[Arabic language|Arabic]] by [[Mohammad al-Fazari]], and was subsequently translated into [[Latin]] in 1126.&lt;ref&gt;George G. Joseph (2000). ''The Crest of the Peacock'', p. 306. [[Princeton University Press]]. ISBN 0-691-00659-8.&lt;/ref&gt; The identity later appeared in [[Fibonacci]]'s ''[[The Book of Squares|Book of Squares]]'' in 1225.

==Related identities==

Analogous identities are [[Euler's four-square identity|Euler's four-square]] related to [[quaternions]], and [[Degen's eight-square identity|Degen's eight-square]] derived from the [[octonions]] which has connections to [[Bott periodicity]]. There is also [[Pfister's sixteen-square identity]], though it is no longer bilinear.

== Relation to complex numbers ==

If ''a'', ''b'', ''c'', and ''d'' are [[real number]]s, this identity is equivalent to the multiplication property for absolute values of [[complex numbers]] namely that:

:&lt;math&gt;  | a+bi |  | c+di | = | (a+bi)(c+di) | \,&lt;/math&gt;

since

:&lt;math&gt;  | a+bi |  | c+di | = | (ac-bd)+i(ad+bc) |,\,&lt;/math&gt;

by squaring both sides

:&lt;math&gt;  | a+bi |^2  | c+di |^2 = | (ac-bd)+i(ad+bc) |^2,\,&lt;/math&gt;

and by the definition of absolute value,

:&lt;math&gt;  (a^2+b^2)(c^2+d^2)= (ac-bd)^2+(ad+bc)^2. \,&lt;/math&gt;

== Interpretation via norms ==

In the case that the variables ''a'', ''b'', ''c'', and ''d'' are [[rational number]]s, the identity may be interpreted as the statement that the [[field norm|norm]] in the [[field (mathematics)|field]] '''Q'''(''i'') is ''multiplicative''. That is, we have

: &lt;math&gt;N(a+bi) = a^2 + b^2 \text{ and }N(c+di) = c^2 + d^2, \,&lt;/math&gt;

and also

: &lt;math&gt;N((a+bi)(c+di)) = N((ac-bd)+i(ad+bc)) = (ac-bd)^2 + (ad+bc)^2. \,&lt;/math&gt;

Therefore the identity is saying that

: &lt;math&gt;N((a+bi)(c+di)) = N(a+bi) \cdot N(c+di). \,&lt;/math&gt;

== Application to Pell's equation ==
In its original context, Brahmagupta applied his discovery (the [[Brahmagupta identity]]) to the solution of [[Pell's equation]], namely ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''Ny''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1. Using the identity in the more general form

:&lt;math&gt;(x_1^2 - Ny_1^2)(x_2^2 - Ny_2^2) = (x_1x_2 + Ny_1y_2)^2 - N(x_1y_2 + x_2y_1)^2, \, &lt;/math&gt;

he was able to &quot;compose&quot; triples (''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''y''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''k''&lt;sub&gt;1&lt;/sub&gt;) and (''x''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''y''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''k''&lt;sub&gt;2&lt;/sub&gt;) that were solutions of ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''Ny''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''k'', to generate the new triple

:&lt;math&gt;(x_1x_2 + Ny_1y_2 \,,\, x_1y_2 + x_2y_1 \,,\, k_1k_2).&lt;/math&gt;

Not only did this give a way to generate infinitely many solutions to ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''Ny''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1 starting with one solution, but also, by dividing such a composition by ''k''&lt;sub&gt;1&lt;/sub&gt;''k''&lt;sub&gt;2&lt;/sub&gt;, integer or &quot;nearly integer&quot; solutions could often be obtained. The general method for solving the Pell equation given by [[Bhaskara II]] in 1150, namely the [[chakravala method|chakravala (cyclic) method]], was also based on this identity.&lt;ref name=stillwell&gt;{{citation | year=2002 | title = Mathematics and its history | author1=[[John Stillwell]] | edition=2 | publisher=Springer | isbn=978-0-387-95336-6 | pages=72–76 | url=http://books.google.com/books?id=WNjRrqTm62QC&amp;pg=PA72}}&lt;/ref&gt;

==See also==
* [[Brahmagupta matrix]]
* [[Indian mathematics]]
* [[List of Indian mathematicians]]
* [[Euler's four-square identity]]

==References==
{{reflist}}

==External links==
*[http://planetmath.org/encyclopedia/BrahmaguptasIdentity.html Brahmagupta's identity at [[PlanetMath]]]
*[http://mathworld.wolfram.com/BrahmaguptaIdentity.html Brahmagupta Identity] on [[MathWorld]]
*[http://sites.google.com/site/tpiezas/005b/  A Collection of Algebraic Identities]

{{DEFAULTSORT:Brahmagupta-Fibonacci identity}}
[[Category:Algebra]]
[[Category:Elementary algebra]]
[[Category:Mathematical identities]]
[[Category:Brahmagupta]]</text>
      <sha1>414yq0ytu2ezg4wzcrqzfltpy947nwn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Temperley–Lieb algebra</title>
    <ns>0</ns>
    <id>2802848</id>
    <revision>
      <id>603681506</id>
      <parentid>603681436</parentid>
      <timestamp>2014-04-11T01:27:11Z</timestamp>
      <contributor>
        <ip>131.91.7.56</ip>
      </contributor>
      <comment>/* Further reading */</comment>
      <text xml:space="preserve" bytes="8783">In [[statistical mechanics]], the '''Temperley–Lieb algebra''' is an algebra from which are built certain [[transfer matrix|transfer matrices]], invented by [[Harold Neville Vazeille Temperley|Neville Temperley]] and [[Elliott H. Lieb|Elliott Lieb]]. It is also related to  [[integrable model]]s, [[knot theory]] and the [[braid group]], [[quantum groups]] and [[subfactor]]s of [[von Neumann algebra]]s.

==Definition==

Let &lt;math&gt;R&lt;/math&gt; be a [[commutative ring]] and fix &lt;math&gt;\delta \in R&lt;/math&gt;. The Temperley–Lieb algebra &lt;math&gt;TL_n(\delta)&lt;/math&gt; is the [[algebra (ring theory)|&lt;math&gt;R&lt;/math&gt;-algebra]] generated by the elements &lt;math&gt;U_1, U_2, \ldots, U_{n-1}&lt;/math&gt;, subject to the Jones relations: 
*&lt;math&gt;U_i^2 = \delta U_i&lt;/math&gt; for all &lt;math&gt;1 \leq i \leq n-1&lt;/math&gt;
*&lt;math&gt;U_i U_{i+1} U_i = U_i&lt;/math&gt; for all &lt;math&gt;1 \leq i \leq n-2&lt;/math&gt;
*&lt;math&gt;U_i U_{i-1} U_i = U_i&lt;/math&gt; for all &lt;math&gt;2 \leq i \leq n-1&lt;/math&gt;
*&lt;math&gt;U_i U_j = U_j U_i&lt;/math&gt; for all &lt;math&gt;1 \leq i,j \leq n-1&lt;/math&gt; such that &lt;math&gt;|i-j| \neq 1&lt;/math&gt;

&lt;math&gt;TL_n(\delta)&lt;/math&gt; may be represented diagrammatically as the vector space over noncrossing pairings on a rectangle with ''n'' points on two opposite sides.  The five basis elements of &lt;math&gt;TL_3(\delta)&lt;/math&gt; are the following:

[[File:Temperley-lieb (horizontal).svg|340px|Basis of the Temperley–Lieb algebra &lt;math&gt;TL_3(\delta)&lt;/math&gt;]].

Multiplication on basis elements can be performed by placing two rectangles side by side, and replacing any closed loops by a factor of ''δ'', for example:

[[File:Factor-a.svg|50px]]  ×  [[File:Factor-b.svg|50px]]  =  [[File:Factor-a.svg|50px]][[File:Factor-b.svg|50px]]  =  δ  [[File:Concatenation-ab.svg|50px]].

The identity element is the diagram in which each point is connected to the one directly across the rectangle from it, and the generator &lt;math&gt;U_i&lt;/math&gt; is the diagram in which the ''i''th point is connected to the ''i+1''th point, the ''2n − i + 1''th point is connected to the ''2n − i''th point, and all other points are connected to the point directly across the rectangle.  The generators of &lt;math&gt;TL_5(\delta)&lt;/math&gt; are:

[[File:Temperley-Lieb (generateurs).svg|340px|Generators of the Temperley–Lieb algebra &lt;math&gt;TL_5(\delta)&lt;/math&gt;]]

From left ot right, the unit 1 and the generators U&lt;sub&gt;1&lt;/sub&gt;, U&lt;sub&gt;2&lt;/sub&gt;, U&lt;sub&gt;3&lt;/sub&gt;, U&lt;sub&gt;4&lt;/sub&gt;.

The Jones relations can be seen graphically:

[[File:E 2 Temperley.svg|50px]] [[File:E 2 Temperley.svg|50px]]  =  δ  [[File:E 2 Temperley.svg|50px]]

[[File:E 2 Temperley.svg|50px]] [[File:E 3 Temperley.svg|50px]] [[File:E 2 Temperley.svg|50px]]  =  [[File:E 2 Temperley.svg|50px]]

[[File:E 1 Temperley.svg|50px]] [[File:E 4 Temperley.svg|50px]]  =  [[File:E 4 Temperley.svg|50px]] [[File:E 1 Temperley.svg|50px]]

==The Temperley-Lieb Hamiltonian==

Consider an interaction-round-a-face model e.g. a square [[Lattice model (physics)|lattice model]] and let &lt;math&gt;L&lt;/math&gt; be the number of sites on the lattice. Following Temperley and Lieb&lt;ref&gt;Temperley N. and Lieb E., (1971), ''Relations between the 'Percolation' and 'Colouring' Problem and other Graph-Theoretical Problems Associated with Regular Planar Lattices: Some Exact Results for the 'Percolation' Problem'', Proc. R. Soc. A 322 251.&lt;/ref&gt; we define the Temperley-Lieb [[Hamiltonian (quantum mechanics)|hamiltonian]] (the TL hamiltonian) as

&lt;math&gt; \mathcal{H} = \sum_{j=1}^{L-1} (1 - e_j) &lt;/math&gt;

where &lt;math&gt;e_j =  U(\lambda)/\sin\lambda&lt;/math&gt;, for some spectral parameter &lt;math&gt;\lambda \in R&lt;/math&gt;.

===Applications===

We will firstly consider the case &lt;math&gt;L = 3&lt;/math&gt;. The TL hamiltonian is &lt;math&gt;\mathcal{H} = 2 - e_1 - e_2 &lt;/math&gt;, namely 

&lt;math&gt;\mathcal{H}&lt;/math&gt;  =  2  [[File:Unit 3 Temperley.svg|50px]]  -  [[File:E 1 3 Temperley.svg|50px]]  -  [[File:E 2 3 Temperley.svg|50px]].

We have two possible states,

[[File:BS1-Temperley-Lieb.svg|40px]] and [[File:BS2-Temperley-Lieb.svg|40px]].

In acting by &lt;math&gt;\mathcal{H}&lt;/math&gt; on these states, we find

&lt;math&gt;\mathcal{H}&lt;/math&gt; [[File:BS1-Temperley-Lieb.svg|40px]]  =  2  [[File:Unit 3 Temperley.svg|50px]][[File:BS1-Temperley-Lieb.svg|40px]]  -  [[File:E 1 3 Temperley.svg|50px]][[File:BS1-Temperley-Lieb.svg|40px]]  -  [[File:E 2 3 Temperley.svg|50px]][[File:BS1-Temperley-Lieb.svg|40px]]  =  [[File:BS1-Temperley-Lieb.svg|40px]]  -  [[File:BS2-Temperley-Lieb.svg|40px]],

and

&lt;math&gt;\mathcal{H}&lt;/math&gt; [[File:BS2-Temperley-Lieb.svg|40px]]  =  2  [[File:Unit 3 Temperley.svg|50px]][[File:BS2-Temperley-Lieb.svg|40px]]  -  [[File:E 1 3 Temperley.svg|50px]][[File:BS2-Temperley-Lieb.svg|40px]]  -  [[File:E 2 3 Temperley.svg|50px]][[File:BS2-Temperley-Lieb.svg|40px]]  =  -  [[File:BS1-Temperley-Lieb.svg|40px]]  +  [[File:BS2-Temperley-Lieb.svg|40px]].

Writing &lt;math&gt;\mathcal{H}&lt;/math&gt; as a matrix in the basis of possible states we have,

&lt;math&gt; \mathcal{H} = \left(\begin{array}{rr}
1 &amp; -1\\
-1 &amp; 1
\end{array}\right)
&lt;/math&gt;

The eigenvector of &lt;math&gt;\mathcal{H}&lt;/math&gt; with the ''lowest'' [[Eigenvalues and eigenvectors|eigenvalue]] is known as the [[ground state]]. In this case, the lowest eigenvalue &lt;math&gt;\lambda_0&lt;/math&gt; for &lt;math&gt;\mathcal{H}&lt;/math&gt; is &lt;math&gt;\lambda_0 = 0&lt;/math&gt;. The corresponding [[Eigenvalues and eigenvectors|eigenvector]] is &lt;math&gt;\psi_0 = (1, 1)&lt;/math&gt;. As we vary the number of sites &lt;math&gt;L&lt;/math&gt; we find the following table&lt;ref name=&quot;bach&quot;&gt;Batchelor M., de Gier J. and Nienhuis B., (2001), The quantum symmetric &lt;math&gt;XXZ&lt;/math&gt; chain at &lt;math&gt;\Delta = -1/2&lt;/math&gt;, alternating-sign matrices and plane partitions, J. Phys. A 34, L265-L270.&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
! &lt;math&gt;L&lt;/math&gt;
! &lt;math&gt;\psi_0&lt;/math&gt;
! &lt;math&gt;L&lt;/math&gt;
! &lt;math&gt;\psi_0&lt;/math&gt;
|-
| 2
| (1)
|3
|(1, 1)
|-
| 4
|(2, 1)
|5
|&lt;math&gt;(3_3, 1_2)&lt;/math&gt;
|-
| 6
| &lt;math&gt;(11, 5_2,4, 1)&lt;/math&gt;
|7
|&lt;math&gt;(26_4, 10_2, 9_2, 8_2, 5_2, 1_2)&lt;/math&gt;
|-
|8
|&lt;math&gt;(170, 75_2, 71, 56_2, 50, 30, 14_4, 6, 1)&lt;/math&gt;
|9
|&lt;math&gt;(646, \ldots)&lt;/math&gt;
|-
|&lt;math&gt;\vdots&lt;/math&gt;
|&lt;math&gt;\vdots&lt;/math&gt;
|&lt;math&gt;\vdots&lt;/math&gt;
|&lt;math&gt;\vdots&lt;/math&gt;
|-
|}

where we have use the notation &lt;math&gt;m_n = (m, \ldots, m)&lt;/math&gt; &lt;math&gt;n&lt;/math&gt;-times i.e. &lt;math&gt;5_2 = (5, 5)&lt;/math&gt;.

===Combinatorial Properties===
An interesting observation is that the largest components of the ground state of &lt;math&gt;\mathcal{H}&lt;/math&gt; have a combinatorial enumeration as we vary the number of sites,&lt;ref&gt;de Gier J., (2005), Loops, matchings and alternating-sign matrices, Discrete Mathematics Volume 298, Issues 1-3, Pages 365-388.&lt;/ref&gt; as was first observed by [[Murray Batchelor]], Jan de Gier and Bernard Nienhuis.&lt;ref name=&quot;bach&quot;&gt;Batchelor M., de Gier J. and Nienhuis B., (2001), The quantum symmetric &lt;math&gt;XXZ&lt;/math&gt; chain at &lt;math&gt;\Delta = -1/2&lt;/math&gt;, alternating-sign matrices and plane partitions, J. Phys. A 34, L265-L270.&lt;/ref&gt; Using the resources of the [[on-line encyclopedia of integer sequences]], Batchelor ''et al.'' found, for an even numbers of sites  

&lt;math&gt;
1, 2, 11, 170, \ldots = \prod_{j=0}^{n-1} \left( 3j + 1\right)\frac{ (2j)!(6j)!}{(4j)!(4j + 1)!}
&lt;/math&gt;

and for an odd numbers of sites

&lt;math&gt;
1, 3, 26, 646, \ldots = \prod_{j=0}^{n-1} (3j+2)\frac{ (2j + 2)!(6j + 3)!}{(4j + 2)!(4j + 3)!}.
&lt;/math&gt;

Surprisingly, these sequences corresponded to well known combinatorial objects. For &lt;math&gt;L&lt;/math&gt; even, this sequence corresponded to cyclically symmetric transpose complement plane partitions and for &lt;math&gt;L&lt;/math&gt; odd these corresponded to &lt;math&gt;(2n+1)\times(2n+1)&lt;/math&gt; [[Alternating sign matrix|alternating sign matrices]] symmetric about the vertical axis.

==References==

&lt;references/&gt;

==Further reading==
*[[Louis H. Kauffman]], [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6V1J-45DHSCR-J&amp;_user=10&amp;_coverDate=12%2F31%2F1987&amp;_rdoc=9&amp;_fmt=high&amp;_orig=browse&amp;_srch=doc-info(%23toc%235676%231987%23999739996%23292694%23FLP%23display%23Volume)&amp;_cdi=5676&amp;_sort=d&amp;_docanchor=&amp;_ct=9&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=cca311a0762fc3d6a7a6f284a10a5c68 ''State Models and the Jones Polynomial''.] [[Topology (journal)|Topology]], 26(3):395-407, 1987.
*[[Rodney J. Baxter|R.J. Baxter]], [http://tpsrv.anu.edu.au/Members/baxter/book ''Exactly solved models in statistical mechanics''] Academic Press Inc. (1982)
*[[Harold Neville Vazeille Temperley|N. Temperley]], [[Elliott H. Lieb|E. Lieb]], [http://www.jstor.org/stable/77727 ''Relations between the 'Percolation' and 'Colouring' Problem and other Graph-Theoretical Problems Associated with Regular Planar Lattices: Some Exact Results for the 'Percolation' Problem''.] Proceedings of the Royal Society Series A 322 (1971), 251-280.

{{DEFAULTSORT:Temperley-Lieb algebra}}
[[Category:Von Neumann algebras]]
[[Category:Algebra]]
[[Category:Knot theory]]
[[Category:Braids]]
[[Category:Diagram algebras]]</text>
      <sha1>rgqs272d6yyysvgttew54q9fq5enryt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Distribution (number theory)</title>
    <ns>0</ns>
    <id>36785973</id>
    <revision>
      <id>601857694</id>
      <parentid>597583780</parentid>
      <timestamp>2014-03-29T19:38:54Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes, removed orphan tag using [[Project:AWB|AWB]] (10067)</comment>
      <text xml:space="preserve" bytes="7596">In [[algebra]] and [[number theory]], a '''distribution''' is a function on a system of finite sets into an [[abelian group]] which is analogous to an integral: it is thus the algebraic analogue of a [[distribution (mathematics)|distribution]] in the sense of [[generalised function]].

The original examples of distributions occur, unnamed, as functions φ on '''Q'''/'''Z''' satisfying&lt;ref&gt;Kubert &amp; Lang (1981) p.1&lt;/ref&gt;

:&lt;math&gt; \sum_{r=0}^{N-1} \phi\left(x + \frac r N\right) = \phi(Nx) \ . &lt;/math&gt;

We shall call these '''ordinary distributions'''.&lt;ref&gt;Lang (1990) p.53&lt;/ref&gt;  They also occur in ''p''-adic integration theory in [[Iwasawa theory]].&lt;ref name=MSD36&gt;Mazur &amp; Swinnerton-Dyer (1972) p.&amp;nbsp;36&lt;/ref&gt;

Let ... → ''X''&lt;sub&gt;''n''+1&lt;/sub&gt; → ''X''&lt;sub&gt;''n''&lt;/sub&gt; → ... be a [[projective system]] of finite sets with surjections, indexed by the natural numbers, and let ''X'' be their [[projective limit]].  We give each ''X''&lt;sub&gt;''n''&lt;/sub&gt; the [[discrete topology]], so that ''X'' is [[compact space|compact]].  Let φ = (φ&lt;sub&gt;''n''&lt;/sub&gt;) be a family of functions on ''X''&lt;sub&gt;''n''&lt;/sub&gt; taking values in an abelian group ''V'' and compatible with the projective system:

:&lt;math&gt; w(m,n) \sum_{y \mapsto x} \phi(y) = \phi(x) &lt;/math&gt;

for some ''weight function'' ''w''.  The family φ is then a ''distribution'' on the projective system ''X''.

A function ''f'' on ''X'' is &quot;locally constant&quot;, or a &quot;step function&quot; if it factors through some ''X''&lt;sub&gt;''n''&lt;/sub&gt;.  We can define an integral of a step function against φ as

:&lt;math&gt; \int f \, d\phi = \sum_{x \in X_n} f(x) \phi_n(x) \ . &lt;/math&gt;

The definition extends to more general projective systems, such as those indexed by the positive integers ordered by divisibility.  As an important special case consider the projective system '''Z'''/''n''&lt;nowiki&gt;&lt;/nowiki&gt;'''Z''' indexed by positive integers ordered by divisibility.   We identify this with the system (1/''n'')'''Z'''/'''Z''' with limit '''Q'''/'''Z'''.

For ''x'' in ''R'' we let ⟨''x''⟩ denote the fractional part of ''x'' normalised to 0 ≤ ⟨''x''⟩ &lt; 1, and let {''x''} denote the fractional part normalised to 0&amp;nbsp;&lt;&amp;nbsp;{''x''}&amp;nbsp;≤&amp;nbsp;1.

==Examples==

===Hurwitz zeta function===
The [[multiplication theorem]] for the [[Hurwitz zeta function]]

:&lt;math&gt;\zeta(s,a) = \sum_{n=0}^\infty (n+a)^{-s} &lt;/math&gt;

gives a distribution relation

:&lt;math&gt;\sum_{p=0}^{q-1}\zeta(s,a+p/q)=q^s\,\zeta(s,qa) \ .&lt;/math&gt;

Hence for given ''s'', the map &lt;math&gt;t \mapsto \zeta(s,\{t\})&lt;/math&gt; is a distribution on '''Q'''/'''Z'''.

===Bernoulli distribution===
Recall that the ''[[Bernoulli polynomials]]''  ''B''&lt;sub&gt;''n''&lt;/sub&gt; are defined by

:&lt;math&gt;B_n(x) = \sum_{k=0}^n {n \choose n-k} b_k x^{n-k} \ ,&lt;/math&gt;

for ''n'' ≥ 0, where ''b''&lt;sub&gt;''k''&lt;/sub&gt; are the [[Bernoulli number]]s, with  [[generating function]]

:&lt;math&gt;\frac{t e^{xt}}{e^t-1}= \sum_{n=0}^\infty B_n(x) \frac{t^n}{n!} \ .&lt;/math&gt;

They satisfy the ''distribution relation''

:&lt;math&gt; B_k(x) = n^{k-1} \sum_{a=0}^{n-1} b_k\left({\frac{x+a}{n}}\right)\ . &lt;/math&gt;

Thus the map

:&lt;math&gt; \phi_n : \frac{1}{n}\mathbb{Z}/\mathbb{Z} \rightarrow \mathbb{Q} &lt;/math&gt;

defined by

:&lt;math&gt; \phi_n : x \mapsto n^{k-1} B_k(\langle x \rangle) &lt;/math&gt;

is a distribution.&lt;ref&gt;Lang (1990) p.36&lt;/ref&gt;

===Cyclotomic units===
The [[cyclotomic unit]]s satisfy ''distribution relations''.  Let ''a'' be an element of '''Q'''/'''Z''' prime to ''p'' and let ''g''&lt;sub&gt;''a''&lt;/sub&gt; denote exp(2πi''a'')−1.  Then for ''a''≠ 0 we have&lt;ref&gt;Lang (1990) p.157&lt;/ref&gt;

:&lt;math&gt; \prod_{p b=a} g_b = g_a \ . &lt;/math&gt;

==Universal distribution==
We consider the distributions on ''Z'' with values in some abelian group ''V'' and seek the &quot;universal&quot; or most general distribution possible.

==Stickelberger distributions==
Let ''h'' be an ordinary distribution on '''Q'''/'''Z''' taking values in a field ''F''.  Let ''G''(''N'') denote the multiplicative group of '''Z'''/''N''&lt;nowiki&gt;&lt;/nowiki&gt;'''Z''', and for any function ''f'' on ''G''(''N'') we extend ''f'' to a function on '''Z'''/''N''&lt;nowiki&gt;&lt;/nowiki&gt;'''Z''' by taking ''f'' to be zero off ''G''(''N'').  Define an element of the group algebra ''F''[''G''(''N'')] by

:&lt;math&gt; g_N(r) = \frac{1}{|G(N)|} \sum_{a \in G(N)} h\left({\left\langle{\frac{ra}{N}}\right\rangle}\right) \sigma_a^{-1} \ . &lt;/math&gt;

The group algebras form a projective system with limit ''X''.  Then the functions ''g''&lt;sub&gt;''N''&lt;/sub&gt; form a distribution on '''Q'''/'''Z''' with values in ''X'', the '''Stickelberger distribution''' associated with ''h''.

==p-adic measures==
Consider the special case when the value group ''V'' of a distribution φ on ''X'' takes values in a [[local field]] ''K'', finite over '''Q'''&lt;sub&gt;''p''&lt;/sub&gt;, or more generally, in a finite-dimensional
''p''-adic Banach space ''W'' over ''K'', with valuation |·|.  We call φ a '''measure''' if |φ| is bounded on compact open subsets of ''X''.&lt;ref name=MSD37&gt;Mazur &amp; Swinnerton-Dyer (1974) p.37&lt;/ref&gt;  Let ''D'' be the ring of integers of ''K'' and ''L'' a lattice in ''W'', that is, a free ''D''-submodule of ''W'' with ''K''⊗''L'' = ''W''.  Up to scaling a measure may be taken to have values in ''L''.

===Hecke operators and measures===
Let ''D'' be a fixed integer prime to ''p'' and consider '''Z'''&lt;sub&gt;''D''&lt;/sub&gt;, the limit of the system '''Z'''/''p''&lt;sup&gt;''n''&lt;/sup&gt;''D''.  Consider any [[eigenfunction]] of the [[Hecke operator]] ''T''&lt;sup&gt;''p''&lt;/sup&gt; with eigenvalue ''λ''&lt;sub&gt;''p''&lt;/sub&gt; prime to ''p''.  We describe a procedure for deriving a measure of '''Z'''&lt;sub&gt;''D''&lt;/sub&gt;.

Fix an integer ''N'' prime to ''p'' and to ''D''.  Let ''F'' be the ''D''-module of all functions on rational numbers with denominator coprime to ''N''.  For any prime ''l'' not dividing ''N'' we define the ''Hecke operator'' ''T''&lt;sub&gt;''l''&lt;/sub&gt; by

:&lt;math&gt; (T_l f)\left(\frac a b\right) = f\left(\frac{la}{b}\right) + \sum_{k=0}^{l-1} f\left({\frac{a+kb}{lb}}\right) - \sum_{k=0}^{l-1} f\left(\frac k l \right) \ . &lt;/math&gt;

Let ''f'' be an eigenfunction for ''T''&lt;sub&gt;''p''&lt;/sub&gt; with eigenvalue λ&lt;sub&gt;''p''&lt;/sub&gt; in ''D''.  The quadratic equation ''X''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;λ&lt;sub&gt;''p''&lt;/sub&gt;''X''&amp;nbsp;+&amp;nbsp;''p''&amp;nbsp;=&amp;nbsp;0 has roots π&lt;sub&gt;1&lt;/sub&gt;, π&lt;sub&gt;2&lt;/sub&gt; with π&lt;sub&gt;1&lt;/sub&gt; a unit and  π&lt;sub&gt;2&lt;/sub&gt; divisible by ''p''.  Define a sequence ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;2, ''a''&lt;sub&gt;1&lt;/sub&gt; =&amp;nbsp;π&lt;sub&gt;1&lt;/sub&gt;+π&lt;sub&gt;2&lt;/sub&gt; =&amp;nbsp;''λ''&lt;sub&gt;''p''&lt;/sub&gt; and

:&lt;math&gt;a_{k+2} = \lambda_p a_{k+1} - p a_k \ , &lt;/math&gt;

so that

:&lt;math&gt;a_k = \pi_1^k + \pi_2^k \ . &lt;/math&gt;

==References==
{{reflist}}
* {{cite book | first1=Daniel S. | last1=Kubert | authorlink1=Daniel Kubert | first2=Serge | last2=Lang | authorlink2=Serge Lang | title=Modular Units | series= Grundlehren der Mathematischen Wissenschaften | volume=244 | publisher=[[Springer-Verlag]] | year=1981 | isbn=0-387-90517-0 | zbl=0492.12002 }}
* {{cite book | last=Lang | first=Serge | authorlink=Serge Lang | title=Cyclotomic Fields I and II | edition=second combined | publisher=[[Springer Verlag]] | series=[[Graduate Texts in Mathematics]] | volume=121 | isbn=3-540-96671-4 | zbl=0704.11038 | year=1990 }}
* {{cite journal | zbl=0281.14016 | last1=Mazur | first1=B. | author1-link=Barry Mazur | last2=Swinnerton-Dyer | first2=P. | author2-link=Peter Swinnerton-Dyer | title=Arithmetic of Weil curves | journal=[[Inventiones Mathematicae]] | volume=25 | pages=1–61 | year=1974 | url=http://www.springerlink.com/content/l30185r823104886/ | doi=10.1007/BF01389997 }}

[[Category:Algebra]]
[[Category:Number theory]]</text>
      <sha1>2kj22xuejikxj8w15hdfa66r9mrzach</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Brahmagupta's identity</title>
    <ns>0</ns>
    <id>5161105</id>
    <revision>
      <id>559092695</id>
      <parentid>555027491</parentid>
      <timestamp>2013-06-09T17:29:59Z</timestamp>
      <contributor>
        <username>Mild Bill Hiccup</username>
        <id>5202324</id>
      </contributor>
      <minor/>
      <comment>stray comma</comment>
      <text xml:space="preserve" bytes="3797">In [[algebra]], '''Brahmagupta's identity''' says that the product of two numbers of the form &lt;math&gt;a^2+nb^2&lt;/math&gt; is itself a number of that form. In other words, the set of such numbers is [[closure (mathematics)|closed]] under multiplication. Specifically:

:&lt;math&gt;\begin{align}
\left(a^2 + nb^2\right)\left(c^2 + nd^2\right) &amp; {}= \left(ac-nbd\right)^2 + n\left(ad+bc\right)^2 &amp; &amp; &amp; (1) \\
                                               &amp; {}= \left(ac+nbd\right)^2 + n\left(ad-bc\right)^2, &amp; &amp; &amp; (2)
\end{align}&lt;/math&gt;

Both (1) and (2) can be verified by [[polynomial expansion|expanding]] each side of the equation.  Also, (2) can be obtained from (1), or (1) from (2), by changing ''b'' to&amp;nbsp;&amp;minus;''b''.

This identity holds in both the [[integer|ring of integers]] and the [[rational number|ring of rational numbers]], and more generally in any [[commutative ring]].

==History==
The identity is a generalization of the so-called [[Brahmagupta–Fibonacci identity|Fibonacci identity]] (where ''n''=1) which is actually found in [[Diophantus]]' ''[[Arithmetica]]'' (III, 19).
That identity was rediscovered by [[Brahmagupta]] (598&amp;ndash;668), an [[Indian mathematicians|Indian mathematician]] and [[Indian astronomy|astronomer]], who generalized it and used it in his study of what is now called [[Pell's equation]]. His ''[[Brahmasphutasiddhanta]]'' was translated from [[Sanskrit]] into [[Arabic language|Arabic]] by [[Mohammad al-Fazari]], and was subsequently translated into [[Latin]] in 1126.&lt;ref&gt;George G. Joseph (2000). ''The Crest of the Peacock'', p. 306. [[Princeton University Press]]. ISBN 0-691-00659-8.&lt;/ref&gt; The identity later appeared in [[Fibonacci]]'s ''[[The Book of Squares|Book of Squares]]'' in 1225.

== Application to Pell's equation ==
In its original context, Brahmagupta applied his discovery to the solution of what was later called [[Pell's equation]], namely ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''Ny''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1. Using the identity in the form

:&lt;math&gt;(x_1^2 - Ny_1^2)(x_2^2 - Ny_2^2) = (x_1x_2 + Ny_1y_2)^2 - N(x_1y_2 + x_2y_1)^2, \, &lt;/math&gt;

he was able to &quot;compose&quot; triples (''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''y''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''k''&lt;sub&gt;1&lt;/sub&gt;) and (''x''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''y''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''k''&lt;sub&gt;2&lt;/sub&gt;) that were solutions of ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''Ny''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''k'', to generate the new triple

:&lt;math&gt;(x_1x_2 + Ny_1y_2 \,,\, x_1y_2 + x_2y_1 \,,\, k_1k_2).&lt;/math&gt;

Not only did this give a way to generate infinitely many solutions to ''x''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''Ny''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1 starting with one solution, but also, by dividing such a composition by ''k''&lt;sub&gt;1&lt;/sub&gt;''k''&lt;sub&gt;2&lt;/sub&gt;, integer or &quot;nearly integer&quot; solutions could often be obtained. The general method for solving the Pell equation given by [[Bhaskara II]] in 1150, namely the [[chakravala method|chakravala (cyclic) method]], was also based on this identity.&lt;ref name=stillwell&gt;{{citation | year=2002 | title = Mathematics and its history | author1=[[John Stillwell]] | edition=2 | publisher=Springer | isbn=978-0-387-95336-6 | pages=72–76 | url=http://books.google.com/books?id=WNjRrqTm62QC&amp;pg=PA72}}&lt;/ref&gt;

==See also==
* [[Brahmagupta matrix]]
* [[Brahmagupta–Fibonacci identity]]
* [[Indian mathematics]]
* [[List of Indian mathematicians]]

==References==
{{reflist}}

==External links==
*[http://planetmath.org/encyclopedia/BrahmaguptasIdentity.html Brahmagupta's identity at [[PlanetMath]]]
*[http://mathworld.wolfram.com/BrahmaguptaIdentity.html Brahmagupta Identity] on [[MathWorld]]
*[http://sites.google.com/site/tpiezas/005b/  A Collection of Algebraic Identities]

[[Category:Algebra]]
[[Category:Elementary algebra]]
[[Category:Mathematical identities]]
[[Category:Brahmagupta]]</text>
      <sha1>dfbqxoezh9c7r1lhscdbpyfq52i2in5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Partial fraction decomposition</title>
    <ns>0</ns>
    <id>247288</id>
    <revision>
      <id>609526515</id>
      <parentid>607155980</parentid>
      <timestamp>2014-05-21T13:59:44Z</timestamp>
      <contributor>
        <ip>80.74.111.178</ip>
      </contributor>
      <comment>/* Basic principles */</comment>
      <text xml:space="preserve" bytes="27090">{{more footnotes|date=September 2012}}
In [[algebra]], the '''partial fraction decomposition''' or '''partial fraction expansion''' of a [[rational fraction]] (that is a [[fraction (mathematics)|fraction]] such that the numerator and the denominator are both [[polynomial]]s) is the operation that consists in expressing the fraction as a sum of a polynomial (possibly zero) and one or several fractions with a simpler denominator.

The importance of the partial fraction decomposition lies in the fact that it provides an [[algorithm]] for computing the [[antiderivative]] of a [[rational function]].

In symbols, one can use ''partial fraction expansion'' to change a rational fraction in the form

: &lt;math&gt; \frac{f(x)}{g(x)} &lt;/math&gt;

where ''ƒ'' and ''g'' are polynomials, into an expression of the form

: &lt;math&gt; \sum_j \frac{f_j(x)}{g_j(x)} &lt;/math&gt;

where ''g''&lt;sub&gt;''j''&lt;/sub&gt;&amp;nbsp;(''x'') are polynomials that are factors of ''g''(''x''), and are in general of lower degree. 
Thus, the partial fraction decomposition may be seen as the inverse procedure of the more elementary operation of addition of [[rational fraction]]s, which produces a single rational fraction with a numerator and denominator usually of high degree.
The ''full'' decomposition pushes the reduction as far as it will go: in other words, the factorization of ''g'' is used as much as possible. Thus, the outcome of a full partial fraction expansion expresses that fraction as a sum of fractions, where:

* the [[denominator]] of each term is a [[Exponentiation|power]] of an [[irreducible polynomial|irreducible]] (not factorable) [[polynomial]] and
* the [[numerator]] is a polynomial of smaller degree than that irreducible polynomial. To decrease the degree of the numerator directly, the [[Euclidean division of polynomials|Euclidean division]] can be used, but in fact if ''ƒ'' already has lower degree than ''g'' this isn't helpful.

== Basic principles ==

The basic principles involved are quite simple; it is the algorithmic aspects that require attention in particular cases. On the other hand, the existence of a decomposition of a certain kind is an assumption in practical cases, and the principles should explain which assumptions are justified.

Assume a rational function &lt;math&gt; R(x) = \frac{f(x)}{g(x)}  &lt;/math&gt;      in one [[Indeterminate (variable)|indeterminate]] ''x'' has a [[denominator]] that factors as

:&lt;math&gt; g(x) = P(x) \cdot Q(x) \, &lt;/math&gt;

over a [[field (mathematics)|field]] ''K'' (we can take this to be [[real number]]s, or [[complex number]]s). If ''P'' and ''Q'' have no common factor, then ''R'' may be written as

:&lt;math&gt; \frac{A}{P} + \frac{B}{Q}&lt;/math&gt;

for some polynomials ''A''(''x'') and ''B''(''x'') over ''K''. The ''existence'' of such a decomposition is a consequence of the fact that the [[polynomial ring]] over ''K'' is a [[principal ideal domain]], so that

:&lt;math&gt;CP + DQ = 1 \, &lt;/math&gt;

for some polynomials ''C''(''x'') and ''D''(''x'') (see [[Bézout's identity]]).

Using this idea inductively we can write ''R''(''x'') as a sum with denominators powers of [[irreducible polynomial]]s. To take this further, if required, write:

:&lt;math&gt;\frac {G(x)}{F(x)^n}&lt;/math&gt;

as a sum with denominators powers of ''F'' and [[numerator]]s of degree less than ''F'', plus a possible extra polynomial. This can be done by the [[Euclidean algorithm]], polynomial case.  The result is the following [[theorem]]:

{{quotation|1=Let ''ƒ'' and ''g'' be nonzero polynomials over a field ''K''. Write ''g'' as a product of powers of distinct irreducible polynomials :

: &lt;math&gt;g=\prod_{i=1}^k p_i^{n_i}.&lt;/math&gt;

There are (unique) polynomials ''b'' and ''a''&lt;sub&gt;''ij''&lt;/sub&gt; with deg&amp;nbsp;''a''&lt;sub&gt;''ij''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;deg&amp;nbsp;''p''&lt;sub&gt;''i''&lt;/sub&gt; such that

: &lt;math&gt;\frac{f}{g}=b+\sum_{i=1}^k\sum_{j=1}^{n_i}\frac{a_{ij}}{p_i^j}.&lt;/math&gt;

If deg&amp;nbsp;''ƒ''&amp;nbsp;&lt;&amp;nbsp;deg&amp;nbsp;''g'', then ''b''&amp;nbsp;=&amp;nbsp;0.}}

Therefore, when the field ''K'' is the complex numbers, we can assume that each ''p''&lt;sub&gt;''i''&lt;/sub&gt; has degree 1 (by the [[fundamental theorem of algebra]]) the numerators will be constant. When ''K'' is the real numbers, some of the ''p''&lt;sub&gt;''i''&lt;/sub&gt; might be quadratic, so in the partial fraction decomposition a quotient of a linear polynomial by a power of a quadratic might occur.

In the preceding theorem, one may replace &quot;distinct irreducible polynomials&quot; by &quot;[[pairwise coprime]] polynomials that are coprime with their derivative&quot;. For example, the ''p''&lt;sub&gt;''i''&lt;/sub&gt; may be the factors of the [[square-free factorization]] of ''g''. When ''K'' is the field of the rational numbers, as it is typically the case in [[computer algebra]], this allows to replace factorization by [[polynomial greatest common divisor|greatest common divisor]] to compute the partial fraction decomposition.

==Application to symbolic integration==

For the purpose of [[symbolic integration]], the preceding result may be refined into

{{quotation|1=Let ''ƒ'' and ''g'' be nonzero polynomials over a field ''K''. Write ''g'' as a product of powers of pairwise coprime polynomials which have no multiple root in an algebraically closed field:

: &lt;math&gt;g=\prod_{i=1}^k p_i^{n_i}.&lt;/math&gt;

There are (unique) polynomials ''b'' and ''c''&lt;sub&gt;''ij''&lt;/sub&gt; with deg&amp;nbsp;''c''&lt;sub&gt;''ij''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;deg&amp;nbsp;''p''&lt;sub&gt;''i''&lt;/sub&gt; such that

: &lt;math&gt;\frac{f}{g}=b+\sum_{i=1}^k\sum_{j=2}^{n_i}\left(\frac{c_{ij}}{p_i^{j-1}}\right)' + 
\sum_{i=1}^k \frac{c_{i1}}{p_i}.&lt;/math&gt;

where &lt;math&gt; X'&lt;/math&gt; denotes the derivative of &lt;math&gt;X.&lt;/math&gt;}}

This reduces the computation of the [[antiderivative]] of a rational function to the integration of the last sum, with is called the ''logarithmic part'', because its antiderivative is a linear combination of logarithms. In fact, we have
:&lt;math&gt;\frac{c_{i1}}{p_i}=\sum_{\alpha_j:p_i(\alpha_j)=0}\frac{c_{i1}(\alpha_j)}{p'_i(\alpha_j)}\frac{1}{x-\alpha_j}.&lt;/math&gt;

There are various methods to compute above decomposition. The one that is the simplest to describe is probably the so-called [[Charles Hermite|Hermite]]'s method. As the degree of ''c''&lt;sub&gt;''ij''&lt;/sub&gt; is bounded by the degree of ''p''&lt;sub&gt;''i''&lt;/sub&gt;, and 
the degree of ''b'' is the difference of the degrees of ''f'' and ''g'' (if this difference is non negative; otherwise, ''b''=0), one may write these unknowns polynomials as polynomials with unknown coefficients. Reducing the two members of above formula to the same denominator and writing that the coefficients of each power of ''x'' are the same in the two numerators, one gets a [[system of linear equations]] which can be solved to obtain the desired values for the unknowns coefficients.

== Procedure ==

Given two polynomials &lt;math&gt;P(x)&lt;/math&gt; and &lt;math&gt;Q(x) = (x-\alpha_1)(x-\alpha_2) \cdots (x-\alpha_n)&lt;/math&gt;, where the ''α''&lt;sub&gt;''i''&lt;/sub&gt; are distinct constants and deg&amp;nbsp;''P''&amp;nbsp;&lt;&amp;nbsp;''n'', partial fractions are generally obtained by supposing that

: &lt;math&gt;\frac{P(x)}{Q(x)} = \frac{c_1}{x-\alpha_1} + \frac{c_2}{x-\alpha_2} + \cdots + \frac{c_n}{x-\alpha_n}&lt;/math&gt;

and solving for the ''c''&lt;sub&gt;''i''&lt;/sub&gt; constants, by substitution, by [[equating the coefficients]] of terms involving the powers of ''x'', or otherwise.  (This is a variant of the [[method of undetermined coefficients]].)

A more direct computation, which is strongly related with [[Lagrange interpolation]] consists in writing 
: &lt;math&gt;\frac{P(x)}{Q(x)} = \sum_{i=1}^n \frac{P(\alpha_i)}{Q'(\alpha_i)}\frac{1}{(x-\alpha_i)} &lt;/math&gt;
where &lt;math&gt;Q'&lt;/math&gt; is the derivative of the polynomial &lt;math&gt;Q&lt;/math&gt;.

This approach does not account for several other cases, but can be modified accordingly:

* If deg&amp;nbsp;''P''&amp;nbsp; &lt;math&gt; \ge &lt;/math&gt; &amp;nbsp;deg&amp;nbsp;''Q'', then it is necessary to perform the [[Polynomial#Divisibility|Euclidean division]] of ''P'' by ''Q'', using [[polynomial long division]], giving ''P''(''x'') = ''E''(''x'') ''Q''(''x'') + ''R''(''x'') with deg&amp;nbsp;''R''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''n''. Dividing by ''Q''(''x'') this gives
:: &lt;math&gt;\frac{P(x)}{Q(x)} = E(x) + \frac{R(x)}{Q(x)},&lt;/math&gt;
:and then seek partial fractions for the remainder fraction (which by definition satisfies deg&amp;nbsp;''R''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;deg&amp;nbsp;''Q'').
* If ''Q''(''x'') contains factors which are irreducible over the given field, then the numerator ''N''(''x'') of each partial fraction with such a factor ''F''(''x'') in the denominator must be sought as a polynomial with deg&amp;nbsp;''N''&amp;nbsp;&lt;&amp;nbsp;deg&amp;nbsp;''F'', rather than as a constant. For example, take the following decomposition over '''R''':
:: &lt;math&gt;\frac{x^2 + 1}{(x+2)(x-1)\color{Blue}(x^2+x+1)} = \frac{a}{x+2} + \frac{b}{x-1} + \frac{\color{OliveGreen}cx + d}{\color{Blue}x^2 + x + 1}.&lt;/math&gt;
* Suppose ''Q''(''x'') = (''x'' − ''α'')&lt;sup&gt;''r''&lt;/sup&gt;''S''(''x'') and ''S''(''α'')&amp;nbsp;≠&amp;nbsp;0. Then ''Q''(''x'') has a zero ''α'' of [[Multiplicity (mathematics)#Multiplicity of a root of a polynomial|multiplicity]] ''r'', and in the partial fraction decomposition, ''r'' of the partial fractions will involve the powers of (''x'' − ''α''). For illustration, take ''S''(''x'') =&amp;nbsp;1 to get the following decomposition:
:: &lt;math&gt;\frac{P(x)}{Q(x)} = \frac{P(x)}{(x-\alpha)^r} = \frac{c_1}{x-\alpha} + \frac{c_2}{(x-\alpha)^2} + \cdots + \frac{c_r}{(x-\alpha)^r}.&lt;/math&gt;

=== Illustration ===

In an example application of this procedure, {{nowrap|(3''x'' + 5)/(1 &amp;minus; 2''x'')&lt;sup&gt;2&lt;/sup&gt;}} can be decomposed in the form

: &lt;math&gt;\frac{3x + 5}{(1-2x)^2} = \frac{A}{(1-2x)^2} + \frac{B}{(1-2x)}.&lt;/math&gt;

Clearing denominators shows that {{nowrap|1=3''x'' + 5 = ''A'' + ''B''(1&amp;nbsp;&amp;minus;&amp;nbsp;2''x'')}}. Expanding and equating the coefficients of powers of ''x'' gives

: 5 = ''A'' + ''B'' and  3''x'' = &amp;minus;2''Bx''

Solving for ''A'' and ''B'' yields ''A'' = 13/2 and ''B'' = &amp;minus;3/2.  Hence,

: &lt;math&gt;\frac{3x + 5}{(1-2x)^2} = \frac{13/2}{(1-2x)^2} + \frac{-3/2}{(1-2x)}.&lt;/math&gt;

=== Residue method ===
{{see also|Heaviside cover-up method}}
Over the complex numbers, suppose ''ƒ''(''x'') is a rational proper fraction, and can be decomposed into

: &lt;math&gt;f(x) = \sum_i \left( \frac{a_{i1}}{x - x_i} + \frac{a_{i2}}{( x - x_i)^2} + \cdots + \frac{a_{i k_i}}{(x - x_i)^{k_i}} \right). &lt;/math&gt;

Let

: &lt;math&gt; g_{ij}(x)=(x-x_{i})^{j-1}f(x),&lt;/math&gt;

then according to the [[Laurent series#Uniqueness|uniqueness of Laurent series]], ''a''&lt;sub&gt;''ij''&lt;/sub&gt; is the coefficient of the term (''x''&amp;nbsp;&amp;minus;&amp;nbsp;''x''&lt;sub&gt;''i''&lt;/sub&gt;)&lt;sup&gt;&amp;minus;1&lt;/sup&gt; in the Laurent expansion of ''g''&lt;sub&gt;''ij''&lt;/sub&gt;(''x'') about the point ''x''&lt;sub&gt;''i''&lt;/sub&gt;, i.e., its [[residue (complex analysis)|residue]]

: &lt;math&gt;a_{ij} = \operatorname{Res}(g_{ij},x_i).&lt;/math&gt;

This is given directly by the formula

: &lt;math&gt;a_{ij}=\frac{1}{(k_{i}-j)!}\lim_{x\to x_i}\frac{d^{k_{i}-j}}{dx^{k_{i}-j}}\left((x-x_{i})^{k_{i}}f(x)\right),&lt;/math&gt;

or in the special case when ''x''&lt;sub&gt;''i''&lt;/sub&gt; is a simple root,

: &lt;math&gt;a_{i1}=\frac{P(x_{i})}{Q'(x_{i})},&lt;/math&gt;

when

: &lt;math&gt;f(x)=\frac{P(x)}{Q(x)}.&lt;/math&gt;

Note that ''P''(''x'') and ''Q''(''x'') may or may not be polynomials.

== Over the reals ==

Partial fractions are used in [[real number|real-variable]] [[integral calculus]] to find real-valued [[antiderivative]]s of [[rational function]]s. Partial fraction decomposition of real [[rational function]]s is also used to find their [[Inverse Laplace transform]]s. For applications of '''partial fraction decomposition over the reals''', see

* [[#Application to symbolic integration|Application to symbolic integration]], above
* [[Partial fractions in Laplace transforms]]

=== General result ===

Let ''ƒ''(''x'') be any rational function over the [[real number]]s.  In other words, suppose there exist real polynomials functions ''p''(''x'') and ''q''(''x'')≠ 0, such that

: &lt;math&gt;f(x) = \frac{p(x)}{q(x)}&lt;/math&gt;

By dividing both the numerator and the denominator by the leading coefficient of ''q''(''x''), we may assume [[without loss of generality]] that ''q''(''x'') is [[monic polynomial|monic]].  By the [[fundamental theorem of algebra]], we can write

: &lt;math&gt;q(x) = (x-a_1)^{j_1}\cdots(x-a_m)^{j_m}(x^2+b_1x+c_1)^{k_1}\cdots(x^2+b_nx+c_n)^{k_n}&lt;/math&gt;

where ''a''&lt;sub&gt;1&lt;/sub&gt;,..., ''a''&lt;sub&gt;''m''&lt;/sub&gt;, ''b''&lt;sub&gt;1&lt;/sub&gt;,..., ''b''&lt;sub&gt;''n''&lt;/sub&gt;, ''c''&lt;sub&gt;1&lt;/sub&gt;,..., ''c''&lt;sub&gt;''n''&lt;/sub&gt; are real numbers with ''b''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; − 4''c''&lt;sub&gt;''i''&lt;/sub&gt; &lt; 0, and ''j''&lt;sub&gt;1&lt;/sub&gt;,..., ''j''&lt;sub&gt;''m''&lt;/sub&gt;, ''k''&lt;sub&gt;1&lt;/sub&gt;,..., ''k''&lt;sub&gt;''n''&lt;/sub&gt; are positive integers.  The terms (''x'' − ''a''&lt;sub&gt;''i''&lt;/sub&gt;) are the ''linear factors'' of ''q''(''x'') which correspond to real roots of ''q''(''x''), and the terms (''x''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; + ''b''&lt;sub&gt;''i''&lt;/sub&gt;''x'' + ''c''&lt;sub&gt;''i''&lt;/sub&gt;) are the ''irreducible quadratic factors'' of ''q''(''x'') which correspond to pairs of [[complex number|complex]] conjugate roots of ''q''(''x'').

Then the partial fraction decomposition of ''ƒ''(''x'') is the following:

: &lt;math&gt;f(x) = \frac{p(x)}{q(x)} = P(x) + \sum_{i=1}^m\sum_{r=1}^{j_i} \frac{A_{ir}}{(x-a_i)^r} + \sum_{i=1}^n\sum_{r=1}^{k_i} \frac{B_{ir}x+C_{ir}}{(x^2+b_ix+c_i)^r}&lt;/math&gt;

Here, ''P''(''x'') is a (possibly zero) polynomial, and the ''A''&lt;sub&gt;''ir''&lt;/sub&gt;, ''B''&lt;sub&gt;''ir''&lt;/sub&gt;, and ''C''&lt;sub&gt;''ir''&lt;/sub&gt; are real constants.  There are a number of ways the constants can be found.

The most straightforward method is to multiply through by the common denominator ''q''(''x'').  We then obtain an equation of polynomials whose left-hand side is simply ''p''(''x'') and whose right-hand side has coefficients which are linear expressions of the constants ''A''&lt;sub&gt;''ir''&lt;/sub&gt;, ''B''&lt;sub&gt;''ir''&lt;/sub&gt;, and ''C''&lt;sub&gt;''ir''&lt;/sub&gt;.  Since two polynomials are equal if and only if their corresponding coefficients are equal, we can equate the coefficients of like terms.  In this way, a system of linear equations is obtained which ''always'' has a unique solution.  This solution can be found using any of the standard methods of [[linear algebra]]. It can also be found with [[limit (mathematics)|limits]] (see [[#Example 5 (limit method)|Example 5]]).

== Examples ==

=== Example 1 ===

: &lt;math&gt;f(x)=\frac{1}{x^2+2x-3}&lt;/math&gt;

Here, the denominator splits into two distinct linear factors:

: &lt;math&gt;q(x)=x^2+2x-3=(x+3)(x-1)&lt;/math&gt;

so we have the partial fraction decomposition

: &lt;math&gt;f(x)=\frac{1}{x^2+2x-3} =\frac{A}{x+3}+\frac{B}{x-1}&lt;/math&gt;

Multiplying through by ''x''&lt;sup&gt;2&lt;/sup&gt; + 2''x'' − 3, we have the polynomial identity

: &lt;math&gt;1=A(x-1)+B(x+3)&lt;/math&gt;

Substituting ''x''  = −3 into this equation gives ''A'' = −1/4, and substituting ''x''  = 1 gives ''B'' = 1/4, so that

: &lt;math&gt;f(x) =\frac{1}{x^2+2x-3} =\frac{1}{4}\left(\frac{-1}{x+3}+\frac{1}{x-1}\right)&lt;/math&gt;

'''

=== Example 2 ===

: &lt;math&gt;f(x)=\frac{x^3+16}{x^3-4x^2+8x}&lt;/math&gt;

After [[Polynomial long division|long-division]], we have

: &lt;math&gt;f(x)=1+\frac{4x^2-8x+16}{x^3-4x^2+8x}=1+\frac{4x^2-8x+16}{x(x^2-4x+8)}&lt;/math&gt;

Since (&amp;minus;4)&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;&amp;minus;&amp;nbsp;4×8 = &amp;minus;16 &lt; 0, the factor ''x''&lt;sup&gt;2&lt;/sup&gt; &amp;minus; 4''x'' + 8 is irreducible, and the partial fraction decomposition over the reals has the shape

: &lt;math&gt;\frac{4x^2-8x+16}{x(x^2-4x+8)}=\frac{A}{x}+\frac{Bx+C}{x^2-4x+8}&lt;/math&gt;

Multiplying through by ''x''&lt;sup&gt;3&lt;/sup&gt; &amp;minus; 4''x''&lt;sup&gt;2&lt;/sup&gt; + 8''x'', we have the polynomial identity

: &lt;math&gt;4x^2-8x+16 = A(x^2-4x+8)+(Bx+C)x&lt;/math&gt;

Taking ''x'' = 0, we see that 16 = 8''A'', so ''A'' = 2.  Comparing the ''x''&lt;sup&gt;2&lt;/sup&gt; coefficients, we see that 4 = ''A'' + ''B'' = 2 + ''B'', so ''B'' = 2.  Comparing linear coefficients, we see that &amp;minus;8 = &amp;minus;4''A'' + ''C'' = &amp;minus;8 + ''C'', so ''C'' = 0.  Altogether,

: &lt;math&gt;f(x)=1+2\left(\frac{1}{x}+\frac{x}{x^2-4x+8}\right)&lt;/math&gt;

The following example illustrates almost all the &quot;tricks&quot; one would need to use short of consulting a [[computer algebra system]]'''.

=== Example 3 ===

: &lt;math&gt;f(x)=\frac{x^9-2x^6+2x^5-7x^4+13x^3-11x^2+12x-4}{x^7-3x^6+5x^5-7x^4+7x^3-5x^2+3x-1}&lt;/math&gt;

After [[Polynomial long division|long-division]] and [[polynomial factorization|factoring]] the denominator, we have

: &lt;math&gt;f(x)=x^2+3x+4+\frac{2x^6-4x^5+5x^4-3x^3+x^2+3x}{(x-1)^3(x^2+1)^2}&lt;/math&gt;

The partial fraction decomposition takes the form

: &lt;math&gt;\frac{2x^6-4x^5+5x^4-3x^3+x^2+3x}{(x-1)^3(x^2+1)^2}=\frac{A}{x-1}+\frac{B}{(x-1)^2}+\frac{C}{(x-1)^3}+\frac{Dx+E}{x^2+1}+\frac{Fx+G}{(x^2+1)^2}&lt;/math&gt;

Multiplying through by (''x''&amp;nbsp;&amp;minus;&amp;nbsp;1)&lt;sup&gt;3&lt;/sup&gt;(''x''&lt;sup&gt;2&lt;/sup&gt; + 1)&lt;sup&gt;2&lt;/sup&gt; we have the polynomial identity

: &lt;math&gt;

\begin{align}
&amp; {} \quad 2x^6-4x^5+5x^4-3x^3+x^2+3x \\
&amp; =A(x-1)^2(x^2+1)^2+B(x-1)(x^2+1)^2+C(x^2+1)^2+(Dx+E)(x-1)^3(x^2+1)+(Fx+G)(x-1)^3
\end{align}
&lt;/math&gt;

Taking ''x'' = 1 gives 4 = 4''C'', so ''C'' = 1.  Similarly, taking ''x'' = [[complex number|''i'']] gives 2 + 2''i'' = (''Fi'' + ''G'')(2 + 2''i''), so ''Fi'' + ''G'' = 1, so ''F'' = 0 and ''G'' = 1 by equating real and [[complex number|imaginary]] parts.  With ''C'' = ''G'' = 1 and ''F'' = 0, taking ''x'' = 0 we get ''A'' − ''B'' + 1 − ''E'' − 1 = 0, thus ''E'' = ''A'' − ''B''.

We now have the identity

: &lt;math&gt;
\begin{align}
 &amp; {} 2x^6-4x^5+5x^4-3x^3+x^2+3x \\
 &amp; = A(x-1)^2(x^2+1)^2+B(x-1)(x^2+1)^2+(x^2+1)^2+(Dx+(A-B))(x-1)^3(x^2+1)+(x-1)^3 \\
 &amp; = A((x-1)^2(x^2+1)^2 + (x-1)^3(x^2+1)) + B((x-1)(x^2+1) - (x-1)^3(x^2+1)) + (x^2+1)^2 + Dx(x-1)^3(x^2+1)+(x-1)^3
\end{align}
&lt;/math&gt;

Expanding and sorting by exponents of x we get

: &lt;math&gt; 
\begin{align}
 &amp; {} 2 x^6 -4 x^5 +5 x^4 -3 x^3 + x^2 +3 x \\
 &amp; = (A + D) x^6 + (-A - 3D) x^5 + (2B + 4D + 1) x^4 + (-2B - 4D + 1) x^3 + (-A + 2B + 3D - 1) x^2 + (A - 2B - D + 3) x

\end{align}
&lt;/math&gt;

We can now compare the coefficients and see that

: &lt;math&gt; 
\begin{align}
 A + D &amp;=&amp; 2  \\
 -A - 3D &amp;=&amp;  -4 \\
2B + 4D + 1 &amp;=&amp; 5 \\
-2B - 4D + 1 &amp;=&amp; -3 \\
-A + 2B + 3D - 1 &amp;=&amp; 1 \\
A - 2B - D + 3 &amp;=&amp; 3 ,
\end{align}
&lt;/math&gt;

with ''A'' = 2 − ''D'' and −''A'' −3 ''D'' =−4 we get ''A'' = ''D'' = 1 and so ''B'' = 0, furthermore is ''C'' = 1, ''E'' = ''A'' − ''B'' = 1, ''F'' = 0 and ''G'' = 1.

The partial fraction decomposition of ''ƒ''(''x'') is thus

: &lt;math&gt;f(x)=x^2+3x+4+\frac{1}{(x-1)} + \frac{1}{(x - 1)^3} + \frac{x + 1}{x^2+1}+\frac{1}{(x^2+1)^2}.&lt;/math&gt;

Alternatively, instead of expanding, one can obtain other linear dependences on the coefficients computing some derivatives at ''x=1'' and at ''x=i'' in the above polynomial identity. (To this end, recall that the derivative at ''x=a'' of ''(x−a)&lt;sup&gt;m&lt;/sup&gt;p(x)'' vanishes if ''m &gt; 1'' and it is just ''p(a)'' if ''m=1''.)
Thus, for instance the first derivative at ''x=1'' gives 
: &lt;math&gt; 2\cdot6-4\cdot5+5\cdot4-3\cdot3+2+3   = A\cdot(0+0) + B\cdot( 2+ 0) + 8 + D\cdot0 &lt;/math&gt;
that is ''8   =   2B + 8''  so ''B=0''.

===Example 4 (residue method)===

:&lt;math&gt; f(z)=\frac{z^{2}-5}{(z^2-1)(z^2+1)}=\frac{z^{2}-5}{(z+1)(z-1)(z+i)(z-i)}&lt;/math&gt;

Thus, ''f''(''z'') can be decomposed into rational functions whose denominators are ''z''+1, ''z''−1, ''z''+i, ''z''−i. Since each term is of power one, −1, 1, −''i'' and ''i'' are simple poles.

Hence, the residues associated with each pole, given by
:&lt;math&gt;\frac{P(z_i)}{Q'(z_i)} = \frac{z_i^2 - 5}{4z_i^3}&lt;/math&gt;,
are 
:&lt;math&gt; 1, -1, \tfrac{3i}{2}, -\tfrac{3i}{2}&lt;/math&gt;,
respectively, and

:&lt;math&gt; f(z)=\frac{1}{z+1}-\frac{1}{z-1}+\frac{3i}{2}\frac{1}{z+i}-\frac{3i}{2}\frac{1}{z-i}&lt;/math&gt;.

===Example 5 (limit method)===

[[Limit (mathematics)|Limits]] can be used to find a partial fraction decomposition.&lt;ref&gt;{{cite book|last=Bluman|first=George W.|title=Problem Book for First Year Calculus|year=1984|publisher=Springer-Verlag|location=New York|pages=250–251}}&lt;/ref&gt;

:&lt;math&gt;f(x) = \frac{1}{x^3 - 1}&lt;/math&gt;

First, factor the denominator:

:&lt;math&gt;f(x) = \frac{1}{(x - 1)(x^2 + x + 1)}&lt;/math&gt;

The decomposition takes the form of

:&lt;math&gt;\frac{1}{(x-1)(x^2+x+1)} = \frac{A}{x - 1} + \frac{Bx + C}{x^2 + x + 1}&lt;/math&gt;

As &lt;math&gt;x \to 1&lt;/math&gt;, the ''A'' term dominates, so the right-hand side approaches &lt;math&gt;\frac{A}{x - 1}&lt;/math&gt;. Thus, we have

:&lt;math&gt;\frac{1}{(x - 1)(x^2 + x + 1)} = \frac{A}{x - 1}&lt;/math&gt;
:&lt;math&gt;A = \lim_{x \to 1}{\frac{1}{x^2 + x + 1}} = \frac{1}{3}&lt;/math&gt;

As &lt;math&gt;x \to \infty&lt;/math&gt;, the right-hand side is

:&lt;math&gt;\lim_{x \to \infty}{\frac{A}{x - 1} + \frac{Bx + C}{x^2 + x + 1}} = \frac{A}{x} + \frac{Bx}{x^2} = \frac{A + B}{x}.&lt;/math&gt;

:&lt;math&gt;\frac{A + B}{x} = \lim_{x \to \infty}{\frac{1}{x^3 - 1}} = 0&lt;/math&gt;

Thus, &lt;math&gt;B = -\frac{1}{3}&lt;/math&gt;.

At &lt;math&gt;x=0&lt;/math&gt;, &lt;math&gt;-1 = -A + C&lt;/math&gt;. Therefore,  &lt;math&gt;C = -\frac{2}{3}&lt;/math&gt;.

The decomposition is thus &lt;math&gt;\frac{\frac{1}{3}}{x - 1} + \frac{-\frac{1}{3}x - \frac{2}{3}}{x^2 + x + 1}&lt;/math&gt;.

== The role of the Taylor polynomial ==

The partial fraction decomposition of a rational function can be related to [[Taylor's theorem]] as follows. Let

:&lt;math&gt;P(x), Q(x), A_1(x),\dots, A_r(x)&lt;/math&gt;

be real or complex  polynomials; assume that

:&lt;math&gt;\textstyle Q=\prod_{j=1}^{r}(x-\lambda_j)^{\nu_j},&lt;/math&gt;

that

:&lt;math&gt;\textstyle\deg(P)&lt;\deg(Q)=\sum_{j=1}^{r}\nu_j,&lt;/math&gt;

and that
:&lt;math&gt;\textstyle\deg A_j&lt;\nu_j\text{ for }j=1,\dots,r.&lt;/math&gt;

Define also

:&lt;math&gt;\textstyle Q_i=\prod_{j\neq i}(x-\lambda_j)^{\nu_j}=\frac{Q}{(x-\lambda_i)^{\nu_i}} \text{ for }i=1,\dots,r.&lt;/math&gt;

Then we have
:&lt;math&gt;\frac{P}{Q}=\sum_{j=1}^{r}\frac{A_j}{(x-\lambda_j)^{\nu_j}}&lt;/math&gt;

if, and only if, for each &lt;math&gt;\textstyle i&lt;/math&gt; the polynomial &lt;math&gt;\textstyle A_i(x)&lt;/math&gt; is the Taylor polynomial of &lt;math&gt;\textstyle\frac{P}{Q_i}&lt;/math&gt; of order &lt;math&gt;\textstyle\nu_i-1&lt;/math&gt; at the point &lt;math&gt;\textstyle\lambda_i&lt;/math&gt;:

:&lt;math&gt;A_i(x):=\sum_{k=0}^{\nu_i-1} \frac{1}{k!}\left(\frac{P}{Q_i}\right)^{(k)}(\lambda_i)\ (x-\lambda_i)^k.
&lt;/math&gt;

Taylor's theorem (in the real or complex case) then provides a proof of the existence and uniqueness of the partial fraction decomposition, and a characterization of the coefficients.

'''Sketch of the proof:''' The above partial fraction decomposition implies, for each 1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;≤&amp;nbsp;''r'', a polynomial expansion

:&lt;math&gt;\frac{P}{Q_i}=A_i + O((x-\lambda_i)^{\nu_i})\qquad &lt;/math&gt;, as &lt;math&gt;x\to\lambda_i;&lt;/math&gt;

so &lt;math&gt;\textstyle A_i&lt;/math&gt; is the Taylor polynomial of &lt;math&gt;\textstyle\frac{P}{Q_i}&lt;/math&gt;, because of the unicity of the polynomial expansion of order &lt;math&gt;\textstyle\nu_i-1&lt;/math&gt;, and by assumption &lt;math&gt;\textstyle\deg A_i&lt;\nu_i&lt;/math&gt;.

Conversely, if the &lt;math&gt;\textstyle A_i&lt;/math&gt; are the Taylor polynomials, the above expansions at each &lt;math&gt;\textstyle\lambda_i&lt;/math&gt; hold, therefore we also have

:&lt;math&gt;P-Q_i A_i = O((x-\lambda_i)^{\nu_i})\qquad &lt;/math&gt;, as &lt;math&gt;x\to\lambda_i,&lt;/math&gt;

which implies that the polynomial &lt;math&gt;\textstyle P-Q_iA_i&lt;/math&gt; is divisible by &lt;math&gt;\textstyle (x-\lambda_i)^{\nu_i}.&lt;/math&gt;

For &lt;math&gt;\textstyle j\neq i&lt;/math&gt; also &lt;math&gt;\textstyle Q_jA_j&lt;/math&gt; is divisible by &lt;math&gt;\textstyle (x-\lambda_i)^{\nu_i}&lt;/math&gt;, so we have in turn that &lt;math&gt;\textstyle P- \sum_{j=1}^{r}Q_jA_j&lt;/math&gt; is divisible by &lt;math&gt;\textstyle Q&lt;/math&gt;. Since &lt;math&gt;\textstyle\deg\left( P- \sum_{j=1}^{r}Q_jA_j \right) &lt; \deg(Q)&lt;/math&gt; we then have
&lt;math&gt;\textstyle P- \sum_{j=1}^{r}Q_jA_j=0&lt;/math&gt;, and we find the partial fraction decomposition dividing by &lt;math&gt;\textstyle Q&lt;/math&gt;.

== Fractions of integers ==

The idea of partial fractions can be generalized to other [[integral domain]]s,
say the ring of [[integer]]s where [[prime numbers]] take the role of irreducible denominators.
For example:

: &lt;math&gt;\frac{1}{18} = \frac{1}{2} - \frac{1}{3} - \frac{1}{3^2}. &lt;/math&gt;

== Notes ==
{{Reflist}}

== References ==
*{{cite news
|first1=K. R. | last1=Rao
|first2=N. | last2=Ahmed
|title=Recursive techniques for obtaining the partial fraction expansion of a rational function
|year=1968
|volume=11 | number=2
|journal=IEEE Trans. Educ.
|pages=152–154
|doi=10.1109/TE.1968.4320370
}}
*{{cite news | first1=Peter | last1=Henrici
|title=An algorithm for the incomplete decomposition of a rational function into partial fractions
|journal=Z. f. Angew. Mathem. Physik
|year=1971
|volume=22 | number=4 | pages=751–755
|doi=10.1007/BF01587772 }}
*{{cite news | first1=Feng-Cheng | last1=Chang
|title=Recursive formulas for the partial fraction expansion of a rational function with multiple poles
|year=1973
|journal = Proc. IEEE
|volume=61 | number=8 |pages=1139–1140
|doi=10.1109/PROC.1973.9216 }}
*{{cite doi|10.1137/0206042|noedit}}
* {{cite news | first1=Dan |last1=Eustice 
|first2=M. S.|last2=Klamkin
|title=On the coefficients of a partial fraction decomposition
|year=1979| jstor=2320421 |volume=86
|number=6 | pages=478–480
}}
*{{cite news
|first1=J. J. | last1=Mahoney
|first2=B. D. | last2=Sivazlian
|title=Partial fractions expansion: a review of computational methodology and efficiency
|journal=J. Comp. Appl. Math.
|year=1983
|doi=10.1016/0377-0427(83)90018-3
|volume=9
|pages=247–269
}}
*{{cite book |last1=Miller |first1=Charles D. |last2=Lial |first2=Margaret L. |last3=Schneider |first3=David I. |title=Fundamentals of College Algebra |edition=3rd ed. |year=1990|publisher=Addison-Wesley Educational Publishers, Inc. |isbn=0-673-38638-4|pages=364–370}}
*{{cite news 
|first1=David | last1=Westreich
|title=partial fraction expansion without derivative evaluation
|year=1991
|journal=IEEE Trans. Circ. Syst.
|volume=38 | number=6
|pages=658–660
|doi=10.1109/31.81863
}}
*{{springer|id=u/u095160|title=Undetermined coefficients, method of|first=L. D.|last=Kudryavtsev}}
*{{cite news
|first1=Daniel J.
|last1=Velleman
|title=Partial fractions, binomial coefficients and the integral of an odd power of sec theta
|year=2002
|journal= Am. Math. Monthly
|volume=109
|number=8
|pages=746–749
|jstor=3072399 }}
*{{cite news | first1=Damian | last1=Slota | first2=Roman | last2=Witula
|year=2005 | series=Lect. Not. Computer Sci.
|title=Three brick method of the partial fraction decomposition of some type of rational expression
|pages=659–662 | volume=33516 |
doi=10.1007/11428862_89}}
*{{cite journal | first1=Sidney H. | last1=Kung
|journal=Coll. Math. J.
|title= Partial fraction decomposition by division
|year=2006 | volume=37 | number=2 | pages=132–134
|jstor=27646303
}}
*{{cite news | first2=Damian | last2=Slota | first1=Roman | last1=Witula
|year=2008 | journal=Appl. Math. Comput.
|title=Partial fractions decompositions of some rational functions
|pages=328–336 | volume=197 | doi=10.1016/j.amc.2007.07.048 | mr=2396331
}}

== External links ==
* {{MathWorld |urlname=PartialFractionDecomposition |title=Partial Fraction Decomposition}}
* {{cite web|first1= Sam | last1=Blake
|url=http://calc101.com/webMathematica/partial-fractions.jsp
|title=Step-by-Step Partial Fractions}}
* [http://cajael.com/eng/control/LaplaceT/LaplaceT-1_Example_2_6_OGATA_4editio.php] Make partial fraction decompositions with [[Scilab]].

[[Category:Algebra]]
[[Category:Elementary algebra]]
[[Category:Partial fractions]]</text>
      <sha1>83dc4w3cqsf23noika6jtn5n70ltpv9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Gelfand–Kirillov dimension</title>
    <ns>0</ns>
    <id>35987396</id>
    <revision>
      <id>582461732</id>
      <parentid>569562704</parentid>
      <timestamp>2013-11-20T01:40:47Z</timestamp>
      <contributor>
        <username>Mark viking</username>
        <id>17698045</id>
      </contributor>
      <comment>Added wl</comment>
      <text xml:space="preserve" bytes="1562">In [[algebra]], the '''Gelfand–Kirillov dimension''' of a [[right module]] ''M'' over a [[K-algebra|''k''-algebra]] ''A'' is:

:&lt;math&gt;\operatorname{GKdim} = \sup \limsup_{n \to \infty} \log_n \dim_k M_0 V^n&lt;/math&gt;

where the sup is taken over all finite-dimensional [[Linear subspace|subspace]]s &lt;math&gt;V \subset A&lt;/math&gt; and &lt;math&gt;M_0 \subset M&lt;/math&gt;.

== In the theory of D-Modules ==

Given a right module ''M'' over the [[Weyl algebra]] &lt;math&gt;A_n&lt;/math&gt;, the Gelfand–Kirillov dimension of ''M'' over the Weyl algebra coincides with the dimension of ''M'', which is by definition the degree of the [[Hilbert polynomial]] of ''M''. This enables to prove additivity in [[short exact sequence]]s for the Gelfand–Kirillov dimension and finally to prove [[Weyl_algebra#Properties of the Weyl algebra|Bernstein's inequality]], which states that the dimension of ''M'' must be at least ''n''. This leads to the definition of [[D-module|holonomic D-Module]]s as those with the minimal dimension ''n'', and these modules play a great role in the [[geometric Langlands program]].

== References ==
*{{cite journal|title=A remark on Gelfand–Kirillov dimension|url=http://www.math.washington.edu/~smith/Research/GK-rmk.pdf |last1=Smith |first1=S. Paul |last2=Zhang |first2=James J. |year=1998 |journal=[[Proceedings of the American Mathematical Society]] |volume=126 |number=2 |pages=349–352}}
* Coutinho: A primer of algebraic D-modules. Cambridge, 1995

{{DEFAULTSORT:Gelfand-Kirillov dimension}}
[[Category:Algebra]]
[[Category:Dimension]]


{{algebra-stub}}</text>
      <sha1>fqa5cjq4hliee15gvp8rhq0buwnz6jy</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Constant (mathematics)</title>
    <ns>0</ns>
    <id>24758132</id>
    <revision>
      <id>599144990</id>
      <parentid>599144525</parentid>
      <timestamp>2014-03-11T15:26:21Z</timestamp>
      <contributor>
        <username>Salix alba</username>
        <id>212526</id>
      </contributor>
      <comment>/* Context-dependence */ use mathbf for the constant to make it stand out</comment>
      <text xml:space="preserve" bytes="6084">{{refimprove|date=August 2012}}
In [[mathematics]], the adjective '''constant''' means non-varying. The noun '''constant''' may have two different meanings. It may refer to a fixed and well defined number or other [[mathematical object]]. The term [[mathematical constant]] (and also [[physical constant]]) is sometimes used to distinguish this meaning from the other one.  A '''constant''' may also refer to a [[constant function]] or its [[value (mathematics)|value]] (it is a common usage to identify them). Such a constant is commonly represented by a [[variable (mathematics)|variable]] which does not depend on the main variable(s) of the studied problem. This is the case, for example, for a [[constant of integration]] which is an arbitrary constant function (not depending on the variable of integration) added to a particular [[antiderivative]] to get all the antiderivatives of the given function.

For example, a general quadratic function is commonly written as:

:&lt;math&gt;a x^2 + b x + c\, ,&lt;/math&gt;

where ''a'', ''b'' and ''c'' are constants (or parameters), while ''x'' is the variable, a placeholder for the argument of the function being studied. A more explicit way to denote this function is

:&lt;math&gt;x\mapsto a x^2 + b x + c \, ,&lt;/math&gt;

which makes the function-argument status of ''x'' clear, and thereby implicitly the constant status of ''a'', ''b'' and ''c''. In this example ''a'', ''b'' and ''c'' are [[coefficient]]s of the polynomial. Since ''c'' occurs in a term that does not involve ''x'', it is called the [[Constant term|constant term of the polynomial]] and can be thought of as the coefficient of ''x''&lt;sup&gt;0&lt;/sup&gt;; any polynomial term or expression of [[Degree of a polynomial|degree]] zero is a constant.&lt;ref&gt;{{cite book | last = Foerster | first = Paul A. | title = Algebra and Trigonometry: Functions and Applications, Teacher's Edition | edition = Classics | year = 2006 | isbn = 0-13-165711-9 | publisher = [[Prentice Hall]] | location = Upper Saddle River, NJ}}&lt;/ref&gt;{{rp|18}}

== Constant function ==
{{Main|Constant function|Nullary}}

A constant may be used to define a [[constant function]] that ignores its arguments and always gives the same value. A constant function of a single variable, such as &lt;math&gt;f(x)=5&lt;/math&gt;, has a graph that is a horizontal straight line, parallel to the x-axis. Such a function always takes the same value (in this case, 5) because its argument does not appear in the expression defining the function.

== Context-dependence ==

The context-dependent nature of the concept of &quot;constant&quot; can be seen in this example from elementary calculus:

:&lt;math&gt;\begin{array}{lll}
\frac{d}{dx} 2^x  &amp; = \lim_{h\to 0} \frac{2^{x+h} - 2^x}{h} &amp; = \lim_{h\to 0} 2^x\frac{2^h - 1}{h} \\
 &amp; = 2^x \lim_{h\to 0} \frac{2^h - 1}{h} &amp; \text{since }x\text{ is constant (i.e. does not depend on }h\text{)}\\
 &amp; = 2^x \cdot\mathbf{constant,} &amp; \text{ where }\mathbf{constant}\text{ means not depending on }x.
\end{array}&lt;/math&gt;
&quot;Constant&quot; means not depending on some variable; not changing as that variable changes. In the first case above, it means not depending on&amp;nbsp;''h''; in the second, it means not depending on&amp;nbsp;''x''.

==Notable mathematical constants==
{{main|Mathematical constant}}
Some values occur frequently in mathematics and are conventionally denoted by a specific symbol. These standard symbols and their values are called mathematical constants. Examples include:
* ''0 ([[zero]])''.
* ''1 ([[one]])'', the [[natural number]] after zero.
* ''π ([[pi]])'', the constant representing the [[ratio]] of a circle's circumference to its diameter, approximately equal to 3.141592653589793238462643...&lt;ref&gt;{{cite book | last = Arndt | first = Jörg | last2 = Haenel | first2 = Christoph | title = Pi - Unleashed | page = 240 | year = 2001 | publisher = Springer | isbn = 978-3540665724}}&lt;/ref&gt;
* ''[[e (mathematical constant)|e]]'', approximately equal to 2.718281828459045235360287...
* ''i'', the [[imaginary unit]] such that ''i''&lt;sup&gt;2&lt;/sup&gt; = -1.
*''&lt;math alt=&quot;Square root of 2&quot;&gt;\sqrt{2}&lt;/math&gt; ([[square root of 2]])'', the length of the diagonal of a square with unit sides, approximately equal to 1.414213562373095048801688.
*''φ ([[golden ratio]])'', approximately equal to 1.618033988749894848204586, or algebraically, &lt;math&gt;1+ \sqrt{5} \over 2&lt;/math&gt;.

==Constants in calculus==
In [[calculus]], constants are treated in several different ways depending on the operation. For example, the [[derivative]] of a constant function is zero. This is because the derivative measures the rate of change of a function with respect to a variable, and since constants, by definition, do not change, their derivative is therefore zero. Conversely, when [[Antiderivative|integrating]] a constant function, the constant is multiplied by the variable of integration. During the evaluation of a [[limit (mathematics)|limit]], the constant remains the same as it was before and after evaluation.

Integration of a function of one variable often involves a [[constant of integration]]. This arises because of the integral operator's nature as the inverse of the [[derivative|differential operator]], meaning the aim of integration is to recover the original function before differentiation. The differential of a constant function is zero, as noted above, and the differential operator is a linear operator, so functions that only differ by a constant term have the same derivative. To acknowledge this, a constant of integration is added to an [[indefinite integral]]; this ensures that all possible solutions are included. The constant of integration is generally written as 'c' and represents a constant with a fixed but undefined value.

===Examples===
&lt;math&gt;f(x)=72 \Rightarrow f'(x)=0&lt;/math&gt;&lt;br /&gt;
&lt;math&gt;f(x)=72 \Rightarrow \int 72 \, dx = 72x+c&lt;/math&gt;&lt;br /&gt;
&lt;math&gt;f(x)=72 \Rightarrow \lim_{x \to \infty} 72 = 72&lt;/math&gt;

==See also==
*[[Expression (mathematics)|Expression]]
*[[Physical constant]]
*[[Constant (disambiguation)]]

==References==
{{reflist}}

[[Category:Algebra]]
[[Category:Elementary mathematics]]</text>
      <sha1>njeb8gck5vnvhv56dsoc9mw6cqqpv1l</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Polynomial</title>
    <ns>0</ns>
    <id>23000</id>
    <revision>
      <id>612432812</id>
      <parentid>612425212</parentid>
      <timestamp>2014-06-11T01:00:00Z</timestamp>
      <contributor>
        <username>Quondum</username>
        <id>12331483</id>
      </contributor>
      <comment>Undid revision 612425212 by [[Special:Contributions/68.119.140.163|68.119.140.163]] ([[User talk:68.119.140.163|talk]]) – unmotivated removal of phrase, no longer grammatical</comment>
      <text xml:space="preserve" bytes="50795">{{Merge from|Polynomial expression|discuss=Talk:Polynomial#Merger proposal|date=April 2014}}

[[File:Polynomialdeg3.svg|The [[graph of a function|graph]] of a polynomial function of degree 3|thumb|upright]]
In [[mathematics]], a '''polynomial''' is an [[mathematical expression|expression]] consisting of [[variable (mathematics)|variables]] (or [[indeterminate (variable)|indeterminates]]) and [[coefficient]]s, that involves only the operations of [[addition]], [[subtraction]], [[multiplication]], and non-negative [[integer]] [[exponent]]s. An example of a polynomial of a single indeterminate (or variable), {{math|''x''}}, is {{math|''x''&lt;sup&gt;2&lt;/sup&gt; &amp;minus; 4''x'' + 7}}, which is a [[Quadratic function|quadratic polynomial]].

Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form '''polynomial equations''', which encode a wide range of problems, from elementary [[word problem (mathematics education)|word problems]] to complicated problems in the sciences; they are used to define '''polynomial functions''', which appear in settings ranging from basic [[chemistry]] and [[physics]] to [[economics]] and [[social science]]; they are used in [[calculus]] and [[numerical analysis]] to approximate other functions.  In advanced mathematics, polynomials are used to construct [[polynomial ring]]s and [[algebraic variety|algebraic varieties]], central concepts in [[algebra]] and [[algebraic geometry]].

==Etymology==
According to the [[Oxford English Dictionary]], ''polynomial'' succeeded the term ''[[binomial]]'', and was made simply by replacing the Latin root ''bi-'' with the Greek ''poly-'', which comes from the Greek word for ''many''. The word ''polynomial'' was first used in the 17th century.&lt;ref&gt;Etymology of &quot;polynomial&quot;. ''Compact Oxford English Dictionary''&lt;/ref&gt;

=={{anchor|Polynomial notation}} Notation and terminology==
The ''x'' occurring in a polynomial is commonly called either a ''variable'' or an ''indeterminate''. When the polynomial is considered for itself, ''x'' is a fixed symbol which does not have any value (its value is &quot;indeterminate&quot;). It is thus more correct to call it an &quot;indeterminate&quot;. However, when one considers the [[function (mathematics)|function]] defined by the polynomial, then ''x'' represents the argument of the function, and is therefore called a &quot;variable&quot;. Many authors use these two words indifferently, but this may be sometimes confusing and is not done in this article.

It is a common convention to use upper case letters for the indeterminates and the corresponding lower case letters for the variables (arguments) of the associated function.

It may be confusing that a polynomial ''P'' in the indeterminate ''X'' may appear in the formulas either as ''P'' or as ''P''(''X'').

Normally, the name of the polynomial is ''P'', not ''P''(''X''). However, if ''a'' denotes a number, a variable, another polynomial, or, more generally any expression, then ''P''(''a'') denotes, by convention, the result of substituting ''X'' by ''a'' in ''P''. For example, the polynomial ''P'' defines the function 
:&lt;math&gt;x\mapsto P(x)&lt;/math&gt;

In particular, if ''a'' = ''X'', then the definition of ''P''(''a'') implies 
:&lt;math&gt;P=P(X).&lt;/math&gt;

This equality allows writing &quot;let ''P''(''X'') be a polynomial&quot; as a shorthand for &quot;let ''P'' be a polynomial in the indeterminate ''X''&quot;. On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.

==Definition==
A polynomial in a single indeterminate can be written in the form
:&lt;math&gt;a_n x^n + a_{n-1}x^{n-1} + \dotsb + a_2 x^2 + a_1 x + a_0,&lt;/math&gt;
where &lt;math&gt;a_0, \ldots, a_n&lt;/math&gt; are numbers, or more generally elements of a [[ring (mathematics)|ring]], and &lt;math&gt;x&lt;/math&gt; is a symbol which is called an [[indeterminate (variable)|indeterminate]] or, for historical reasons, a [[variable (mathematics)|variable]]. The symbol &lt;math&gt;x&lt;/math&gt; does not represent any value, although the usual (commutative, distributive) laws valid for [[arithmetic]] operations also apply to it.

This can be expressed more concisely by using [[Capital-sigma notation#Capital-sigma notation|summation notation]]:

:&lt;math&gt;\sum_{i=0}^n a_i x^i&lt;/math&gt;

That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero [[term (mathematics)|terms]]. Each term consists of the product of a number—called the [[coefficient]] of the term&lt;ref&gt;The coefficient of a term may be any number from a specified set.  If that set is the set of real numbers, we speak of &quot;polynomials over the reals&quot;.  Other common kinds of polynomials are polynomials with integer coefficients, polynomials with complex coefficients, and polynomials with coefficients that are integers [[modular arithmetic|modulo]] of some [[prime number]] {{math|''p''}}.&lt;/ref&gt;—and a finite number of indeterminates, raised to integer powers. The exponent on an indeterminate in a term is called the [[Degree of a polynomial|degree]] of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient.  Since {{math|''x'' {{=}} ''x''&lt;sup&gt;1&lt;/sup&gt;}}, the degree of an indeterminate without a written exponent is one. A term and a polynomial with no indeterminates are called respectively a [[constant term]] and a constant polynomial;&lt;ref&gt;This terminology date from the time where the distinction was not clear between a polynomial and the function that it defines: a constant term and a constant polynomial define [[constant function]]s.&lt;/ref&gt; the degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial (which has no term) is not defined.&lt;ref name=Barbeau-2003-pp1-2&gt;{{cite book|author=Barbeau, E.J.|title=Polynomials|publisher=Springer|year=2003|isbn=9780387406275|pages=1–2|url=http://books.google.com/books?id=CynRMm5qTmQC&amp;pg=PA1}}&lt;/ref&gt;

For example:

: &lt;math&gt; -5x^2y\,&lt;/math&gt;

is a term. The coefficient is {{math|&amp;minus;5}}, the indeterminates are {{math|''x''}} and {{math|''y''}}, the degree of {{math|''x''}} is two, while the degree of {{math|''y''}} is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is {{math|2 + 1 {{=}} 3}}.

Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:

:&lt;math&gt;\underbrace{_\,3x^2}_{\begin{smallmatrix}\mathrm{term}\\\mathrm{1}\end{smallmatrix}} \underbrace{-_\,5x}_{\begin{smallmatrix}\mathrm{term}\\\mathrm{2}\end{smallmatrix}} \underbrace{+_\,4}_{\begin{smallmatrix}\mathrm{term}\\\mathrm{3}\end{smallmatrix}}. &lt;/math&gt;

It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.

Polynomials of small degree have been given specific names. A polynomial of degree zero is a ''constant polynomial'' or simply a ''constant''. Polynomials of degree one, two or three are respectively ''linear polynomials,'' ''quadratic polynomials'' and ''cubic polynomials''. For higher degrees the specific names are not commonly used, although ''quartic polynomial'' (for degree four) and ''quintic polynomial'' (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in {{math|''x''&lt;sup&gt;2&lt;/sup&gt; + 2''x'' + 1}} the term {{math|2''x''}} is a linear term in a quadratic polynomial.

{{anchor|zero polynomial}}The polynomial 0, which may be considered to have no terms at all, is called the '''zero polynomial'''. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞).&lt;ref&gt;{{MathWorld|urlname=ZeroPolynomial|title=Zero Polynomial}}&lt;/ref&gt; These conventions are useful when defining [[Euclidean division of polynomials]]. The zero polynomial is also unique in that it is the only polynomial having an infinite number of [[Root of a function|roots]]. In the case of polynomials in more than one indeterminate, a polynomial is called ''homogeneous'' of {{nowrap|degree {{math|''n''}}}} if ''all'' its terms have {{nowrap|degree {{math|''n''}}}}. For example, {{math|''x''&lt;sup&gt;3&lt;/sup&gt;''y''&lt;sup&gt;2&lt;/sup&gt; + 7''x''&lt;sup&gt;2&lt;/sup&gt;''y''&lt;sup&gt;3&lt;/sup&gt; − 3''x''&lt;sup&gt;5&lt;/sup&gt;}} is homogeneous of degree 5. For more details, see [[homogeneous polynomial]].

The [[commutative law]] of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in &quot;descending powers of {{math|''x''}}&quot;, with the term of largest degree first, or in &quot;ascending powers of {{math|''x''}}&quot;.  The polynomial in the example above is written in descending powers of {{math|''x''}}.  The first term has coefficient {{math|3}}, indeterminate {{math|''x''}}, and exponent {{math|2}}.  In the second term, the coefficient {{nowrap|is {{math|&amp;minus;5}}}}. The third term is a constant.  Since the ''degree'' of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.&lt;ref&gt;{{cite book|author=Edwards, Harold M.|title=Linear Algebra|publisher=Springer|year=1995|isbn=9780817637316|page=78|url=http://books.google.com/books?id=ylFR4h5BIDEC&amp;pg=PA78}}&lt;/ref&gt;

Two terms with the same indeterminates raised to the same powers are called &quot;similar terms&quot; or &quot;like terms&quot;, and they can be combined, using the [[distributive law]], into a single term  whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0.&lt;ref name=&quot;Edwards-1995-p47&quot; /&gt; Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a [[monomial]],&lt;ref&gt;Some authors use &quot;monomial&quot; to mean &quot;[[monic polynomial|monic]] monomial&quot;. See {{cite book |author=Anthony W. Knapp |title=Advanced Algebra: Along with a Companion Volume Basic Algebra |page=457 |year=2007 |publisher=Springer |isbn=0-8176-4522-5}}&lt;/ref&gt; a two-term polynomial is called a [[binomial]], and so on.
{{anchor|univariate}}A polynomial in one indeterminate is called a ''[[univariate]] polynomial'', a polynomial in more than one indeterminate is called a ''multivariate polynomial''. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as ''bivariate'', ''trivariate'', and so on, according to the maximum number of indeterminates allowed.  Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on.  It is common, also, to say simply &quot;polynomials in {{math|''x'', ''y''}}, and {{math|''z''}}&quot;, listing the indeterminates allowed.

The ''evaluation of a polynomial'' consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using the [[Horner scheme]]:
:&lt;math&gt;(((\dotsb((a_n x + a_{n-1})x + a_{n-2})x + \dotsb + a_3)x + a_2)x + a_1)x + a_0.&lt;/math&gt;

==Arithmetic of polynomials==
Polynomials can be added using the [[associative law|associative]] law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms.&lt;ref name=&quot;Edwards-1995-p47&quot;&gt;{{cite book|author=Edwards, Harold M.|title=Linear Algebra|publisher=Springer|year=1995|isbn=9780817637316|page=47|url=http://books.google.com/books?id=ylFR4h5BIDEC&amp;pg=PA47}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author=Salomon, David|title=Coding for Data and Computer Communications|publisher=Springer|year=2006|isbn=9780387238043|page=459|url=http://books.google.com/books?id=Zr9bjEpXKnIC&amp;pg=PA459}}&lt;/ref&gt; For example, if
:&lt;math&gt;\begin{align}
  P &amp;=  3x^2 - 2x + 5xy - 2 \\
  Q &amp;= -3x^2 + 3x + 4y^2 + 8
\end{align}&lt;/math&gt;

then
:&lt;math&gt;P + Q = 3x^2 - 2x + 5xy - 2 - 3x^2 + 3x + 4y^2 + 8 &lt;/math&gt;

which can be simplified to
:&lt;math&gt;P + Q = x + 5xy + 4y^2 + 6 &lt;/math&gt;

To work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other.&lt;ref name=&quot;Edwards-1995-p47&quot; /&gt; For example, if
:&lt;math&gt;\begin{align}
  \color{Brown} P &amp;\color{Brown}{= 2x + 3y + 5} \\
  \color{RoyalBlue} Q &amp;\color{RoyalBlue}{= 2x + 5y + xy + 1}
\end{align}&lt;/math&gt;

then
:&lt;math&gt;\begin{array}{rccrcrcrcr}
{\color{Brown}{P}} {\color{RoyalBlue}{Q}} &amp; {{=}}&amp;&amp;({\color{Brown}{2x}}\cdot{\color{RoyalBlue}{2x}})
&amp;+&amp;({\color{Brown}{2x}}\cdot{\color{RoyalBlue}{5y}})&amp;+&amp;({\color{Brown}{2x}}\cdot {\color{RoyalBlue}{xy}})&amp;+&amp;({\color{Brown}{2x}}\cdot{\color{RoyalBlue}{1}})
\\&amp;&amp;+&amp;({\color{Brown}{3y}}\cdot{\color{RoyalBlue}{2x}})&amp;+&amp;({\color{Brown}{3y}}\cdot{\color{RoyalBlue}{5y}})&amp;+&amp;({\color{Brown}{3y}}\cdot {\color{RoyalBlue}{xy}})&amp;+&amp;
({\color{Brown}{3y}}\cdot{\color{RoyalBlue}{1}})
\\&amp;&amp;+&amp;({\color{Brown}{5}}\cdot{\color{RoyalBlue}{2x}})&amp;+&amp;({\color{Brown}{5}}\cdot{\color{RoyalBlue}{5y}})&amp;+&amp;
({\color{Brown}{5}}\cdot {\color{RoyalBlue}{xy}})&amp;+&amp;({\color{Brown}{5}}\cdot{\color{RoyalBlue}{1}})
\end{array}&lt;/math&gt;

which can be simplified to
:&lt;math&gt;PQ = 4x^2 + 21xy + 2x^2y + 12x + 15y^2 + 3xy^2 + 28y + 5&lt;/math&gt;

Polynomial evaluation can be used to compute the remainder of [[Euclidean division of polynomials|polynomial division]] by a polynomial of degree one, since the remainder of the division of {{math|''f''(''x'')}} by {{math|(''x'' &amp;minus; ''a'')}} is {{math|''f''(''a'')}}; see the [[polynomial remainder theorem]]. This is more efficient than the usual algorithm of division when the quotient is not needed.

* A [[sum]] of polynomials is a polynomial.&lt;ref name=Barbeau-2003-pp1-2 /&gt;
* A [[product (mathematics)|product]] of polynomials is a polynomial.&lt;ref name=Barbeau-2003-pp1-2 /&gt;
* A [[function composition|composition]] of two polynomials is a polynomial, which is obtained by substituting a variable of the first polynomial by the second polynomial.&lt;ref name=Barbeau-2003-pp1-2 /&gt;
* The [[derivative]] of the polynomial {{math|''a''&lt;sub&gt;n&lt;/sub&gt;''x''&lt;sup&gt;n&lt;/sup&gt; + ''a''&lt;sub&gt;n&amp;minus;1&lt;/sub&gt;''x''&lt;sup&gt;n&amp;minus;1&lt;/sup&gt; + ... +  ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sup&gt;2&lt;/sup&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt;''x'' + ''a''&lt;sub&gt;0&lt;/sub&gt;}} is the polynomial {{math|n''a''&lt;sub&gt;n&lt;/sub&gt;''x''&lt;sup&gt;n&amp;minus;1&lt;/sup&gt; + (n&amp;minus;1)''a''&lt;sub&gt;n&amp;minus;1&lt;/sub&gt;''x''&lt;sup&gt;n&amp;minus;2&lt;/sup&gt;  + ... +  2''a''&lt;sub&gt;2&lt;/sub&gt;''x'' + ''a''&lt;sub&gt;1&lt;/sub&gt;}}. If the set of the coefficients does not contain the integers (for example if the coefficients are integers [[modular arithmetic|modulo]] some [[prime number]] {{math|''p''}}), then {{math|k''a''&lt;sub&gt;k&lt;/sub&gt;}} should be interpreted as the sum of {{math|''a''&lt;sub&gt;k&lt;/sub&gt;}} with itself, {{math|k}} times. For example, over the integers modulo {{math|''p''}}, the derivative of the polynomial {{math|''x''&lt;sup&gt;''p''&lt;/sup&gt; + 1}} is the polynomial {{math|0}}.&lt;ref name=Barbeau-2003-pp64-65&gt;{{cite book|author=Barbeau, E.J.|title=Polynomials|publisher=Springer|year=2003|isbn=9780387406275|pages=64–65|url=http://books.google.com/books?id=CynRMm5qTmQC&amp;pg=PA64}}&lt;/ref&gt;
* A primitive or [[antiderivative]] of the polynomial {{math|''a''&lt;sub&gt;n&lt;/sub&gt;''x''&lt;sup&gt;n&lt;/sup&gt;  + ''a''&lt;sub&gt;n&amp;minus;1&lt;/sub&gt;''x''&lt;sup&gt;n&amp;minus;1&lt;/sup&gt;  + ... +  ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sup&gt;2&lt;/sup&gt;  + ''a''&lt;sub&gt;1&lt;/sub&gt;''x'' + ''a''&lt;sub&gt;0&lt;/sub&gt;}}  is the polynomial  {{math|''a''&lt;sub&gt;n&lt;/sub&gt;''x''&lt;sup&gt;n+1&lt;/sup&gt;/(n+1) + ''a''&lt;sub&gt;n&amp;minus;1&lt;/sub&gt;''x''&lt;sup&gt;n&lt;/sup&gt;/n  + ... +  ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sup&gt;3&lt;/sup&gt;/3  + ''a''&lt;sub&gt;1&lt;/sub&gt;''x''&lt;sup&gt;2&lt;/sup&gt;/2 + ''a''&lt;sub&gt;0&lt;/sub&gt;''x'' + ''c''}}, where {{math|''c''}} is an arbitrary constant. For instance, the antiderivatives of {{math|''x''&lt;sup&gt;2&lt;/sup&gt; + 1}} have the form {{math|{{sfrac|3}}''x''&lt;sup&gt;3&lt;/sup&gt; + ''x'' + ''c''}}.

As for the integers, two kinds of divisions are considered for the polynomials. The ''[[Euclidean division of polynomials]]'' that generalizes the [[Euclidean division]] of the integers. It results in two polynomials, a ''quotient'' and a ''remainder'' that are characterized by the following property of the polynomials: given two polynomials ''a'' and ''b'' such that ''b'' ≠ 0, there exists a unique pair of polynomials, ''q'', the quotient, and ''r'', the remainder, such that {{math|''a'' {{=}} ''b'' ''q'' + ''r''}} and {{math|degree(''r'') &lt; degree(''b'')}} (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the [[polynomial long division]] algorithm.&lt;ref&gt;Peter H. Selby, Steve Slavin, ''Practical Algebra: A Self-Teaching Guide, 2nd Edition'', Wiley, ISBN 0-471-53012-3 ISBN 978-0471530121&lt;/ref&gt;

All polynomials with coefficients in a [[unique factorization domain]] (for example, the integers or a [[field (mathematics)|field]]) also have a factored form in which the polynomial is written as a product of [[irreducible polynomial]]s and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant.  In the case of the field of [[complex number]]s, the irreducible factors are linear. Over the [[real number]]s, they have the degree either one or two. Over the integers and the [[rational number]]s the irreducible factors may have any degree.&lt;ref name=Barbeau-2003-pp80-82&gt;{{cite book|author=Barbeau, E.J.|title=Polynomials|publisher=Springer|year=2003|isbn=9780387406275|pages=80–82|url=http://books.google.com/books?id=CynRMm5qTmQC&amp;pg=PA80}}&lt;/ref&gt; For example, the factored form of 
:&lt;math&gt; 5x^3-5&lt;/math&gt;
is 
:&lt;math&gt;5(x - 1)\left(x^2 + x + 1\right)&lt;/math&gt;

over the integers and the reals and 
:&lt;math&gt; 5(x - 1)\left(x + \frac{1 + i\sqrt{3}}{2}\right)\left(x + \frac{1 - i\sqrt{3}}{2}\right)&lt;/math&gt;

over the complex numbers.

The computation of the factored form, called ''factorization'' is, in general, too difficult to be done by hand-written computation. However, there are efficient [[polynomial factorization]] [[algorithms]] that are available in most [[computer algebra system]]s.

A formal quotient of polynomials, that is, an [[algebraic fraction]] where the numerator and denominator are polynomials, is called a &quot;[[Rational function|rational expression]]&quot; or &quot;rational fraction&quot; and is not, in general, a polynomial.  Division of a polynomial by a number, however, does yield another polynomial.  For example, {{math|''x''&lt;sup&gt;3&lt;/sup&gt;/12}} is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to {{math|(1/12)''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{math|1/12}} is just a constant. When this expression is used as a term, its coefficient is therefore {{math|1/12}}. For similar reasons, if complex coefficients are allowed, one may have a single term like {{math|(2 + 3''i'') ''x''&lt;sup&gt;3&lt;/sup&gt;}}; even though it looks like it should be expanded to two terms, the complex number {{math|2 + 3''i''}} is one complex number, and is the coefficient of that term. The expression {{math|1/(''x''&lt;sup&gt;2&lt;/sup&gt; + 1)}} is not a polynomial because it includes division by a non-constant polynomial. The expression {{math|(5 + ''y'')&lt;sup&gt;''x''&lt;/sup&gt;}} is not a polynomial, because it contains an indeterminate used as exponent.

Since subtraction can be replaced by addition of the opposite quantity, and since positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.

==Polynomial functions==&lt;!-- &quot;Polynomial function&quot; redirects here --&gt;
{{see also|Ring of polynomial functions}}
A ''polynomial function'' is a function that can be defined by [[expression (mathematics)|evaluating]] a polynomial. A function {{math|''f''}} of one [[Argument of a function|argument]] is called a polynomial function if it satisfies{{citation needed|date=August 2013}}

: &lt;math&gt; f(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_2 x^2 + a_1 x + a_0 \, &lt;/math&gt;

for all arguments {{math|''x''}}, where {{math|''n''}} is a non-negative integer and {{math|''a''&lt;sub&gt;0&lt;/sub&gt;,  ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ..., ''a&lt;sub&gt;n&lt;/sub&gt;''}} are constant coefficients.

For example, the function {{math|''f''}}, taking real numbers to real numbers, defined by

:&lt;math&gt; f(x) = x^3 - x\,&lt;/math&gt;

is a polynomial function of one variable. Polynomial functions of multiple variables can also be defined, using polynomials in multiple indeterminates, as in

: &lt;math&gt;f(x,y)= 2x^3+4x^2y+xy^5+y^2-7.\,&lt;/math&gt;
An example is also the function &lt;math&gt;f(x)=\cos(2\arccos(x))&lt;/math&gt; which, although it doesn't look like a polynomial, is a polynomial function on &lt;math&gt;[-1,1]&lt;/math&gt; since for every &lt;math&gt;x&lt;/math&gt; from &lt;math&gt;[-1,1]&lt;/math&gt; it is true that &lt;math&gt;f(x)=2x^2-1&lt;/math&gt; (see [[Chebyshev polynomials]]).

Polynomial functions are a class of functions having many important properties. They are all [[Continuous function|continuous]], [[smooth function|smooth]], [[entire function|entire]], [[computable function|computable]], etc.{{citation needed|date=August 2013}}

===Graphs of polynomial functions===
&lt;div class=&quot;floatright&quot;&gt;
&lt;gallery perrow=&quot;2&quot; widths=&quot;200px&quot; heights=&quot;200px&quot;&gt;
File:Polynomialdeg2.svg|Polynomial of degree 2:&lt;br&gt;&lt;small&gt;{{math|''f''(''x'') {{=}} ''x''&lt;sup&gt;2&lt;/sup&gt; &amp;minus; ''x'' &amp;minus; 2}}&lt;br&gt;{{math|{{=}} (''x'' + 1)(''x'' &amp;minus; 2)}}&lt;/small&gt;
File:Polynomialdeg3.svg|Polynomial of degree 3:&lt;br&gt;&lt;small&gt;{{math|''f''(''x'') {{=}} ''x''&lt;sup&gt;3&lt;/sup&gt;/4 + 3''x''&lt;sup&gt;2&lt;/sup&gt;/4 &amp;minus; 3''x''/2 &amp;minus; 2}}&lt;br&gt;{{math|{{=}}  1/4 (''x'' + 4)(''x'' + 1)(''x'' &amp;minus; 2)}}&lt;/small&gt;
File:Polynomialdeg4.svg|Polynomial of degree 4:&lt;br&gt;&lt;small&gt;{{math|''f''(''x'') {{=}} 1/14 (''x'' + 4)(''x'' + 1)(''x'' &amp;minus; 1)(''x'' &amp;minus; 3) + 0.5}}&lt;/small&gt;
File:Polynomialdeg5.svg|Polynomial of degree 5:&lt;br&gt;&lt;small&gt;{{math|''f''(''x'') {{=}} 1/20 (''x'' + 4)(''x'' + 2)(''x'' + 1 )(''x'' &amp;minus; 1)(''x'' &amp;minus; 3)}}&lt;br&gt;{{math|+ 2}}&lt;/small&gt;
File:Sextic Graph.svg|Polynomial of degree 6:&lt;br&gt;&lt;small&gt;{{math|''f''(''x'') {{=}} 1/30 (''x''+3.5)(''x''+2)(''x'' + 1)(''x'' &amp;minus; 1)(''x'' &amp;minus; 3)}}&lt;br&gt;{{math|(''x'' &amp;minus; 4) + 2}}&lt;/small&gt;
File:Septic graph.svg|Polynomial of degree 7:&lt;br&gt;&lt;small&gt;{{math|''f''(''x'') {{=}} (''x'' &amp;minus; 3)(''x'' &amp;minus; 2)(''x'' &amp;minus; 1)(''x'')(''x'' + 1)(''x'' + 2)}}&lt;br&gt;{{math|(''x'' + 3)}}&lt;/small&gt;
&lt;/gallery&gt;
&lt;/div&gt;
A polynomial function in one real variable can be represented by a [[graph of a function|graph]].
* The graph of the zero polynomial
::{{math|''f''(''x'') {{=}} 0}}
:is the {{math|''x''}}-axis.

* The graph of a degree 0 polynomial
::{{math|''f''(''x'') {{=}} ''a''&lt;sub&gt;0&lt;/sub&gt;}}, where {{math|''a''&lt;sub&gt;0&lt;/sub&gt; ≠ 0}},
:is a horizontal line with {{math|''y''-intercept ''a''&lt;sub&gt;0&lt;/sub&gt;}}

* The graph of a degree 1 polynomial (or linear function)
::{{math|''f''(''x'') {{=}} ''a''&lt;sub&gt;0&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt;''x''}} , where {{math|''a''&lt;sub&gt;1&lt;/sub&gt; ≠ 0}},
:is an oblique line with {{math|''y''-intercept ''a''&lt;sub&gt;0&lt;/sub&gt;}} and [[slope]] {{math|''a''&lt;sub&gt;1&lt;/sub&gt;}}.

* The graph of a degree 2 polynomial
::{{math|''f''(''x'') {{=}} ''a''&lt;sub&gt;0&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt;''x'' + ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sup&gt;2&lt;/sup&gt;}}, where {{math|''a''&lt;sub&gt;2&lt;/sub&gt; ≠ 0}}
:is a [[parabola]].

* The graph of a degree 3 polynomial
::{{math|''f''(''x'') {{=}} ''a''&lt;sub&gt;0&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt;''x'' + ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sup&gt;2&lt;/sup&gt;, + ''a''&lt;sub&gt;3&lt;/sub&gt;''x''&lt;sup&gt;3&lt;/sup&gt;}}, where {{math|''a''&lt;sub&gt;3&lt;/sub&gt; ≠ 0}}
:is a cubic curve.

* The graph of any polynomial with degree 2 or greater
::{{math|''f''(''x'') {{=}} ''a''&lt;sub&gt;0&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt;''x'' + ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sup&gt;2&lt;/sup&gt; + ... + ''a''&lt;sub&gt;''n''&lt;/sub&gt;''x''&lt;sup&gt;''n''&lt;/sup&gt;}} , where {{math|''a''&lt;sub&gt;''n''&lt;/sub&gt; ≠ 0 and ''n'' ≥ 2}}
:is a continuous non-linear curve.

The graph of a non-constant (univariate) polynomial always [[Infinity#Calculus|tends to infinity]] when the variable increases indefinitely (in [[absolute value]]).{{citation needed|date=August 2013}}

Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.

==Polynomial equations==
{{main|Algebraic equation}}
A ''polynomial equation'', also called ''[[algebraic equation]]'', is an [[equation]] of the form&lt;ref&gt;{{Cite book|author=Proskuryakov, I.V.|chapter=Algebraic equation|editor=Hazewinkel, Michiel| editor-link=Michiel Hazewinkel |title=Encyclopaedia of Mathematics|volume=vol. 1|publisher=Springer|year=1994|isbn=9781556080104|url=http://books.google.com/books?id=PE1a-EIG22kC&amp;pg=PA88}}&lt;/ref&gt;

:&lt;math&gt;a_n x^n + a_{n-1}x^{n-1} + \dotsb + a_2 x^2 + a_1 x + a_0 = 0&lt;/math&gt;

For example,

: &lt;math&gt; 3x^2 + 4x -5 = 0 \,&lt;/math&gt;

is a polynomial equation.

In case of a univariate polynomial equation, the variable is considered an [[Variable (mathematics)|unknown]], and one seeks to find the possible values for which both members of the equation evaluate to the same value (in general more than one solution may exist). A polynomial equation stands in contrast to a ''polynomial identity'' like {{math|(''x'' + ''y'')(''x'' &amp;minus; ''y'') {{=}} ''x''&lt;sup&gt;2&lt;/sup&gt; &amp;minus; ''y''&lt;sup&gt;2&lt;/sup&gt;}}, where both members represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality. This means that a polynomial identity is a polynomial equation for which all possible values of the unknowns are solutions.{{citation needed|date=August 2013}}

In elementary [[algebra]], methods such as the [[quadratic formula]] are given for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the [[cubic function|cubic]] and [[quartic equations]]. For higher degrees, [[Abel–Ruffini theorem]] asserts that there can not exist a general formula. Therefore, only [[numerical approximation]]s of the roots may be computed (see [[Root-finding algorithm]]). The number of solutions may not exceed the degree, and equals the degree when the [[complex number|complex]] solutions are counted with their [[Multiplicity (mathematics)|multiplicity]]. This fact is called the [[fundamental theorem of algebra]].

===Solving polynomial equations===&lt;!-- &quot;Simple root (polynomial)&quot; redirects here --&gt;
{{Further|Properties of polynomial roots}}

Every polynomial {{math|''P''}} in {{math|''x''}} corresponds to a function, {{math|''f''(''x'') {{=}} ''P''}} (where the occurrences of {{math|''x''}} in {{math|''P''}} are interpreted as the argument of {{math|''f''}}), called the ''polynomial function'' of {{math|''P''}}; the equation in {{math|''x''}} setting {{math|''f''(''x'') {{=}} 0}} is the ''polynomial equation'' corresponding to {{math|''P''}}. The solutions of this equation are called the ''roots'' of the polynomial; they are the ''zeroes'' of the function {{math|''f''}} (corresponding to the points where the graph of {{math|''f''}} meets the {{math|''x''}}-axis). A number {{math|''a''}} is a root of {{math|''P''}} if and only if the polynomial {{math|''x'' &amp;minus; ''a''}} (of degree one in {{math|''x''}}) divides {{math|''P''}}. It may happen that {{math|''x'' &amp;minus; ''a''}} divides {{math|''P''}} more than once: if {{math|(''x'' &amp;minus; ''a'')&lt;sup&gt;2&lt;/sup&gt;}} divides {{math|''P''}} then {{math|''a''}} is called a ''multiple root'' of {{math|''P''}}, and otherwise {{math|''a''}} is called a ''simple root'' of {{math|''P''}}. If {{math|''P''}} is a nonzero polynomial, there is a highest power {{math|''m''}} such that {{math|(''x''&amp;nbsp;−&amp;nbsp;''a'')&lt;sup&gt;''m''&lt;/sup&gt;}} divides {{math|''P''}}, which is called the ''multiplicity'' of the root {{math|''a''}} in {{math|''P''}}. When {{math|''P''}} is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots: with the above definitions every number would be a root of the zero polynomial, with undefined (or infinite) multiplicity. With this exception made, the number of roots of {{math|''P''}}, even counted with their respective multiplicities, cannot exceed the degree of {{math|''P''}}.&lt;ref&gt;{{cite book|author=Leung, Kam-tim et al|title=Polynomials and Equations|publisher=Hong Kong University Press|year=1992|isbn=9789622092716|page=134|url=http://books.google.com/books?id=v5uXkwIUbC8C&amp;pg=PA134}}&lt;/ref&gt; The relation between the roots of a polynomial and its coefficients is described by [[Viète's formulas]].

Some polynomials, such as {{math|''x''&lt;sup&gt;2&lt;/sup&gt; + 1}}, do not have any roots among the [[real number]]s. If, however, the set of allowed candidates is expanded to the [[complex number]]s, every non-constant polynomial has at least one root; this is the [[fundamental theorem of algebra]]. By successively dividing out factors {{math|''x'' &amp;minus; ''a''}}, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree&amp;nbsp;1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.

There is a difference between approximating roots and finding exact expressions for roots. Formulas for expressing the roots of polynomials of [[Degree of a polynomial|degree]] 2 in terms of square roots have been known since ancient times (see [[quadratic equation]]), and for polynomials of degree 3 or 4 similar formulas (using cube roots in addition to square roots) were found in the 16th century (see [[cubic function]] and [[quartic function]] for the formulas and [[Niccolò Fontana Tartaglia]], [[Lodovico Ferrari]], [[Gerolamo Cardano]], and [[Franciscus Vieta|Vieta]] for historical details). But formulas for degree 5 eluded researchers. In 1824, [[Niels Henrik Abel]] proved the striking result that there can be no general (finite) formula, involving only arithmetic operations and radicals, that expresses the roots of a polynomial of degree 5 or greater in terms of its coefficients (see [[Abel–Ruffini theorem]]). In 1830, [[Évariste Galois]], studying the permutations of the roots of a polynomial, extended the [[Abel–Ruffini theorem]] by showing that, given a polynomial equation, one may decide if it is solvable by radicals, and, if it is, solve it. This result marked the start of [[Galois theory]] and [[Group theory]], two important branches of modern mathematics. Galois himself noted that the computations implied by his method were impracticable. Nevertheless, formulas for solvable equations of degrees 5 and 6 have been published (see [[quintic function]] and [[sextic equation]]).

Numerical approximations of roots of polynomial equations in one unknown is easily done on a computer by the [[Jenkins–Traub method]], [[Laguerre's method]], [[Durand–Kerner method]] or by some other [[root-finding algorithm]].&lt;ref&gt;{{cite book|author=McNamee, J.M.|title=Numerical Methods for Roots of Polynomials, Part 1|publisher=Elsevier|year=2007|isbn=9780080489476|url=http://books.google.com/books?id=4PMqxwG-eqQC&amp;printsec=frontcover}}&lt;/ref&gt;

For polynomials in more than one indeterminate the notion of root does not exist, and there are usually infinitely many combinations of values for the variables for which the polynomial function takes the value zero. However for certain ''sets'' of such polynomials it may happen that for only finitely many combinations all polynomial functions take the value zero.

For a set of polynomial equations in several unknowns, there are [[algorithm]]s to decide if they have a finite number of complex solutions. If the number of solutions is finite, there are algorithms to compute the solutions. The methods underlying these algorithms are described in the article [[systems of polynomial equations]].

The special case where all the polynomials are of degree one is called a [[system of linear equations]], for which another range of different [[System of linear equations#Solving a linear system|solution methods]] exist, including the classical [[Gaussian elimination]].

==Polynomials associated to other objects==

===Calculus===
{{Main|Calculus with polynomials}}
{{See also|Polynomial interpolation}}
The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in [[calculus]] is [[Taylor's theorem]], which roughly states that every [[differentiable]] function locally looks like a polynomial function, and the [[Stone–Weierstrass theorem]], which states that every [[continuous function]] defined on a [[compact space|compact]] [[interval (mathematics)|interval]] of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.

Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function
:&lt;math&gt;\sum_{i=0}^n a_i x^i&lt;/math&gt;
the derivative with respect to ''x'' is
:&lt;math&gt;\sum_{i=1}^n a_i i x^{i-1}&lt;/math&gt;
and the indefinite integral is
:&lt;math&gt;\sum_{i=0}^n {a_i\over i+1} x^{i+1}+c.&lt;/math&gt;

=== Abstract algebra ===
{{Main|Polynomial ring}}

In [[abstract algebra]], one distinguishes between ''polynomials'' and ''polynomial functions''. A ''polynomial'' {{math|''f''}} in one indeterminate {{math|''X''}} over a [[ring (mathematics)|ring]] {{math|''R''}} is defined as a formal expression of the form
: &lt;math&gt;f = a_n X^n + a_{n - 1} X^{n - 1} + \cdots + a_1 X^1 + a_0X^0&lt;/math&gt;
where {{math|''n''}} is a natural number, the coefficients {{math|''a''&lt;sub&gt;0&lt;/sub&gt;, . . ., ''a''&lt;sub&gt;n&lt;/sub&gt;}} are elements of {{math|''R''}}, and {{math|''X''}} is a formal symbol, whose powers {{math|''X&lt;sup&gt;i&lt;/sup&gt;''}} are just placeholders for the corresponding coefficients {{math|''a&lt;sub&gt;i&lt;/sub&gt;''}}, so that the given formal expression is just a way to encode the sequence {{math|(''a''&lt;sub&gt;0&lt;/sub&gt;, ''a''&lt;sub&gt;1&lt;/sub&gt;, . . .)}}, where there is an {{math|''n''}} such that {{math|''a&lt;sub&gt;i&lt;/sub&gt;'' {{=}} 0}}  for all {{math|''i'' &gt; ''n''}}. Two polynomials sharing the same value of ''n'' are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of {{math|''n''}} obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term {{math|''a&lt;sub&gt;i&lt;/sub&gt;X&lt;sup&gt;i&lt;/sup&gt;''}} is interpreted as a polynomial that has zero coefficients at all powers of {{math|''X''}} other than {{math|''X&lt;sup&gt;i&lt;/sup&gt;''}}. Then to define multiplication, it suffices by the [[distributive law]] to describe the product of any two such terms, which is given by the rule

:&lt;div style=&quot;vertical-align:30%;display:inline&quot;&gt;&lt;math&gt;
 a X^k \; b X^l = ab X^{k+l}&lt;/math&gt;&lt;/div&gt;{{nbsp|2}} for all elements ''a'', ''b'' of the ring ''R'' and all [[natural numbers]] ''k'' and ''l''.

Thus the set of all polynomials with coefficients in the ring {{math|''R''}} forms itself a ring, the ''ring of polynomials'' over {{math|''R''}}, which is denoted by {{math|''R''[''X'']}}. The map from  {{math|''R''}} to {{math|''R''[''X'']}} sending  {{math|''R''}} to {{math|''rX''&lt;sup&gt;0&lt;/sup&gt;}} is an injective homomorphism of rings, by which  {{math|''R''}} is viewed as a subring of {{math|''R''[''X'']}}. If  {{math|''R''}} is [[commutative ring|commutative]], then {{math|''R''[''X'']}} is an [[Algebra (ring theory)|algebra]] over {{math|''R''}}.

One can think of the ring {{math|''R''[''X'']}} as arising from  {{math|''R''}} by adding one new element ''X'' to ''R'', and extending in a minimal way to a ring in which {{math|''X''}} satisfies no other relations than the obligatory ones, plus commutation with all elements of  {{math|''R''}} (that is {{math|''Xr'' {{=}} ''rX''}}). To do this, one must add all powers of {{math|''X''}} and their linear combinations as well.

Formation of the polynomial ring, together with forming factor rings by factoring out [[ideal (ring theory)|ideals]], are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring {{math|''R''[''X'']}} over the real numbers by factoring out the ideal of multiples of the polynomial {{math|''X''&lt;sup&gt;2&lt;/sup&gt; + 1}}. Another example is the construction of [[finite field]]s, which proceeds similarly, starting out with the field of integers modulo some [[prime number]] as the coefficient ring  {{math|''R''}} (see [[modular arithmetic]]).

If  {{math|''R''}} is commutative, then one can associate to every polynomial {{math|''P''}} in {{math|''R''[''X'']}}, a ''polynomial function'' {{math|''f''}} with domain and range equal to  {{math|''R''}} (more generally one can take domain and range to be the same [[unital algebra|unital]] [[associative algebra]] over {{math|''R''}}). One obtains the value {{math|''f''(''r'')}} by [[substitution (algebra)|substitution]] of the value  {{math|''R''}} for the symbol {{math|''X''}} in {{math|''P''}}. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see [[Fermat's little theorem]] for an example where  {{math|''R''}} is the integers modulo {{math|''p''}}). This is not the case when  {{math|''R''}} is the real or complex numbers, whence the two concepts are not always distinguished in [[analysis (mathematics)|analysis]]. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like [[Euclidean division]]) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for {{math|''X''}}.

====Divisibility====
{{Main|Polynomial greatest common divisor|Factorization of polynomials}}
In [[commutative algebra]], one major focus of study is ''divisibility'' among polynomials. If {{math|''R''}} is an [[integral domain]] and {{math|''f''}} and {{math|''g''}} are polynomials in {{math|''R''[''X'']}}, it is said that {{math|''f''}} ''divides'' {{math|''g''}} or {{math|''f''}} is a divisor of {{math|''g''}} if there exists a polynomial {{math|''q''}} in {{math|''R''[''X'']}} such that {{math|''f'' ''q'' {{=}} ''g''}}. One can show that every zero gives rise to a linear divisor, or more formally, if {{math|''f''}} is a polynomial in {{math|''R''[''X'']}} and {{math|''r''}} is an element of {{math|''R''}} such that {{math|''f''(''r'') {{=}} 0}}, then the polynomial ({{math|''X'' &amp;minus; ''r''}}) divides {{math|''f''}}. The converse is also true. The quotient can be computed using the [[polynomial long division]].&lt;ref&gt;{{Cite book|author=Irving, Ronald S.|title=Integers, Polynomials, and Rings: A Course in Algebra|publisher=Springer|year=2004|isbn=9780387201726|page=129|url=http://books.google.com/books?id=B4k6ltaxm5YC&amp;pg=PA129}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author=Jackson, Terrence H.|title=From Polynomials to Sums of Squares|publisher=CRC Press|year=1995|isbn=9780750303293|page=143|url=http://books.google.com/books?id=LCEOri2-doMC&amp;pg=PA143}}&lt;/ref&gt;

If {{math|''F''}} is a [[field (mathematics)|field]] and {{math|''f''}} and {{math|''g''}} are polynomials in {{math|''F''[''X'']}} with {{math|''g'' ≠ 0}}, then there exist unique polynomials {{math|''q''}} and {{math|''r''}} in {{math|''F''[''X'']}} with
:&lt;math&gt; f = q \, g + r &lt;/math&gt;
and such that the degree of {{math|''r''}} is smaller than the degree of {{math|''g''}} (using the convention that the polynomial 0 has a negative degree). The polynomials {{math|''q''}} and {{math|''r''}} are uniquely determined by {{math|''f''}} and {{math|''g''}}. This is called ''[[Euclidean division of polynomials|Euclidean division]], division with remainder'' or ''polynomial long division'' and shows that the ring {{math|''F''[''X'']}} is a [[Euclidean domain]].

Analogously, ''prime polynomials'' (more correctly, ''[[irreducible element|irreducible]] polynomials'') can be defined as ''non zero polynomials which cannot be factorized into the product of two non constant polynomials''. In the case of coefficients in a ring, ''&quot;non constant&quot;'' must be replaced by ''&quot;non constant or non [[unit (ring theory)|unit]]&quot;'' (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a [[unique factorization domain]] this decomposition is unique up to the order of the factors and the multiplication of any non unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see [[Factorization of polynomials]]). These algorithms are not practicable for hand written computation, but are available in any [[computer algebra system]]. [[Eisenstein's criterion]] can also be used in some cases to determine irreducibility.

===Other applications===
{{see also|Orthogonal polynomial|B-spline|spline interpolation}}
Polynomials serve to approximate other [[function (mathematics)|functions]],&lt;ref&gt;{{cite book|author=de Villiers, Johann |title=Mathematics of Approximation|publisher=Springer|year=2012|isbn=9789491216503|url=http://books.google.com/books?id=l5mIro_6RlUC&amp;printsec=frontcover}}&lt;/ref&gt; such as the use of [[Spline (mathematics)|splines]].

Polynomials are frequently used to encode information about some other object. The [[characteristic polynomial]] of a matrix or linear operator contains information about the operator's [[eigenvalue]]s. The [[minimal polynomial (field theory)|minimal polynomial]] of an [[algebraic element]] records the simplest algebraic relation satisfied by that element. The [[chromatic polynomial]] of a [[Graph (mathematics)|graph]] counts the number of proper colourings of that graph.

The term &quot;polynomial&quot;, as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in [[computational complexity theory]] the phrase ''[[polynomial time]]'' means that the time it takes to complete an [[algorithm]] is bounded by a polynomial function of some variable, such as the size of the input.

==Extensions of the concept of a polynomial==
Rings of polynomials in a finite number of variables are of fundamental importance in [[algebraic geometry]] which studies the simultaneous zero sets of several such multivariate polynomials. These rings can alternatively be constructed by repeating the construction of univariate polynomials with as coefficient ring another ring of polynomials: thus the ring {{math|''R''[''X'', ''Y'']}} of polynomials in {{math|''X''}} and {{math|''Y''}} can be viewed as the ring {{math|(''R''[''X''])[''Y'']}} of polynomials in {{math|''Y''}} with as coefficients polynomials in {{math|''X''}}, or as the ring
{{math|(''R''[''Y''])[''X'']}} of polynomials in ''X'' with as coefficients polynomials in {{math|''Y''}}. These identifications are compatible with arithmetic operations (they are [[isomorphism]]s of rings), but some notions such as degree or whether a polynomial is considered monic can change between these points of view. One can construct rings of polynomials in infinitely many indeterminates, but since polynomials are (finite) expressions, any individual polynomial can only contain finitely many indeterminates.{{citation needed|date=August 2013}}

A bivariate polynomial where the second variable is substituted by an exponential function applied to the first variable, for example {{math|''P''(''X'', ''e''&lt;sup&gt;''X''&lt;/sup&gt;)}}, may be called an [[exponential polynomial]].

[[Laurent polynomial]]s are like polynomials, but allow negative powers of the variable(s) to occur.

[[Quotient]]s of polynomials are called [[rational expression]]s (or rational fractions), and functions that evaluate rational expressions are called [[rational function]]s. Rational fractions are formal quotients of polynomials (they are formed from polynomials just as [[rational number]]s are formed from [[integer]]s, writing a [[Algebraic fraction|fraction]] of two of them; fractions related by the canceling of common factors are identified with each other). The rational function defined by a rational fraction is the quotient of the polynomial functions defined by the numerator and the denominator of the rational fraction. The rational fractions contain the Laurent polynomials, but do not limit denominators to powers of an indeterminate. While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not null. A rational function produces rational output for any rational input for which it is defined; this is not true of other functions such as [[trigonometric function]]s, [[logarithm]]s and [[exponential function]]s.{{citation needed|date=August 2013}}

[[Formal power series]] are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like [[real number]]s cannot), but the rules for manipulating their terms are the same as for polynomials.

==History==
Determining the roots of polynomials, or &quot;solving algebraic equations&quot;, is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese [[The Nine Chapters on the Mathematical Art|Arithmetic in Nine Sections]], circa 200 BCE, begins &quot;Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou.&quot; We would write {{math|3''x''&amp;nbsp;+&amp;nbsp;2''y''&amp;nbsp;+&amp;nbsp;''z'' {{=}}&amp;nbsp;29}}.

===History of the notation===
{{Main|History of mathematical notation}}
The earliest known use of the equal sign is in [[Robert Recorde]]'s ''[[The Whetstone of Witte]]'', 1557. The signs + for addition, &amp;minus; for subtraction, and the use of a letter for an unknown appear in [[Michael Stifel]]'s ''Arithemetica integra'', 1544. [[René Descartes]], in ''La géometrie'', 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the {{math|''a''}}'s denote constants and {{math|''x''}} denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.&lt;ref&gt;Howard Eves, ''An Introduction to the History of Mathematics, Sixth Edition, Saunders, ISBN 0-03-029558-0&lt;/ref&gt;

==See also==
*[[Lill's method]]
*[[List of polynomial topics]]
*[[Polynomials on vector spaces]]
*[[Power series]]

==Notes==
{{Reflist|colwidth=35em}}

==References==
&lt;!-- * {{cite book|author=|title=|publisher=|year=|isbn=|url=}} --&gt;
{{Refbegin}}
* {{cite book|author=Barbeau, E.J.|title=Polynomials|publisher=Springer|year=2003|isbn=9780387406275|url=http://books.google.com/books?id=CynRMm5qTmQC&amp;printsec=frontcover}}
* {{cite book|editors=Bronstein, Manuel et al|title=Solving Polynomial Equations: Foundations, Algorithms, and Applications|publisher=Springer|year=2006|isbn=9783540273578|url=http://books.google.com/books?id=aIlSmBV3yf8C&amp;printsec=frontcover}}
* {{cite book|authors=Cahen, Paul-Jean &amp; Chabert, Jean-Luc|title=Integer-Valued Polynomials|publisher=American Mathematical Society|year=1997|isbn=9780821803882|url=http://books.google.com/books?id=AlAluH5is6AC&amp;printsec=frontcover}}
* {{Lang Algebra}}. This classical book covers most of the content of this article.
* {{cite book|author=Leung, Kam-tim et al|title=Polynomials and Equations|publisher=Hong Kong University Press|year=1992|isbn=9789622092716|url=http://books.google.com/books?id=v5uXkwIUbC8C&amp;printsec=frontcover}}
* Mayr, K. Über die Auflösung algebraischer Gleichungssysteme durch hypergeometrische Funktionen. ''Monatshefte für Mathematik und Physik'' vol. 45, (1937) pp.&amp;nbsp;280–313.
* {{cite book|author=Prasolov, Victor V.|title=Polynomials|publisher=Springer|year=2005|isbn=9783642040122|url=http://books.google.com/books?id=qIJPxdwSqlcC&amp;printsec=frontcover}}
* {{cite book|author=Sethuraman, B.A.|chapter=Polynomials|title=Rings, Fields, and Vector Spaces: An Introduction to Abstract Algebra Via Geometric Constructibility|publisher=Springer|year=1997|isbn=9780387948485|url=http://books.google.com/books?id=yWnTIqmUOFgC&amp;pg=PA119}} 
* Umemura, H. Solution of algebraic equations in terms of theta constants. In D. Mumford, ''Tata Lectures on Theta II'', Progress in Mathematics 43, Birkhäuser, Boston, 1984.
* von Lindemann, F. [http://dz-srv1.sub.uni-goettingen.de/sub/digbib/loader?ht=VIEW&amp;did=D55215 Über die Auflösung der algebraischen Gleichungen durch transcendente Functionen]. Nachrichten von der Königl. Gesellschaft der Wissenschaften, vol. 7, 1884. Polynomial solutions in terms of theta functions.
* von Lindemann, F. [http://dz-srv1.sub.uni-goettingen.de/sub/digbib/loader?did=D55847 Über die Auflösung der algebraischen Gleichungen durch transcendente Functionen II]. Nachrichten von der Königl. Gesellschaft der Wissenschaften und der Georg-Augusts-Universität zu Göttingen, 1892 edition.
{{Refend}}

==External links==
{{Commons category|Polynomials}}
{{Wiktionary|polynomial}}
*{{springer|title=Polynomial|id=p/p073690}}
*[http://mathdl.maa.org/convergence/1/?pa=content&amp;sa=viewDocument&amp;nodeId=640&amp;bodyId=1038 Euler's work on Imaginary Roots of Polynomials]
*{{MathWorld |title=Polynomial |id=Polynomial}}

{{Polynomials}}

[[Category:Polynomials| ]]
[[Category:Algebra]]

{{Link GA|yo}}</text>
      <sha1>i5jqh1ukqmj344dllu61x5qsrooffyc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Order of operations</title>
    <ns>0</ns>
    <id>212980</id>
    <revision>
      <id>612494965</id>
      <parentid>612493887</parentid>
      <timestamp>2014-06-11T13:17:42Z</timestamp>
      <contributor>
        <username>Joel B. Lewis</username>
        <id>13974845</id>
      </contributor>
      <comment>Undo 2 changes</comment>
      <text xml:space="preserve" bytes="18665">In [[mathematics]] and [[computer programming]], the '''order of operations''' (sometimes called '''operator precedence''') is a rule used to clarify which procedures should be performed first in a given [[Expression (mathematics)|mathematical expression]].

For example, in mathematics and most computer languages multiplication is done before addition; in the expression 2 + 3 × 4, the answer is 14. Brackets, &quot;( and ), { and }, or [ and ]&quot;, which have their own rules, may be used to avoid confusion, thus the preceding expression may also be rendered 2 + (3 × 4), but the brackets are unnecessary as multiplication still has precedence without them.

Since the introduction of modern algebraic notation, multiplication has taken precedence over addition.&lt;ref&gt;{{Cite web|url=http://mathforum.org/library/drmath/view/52582.html |title=Ask Dr. Math |publisher=Math Forum |date=22 November 2000 |accessdate=5 March 2012}}&lt;/ref&gt;  Thus 3&amp;nbsp;+&amp;nbsp;4&amp;nbsp;&amp;times;&amp;nbsp;5 =&amp;nbsp;4&amp;nbsp;&amp;times;&amp;nbsp;5&amp;nbsp;+&amp;nbsp;3 = 23. When exponents were first introduced in the 16th and 17th centuries, exponents took precedence over both addition and multiplication and could be placed only as a superscript to the right of their base. Thus 3&amp;nbsp;+&amp;nbsp;5&lt;sup&gt;2&lt;/sup&gt; = 28 and 3&amp;nbsp;&amp;times;&amp;nbsp;5&lt;sup&gt;2&lt;/sup&gt; =&amp;nbsp;75. To change the order of operations, originally a [[vinculum (symbol)|vinculum]] (an overline or underline) was used. Today, [[bracket (mathematics)|parentheses or brackets]] are used to explicitly denote precedence by grouping parts of an expression that should be evaluated first. Thus, to force addition to precede multiplication, we write (2&amp;nbsp;+&amp;nbsp;3)&amp;nbsp;&amp;times;&amp;nbsp;4 =&amp;nbsp;20, and to force addition to precede exponentiation, we write (3&amp;nbsp;+&amp;nbsp;5)&lt;sup&gt;2&lt;/sup&gt; = 64.
&lt;!-- It is tempting to talk about PEMDAS or BEDMAS or &quot;Please excuse my dear Aunt Sally&quot; here, but these mnemonics are covered below and there are more choices of them than easily fit in an intro. You should at least read the discussion (linked above the article head) before adding a sentence about the mnemonics in the intro. --&gt;

==The standard order of operations==

The order of operations used throughout mathematics, science, technology and many computer [[programming language]]s is expressed here:&lt;ref&gt;{{Cite web|url=http://www.algebrahelp.com/lessons/simplifying/oops/ |title=Order of Operations Lessons |publisher=Algebra.Help |date= |accessdate=5 March 2012}}&lt;/ref&gt;

: '''[[Exponentiation|exponents]]''' and '''[[Nth root|roots]]'''
: '''[[multiplication]]''' and  '''[[Division (mathematics)|division]]'''
: '''[[addition]]''' and '''[[subtraction]]'''

This means that if a mathematical expression is preceded by one binary operator and followed by another, the operator higher on the list should be applied first. The [[Commutative property|commutative]] and [[Associative property|associative]] laws of addition and multiplication allow terms to be added in any order and factors to be multiplied in any order, but mixed operations must obey the standard order of operations.

It is helpful to treat division as multiplication by the reciprocal (multiplicative inverse) and subtraction as addition of the negation (additive inverse). Thus 3/4 =&amp;nbsp;3&amp;nbsp;÷&amp;nbsp;4 = 3&amp;nbsp;•&amp;nbsp;¼; in other words the quotient of 3 and 4 equals the product of 3 and &amp;nbsp;¼. Also 3&amp;nbsp;−&amp;nbsp;4 = 3&amp;nbsp;+&amp;nbsp;(−4); in other words the difference of 3 and 4 equals the sum of positive three and negative four. With this understanding, we can think of 1 − 3 + 7 as the sum of 1, negative 3, and 7, and add in any order: (1 − 3) + 7 = −2 + 7 = 5 and in reverse order (7 − 3) + 1 = 4 + 1 = 5. The important thing is to keep the negative sign with the 3.

The root symbol, √, requires a symbol of grouping around the radicand. The usual symbol of grouping is a bar (called [[vinculum (symbol)|vinculum]]) over the radicand. Other functions use parentheses around the input to avoid ambiguity.  The parentheses are sometimes omitted if the input is a [[monomial]].  Thus, sin x = sin(x), but sin x + y = sin(x) + y, because x + y is not a monomial. Some calculators and programming languages require parentheses around function inputs, some do not.

Stacked exponents are applied from the top down, i.e., from right to left.

Symbols of grouping can be used to override the usual order of operations. Grouped symbols can be treated as a single expression. Symbols of grouping can be removed using the associative and distributive laws, also they can be removed if the expression inside the symbol of grouping is sufficiently simplified so no ambiguity results from their removal.

===Examples===

: &lt;math&gt;\sqrt{1+3}+5=\sqrt4+5=2+5=7.\,&lt;/math&gt;

A horizontal fractional line also acts as a symbol of grouping:

: &lt;math&gt;\frac{1+2}{3+4}+5=\frac37+5.&lt;/math&gt;

For ease in reading, other grouping symbols such as braces, sometimes called curly braces { }, or brackets, sometimes called square brackets [ ], are often used along with parentheses ( ). For example,

: &lt;math&gt;[(1+2)-3]-(4-5) = [3-3]-(-1) = 1. \, &lt;/math&gt;

===Exceptions to the standard===

There exist differing conventions concerning the unary operator − (usually read &quot;minus&quot;). In written or printed mathematics, the expression &amp;minus;3&lt;sup&gt;2&lt;/sup&gt; is interpreted to mean &amp;minus;(3&lt;sup&gt;2&lt;/sup&gt;)&amp;nbsp;=&amp;nbsp;&amp;minus;9,&lt;ref&gt;Allen R. Angel, Elementary Algebra for College Students 8/E; Chapter 1, Section 9, Objective 3&lt;/ref&gt;  but in some applications and programming languages, notably the [[BASIC_language| BASIC programming language]], the application [[Microsoft Office Excel]] and [[bc programming language|the programming language bc]], unary operators have a higher priority than binary operators, that is, the unary minus (negation) has higher precedence than exponentiation, so in those languages &amp;minus;3&lt;sup&gt;2&lt;/sup&gt; will be interpreted as (&amp;minus;3)&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;9.&lt;ref&gt;{{Cite web|url=http://support.microsoft.com/kb/q132686/ |title=Formula Returns Unexpected Positive Value |publisher=Support.microsoft.com |date=15 August 2005 |accessdate=5 March 2012}}&lt;/ref&gt; Note this does not apply to the binary operator −; for example while the formulas &lt;code&gt;=-2^2&lt;/code&gt; and &lt;code&gt;=0+-2^2&lt;/code&gt; return 4 in Microsoft Excel, the formula &lt;code&gt;=0-2^2&lt;/code&gt; returns −4. In cases where there is the possibility that the notation might be misinterpreted, parentheses are usually used to clarify the intended meaning, however due to the syntax of most major programming languages, it is usually hard or impossible to be ambiguous.

Similarly, there can be ambiguity in the use of the [[Slash (punctuation)#Arithmetic|slash]] ('/') symbol in expressions such as 1/2''x''. If one rewrites this expression as 1 ÷ 2 &amp;times; ''x'' and then interprets the division symbol as indicating multiplication by the reciprocal, this becomes
: &lt;math&gt;1 \div 2 \times x = 1 \times \tfrac{1}{2} \times x = \tfrac{1}{2}x.&lt;/math&gt;
Hence, with this interpretation we have that 1/2''x'' is equal to (1/2)''x'', and not 1/(2''x''). However, there are examples, including in published literature, where implied multiplication is interpreted as having higher precedence than division, so that 1/2''x'' equals 1/(2''x''), not (1/2)''x''. For example, the manuscript submission instructions for the ''[[Physical Review]]'' journals state that multiplication is of higher precedence than division with a slash,&lt;ref&gt;{{Cite web|url=https://publish.aps.org/files/styleguide-pr.pdf |title=Physical Review Style and Notation Guide |publisher=[[American Physical Society]]|accessdate=5 August 2012|at=Section IV&amp;ndash;E&amp;ndash;2&amp;ndash;e}}&lt;/ref&gt; and this is also the convention observed in prominent physics textbooks such as the ''[[Course of Theoretical Physics]]'' by [[Lev Landau|Landau]] and [[Evgeny Lifshitz|Lifshitz]] and the ''[[Feynman Lectures on Physics]]''.&lt;ref&gt;For example, the third edition of ''Mechanics'' by Landau and Lifshitz contains expressions such as ''hP&lt;sub&gt;z&lt;/sub&gt;''/2π (p. 22), and the first volume of the ''Feynman Lectures'' contains expressions such as 1/2{{sqrt|''N''}} (p. 6&amp;ndash;8). In both books these expressions are written with the convention that the solidus is evaluated last.&lt;/ref&gt; [[Wolfram Alpha]] changed in early 2013 to treat implied multiplication the same as explicit multiplication (formerly, implied multiplication without parentheses was assumed to bind stronger than explicit multiplication).  2''x''/2''x'', 2*''x''/2*''x'', and 2(x)/2(x) now all yield x&lt;sup&gt;2&lt;/sup&gt;.&lt;ref&gt;{{Cite web|url=http://www.wolframalpha.com/input/?i=2x%2F2x%2C+2*x%2F2*x%2C+2%28x%29%2F2%28x%29+|title=2x&amp;#x2f;2x, 2&amp;#x2a;x&amp;#x2f;2&amp;#x2a;x, 2&amp;#x28;x&amp;#x29;&amp;#x2f;2&amp;#x28;x&amp;#x29;  - Wolfram&amp;#124;Alpha|publisher=Wolframalpha.com |date= |accessdate=11 February 2013}}&lt;/ref&gt;  The TI 89 and TI 86 calculators also yield ''x''&lt;sup&gt;2&lt;/sup&gt; in all three cases.

==Mnemonics== &lt;!-- Article [[BEDMAS]] redirects to this section. Please use the anchor template if the section name changes --&gt;
{{unreferenced section|date=June 2013}}

[[Mnemonics]] are often used to help students remember the rules, but the rules taught by the use of acronyms can be misleading. In the United States the acronym ''PEMDAS'' is common. It stands for ''P''arentheses, ''E''xponents, ''M''ultiplication, ''D''ivision, ''A''ddition, ''S''ubtraction. PEMDAS is often expanded to &quot;Please Excuse My Dear Aunt Sally&quot; with the first letter of each word creating the acronym PEMDAS. Canada uses ''BEDMAS''. It stands for ''B''rackets, ''E''xponents, ''D''ivision, ''M''ultiplication, ''A''ddition, ''S''ubtraction. The UK and Australia&lt;ref&gt;http://syllabus.bos.nsw.edu.au/assets/global/files/maths_s3_sampleu1.doc&lt;/ref&gt; use ''BODMAS'' or ''BIDMAS''.  
In Canada and other English speaking countries, ''P''arentheses may be called ''B''rackets, or symbols of inclusion and ''E''xponents may be called either ''I''ndices, ''P''owers or ''O''rders, which have the same precedence as ''R''oots or ''R''adicals. Since multiplication and division are of equal precedence, ''M'' and ''D'' are often interchanged, leading to such acronyms as ''BOMDAS''. The original order of operations in most countries was ''BODMAS'' which stood for ''B''rackets, ''O''rders, ''D''ivision, ''M''ultiplication, ''A''ddition, ''S''ubtraction. This mnemonic was used until exponentials were added into the mnemonic.

These mnemonics may be misleading when written this way, especially if the user is not aware that multiplication and division are of equal precedence, as are addition and subtraction. Using any of the above rules in the order &quot;addition first, subtraction afterward&quot; would also give the wrong answer to the problem

::&lt;math&gt;10 - 3 + 2 \,&lt;/math&gt;.

The correct answer is 9 (and not 5, which we get when we do the addition first and then the subtraction).  The best way to understand a combination of addition and subtraction is to think of the subtraction as addition of a negative number.  In this case, we see the problem as the sum of positive ten, negative three, and positive two.

::&lt;math&gt;10 + (-3) + 2 \,&lt;/math&gt;

To emphasize that addition and subtraction have the same precedence (and multiplication and division have the same precedence) the mnemonic is sometimes written P E MD AS; or, simply as PEMA.

All of these acronyms conflate two different ideas, operations on the one hand and symbols of grouping on the other, which can lead to confusion.

==Special cases==
If exponentiation is indicated by stacked symbols, the usual rule is to work from the top down, thus

: &lt;math&gt; a^{b^c} = a^{(b^c)} &lt;/math&gt;,

which typically is not equal to &lt;math&gt;(a^b)^c&lt;/math&gt;.  However, some computer systems may resolve the ambiguous expression differently.  For example, [[Microsoft Office Excel]] evaluates ''a''^''b''^''c'' as  (''a''^''b'')^''c'' which is opposite of normally accepted convention of top-down order of execution for exponentiation.  If a=4, p=3, and q=2, &lt;math&gt;a^{p^q}&lt;/math&gt; is evaluated to be 4096 in Microsoft Excel 2013, the same as &lt;math&gt;(a^p)^q&lt;/math&gt;.  The expression &lt;math&gt; a^{(p^q)} &lt;/math&gt;, on the other hand, results in 262144 using the same program.

==Calculators==

{{Main|Calculator input methods}}

Different calculators follow different orders of operations. Most non-scientific calculators without a stack work left to right without any priority given to different operators, for example giving
:&lt;math&gt;1 + 2 \times 3 = 9, \;&lt;/math&gt;
while more sophisticated calculators will use a more standard priority, for example giving
:&lt;math&gt;1 + 2 \times 3 = 7. \;&lt;/math&gt;
The Microsoft ''[[Calculator (Windows)|Calculator]]'' program uses the former in its standard view and the latter in its scientific and programmer views.

The non-scientific calculator expects two operands and an operator. When the next operator is pressed, the expression is immediately evaluated and the answer becomes the left hand of the next operator. Advanced calculators allow entry of the whole expression, grouped as necessary, and evaluates only when the user uses the equals sign.

Calculators may associate exponents to the left or to the right depending on the model. For example, the expression a ^ b ^ c on the TI-92, the TI-30XII and the TI-30XS MultiView (all Texas Instruments calculators) associate two different ways:

The TI-92 and the TI-30XS MultiView in &quot;MathPrint Mode&quot; associate to the right, that is
::a ^ b ^ c = a ^ (b ^ c) = &lt;math&gt; a^{(b^c)} = a^{b^c}&lt;/math&gt;

whereas, the TI-30XII and the TI-30XS MultiView in &quot;Classic Mode&quot;  associate to the left, that is

::a ^ b ^ c = (a ^ b) ^ c = &lt;math&gt; (a^b)^c.&lt;/math&gt;

An expression like 1/2''x'' is interpreted as 1/(2''x'') by TI-82, but as (1/2)''x'' by TI-83.&lt;ref&gt;{{cite web
  | title = Implied Multiplication Versus Explicit Multiplication on TI Graphing Calculators
  | publisher = Texas Instruments Incorporated
  | date = 16 January 2011
  | url = http://epsstore.ti.com/OA_HTML/csksxvm.jsp?nSetId=103110
  | accessdate = 29 April 2011}}{{citation not found}}&lt;/ref&gt;
&lt;ref&gt;{{cite web
  | title = Google cache for: Implied Multiplication Versus Explicit Multiplication on TI Graphing Calculators
  | publisher = Texas Instruments Incorporated
  | date = 23 Apr 2013
  | url = http://archive.is/20130204120803/http://epsstore.ti.com/OA_HTML/csksxvm.jsp?nSetId=103110
  | accessdate = 10 May 2013}}&lt;/ref&gt; While the first interpretation may be expected by some users, only the latter is in agreement with the standard rule that multiplication and division are of equal precedence,{{citation needed|date=May 2013}} so 1/2''x'' is read one divided by two and the answer multiplied by ''x''.

When the user is unsure how a calculator will interpret an expression, it is a good idea to use parentheses so there is no ambiguity.

Calculators that utilize reverse Polish notation, also known as '''postfix notation,''' use [[Stack (data structure)|stack]] to enter formulas without the need for parentheses.

==Programming languages==

Many [[programming languages]] use precedence levels that conform to the order commonly used in mathematics, though some, such as [[APL (programming language)|APL]] and [[Smalltalk]], have no [[Operator (programming)|operator]] precedence rules (in APL, evaluation is strictly right to left; in [[Smalltalk]], it's strictly left to right).

The [[logical bitwise operator]]s in [[C (programming language)|C]] (and all programming languages that borrowed precedence rules from C, for example, [[C++]], [[Perl]] and [[PHP]]) have a precedence level that the creator of the C language considered to be unsatisfactory.&lt;ref&gt;[[Dennis M. Ritchie]]: [http://cm.bell-labs.com/who/dmr/chist.html The Development of the C Language]. In History of Programming Languages, 2nd ed., ACM Press 1996.&lt;/ref&gt; However, many programmers have become accustomed to this order. The relative precedence levels of [[Operator (programming)|operators]] found in many C-style languages are as follows:

{| class=&quot;wikitable&quot;
|1 || () &amp;nbsp; [] &amp;nbsp; -&gt; &amp;nbsp; . &amp;nbsp; :: || Grouping, scope, array/member access
|-
|2 || ! &amp;nbsp; ~ &amp;nbsp; - &amp;nbsp; + &amp;nbsp; * &amp;nbsp; &amp; &amp;nbsp; sizeof &amp;nbsp; ''type cast'' ++x &amp;nbsp; --x &amp;nbsp; || (most) unary operations, sizeof and type casts
|-
|3 || * &amp;nbsp; / &amp;nbsp; % || Multiplication, division, [[modular arithmetic|modulo]]
|-
|4 || + &amp;nbsp; - || Addition and subtraction
|-
|5 || &lt;&lt; &amp;nbsp; &gt;&gt; || Bitwise shift left and right
|-
|6 || &lt; &amp;nbsp;  &lt;= &amp;nbsp;  &gt; &amp;nbsp; &gt;= || Comparisons: less-than, ...
|-
|7 || ==  &amp;nbsp; != || Comparisons: equal and not equal
|-
|8 || &amp; || Bitwise AND
|-
|9 || ^ || Bitwise exclusive OR
|-
|10 || &lt;nowiki&gt;|&lt;/nowiki&gt; || Bitwise inclusive (normal) OR
|-
|11 || &amp;&amp; || Logical AND
|-
|12 || &lt;nowiki&gt;||&lt;/nowiki&gt; || Logical OR
|-
|13 || ?: &amp;nbsp;  = &amp;nbsp;  += &amp;nbsp; -= &amp;nbsp; *= &amp;nbsp; /= &amp;nbsp; %= &amp;nbsp; &amp;= &amp;nbsp; &lt;nowiki&gt;|=&lt;/nowiki&gt; &amp;nbsp; ^=  &amp;nbsp; &lt;&lt;= &amp;nbsp; &gt;&gt;=  || Conditional expression (ternary) and assignment operators
|-
|14 ||, || [[Comma operator]]
|}

Examples:
* &lt;code&gt;!A + !B&lt;/code&gt; ≡ &lt;code&gt;(!A) + (!B)&lt;/code&gt;
* &lt;code&gt;++A + !B&lt;/code&gt; ≡ &lt;code&gt;(++A) + (!B)&lt;/code&gt;
* &lt;code&gt;A + B * C&lt;/code&gt; ≡ &lt;code&gt;A + (B * C)&lt;/code&gt;
* &lt;code&gt;A || B &amp;&amp; C&lt;/code&gt; ≡ &lt;code&gt;A || (B &amp;&amp; C)&lt;/code&gt;
* &lt;code&gt;(A &amp;&amp; B == C)&lt;/code&gt; ≡ &lt;code&gt;(A &amp;&amp; (B == C) )&lt;/code&gt;

[[Source-to-source compiler]]s that compile to multiple languages need to explicitly deal with the issue of different order of operations across languages. [[Haxe]] for example standardizes the order and enforces it by inserting brackets where it is appropriate.&lt;ref&gt;[http://blog.onthewings.net/2011/05/02/six-divided-by-two-bracket-one-plus-two/ 6÷2(1+2)=?] Andy Li's Blog. 2 May 2011. Retrieved 31 December 2012.&lt;/ref&gt;

The accuracy of software developer knowledge about binary operator precedence has been found to closely follow their frequency of occurrence in source code.&lt;ref&gt;&quot;[http://www.knosof.co.uk/cbook/accu06.html Developer beliefs about binary operator precedence]&quot; Derek M. Jones, CVu 18(4):14&amp;ndash;21&lt;/ref&gt;

==See also==
* [[Common operator notation]] (for a more formal description)
* [[Operator associativity]]
* [[Associativity]]
* [[Commutativity]]
* [[Distributivity]]
* [[Operator (programming)]]
* [[Operator overloading]]
* [[C operator precedence|Operator precedence in C and C++]]
* [[Reverse Polish notation]]
* [[Hyperoperation]]

==References==
&lt;references/&gt;

==External links==
*{{planetmath reference|id=3951|title=Order of operations}}
{{Use dmy dates|date=June 2011}}

{{DEFAULTSORT:Order Of Operations}}
[[Category:Abstract algebra]]
[[Category:Algebra]]
[[Category:Mnemonics]]
[[Category:Operators (programming)]]</text>
      <sha1>nik0ft6mre74tnnlsoz76004o50j7jj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Suspension of a ring</title>
    <ns>0</ns>
    <id>41284472</id>
    <revision>
      <id>585460939</id>
      <parentid>585129404</parentid>
      <timestamp>2013-12-10T17:46:55Z</timestamp>
      <contributor>
        <username>Malcolma</username>
        <id>320496</id>
      </contributor>
      <comment>added [[Category:Algebra]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="754">In algebra, more specifically in [[algebraic K-theory]], the '''suspension &lt;math&gt;\Sigma R&lt;/math&gt; of a ring ''R''''' is given by&lt;ref&gt;Weibel, III, Ex. 1.15&lt;/ref&gt; &lt;math&gt;\Sigma(R) = C(R)/M(R)&lt;/math&gt; where &lt;math&gt;C(R)&lt;/math&gt; is the ring of all infinite matrices with coefficients in ''R'' having only finitely many nonzero elements in each row or column and &lt;math&gt;M(R)&lt;/math&gt; is its ideal of matrices having only finitely many nonzero elements. It is an analog of [[suspension (topology)|suspension]] in topology.

One then has: &lt;math&gt;K_i(R) \simeq K_{i+1}(\Sigma R)&lt;/math&gt;.

== References ==
{{reflist}}
* C. Weibel &quot;[http://www.math.rutgers.edu/~weibel/Kbook.html The K-book: An introduction to algebraic K-theory]&quot;



[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>lzs6x81npaofc655smwvkasql68xero</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Q-construction</title>
    <ns>0</ns>
    <id>41403415</id>
    <revision>
      <id>589510971</id>
      <parentid>588208028</parentid>
      <timestamp>2014-01-06T23:04:17Z</timestamp>
      <contributor>
        <username>Biscuittin</username>
        <id>3955768</id>
      </contributor>
      <comment>add cat</comment>
      <text xml:space="preserve" bytes="9558">In algebra, [[Daniel Quillen|Quillen]]'s '''Q-construction''' associates to an [[exact category]] (e.g., an [[abelian category]]) an [[algebraic K-theory]]. More precisely, given an exact category ''C'', the construction creates a [[topological space]] &lt;math&gt;B^+C&lt;/math&gt; so that &lt;math&gt;\pi_0 (B^+C)&lt;/math&gt; is the [[Grothendieck group]] of ''C'' and, when ''C'' is the category of finitely generated projective modules over a ring ''R'', for &lt;math&gt;i = 0, 1, 2&lt;/math&gt;, &lt;math&gt;\pi_i (B^+C)&lt;/math&gt; is the ''i''-th K-group of ''R'' in the classical sense. (The notation &quot;+&quot; is because it actually provides a model for Quillen's +-construction.) One puts
:&lt;math&gt;K_i(C) = \pi_i(B^+C)&lt;/math&gt;
and call it the ''i''-th K-group of ''C''. Similarly, the ''i''-th K-group of ''C'' with coefficients in a group ''G'' is defined as the [[homotopy group with coefficients]]:
:&lt;math&gt;K_i(C; G) = \pi_i(B^+ C; G)&lt;/math&gt;.

The construction is widely applicable and is used to define an [[algebraic K-theory]] in a non-classical context. For example, one can define [[equivariant K-theory]] as &lt;math&gt;\pi_*&lt;/math&gt; of &lt;math&gt;B^+&lt;/math&gt; of the category of [[equivariant sheaf|equivariant sheaves]] on a scheme.

[[Friedhelm Waldhausen|Waldhausen]]'s [[S-construction]] generalizes the Q-construction; in fact, the former, which uses a more general [[Waldhausen category]], produces a [[spectrum (topology)|spectrum]] instead of a space. [[Grayson's binary complex]] also gives a construction of algebraic K-theory for exact categories.&lt;ref&gt;Daniel R. Grayson, [http://www.math.uiuc.edu/K-theory/0988/ Algebraic K-theory via binary complexes]&lt;/ref&gt;

Every ring homomorphism &lt;math&gt;R \to S&lt;/math&gt; induces &lt;math&gt;B^+P(R) \to B^+P(S)&lt;/math&gt; and thus &lt;math&gt;K_i(P(R)) = K_i(R) \to K_i(S)&lt;/math&gt; where &lt;math&gt;P(R)&lt;/math&gt; is the category of finitely generated projective modules over ''R''. One can easily show this map (called transfer) agrees with one defined in Milnor's ''Introduction to algebraic K-theory''.&lt;ref&gt;{{harvnb|V. Srinivas|1996|loc=The end of Ch. 7.}}&lt;/ref&gt; The construction is also compatible with the [[suspension of a ring]] (cf. Grayson).

== Details ==
Let ''C'' be an exact category; i.e., an additive full subcategory of an abelian category that is closed under extension. If there is an exact sequence &lt;math&gt;0 \to M' \to M \to M'' \to 0&lt;/math&gt; in ''C'', then the arrow from ''M''' is called an admissible mono and the arrow from ''M'' is called an admissible epi.

Let ''QC'' be the category whose objects are the same as those of ''C'' and morphisms from ''X'' to ''Y'' are isomorphism classes of diagrams &lt;math&gt;X \leftarrow Z \to Y&lt;/math&gt; such that the first arrow is an admissible epi and the second admissible mono and two diagrams are isomorphic if they differ only at the middle and there is an isomorphism between them. The composition of morphisms is given by pullback.

Define a topological space &lt;math&gt;B^+ C&lt;/math&gt; by &lt;math&gt;B^+C = \Omega B QC&lt;/math&gt; where &lt;math&gt;\Omega&lt;/math&gt; is a [[loop space functor]] and &lt;math&gt;B QC&lt;/math&gt; is the [[classifying space of a category|classifying space]] of the category ''QC'' (geometric realization of the nerve). As it turns out, it is uniquely defined up to homotopy equivalence (so the notation is justified.)

A theorem of Quillen states that, when ''C'' is the category of finitely generated projective modules over a ring ''R'', &lt;math&gt;\pi_i(B^+C)&lt;/math&gt; is the ''i''-th K-group of ''R'' in the classical sense for &lt;math&gt;i = 0, 1, 2&lt;/math&gt;. The usual proof of the theorem (cf. Weibel) relies on an intermediate homotopy equivalence. If ''S'' is a symmetric monoidal category in which every morphism is an isomorphism, one constructs (cf. Grayson) the category &lt;math&gt;S^{-1} S&lt;/math&gt; that generalizes the Grothendieck group construction of a monoid. Let ''C'' be an exact category in which every exact sequence splits; e.g., the category of finitely generated projective modules, and put &lt;math&gt;S = \operatorname{iso} C&lt;/math&gt;, the subcategory of ''C'' with the same class of objects but with morphisms that are isomorphisms in ''C''. Then there is a &quot;natural&quot; homotopy equivalence:&lt;ref&gt;{{harvnb|Weilbel|2013|loc=Ch. IV. Theorem 7.1}}&lt;/ref&gt;
:&lt;math&gt;\Omega BQC \simeq B(S^{-1} S)&lt;/math&gt;.
The equivalence is constructed as follows. Let ''E'' be the category whose objects are short exact sequences in ''C'' and whose morphisms are isomorphism classes of diagrams between them. Let &lt;math&gt;f: E \to QC&lt;/math&gt; be the functor that sends an short exact sequence to the third term in the sequence. Note the fiber &lt;math&gt;f^{-1}(X)&lt;/math&gt;, which is a subcategory, consists of exact sequences whose third term is ''X''. This makes ''E'' a [[fibered category|category fibered over]] ''QC''. Writing &lt;math&gt;S^{-1} f&lt;/math&gt; for &lt;math&gt;S^{-1} E \to QC&lt;/math&gt;, there is an obvious (hence natural) inclusion &lt;math&gt;\Omega BQC&lt;/math&gt; into the [[homotopy fiber]] &lt;math&gt;F (BS^{-1} f)&lt;/math&gt;, which can be shown to be a homotopy equivalence. On the other hand, by [[Quillen's Theorem B]], one can show that &lt;math&gt;B(S^{-1}S)&lt;/math&gt; is the [[homotopy pullback]] of &lt;math&gt;BS^{-1} f&lt;/math&gt; along &lt;math&gt;* \to BQC&lt;/math&gt; and thus is homotopy equivalent to the &lt;math&gt;F (BS^{-1} f)&lt;/math&gt;.

We now take ''C'' to be the category of finitely generated projective modules over a ring ''R'' and shows that &lt;math&gt;\pi_i B(S^{-1} S)&lt;/math&gt; are the &lt;math&gt;K_i&lt;/math&gt; of ''R'' in the classical sense for &lt;math&gt;i = 0, 1, 2&lt;/math&gt;. First of all, by definition, &lt;math&gt;\pi_0 B(S^{-1} S) = K_0(R)&lt;/math&gt;. Next, &lt;math&gt;GL_n(R) = \operatorname{Aut}(R^n) \to S^{-1}S&lt;/math&gt; gives us:
:&lt;math&gt;BGL(R) = \varinjlim BGL_n(R) \to B(S^{-1}S)&lt;/math&gt;.
(Here, &lt;math&gt;BGL(R)&lt;/math&gt; is either the classifying space of the category &lt;math&gt;GL(R)&lt;/math&gt; or the [[Eilenberg–MacLane space]] of the type &lt;math&gt;K(GL(R), 1)&lt;/math&gt;, amounting to the same thing.) The image actually lies in the identity component of &lt;math&gt;B(S^{-1}S)&lt;/math&gt; and so we get:
:&lt;math&gt;f: BGL(R) \to B(S^{-1}S)^0.&lt;/math&gt;
Let &lt;math&gt;S_n&lt;/math&gt; be the full subcategory of ''S'' consisting of modules isomorphic to &lt;math&gt;R^n&lt;/math&gt; (thus, &lt;math&gt;BS_n&lt;/math&gt; is the connected component containing &lt;math&gt;R^n&lt;/math&gt;). Let &lt;math&gt;e \in \pi_0(BS)&lt;/math&gt; be the component containing ''R''. Then, by a theorem of Quillen,
:&lt;math&gt;H_p(B(S^{-1}S)^0) \subset H_p(B(S^{-1}S)) = H_p(BS)[\pi_0(BS)^{-1}] = H_p(BS)[e^{-1}].&lt;/math&gt;
Thus, a class on the left is of the form &lt;math&gt;x e^{-n}&lt;/math&gt;. But &lt;math&gt;x \mapsto x e^m&lt;/math&gt; is induced by the action of &lt;math&gt;R^m \in S&lt;/math&gt;. Hence,
:&lt;math&gt;H_p(B(S^{-1}S)^0) = \varinjlim H_p(BS_n) = \varinjlim H_p(BGL_n(R)) = H_p(BGL(R)), \quad p \ge 0&lt;/math&gt;.
Since &lt;math&gt;B(S^{-1}S)^0&lt;/math&gt; is an ''H''-group,
:&lt;math&gt;\pi_1(B(S^{-1}S)^0) = \pi_1(B(S^{-1}S)^0)^\text{ab} = H_1(B(S^{-1}S)^0) = H_1(BGL(R)) = H_1(GL(R)) = GL(R)^{\text{ab}} = K_1(R).&lt;/math&gt;
It remains to see &lt;math&gt;\pi_2&lt;/math&gt; is &lt;math&gt;K_2&lt;/math&gt;. Writing &lt;math&gt;Ff&lt;/math&gt; for the homotopy fiber, we have the long exact sequence:
:&lt;math&gt;\pi_2(BGL(R)) = 0 \to \pi_2(B(S^{-1}S)^0) \to \pi_1 (Ff) \to \pi_1(BGL(R)) = GL(R) \to K_1(R)&lt;/math&gt;.
From homotopy theory, we know the second term is central; i.e., &lt;math&gt;\pi_1(Ff) \to E(R)&lt;/math&gt; is a [[central extension (mathematics)|central extension]]. It then follows from the next lemma that &lt;math&gt;\pi_1(Ff)&lt;/math&gt; is the [[universal central extension]] (i.e., &lt;math&gt;\pi_1(Ff)&lt;/math&gt; is the [[Steinberg group]] of ''R'' and the kernel is &lt;math&gt;K_2(R)&lt;/math&gt;.)

{{math_theorem|name=Lemma|Let &lt;math&gt;f: X \to Y&lt;/math&gt; be a continuous map between connected CW-complexes. If &lt;math&gt;f_*: H_*(X, L) \to H_*(Y, f^*L)&lt;/math&gt; is an isomorphism for any [[local coefficient system]] ''L'' on ''X'', then
:&lt;math&gt;H_1(\pi_1(Ff), \mathbb{Z}) = H_2(\pi_1(Ff), \mathbb{Z}) = 0.&lt;/math&gt;}}

Proof: The homotopy type of &lt;math&gt;Ff&lt;/math&gt; does not change if we replace ''f'' by the pullback &lt;math&gt;\widetilde{f}&lt;/math&gt; along the universal covering of ''Y'' &lt;math&gt;\to Y&lt;/math&gt;. Thus, we can replace the hypothesis by one that ''Y'' is simply connected and &lt;math&gt;H_p(X, \mathbb{Z}) \simeq H_p(Y, \mathbb{Z}), p \ge 0&lt;/math&gt;. Now, [[Serre spectral sequence]]s for &lt;math&gt;Ff \to X \to Y&lt;/math&gt; and &lt;math&gt;* \to Y \to Y&lt;/math&gt; say:
:&lt;math&gt;{}^2 E_{pq} = H_p(Y, H_q(Ff, \mathbb{Z})) \Rightarrow H_{p+q}(X, \mathbb{Z}),&lt;/math&gt;
:&lt;math&gt;{}^2 E'_{pq} = H_p(Y, H_q(*, \mathbb{Z})) \Rightarrow H_{p+q}(Y, \mathbb{Z}).&lt;/math&gt;
By the [[comparison theorem for spectral sequences]], it follows that &lt;math&gt;{}^2 E_{0q} = {}^2 E'_{0q}&lt;/math&gt;; i.e., &lt;math&gt;Ff&lt;/math&gt; is [[acyclic space|acyclic]]. (Coincidentally, by reversing argument, one can say this implies &lt;math&gt;H_p(X, \mathbb{Z}) \simeq H_p(Y, \mathbb{Z})&lt;/math&gt;; thus, the hypothesis of the lemma.) Next, the [[spectral sequence for the covering]] &lt;math&gt;\widetilde{Ff} \to Ff&lt;/math&gt; with group &lt;math&gt;G = \pi_1(Ff)&lt;/math&gt; says:
:&lt;math&gt;{}^2 E_{pq} = H_p(G, H_q(\widetilde{Ff}, \mathbb{Z})) \Rightarrow H_{p+q}(Ff, \mathbb{Z}) = H_{p+q}(*, \mathbb{Z})&lt;/math&gt;.
An inspection with this spectral sequence gives the desired result.

==References==
{{reflist}}
*Daniel Grayson, [http://www.math.uiuc.edu/~dan/Papers/HigherAlgKThyII.pdf Higher algebraic K-theory II &lt;nowiki&gt;[after Daniel Quillen]&lt;/nowiki&gt;], 1976
*{{citation | last=Srinivas | first=V. | title=Algebraic ''K''-theory | edition=Paperback reprint of the 1996 2nd | series=Modern Birkhäuser Classics | location=Boston, MA | publisher=[[Birkhäuser]] | year=2008 | isbn=978-0-8176-4736-0 | zbl=1125.19300 }}
*C. Weibel &quot;[http://www.math.rutgers.edu/~weibel/Kbook.html The K-book: An introduction to algebraic K-theory]&quot;

[[Category:Algebra]]</text>
      <sha1>l1ie4s20r4o1pbqmf27ng4mp337tc5v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Monomial</title>
    <ns>0</ns>
    <id>357416</id>
    <revision>
      <id>604279449</id>
      <parentid>604272323</parentid>
      <timestamp>2014-04-15T09:51:56Z</timestamp>
      <contributor>
        <username>Eric Kvaalen</username>
        <id>122969</id>
      </contributor>
      <comment>Return to my version. What I wrote is not an unsourced linguistic opinion. Take a look at Wiktionary! I'm adding a reference to the American Heritage Dictionary.</comment>
      <text xml:space="preserve" bytes="7598">In [[mathematics]], a '''monomial''' is, roughly speaking, a [[polynomial]] which has only one term. Two different definitions of a monomial may be encountered:
*For the first definition, a '''monomial''' is a product of powers of [[Variable (mathematics)|variables]] with [[nonnegative integer]] exponents, or, in other words, a product of variables, possibly with repetitions. The constant 1 is a monomial, being equal to the [[empty product]] and {{mvar|x}}&lt;sup&gt;0&lt;/sup&gt; for any variable {{mvar|x}}. If only a single variable {{mvar|x}} is considered, this means that a monomial is either 1 or a power {{math|''x''&lt;sup&gt;''n''&lt;/sup&gt;}} of {{mvar|x}}, with {{mvar|n}} a positive integer. If several variables are considered, say, &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;y&lt;/math&gt;, &lt;math&gt;z&lt;/math&gt;, then each can be given an exponent, so that any monomial is of the form &lt;math&gt;x^a y^b z^c&lt;/math&gt; with &lt;math&gt;a,b,c&lt;/math&gt; non-negative integers (taking note that any exponent 0 makes the corresponding factor equal to 1).
*For the second definition, a '''monomial''' is a monomial in the first sense multiplied by a nonzero constant, called the ''coefficient'' of the monomial. A monomial in the first sense is also a monomial in the second sense, because the multiplication by 1 is allowed. For example, in this interpretation &lt;math&gt;-7x^5&lt;/math&gt; and &lt;math&gt;(3-4i)x^4yz^{13}&lt;/math&gt; are monomials (in the second example, the variables are &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;y&lt;/math&gt;, &lt;math&gt;z&lt;/math&gt;, and the coefficient is a [[complex number]]).

In the context of [[Laurent polynomial]]s and [[Laurent series]], the exponents of a '''monomial''' may be negative, and in the context of [[Puiseux series]], the exponents may be [[rational number]]s.

Since the word &quot;polynomial&quot; comes from &quot;poly-&quot; plus the Greek word &quot;νομός&quot; (nomós, meaning part, portion), a monomial should theoretically be called a &quot;mononomial&quot;. &quot;Monomial&quot; is a [[syncope (phonetics)|syncope]] of &quot;mononomial&quot;.&lt;ref&gt;''American Heritage Dictionary of the English Language'', 1969.&lt;/ref&gt;

== Comparison of the two definitions ==
With either definition, the set of monomials is a subset of all polynomials that is closed under multiplication.

Both uses of this notion can be found, and in many cases the distinction is simply ignored, see for instance examples for the first&lt;ref&gt;{{cite book | last = Cox | first = David | authorlink = | coauthors = John Little, Donal O'Shea | title = Using Algebraic Geometry | publisher = Springer Verlag | year = 1998 | location = | pages = 1 | url = | doi = | id = | isbn = 0-387-98487-9 }}&lt;/ref&gt; and second&lt;ref&gt;{{Springer|id=M/m064760|title=Monomial}}&lt;/ref&gt; meaning, and an [http://planetmath.org/encyclopedia/Monomial.html unclear definition]. In informal discussions the distinction is seldom important, and tendency is towards the broader second meaning. When studying the structure of polynomials however, one often definitely needs a notion with the first meaning. This is for instance the case when considering a [[monomial basis]] of a [[polynomial ring]], or a [[monomial order]]ing of that basis. An argument in favor of the first meaning is also that no obvious other notion is available to designate these values (the term '''power product''' is in use, but it does not make the absence of constants clear either), while the notion '''term''' of a polynomial unambiguously coincides with the second meaning of monomial. 

''The remainder of this article assumes the first meaning of &quot;monomial&quot;.''

==As bases==

The most obvious fact about monomials (first meaning) is that any polynomial is a [[linear combination]] of them, so they form a [[basis (linear algebra)|basis]] of the [[vector space]] of all polynomials - a fact of constant implicit use in mathematics.

==Number==
The number of monomials of degree ''d'' in ''n'' variables is the number of [[multicombination]]s of ''d'' elements chosen among the ''n'' variables (a variable can be chosen more than once, but order does not matter), which is given by the [[multiset coefficient]] &lt;math&gt;\textstyle{\left(\!\!{n\choose d}\!\!\right)}&lt;/math&gt;. This expression can also be given in the form of a [[binomial coefficient]], as a [[polynomial expression]] in ''d'', or using a [[Pochhammer symbol#Alternate notations|rising factorial power]] of {{nowrap|''d'' + 1}}:
:&lt;math&gt;\left(\!\!{n\choose d}\!\!\right) = \binom{n+d-1}{d} = \binom{d+(n-1)}{n-1}
  = \frac{(d+1)\times(d+2)\times\cdots\times(d+n-1)}{1\times2\times\cdots\times(n-1)} = \frac{1}{(n-1)!}(d+1)^{\overline{n-1}}.&lt;/math&gt;
The latter forms are particularly useful when one fixes the number of variables and lets the degree vary. From these expressions one sees that for fixed ''n'', the number of monomials of degree ''d'' is a polynomial expression in ''d'' of degree &lt;math&gt;n-1&lt;/math&gt; with leading coefficient &lt;math&gt;\tfrac1{(n-1)!}&lt;/math&gt;.

For example, the number of monomials in three variables (&lt;math&gt;n=3&lt;/math&gt;) of degree ''d'' is &lt;math&gt;\textstyle{\frac{1}{2}}(d+1)^{\overline2} = \textstyle{\frac{1}{2}}(d+1)(d+2)&lt;/math&gt;; these numbers form the sequence 1, 3, 6, 10, 15, ... of [[triangular number]]s.

The [[Hilbert series]] is a compact way to express the number of monomials of a given degree: the number of monomials of degree {{mvar|d}} in {{mvar|n}} variables is the coefficient of degree {{mvar|d}} of the [[formal power series]] expansion of
:&lt;math&gt; \frac{1}{(1-t)^n}.&lt;/math&gt;

==Notation==

Notation for monomials is constantly required in fields like [[partial differential equation]]s. If the variables being used form an indexed family like &lt;math&gt;x_1&lt;/math&gt;, &lt;math&gt;x_2&lt;/math&gt;, &lt;math&gt;x_3&lt;/math&gt;, ..., then ''[[multi-index notation]]'' is helpful: if we write 

:&lt;math&gt;\alpha = (a, b, c)&lt;/math&gt;

we can define 
:&lt;math&gt;x^{\alpha} = x_1^a\, x_2^b\, x_3^c&lt;/math&gt;

and save a great deal of space.

==Degree&lt;!-- [[Degree of a monomial]] redirects here --&gt;==

The '''degree''' of a monomial is defined as the sum of all the exponents of the variables, including the implicit exponents of 1 for the variables which appear without exponent; e.g., in the example of the previous section, the degree is &lt;math&gt;a+b+c&lt;/math&gt;. The degree of &lt;math&gt;x y z^2&lt;/math&gt; is 1+1+2=4.

The degree of a monomial is sometimes called '''order''', mainly in the context of series. It is also called '''total degree''' when it is needed to distinguish it from the degree in one of the variables.

Monomial degree is fundamental to the theory of univariate and multivariate polynomials. Explicitly, it is used to define the [[degree of a polynomial]] and the notion of [[homogeneous polynomial]], as well as for graded [[monomial ordering]]s used in formulating and computing [[Gröbner basis|Gröbner bases]]. Implicitly, it is used in grouping the terms of a [[Taylor series#Taylor series in several variables|Taylor series in several variables]].

==Geometry==

In [[algebraic geometry]] the varieties defined by monomial equations &lt;math&gt;x^{\alpha} = 0&lt;/math&gt; for some set of α have special properties of homogeneity. This can be phrased in the language of [[algebraic group]]s, in terms of the existence of a [[group action]] of an [[algebraic torus]] (equivalently by a multiplicative group of [[diagonal matrix|diagonal matrices]]). This area is studied under the name of ''[[Toric geometry|torus embedding]]s''.

==See also==
* [[Monomial representation]]
* [[Generalized permutation matrix|Monomial matrix]]
* [[Homogeneous polynomial]]
* [[Homogeneous function]]
* [[Multilinear form]]
* [[Log-log plot]]
* [[Power law]]

== Notes ==

{{Reflist}}

[[Category:Homogeneous polynomials]]
[[Category:Algebra]]</text>
      <sha1>1wk0mkpj3da1i1mc8t4od7xx1rszmj5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Ratio</title>
    <ns>0</ns>
    <id>87837</id>
    <revision>
      <id>609648406</id>
      <parentid>609415698</parentid>
      <timestamp>2014-05-22T09:49:42Z</timestamp>
      <contributor>
        <ip>115.187.33.253</ip>
      </contributor>
      <comment>/* Examples */</comment>
      <text xml:space="preserve" bytes="18743">{{other uses}}
{{redirect|is to|the grammatical construction|am to}}
[[File:Aspect-ratio-4x3.svg|thumb|The ratio of width to height of [[standard-definition television]].]]

In [[mathematics]], a '''ratio''' is a relationship between two numbers of the same kind&lt;ref&gt;Wentworth, p. 55&lt;/ref&gt; (''e.g.'', [[objects]], persons, students, spoonfuls, units of whatever identical dimension), expressed as ''&quot;a'' to ''b&quot;'' or a:b, sometimes expressed arithmetically as a dimensionless [[quotient]] of the two&lt;ref&gt;New International Encyclopedia&lt;/ref&gt; that explicitly indicates how many times the first number contains the second (not necessarily an [[integer]]).&lt;ref&gt;Penny Cyclopedia, p. 307&lt;/ref&gt;

In layman's terms a ratio represents, for every amount of one thing, how much there is of another thing. For example, supposing one has 8 oranges and 6 lemons in a bowl of fruit, the ratio of oranges to lemons would be 4:3 (which is equivalent to 8:6) while the ratio of lemons to oranges would be 3:4. Additionally, the ratio of oranges to the total amount of fruit is 4:7 (equivalent to 8:14). The 4:7 ratio can be further converted to a [[Fraction (mathematics)|fraction]] of 4/7 to represent how much of the fruit is oranges.

==Notation and terminology==
The ratio of numbers ''A'' and ''B'' can be expressed as:&lt;ref&gt;New International Encyclopedia&lt;/ref&gt;
*the ratio of '''A''' to '''B'''
*'''A''' is to '''B''' '' (often followed by &quot;as ...&quot;)''
*'''A:B''' 
*A [[fraction (mathematics)|fraction]] ([[rational number]]) that is the quotient: '''A''' divided by '''B'''
*&lt;math&gt;\tfrac{A}{B}&lt;/math&gt;
Example 2:10 would be simplified to 1:5
The numbers ''A'' and ''B'' are sometimes called ''terms'' with ''A'' being the ''[[Antecedent (grammar)|antecedent]]'' and ''B'' being the ''[[consequent]]''.{{citation needed|date=June 2012}}
&lt;!-- (Can't find a source for this but leave in as comment for now since it seems plausible.)--&gt;

The proportion expressing the equality of the ratios ''A'':''B'' and ''C'':''D'' is written
''A'':''B'' = ''C'':''D'' or ''A'':''B''::''C'':''D''. this latter form, when spoken or written in the English language, is often expressed as
:''A'' is to ''B'' as ''C'' is to ''D''.

''A'', ''B'', ''C'' and ''D'' are called the terms of the proportion. ''A'' and ''D'' are called the ''extremes'', and ''B'' and ''C'' are called the ''means''. The equality of three or more proportions is called a continued proportion.&lt;ref&gt;New International Encyclopedia&lt;/ref&gt;

Ratios are sometimes used with three or more terms.  The ratio of the dimensions of a &quot;two by four&quot; that is ten inches long is 2:4:10. A good concrete mix is sometimes quoted as 1:2:4 for the ration of cement to sand to gravel.&lt;ref&gt;[http://www.bellegroup.com/es/support/mixingHints.html Belle Group concrete mixing hints]&lt;/ref&gt;

==History and etymology==
It is impossible to trace the origin of the ''concept'' of ratio, because the ideas from which it developed would have been familiar to preliterate cultures. For example, the idea of one village being twice as large as another is so basic that it would have been understood in prehistoric society.&lt;ref&gt;Smith, p. 477&lt;/ref&gt; However, it is possible to trace the origin of the word &quot;ratio&quot; to the [[Ancient Greek]] λόγος (''[[logos]]''). Early translators rendered this into [[Latin]] as ''[[wikt:ratio#Latin|ratio]]'' (&quot;reason&quot;; as in the word &quot;rational&quot;). (A rational number may be expressed as the quotient of two integers.) A more modern interpretation of Euclid's meaning is more akin to computation or reckoning.&lt;ref&gt;Penny Cyclopedia, p. 307&lt;/ref&gt; Medieval writers used the word ''proportio'' (&quot;proportion&quot;) to indicate ratio and ''proportionalitas'' (&quot;proportionality&quot;) for the equality of ratios.&lt;ref&gt;Smith, p. 478&lt;/ref&gt;

Euclid collected the results appearing in the Elements from earlier sources. The [[Pythagoreanism|Pythagoreans]] developed a theory of ratio and proportion as applied to numbers.&lt;ref&gt;Heath, p. 112&lt;/ref&gt; The Pythagoreans' conception of number included only what would today be called rational numbers, casting doubt on the validity of the theory in geometry where, as the Pythagoreans also discovered, incommensurable ratios (corresponding to irrational numbers) exist. The discovery of a theory of ratios that does not assume commensurability is probably due to [[Eudoxus]]. The exposition of the theory of proportions that appears in Book VII of The Elements reflects the earlier theory of ratios of commensurables.&lt;ref&gt;Heath, p. 113&lt;/ref&gt;

The existence of multiple theories seems unnecessarily complex to modern sensibility since ratios are, to a large extent, identified with quotients. This is a comparatively recent development however, as can be seen from the fact that modern geometry textbooks still use distinct terminology and notation for ratios and quotients. The reasons for this are twofold. First, there was the previously mentioned reluctance to accept irrational numbers as true numbers. Second, the lack of a widely used symbolism to replace the already established terminology of ratios delayed the full acceptance of fractions as alternative until the 16th century.&lt;ref&gt;Smith, p. 480&lt;/ref&gt;

===Euclid's definitions===
Book V of [[Euclid's Elements]] has 18 definitions, all of which relate to ratios.&lt;ref&gt;Heath, reference for section&lt;/ref&gt; In addition, Euclid uses ideas that were in such common usage that he did not include definitions for them. The first two definitions say that a ''part'' of a quantity is another quantity that &quot;measures&quot; it and conversely, a ''multiple'' of a quantity is another quantity that it measures. In modern terminology, this means that a multiple of a quantity is that quantity multiplied by an integer greater than one—and a part of a quantity (meaning [[aliquot part]]) is a part that, when multiplied by an integer greater than one, gives the quantity.

Euclid does not define the term &quot;measure&quot; as used here, However, one may infer that if a quantity is taken as a unit of measurement, and a second quantity is given as an integral number of these units, then the first quantity ''measures'' the second. Note that these definitions are repeated, nearly word for word, as definitions 3 and 5 in book VII.

Definition 3 describes what a ratio is in a general way. It is not rigorous in a mathematical sense and some have ascribed it to Euclid's editors rather than Euclid himself.&lt;ref&gt;&quot;Geometry, Euclidean&quot; ''[[Encyclopædia Britannica Eleventh Edition]]'' p682.&lt;/ref&gt; Euclid defines a ratio as between two quantities ''of the same type'', so by this definition the ratios of two lengths or of two areas are defined, but not the ratio of a length and an area. Definition 4 makes this more rigorous. It states that a ratio of two quantities exists when there is a multiple of each that exceeds the other. In modern notation, a ratio exists between quantities ''p'' and ''q'' if there exist integers ''m'' and ''n'' so that ''mp''&gt;''q'' and ''nq''&gt;''m''. This condition is known as the [[Archimedean property]].

Definition 5 is the most complex and difficult. It defines what it means for two ratios to be equal. Today, this can be done by simply stating that ratios are equal when the quotients of the terms are equal, but Euclid did not accept the existence of the quotients of incommensurables, so such a definition would have been meaningless to him. Thus, a more subtle definition is needed where quantities involved are not measured directly to one another. Though it may not be possible to assign a rational value to a ratio, it is possible to compare a ratio with a rational number. Specifically, given two quantities, ''p'' and ''q'', and a rational number ''m''/''n'' we can say that the ratio of ''p'' to ''q'' is less than, equal to, or greater than ''m''/''n'' when ''np'' is less than, equal to, or greater than ''mq'' respectively. Euclid's definition of equality can be stated as that two ratios are equal when they behave identically with respect to being less than, equal to, or greater than any rational number. In modern notation this says that given quantities ''p'', ''q'', ''r'' and ''s'', then ''p'':''q''::''r'':''s'' if for any positive integers ''m'' and ''n'', ''np''&lt;''mq'', ''np''=''mq'', ''np''&gt;''mq'' according as ''nr''&lt;''ms'', ''nr''=''ms'', ''nr''&gt;''ms'' respectively. There is a remarkable similarity between this definition and the theory of [[Dedekind cut]]s used in the modern definition of irrational numbers.&lt;ref&gt;Heath p. 125&lt;/ref&gt;

Definition 6 says that quantities that have the same ratio are ''proportional'' or ''in proportion''. Euclid uses the Greek ἀναλόγον (analogon), this has the same root as λόγος and is related to the English word &quot;analog&quot;.

Definition 7 defines what it means for one ratio to be less than or greater than another and is based on the ideas present in definition 5. In modern notation it says that given quantities ''p'', ''q'', ''r'' and ''s'', then ''p'':''q''&gt;''r'':''s'' if there are positive integers ''m'' and ''n'' so that ''np''&gt;''mq'' and ''nr''≤''ms''.

As with definition 3, definition 8 is regarded by some as being a later insertion by Euclid's editors. It defines three terms ''p'', ''q'' and ''r'' to be in proportion when ''p'':''q''::''q'':''r''. This is extended to 4 terms ''p'', ''q'', ''r'' and ''s'' as ''p'':''q''::''q'':''r''::''r'':''s'', and so on. Sequences that have the property that the ratios of consecutive terms are equal are called [[Geometric progression]]s. Definitions 9 and 10 apply this, saying that if ''p'', ''q'' and ''r'' are in proportion then ''p'':''r'' is the ''duplicate ratio'' of ''p'':''q'' and if ''p'', ''q'', ''r'' and ''s'' are in proportion then ''p'':''s'' is the ''triplicate ratio'' of ''p'':''q''. If ''p'', ''q'' and ''r'' are in proportion then ''q'' is called a ''mean proportional'' to (or the [[geometric mean]] of) ''p'' and ''r''. Similarly, if ''p'', ''q'', ''r'' and ''s'' are in proportion then ''q'' and ''r'' are called two mean proportionals to ''p'' and ''s''.

ed, nor the amount of concrete being made. Equivalently it could be said that the ratio of cement to water is 4:1, that there is 4 times as much cement as water, or that there is a quarter (1/4) as much water as cement..

Older [[television]]s have a 4:3 ''[[aspect ratio]]'', which means that the width is 4/3 of the height; modern widescreen TVs have a 16:9 aspect ratio.

===Fraction===
{{main|Fraction (mathematics)}}
If there are 2 oranges and 3 apples, the ratio of oranges to apples is 2:3, and the ratio of oranges to the total number of pieces of fruit is 2:5.  These ratios can also be expressed in fraction form: there are 2/3 as many oranges as apples, and 2/5 of the pieces of fruit are oranges.  If orange juice concentrate is to be diluted with water in the ratio 1:4, then one part of concentrate is mixed with four parts of water, giving five parts total; the amount of orange juice concentrate is 1/4 the amount of water, while the amount of orange juice concentrate is 1/5 of the total liquid.  In both ratios and fractions, it is important to be clear what is being compared to what, and beginners often make mistakes for this reason.

==Number of terms==
In general, when comparing the quantities of a two-quantity ratio, this can be expressed as a fraction derived from the ratio. For example, in a ratio of 2:3, the amount/size/volume/number of the first quantity is &lt;math&gt;\tfrac{2}{3}&lt;/math&gt; that of the second quantity. This pattern also works with ratios with more than two terms. However, a ratio with more than two terms cannot be completely converted into a single fraction; a single fraction represents only one part of the ratio since a fraction can only compare two numbers.  If the ratio deals with objects or amounts of objects, this is often expressed as &quot;for every two parts of the first quantity there are three parts of the second quantity&quot;.

===Percentage ratio===
If we multiply all quantities involved in a ratio by the same number, the ratio remains valid. For example, a ratio of 3:2 is the same as 12:8. It is usual either to reduce terms to the [[lowest common denominator]], or to express them in parts per hundred ([[percent]]).

If a mixture contains substances A, B, C &amp; D in the ratio 5:9:4:2 then there are 5 parts of A for every 9 parts of B, 4 parts of C and 2 parts of D.  As 5+9+4+2=20, the total mixture contains 5/20 of A (5 parts out of 20), 9/20 of B, 4/20 of C, and 2/20 of D.  If we divide all numbers by the total and multiply by 100, this is converted to percentages: 25% A, 45% B, 20% C, and 10% D (equivalent to writing the ratio as 25:45:20:10).

==Proportions==
If the two or more ratio quantities encompass all of the quantities in a particular situation, for example two apples and three oranges in a fruit basket containing no other types of fruit, it could be said that &quot;the whole&quot; contains five parts, made up of two parts apples and three parts oranges. In this case, &lt;math&gt;\tfrac{2}{5}&lt;/math&gt;, or 40% of the whole are apples and &lt;math&gt;\tfrac{3}{5}&lt;/math&gt;, or 60% of the whole are oranges. This comparison of a specific quantity to &quot;the whole&quot; is sometimes called a proportion. Proportions are sometimes expressed as [[percentages]] as demonstrated above.

==Reduction==
{{main|Reduction (mathematics)}}
Note that ratios can be [[Reduction (mathematics)|reduced]] (as fractions are) by dividing each quantity by the common factors of all the quantities.  This is often called &quot;cancelling.&quot;  As for fractions, the simplest form is considered that in which the numbers in the ratio are the smallest possible integers.

Thus, the ratio 40:60 may be considered equivalent in meaning to the ratio 2:3 within contexts concerned only with relative quantities.

Mathematically, we write: &quot;40:60&quot; = &quot;2:3&quot; (dividing both quantities by 20).
:Grammatically, we would say, &quot;40 to 60 equals 2 to 3.&quot;

An alternative representation is: &quot;40:60::2:3&quot;
:Grammatically, we would say, &quot;40 is to 60 as 2 is to 3.&quot;

A ratio that has integers for both quantities and that cannot be reduced any further (using integers) is said to be in [[Irreducible fraction|simplest form]] or lowest terms.

Sometimes it is useful to write a ratio in the form 1:''n'' or ''n'':1 to enable comparisons of different ratios.

For example, the ratio 4:5 can be written as 1:1.25 (dividing both sides by 4)

Alternatively, 4:5 can be written as 0.8:1 (dividing both sides by 5)

Where the context makes the meaning clear, a ratio in this form is sometimes written without the 1 and the colon, though, mathematically, this makes it a [[divisor|factor]] or [[Multiplication|multiplier]].

===Dilution ratio===
Ratios are often used for simple dilutions applied in chemistry and biology. A simple dilution is one in which a unit volume of a liquid material of interest is combined with an appropriate volume of a solvent liquid to achieve the desired concentration. The dilution factor is the total number of unit volumes in which your material is dissolved. The diluted material must then be thoroughly mixed to achieve the true dilution. For example, a 1:5 dilution (verbalize as &quot;1 to 5&quot; dilution) entails combining 1 unit volume of solute (the material to be diluted) + 4 unit volumes (approximately) of the solvent to give 5 units of the total volume.  (Some solutions and mixtures take up slightly less volume than their components.)

The dilution factor is frequently expressed using exponents: 1:5 would be 5e−1 (5&lt;sup&gt;−1&lt;/sup&gt; i.e. one-fifth:one); 1:100 would be 10e−2 (10&lt;sup&gt;−2&lt;/sup&gt; i.e. one hundredth:one), and so on.

There is often confusion between dilution ratio (1:n meaning 1 part solute to n parts solvent) and dilution factor (1:n+1) where the second number (n+1) represents the total volume of solute + solvent.  In scientific and serial dilutions, the given ratio (or factor) often means the ratio to the final volume, not to just the solvent.  The factors then can easily be multiplied to give an overall dilution factor.

In other areas of science such as pharmacy, and in non-scientific usage, a dilution is normally given as a plain ratio of solvent to solute.

==Odds==
{{main|Odds}}
''Odds'' (as in gambling) are expressed as a ratio.  For example, odds of &quot;7 to 3 against&quot; (7:3) mean that there are seven chances that the event will not happen to every three chances that it will happen.  The probability of success is 30%.  In every ten trials, there are three wins and seven losses.

==Different units==
Ratios are [[Dimensionless quantity|unitless]] when they relate quantities in units of the same [[dimensional analysis|dimension]].

For example, the ratio 1 minute : 40 seconds can be reduced by changing the first value to 60 seconds. Once the units are the same, they can be omitted, and the ratio can be reduced to 3:2.

In chemistry, [[Mass concentration (chemistry)|mass concentration]] &quot;ratios&quot; are usually expressed as w/v percentages, and are really proportions.

For example, a concentration of 3% w/v usually means 3g of substance in every 100mL of solution.  This cannot easily be converted to a pure ratio because of density considerations, and the second figure is the ''total'' amount, not the volume of [[solvent]].

==Financial ratios==
Various [[financial ratio]]s are used in the [[fundamental analysis]] of a business, for example the [[price–earnings ratio]] is commonly quoted for shares.

==See also==

*[[Aspect ratio]]
*[[Fraction (mathematics)]]
*[[Golden ratio]]
*[[Interval (music)]]
*[[Parts-per notation]]
*[[Price–performance ratio]]
*[[Proportionality (mathematics)]]
*[[Ratio distribution]]
*[[Ratio estimator]]
*[[Rule of three (mathematics)]]
*[[Slope]]

==References==
&lt;references/&gt;

==Further reading==
*[http://books.google.com/books?id=ZqsrAAAAYAAJ&amp;pg=PA307 &quot;Ratio&quot; ''The Penny Cyclopædia'' vol. 19], The Society for the Diffusion of Useful Knowledge (1841) Charles Knight and Co., London pp.&amp;nbsp;307ff
*[http://books.google.com/books?id=qgAoAAAAYAAJ&amp;pg=PA270 &quot;Proportion&quot; ''New International Encyclopedia, Vol. 19'' 2nd ed. (1916) Dodd Mead &amp; Co. pp270-271]
*[http://books.google.com/books?id=sqMXAAAAYAAJ&amp;pg=PA55 &quot;Ratio and Proportion&quot; ''Fundamentals of practical mathematics'', George Wentworth, David Eugene Smith, Herbert Druery Harper (1922) Ginn and Co. pp. 55ff]
*{{cite book |title= The thirteen books of Euclid's Elements, vol 2
|others= trans. Sir Thomas Little Heath (1908) |publisher= Cambridge Univ. Press|pages=112ff| url=http://books.google.com/books?id=lxkPAAAAIAAJ&amp;pg=RA2-PA112}}
*D.E. Smith, ''History of Mathematics, vol 2'' Dover (1958) pp.&amp;nbsp;477ff

==External links==
{{wiktionary}}


[[Category:Elementary mathematics]]
[[Category:Algebra]]
[[Category:Ratios| ]]</text>
      <sha1>oj5bv9qsnfi7910srej1fuzpaisqfbw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Antiisomorphism</title>
    <ns>0</ns>
    <id>2152465</id>
    <revision>
      <id>604829817</id>
      <parentid>597928023</parentid>
      <timestamp>2014-04-19T03:37:14Z</timestamp>
      <contributor>
        <ip>2605:A000:160D:805A:8C4D:1C6:6531:EF7A</ip>
      </contributor>
      <comment>/* Simple example */</comment>
      <text xml:space="preserve" bytes="3220">In [[Category theory]], a branch of formal mathematics, an '''antiisomorphism''' (or '''anti-isomorphism''') between [[Mathematical structure|structured sets]] ''A'' and ''B'' is an [[isomorphism]] from ''A'' to the [[Dual (category theory)|opposite]] of ''B'' (or equivalently from the opposite of ''A'' to ''B'').&lt;ref&gt;{{harvnb|Pareigis|loc=p. 19}}&lt;/ref&gt; If there exists an antiisomorphism between two structures, they are ''antiisomorphic.''

Intuitively, to say that two mathematical structures are ''antiisomorphic'' is to say that they are basically opposites of one another.

The concept is particularly useful in an algebraic setting, as, for instance, when applied to [[ring (mathematics)|rings]].

==Simple example==
Let ''A'' be the [[binary relation]] (or [[directed graph]]) consisting of elements {1,2,3} and binary relation &lt;math&gt;\rightarrow&lt;/math&gt; defined as follows:
* &lt;math&gt;1 \rightarrow 2;&lt;/math&gt;
* &lt;math&gt;1 \rightarrow 3;&lt;/math&gt;
* &lt;math&gt;2 \rightarrow 1.&lt;/math&gt;

Let ''B'' be the binary relation set consisting of elements {''a'',''b'',''c''} and binary relation &lt;math&gt;\Rightarrow&lt;/math&gt; defined as follows:
* &lt;math&gt;b \Rightarrow a;&lt;/math&gt;
* &lt;math&gt;c \Rightarrow a;&lt;/math&gt;
* &lt;math&gt;a \Rightarrow b.&lt;/math&gt;

Note that the opposite of ''B'' (called ''B''&lt;sup&gt;op&lt;/sup&gt;) is the same set of elements with the opposite binary relation &lt;math&gt;\Leftarrow&lt;/math&gt; (that is, reverse all the arcs of the directed graph):
* &lt;math&gt;b \Leftarrow a;&lt;/math&gt;
* &lt;math&gt;c \Leftarrow a;&lt;/math&gt;
* &lt;math&gt;a \Leftarrow b.&lt;/math&gt;

If we replace ''a'', ''b'', and ''c'' with 1, 2, and 3 respectively, we will see that each rule in ''B''&lt;sup&gt;op&lt;/sup&gt; is the same as some rule in ''A''. That is, we can define an isomorphism &lt;math&gt;\phi&lt;/math&gt; from ''A'' to ''B''&lt;sup&gt;op&lt;/sup&gt; by

&lt;math&gt;\phi(n) = \begin{cases}a&amp;\mbox{if }n=1;\\ b&amp;\mbox{if }n=2;\\ c&amp;\mbox{if }n=3.\end{cases}&lt;/math&gt;

This &lt;math&gt;\phi&lt;/math&gt; is an antiisomorphism between ''A'' and ''B''.

==Ring anti-isomorphisms==
Specializing the general language of category theory to the algebraic topic of rings, we have:
Let ''R'' and ''S'' be rings and ''f'': ''R'' → ''S'' a [[bijection]] between them, then if
:&lt;math&gt;f(x +_R y) = f(x) +_S f(y) \text{  and  } f(x \cdot_R y) = f(y) \cdot_S f(x) \text{  for all  } x,y \in R&lt;/math&gt;
''f'' will be called a ''ring anti-isomorphism''.&lt;ref&gt;{{harvnb|Jacobson|loc=p. 16}}&lt;/ref&gt; If ''R'' = ''S'' then ''f'' will be called a ring ''anti-automorphism''.

An example of a ring anti-automorphism is given by the conjugate mapping of [[quaternion]]s:&lt;ref&gt;{{harvnb|Baer|loc=p. 96}}&lt;/ref&gt;
:&lt;math&gt; x_0 + x_1 \mathbf{i} + x_2 \mathbf{j} + x_3 \mathbf{k}\quad \mapsto \quad x_0 - x_1 \mathbf{i} - x_2 \mathbf{j} - x_3 \mathbf{k}.&lt;/math&gt;

==Notes==
{{reflist|3}}

==References==
* {{citation|first=Reinhold|last=Baer|title=Linear Algebra and Projective Geometry|year=2005|origyear=1952|publisher=Dover|isbn=0-486-44565-8}}
* {{citation|first=Nathan|last=Jacobson|title=The Theory of Rings|year=1948|publisher=American Mathematical Society|isbn=0-8218-1502-4}}
* {{citation|first=Bodo|last=Pareigis|title=Categories and Functors|year=1970|publisher=Academic Press|isbn=0-12-545150-4}}

[[Category:Morphisms]]
[[Category:Rings]]
[[Category:Algebra]]</text>
      <sha1>pvb34dfz3omkfomjejyh47p4yhc9346</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Coefficient</title>
    <ns>0</ns>
    <id>51203</id>
    <revision>
      <id>606398924</id>
      <parentid>606397205</parentid>
      <timestamp>2014-04-29T22:42:40Z</timestamp>
      <contributor>
        <username>Paul August</username>
        <id>87355</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/74.89.188.7|74.89.188.7]] ([[User talk:74.89.188.7|talk]]) to last version by ClueBot NG</comment>
      <text xml:space="preserve" bytes="5450">{{Other uses}}
In [[mathematics]], a '''coefficient''' is a multiplicative factor in some [[term (mathematics)|term]] of a [[polynomial]], a [[series (mathematics)|series]] or any [[expression (mathematics)|expression]]; it is usually a number, but in any case does not involve any [[variable (mathematics)|variable]]s of the expression. For instance in
:&lt;math&gt;7x^2-3xy+1.5+y&lt;/math&gt;
the first two terms respectively have the coefficients 7 and −3. The third term 1.5 is a constant. The final term does not have any explicitly written coefficient, but is considered to have coefficient 1, since multiplying by that factor would not change the term. Often coefficients are numbers as in this example, although they could be parameters of the problem, as ''a'', ''b'', and ''c'', where &quot;c&quot; is a constant, in
:&lt;math&gt;ax^2+bx+c&lt;/math&gt;
when it is understood that these are not considered as variables.
Thus a [[polynomial]] in one variable ''x'' can be written as
:&lt;math&gt;a_k x^k + \dotsb + a_1 x^1 + a_0&lt;/math&gt;
for some integer &lt;math&gt;k&lt;/math&gt;, where &lt;math&gt;a_k, \dotsc, a_1, a_0&lt;/math&gt; are coefficients; to allow this kind of expression in all cases one must allow introducing terms with 0 as coefficient.
For the largest &lt;math&gt;i&lt;/math&gt; with &lt;math&gt;a_i \ne 0&lt;/math&gt; (if any), &lt;math&gt;a_i&lt;/math&gt; is called the '''leading coefficient''' of the polynomial. So for example the leading coefficient of the polynomial

:&lt;math&gt;\, 4x^5 + x^3 + 2x^2&lt;/math&gt;

is 4.

Specific coefficients arise in mathematical identities, such as the [[binomial theorem]] which involves [[binomial coefficient]]s; these particular coefficients are tabulated in [[Pascal's triangle]].

==Linear algebra==
In [[linear algebra]], the '''leading coefficient''' of a row in a matrix is the first nonzero entry in that row. So, for example, given

:&lt;math&gt;
M = \begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; 6\\
0 &amp; 2 &amp; 9 &amp; 4\\
0 &amp; 0 &amp; 0 &amp; 4\\
\end{pmatrix}
&lt;/math&gt;.

The leading coefficient of the first row is 1; 2 is the leading coefficient of the second row; 4 is the leading coefficient of the third row, and the last row does not have a leading coefficient.

Though coefficients are frequently viewed as [[constant (mathematics)|constants]] in elementary algebra, they can be variables more generally. For example, the [[coordinates]] &lt;math&gt;(x_1, x_2, \dotsc, x_n)&lt;/math&gt; of a [[vector (geometric)|vector]] &lt;math&gt;v&lt;/math&gt; in a [[vector space]] with [[basis (linear algebra)|basis]] &lt;math&gt;\lbrace e_1, e_2, \dotsc, e_n \rbrace &lt;/math&gt;, are the coefficients of the basis vectors in the expression 
:&lt;math&gt; v = x_1 e_1 + x_2 e_2 + \dotsb + x_n e_n .&lt;/math&gt;

==Examples of physical coefficients==
# ''[[coefficient of thermal expansion|Coefficient of Thermal Expansion]]'' ([[thermodynamics]]) (dimensionless) - Relates the change in temperature to the change in a material's dimensions.
# ''[[Partition Coefficient]]'' (''K&lt;sub&gt;D&lt;/sub&gt;'') ([[chemistry]]) - The ratio of concentrations of a compound in two phases of a mixture of two immiscible solvents at equilibrium. H2O is a coefficient
# ''[[Hall Effect|Hall coefficient]]'' (electrical physics) - Relates a magnetic field applied to an element to the voltage created, the amount of current and the element thickness. It is a characteristic of the material from which the conductor is made.
# ''[[Lift coefficient]]'' (''C&lt;sub&gt;L&lt;/sub&gt;'' or ''C&lt;sub&gt;Z&lt;/sub&gt;'') ([[Aerodynamics]]) (dimensionless) - Relates the lift generated by an airfoil with the dynamic pressure of the fluid flow around the airfoil, and the plan-form area of the airfoil.
# ''[[Ballistic coefficient]]'' (BC) ([[Aerodynamics]]) (units of kg/m&lt;sup&gt;2&lt;/sup&gt;) - A measure of a body's ability to overcome air resistance in flight. BC is a function of mass, diameter, and drag coefficient.
# ''[[Transmission coefficient (physics)|Transmission Coefficient]]'' ([[quantum mechanics]]) (dimensionless) - Represents the probability flux of a transmitted wave relative to that of an incident wave. It is often used to describe the probability of a particle [[Quantum tunnelling|tunnelling]] through a barrier.
# ''[[Damping|Damping Factor]]'' a.k.a. ''viscous damping coefficient'' (Physical Engineering) (units of newton-seconds per meter) - relates a damping force with the velocity of the object whose motion is being damped.


A coefficient is a number placed in front of a term in a [[chemical equation]] to indicate how many [[molecule]]s (or atoms)  take part in the reaction. For example, in the formula 
:&lt;math&gt;2H_2 + O_2 \rarr 2H_2O&lt;/math&gt;, 

the number 2's in front of &lt;math&gt;H_2&lt;/math&gt; and &lt;math&gt;H_2O&lt;/math&gt; are [[Stoichiometry|stoichiometric coefficients]].

==See also==
*[[Degree of a polynomial]]
*[[Monic polynomial]]

==References==
*Tango Freshness &amp; Kiva Juice University of Nevada Wireless Network, Quadratic functions and models 3.5http://faculty.palomar.edu/mmumford/110/notes/3.pdf
*Sabah Al-hadad and C.H. Scott (1979) ''College Algebra with Applications'', page 42, Winthrop Publishers, Cambridge Massachusetts ISBN 0-87626-140-3 .
*Gordon Fuller, Walter L Wilson, Henry C Miller, (1982) ''College Algebra'', 5th edition, page 24, Brooks/Cole Publishing, Monterey California ISBN 0-534-01138-1 .
* Steven Schwartzman (1994) ''The Words of Mathematics: an etymological dictionary of mathematical terms used in English'', page 48, [[Mathematics Association of America]], ISBN 0-88385-511-9.

[[Category:Polynomials]]
[[Category:Mathematical terminology]]
[[Category:Algebra]]</text>
      <sha1>q3jbetz5z8wykh7mtlagnmicaoz3s8t</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Bose–Mesner algebra</title>
    <ns>0</ns>
    <id>22649391</id>
    <revision>
      <id>605444567</id>
      <parentid>605430154</parentid>
      <timestamp>2014-04-23T12:24:05Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* Definition */changes per [[WP:ORDINAL]], replaced: 0&lt;sup&gt;th&lt;/sup&gt; → 0th using [[Project:AWB|AWB]] (10080)</comment>
      <text xml:space="preserve" bytes="10987">In [[mathematics]], a '''Bose–Mesner algebra''' is a special set of [[Matrix (mathematics)|matrices]] which arise from a combinatorial structure known as an [[association scheme]], together with the usual set of rules for combining (forming the products of) those matrices, such that they form an [[associative algebra]], or, more precisely, a [[Unital algebra|unitary commutative algebra]]. Among these rules are:
:*the result of a product is also within the set of matrices,
:*there is an identity matrix in the set, and
:*such that taking products is [[Commutativity|commutative]].

Bose–Mesner algebras have applications in [[physics]] to [[spin model]]s, and in [[statistics]] to the [[design of experiments]]. They are named for [[R. C. Bose]] and Dale Marsh Mesner.&lt;ref&gt;Bose &amp; Mesner (1959)&lt;/ref&gt;

==Definition==
Let ''X'' be a set of ''v'' elements. Consider a partition of the 2-element subsets of ''X'' into ''n'' non-empty subsets, ''R''&lt;sub&gt;1&lt;/sub&gt;, ..., ''R''&lt;sub&gt;''n''&lt;/sub&gt; such that:
* given an &lt;math&gt;x \in X&lt;/math&gt;, the number of &lt;math&gt;y \in X&lt;/math&gt; such that &lt;math&gt;\{x,y\} \in R_i&lt;/math&gt; depends only on i (and not on ''x''). This number will be denoted by v&lt;sub&gt;i&lt;/sub&gt;, and
* given &lt;math&gt;x,y \in X&lt;/math&gt; with &lt;math&gt;\{x,y\} \in R_k&lt;/math&gt;, the number of &lt;math&gt;z \in X&lt;/math&gt; such that &lt;math&gt;\{x,z\} \in R_i&lt;/math&gt; and &lt;math&gt;\{z,y\} \in R_j&lt;/math&gt; depends only on ''i'',''j'' and ''k'' (and not on ''x'' and ''y''). This number will be denoted by &lt;math&gt;p^k_{ij}&lt;/math&gt;.
This structure is enhanced by adding all pairs of repeated elements of ''X'' and collecting them in a subset ''R''&lt;sub&gt;0&lt;/sub&gt;. This enhancement permits the parameters ''i'', ''j'', and ''k'' to take on the value of zero, and lets some of ''x'',''y'' or ''z'' be equal.

A set with such an enhanced partition is called an [[Association scheme]].&lt;ref&gt;{{harvnb|Cameron|van Lint|1991|loc=pp.197–198}}&lt;/ref&gt; One may view an association scheme as a partition of the edges of a [[complete graph]] (with vertex set ''X'') into n classes, often thought of as color classes. In this representation, there is a loop at each vertex and all the loops receive the same 0th color.

The association scheme can also be represented algebraically. Consider the [[Matrix (mathematics)|matrices]] ''D''&lt;sub&gt;''i''&lt;/sub&gt; defined by:
: &lt;math&gt;(D_i)_{x,y} = \begin{cases} 
1,&amp; \text{if } \left(x,y\right)\in R_{i},\\ 
0,&amp; \text{otherwise.}  \end{cases} \qquad (1)&lt;/math&gt;

Let &lt;math&gt;\mathcal{A}&lt;/math&gt; be the [[vector space]] consisting of all [[Matrix (mathematics)|matrices]] &lt;math&gt;\sideset{}{_{i=0}^{n}}\sum a_{i}D_{i}&lt;/math&gt;, with &lt;math&gt;a_{i}&lt;/math&gt; complex.&lt;ref&gt;{{harvnb|Camion|1998}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Delsarte|Levenshtein|1998}}&lt;/ref&gt;

The definition of an [[association scheme]] is equivalent to saying that the &lt;math&gt;D_{i}&lt;/math&gt; are ''v''&amp;nbsp;&amp;times;&amp;nbsp;''v'' (0,1)-[[Matrix (mathematics)|matrices]] which satisfy

# &lt;math&gt;D_i&lt;/math&gt; is symmetric,
# &lt;math&gt;\sum_{i=0}^n D_{i}=J &lt;/math&gt; (the all-ones matrix),
# &lt;math&gt;D_0=I,&lt;/math&gt;
# &lt;math&gt;D_i D_j = \sum_{k=0}^n p^k_{ij} D_k = D_j D_i,\qquad i,j=0,\ldots,n.&lt;/math&gt;

The (''x'',''y'')-th entry of the left side of 4. is the number of two colored paths of length two joining ''x'' and ''y'' (using &quot;colors&quot; ''i'' and ''j'') in the graph. Note that the rows and columns of &lt;math&gt;D_i&lt;/math&gt; contain &lt;math&gt;v_i&lt;/math&gt; 1s:

: &lt;math&gt;D_i J=J D_i = v_i J. \qquad (2)&lt;/math&gt;

From 1., these [[Matrix (mathematics)|matrices]] are [[Symmetric matrix|symmetric]]. From 2., &lt;math&gt;D_{0},\ldots,D_{n}&lt;/math&gt; are [[Linear independence|linearly independent]], and the dimension of &lt;math&gt;\mathcal{A}&lt;/math&gt; is &lt;math&gt;n+1&lt;/math&gt;. From 4., &lt;math&gt;\mathcal{A}&lt;/math&gt; is closed under multiplication, and multiplication is always associative. This [[Associative algebra|associative]] [[commutative algebra]] &lt;math&gt;\mathcal{A}&lt;/math&gt; is called the '''Bose–Mesner algebra''' of the [[association scheme]]. Since the [[Matrix (mathematics)|matrices]] in &lt;math&gt;\mathcal{A}&lt;/math&gt; are symmetric and commute with each other, they can be simultaneously diagonalized. This means that there is a [[Matrix (mathematics)|matrix]] &lt;math&gt;S&lt;/math&gt; such that to each &lt;math&gt;A\in\mathcal{A}&lt;/math&gt; there is a [[diagonal matrix]] &lt;math&gt;\Lambda_{A}&lt;/math&gt; with &lt;math&gt;S^{-1}A S=\Lambda_{A}&lt;/math&gt;. This means that &lt;math&gt;\mathcal{A}&lt;/math&gt; is semi-simple and has a unique basis of primitive idempotents &lt;math&gt;J_{0},\ldots,J_{n}&lt;/math&gt;. These are complex n &amp;times; n [[Matrix (mathematics)|matrices]] satisfying

: &lt;math&gt;
J_i^2 =J_i, i=0,\ldots,n, \qquad (3)
&lt;/math&gt;

: &lt;math&gt;
J_i J_k=0, i\neq k, \qquad (4)
&lt;/math&gt;

: &lt;math&gt;
\sum_{i=0}^n J_i = I. \qquad (5)
&lt;/math&gt;

The '''Bose–Mesner algebra''' has two distinguished bases: the basis consisting of the [[Adjacency matrix|adjacency matrices]] &lt;math&gt;D_i&lt;/math&gt;, and the basis consisting of the irreducible [[Idempotent matrix|idempotent matrices]] &lt;math&gt;E_k&lt;/math&gt;. By definition, there exist well-defined [[complex number]]s such that

: &lt;math&gt;
D_{i}=\sum_{k=0}^n p_i (k) E_k, \qquad (6)
&lt;/math&gt;

and

: &lt;math&gt;
|X|E_{k}=\sum_{i=0}^n q_k\left(i\right)D_i. \qquad (7)
&lt;/math&gt;

The p-numbers &lt;math&gt;p_i (k)&lt;/math&gt;, and the q-numbers &lt;math&gt;q_k(i)&lt;/math&gt;, play a prominent role in the theory.&lt;ref&gt;{{harvnb|Camion|1998}}&lt;/ref&gt; They satisfy well-defined orthogonality relations. The p-numbers are the [[eigenvalues]] of the [[adjacency matrix]] &lt;math&gt;D_i&lt;/math&gt;.

==Theorem==

The [[eigenvalues]] of &lt;math&gt;p_{i}\left(k\right)&lt;/math&gt; and &lt;math&gt;q_{k}\left(i\right)&lt;/math&gt;, satisfy the orthogonality conditions:

: &lt;math&gt;
\sum_{k=0}^n \mu_i p_i (k)p_\ell (k)=v v_i \delta_{i \ell}, \quad(8)
&lt;/math&gt;

: &lt;math&gt;
\sum_{k=0}^n \mu_i q_k (i) q_\ell (i)=v \mu_k \delta_{k \ell}. \quad(9)
&lt;/math&gt;

Also

: &lt;math&gt;
\mu_j p_i (j) = v_i q_ j (i),\quad i,j=0,\ldots,n. \quad(10)
&lt;/math&gt;

In [[Matrix (mathematics)|matrix]] notation, these are

: &lt;math&gt;
P^T \Delta_\mu P=v\Delta_v, \quad(11)
&lt;/math&gt;

: &lt;math&gt;
Q^T \Delta_v Q=v\Delta_\mu, \quad(12)
&lt;/math&gt;

where &lt;math&gt;\Delta_v = \operatorname{diag} \{v_0,v_1,\ldots,v_n\},\qquad \Delta_\mu = \operatorname{diag} \{\mu_0,\mu_1,\ldots,\mu_n\}.&lt;/math&gt;

==Proof of theorem==

The [[eigenvalue]]s of &lt;math&gt;D_i D_\ell&lt;/math&gt; are &lt;math&gt;p_i (k)p_\ell (k)&lt;/math&gt; with multiplicities &lt;math&gt;\mu_k&lt;/math&gt;. This implies that

: &lt;math&gt;
v v_i \delta_{i\ell} = \operatorname{trace}D_i D_\ell = \sum_{k=0}^n \mu_i p_i(k) p_\ell (k), \quad(13)
&lt;/math&gt;

which proves Equation &lt;math&gt;\left(8\right)&lt;/math&gt; and Equation &lt;math&gt;\left(11\right)&lt;/math&gt;,

: &lt;math&gt;
Q = v P^{-1} = \Delta_v^{-1} P^T \Delta_\mu, \quad(14)
&lt;/math&gt;

which gives Equations &lt;math&gt;(9)&lt;/math&gt;, &lt;math&gt;(10)&lt;/math&gt; and &lt;math&gt;(12)&lt;/math&gt;.&lt;math&gt;\Box&lt;/math&gt;

There is an analogy between extensions of [[association scheme]]s and [[Kronecker's theorem|extensions]] of [[finite field]]s. The cases we are most interested in are those where the extended schemes are defined on the &lt;math&gt;n&lt;/math&gt;-th [[Cartesian power]] &lt;math&gt;X=\mathcal{F}^{n}&lt;/math&gt; of a set &lt;math&gt;\mathcal{F}&lt;/math&gt; on which a basic [[association scheme]] &lt;math&gt;\left(\mathcal{F},K\right)&lt;/math&gt; is defined. A first [[association scheme]] defined on &lt;math&gt;X=\mathcal{F}^{n}&lt;/math&gt; is called the &lt;math&gt;n&lt;/math&gt;-th [[Kronecker product|Kronecker power]] &lt;math&gt;\left(\mathcal{F},K\right)_{\otimes}^{n}&lt;/math&gt; of &lt;math&gt;\left(\mathcal{F},K\right)&lt;/math&gt;. Next the extension is defined on the same set &lt;math&gt;X=\mathcal{F}^{n}&lt;/math&gt; by gathering classes of &lt;math&gt;\left(\mathcal{F},K\right)_{\otimes}^{n}&lt;/math&gt;. The [[Kronecker product|Kronecker power]] corresponds to the [[polynomial ring]] &lt;math&gt;F\left[X\right]&lt;/math&gt; first defined on a [[Finite field|field]] &lt;math&gt;\mathbb{F}&lt;/math&gt;, while the extension scheme corresponds to the [[extension field]] obtained as a quotient. An example of such an extended scheme is the [[Hamming scheme]].

[[Association scheme]]s may be merged, but merging them leads to non-symmetric [[association scheme]]s, whereas all usual [[code]]s are [[subgroup]]s in symmetric [[Abelian variety|Abelian schemes]].&lt;ref&gt;{{harvnb|Delsarte|Levenshtein|1998}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Camion|1998}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|MacWilliams|Sloane|1978}}&lt;/ref&gt;

==See also==
* [[Association scheme]]

{{More footnotes|date=September 2010}}

==Notes==

{{Reflist}}

==References==

* {{Cite book|first=Rosemary&amp;nbsp;A.|last=Bailey|authorlink=Rosemary A. Bailey|url=http://www.maths.qmul.ac.uk/~rab/Asbook |title=Association schemes: Designed experiments, algebra and combinatorics|series=Cambridge Studies in Advanced Mathematics|volume=84|publisher=Cambridge University Press|year=2004|pages=387|isbn=978-0-521-82446-0| mr=2047311|ref=harv}}

* {{cite book | last1=Bannai | first1=Eiichi | last2=Ito | first2=Tatsuro | title=Algebraic combinatorics I: Association schemes |  publisher=The Benjamin/Cummings Publishing Co., Inc. | location=Menlo Park, CA | year=1984 | pages=xxiv+425 | isbn=0-8053-0490-8 | mr=0882540 | unused_data=&lt;!-- authorlink1=Eiichi Bannai | authorlink2= Tatsuro Ito --&gt; }}

* Bannai, Etsuko (2001) &quot;Bose–Mesner algebras associated with four-weight spin models&quot;, ''Graphs and Combinatorics'', 17 (4),589&amp;ndash;598. {{doi|10.1007/PL00007251}}

* {{citation| last1=Bose|first1=R.&amp;nbsp;C.| authorlink1=R. C. Bose| last2=Mesner|first2=D.&amp;nbsp;M.|year=1959|title=On linear associative algebras corresponding to association schemes of partially balanced designs|journal=[[Annals of Mathematical Statistics]]|volume=30|issue=1|pages=21&amp;ndash;38| url=http://projecteuclid.org/euclid.aoms/1177706356 | doi=10.1214/aoms/1177706356 | mr = 102157 | jstor = 2237117}}

* {{citation|last=Cameron|first=P.&amp;nbsp;J.|last2=van Lint|first2=J.&amp;nbsp;H.|title=Designs, Graphs, Codes and their Links|year=1991|publisher=Cambridge University Press|location=Cambridge|isbn=0-521-42385-6}}

* {{citation|last1= Camion|first1=P.|authorlink1=Paul Camion|chapter=Codes and association schemes: Basic properties of association schemes relevant to coding| title=Handbook of coding theory|editor1-last= Pless|editor1-first=V.&amp;nbsp;S.|editor1-link=Vera Pless|editor2-last=Huffman|editor2-first=W.&amp;nbsp;C.|publisher=Elsevier|place= The Netherlands|year= 1998}}

* {{citation|last1=Delsarte|first1=P.|last2=Levenshtein|first2=V.&amp;nbsp;I.|authorlink2=Vladimir Levenshtein|title=Association schemes and coding theory|journal=IEEE Transactions in Information Theory| volume= 44| issue= 6|pages= 2477&amp;ndash;2504|year= 1998}}

* {{citation| first1=F. J.|last1= MacWilliams|first2=N.&amp;nbsp;J.&amp;nbsp;A.|last2= Sloane|authorlink2=Neil J. A. Sloane|title=The theory of error-correcting codes|publisher= Elsevier|place= New York|year= 1978}}

* Nomura, K. (1997) &quot;An algebra associated with a spin model&quot;, ''Journal of Algebraic Combinatorics'', 6 (1), 53&amp;ndash;58. {{DOI|10.1023/A:1008644201287}}

{{Experimental design|state=expanded}}

{{DEFAULTSORT:Bose-Mesner Algebra}}
[[Category:Algebraic combinatorics]]
[[Category:Design of experiments]]
[[Category:Analysis of variance]]
[[Category:Representation theory]]
[[Category:Algebra]]</text>
      <sha1>8nd7ug9rwhhy0pa6xlfqyzbbaley2mv</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Basic element</title>
    <ns>0</ns>
    <id>42482358</id>
    <revision>
      <id>608949393</id>
      <parentid>606958102</parentid>
      <timestamp>2014-05-17T11:44:16Z</timestamp>
      <contributor>
        <username>Hhhippo</username>
        <id>11916575</id>
      </contributor>
      <comment>+cat</comment>
      <text xml:space="preserve" bytes="416">{{multiple issues|1=
{{orphan|date=April 2014}}
{{CI|date=May 2014}}
{{unreferenced|date=May 2014}}
}}
In algebra, a '''basic element''' ''x'' with respect to an element ''y'' is an element of a [[cochain complex]] &lt;math&gt;(C^*, d)&lt;/math&gt; (e.g., complex of [[differential form]]s on a manifold) that is closed: &lt;math&gt;dx = 0&lt;/math&gt; and the contraction of ''x'' by ''y'' is zero.

[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>73l7t9q5trq5hu34vno7bnjyy6dsq66</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>En-ring</title>
    <ns>0</ns>
    <id>42480494</id>
    <revision>
      <id>608959745</id>
      <parentid>605850204</parentid>
      <timestamp>2014-05-17T13:40:18Z</timestamp>
      <contributor>
        <username>Hhhippo</username>
        <id>11916575</id>
      </contributor>
      <comment>+cat</comment>
      <text xml:space="preserve" bytes="968">{{DISPLAYTITLE:E_n-ring}}
In mathematics, an '''&lt;math&gt;\mathcal{E}_n&lt;/math&gt;-algebra''' in a [[symmetric monoidal infinity category]] ''C'' consists of the following data:
*&lt;math&gt;A(U)&lt;/math&gt; an object for any open subset ''U'' homeomorphic to a disk.
*A multiplication map:
*:&lt;math&gt;\mu: A(U_1) \otimes \cdots \otimes A(U_m) \to A(V)&lt;/math&gt;
:for any disjoint open disks &lt;math&gt;U_j&lt;/math&gt; contained in some open disk ''V''
subject to the requirement that &lt;math&gt;\mu&lt;/math&gt; is an equivalence if &lt;math&gt;m=1&lt;/math&gt;.

For example, &lt;math&gt;X \mapsto C_*(\Omega^n X; \Lambda)&lt;/math&gt; defines an &lt;math&gt;\mathcal{E}_n&lt;/math&gt;-algebra in the infinity category of chain complexes of &lt;math&gt;\Lambda&lt;/math&gt;-modules.

== References ==
*http://www.math.harvard.edu/~lurie/282ynotes/LectureXXII-En.pdf
*http://www.math.harvard.edu/~lurie/282ynotes/LectureXXIII-Koszul.pdf

== External links ==
*http://ncatlab.org/nlab/show/En-algebra

{{CI|date=May 2014}}
[[Category:Algebra]]


{{algebra-stub}}</text>
      <sha1>q6uua6ybk1paifbqn24z4jcd9am98j8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Square (algebra)</title>
    <ns>0</ns>
    <id>659942</id>
    <revision>
      <id>609569163</id>
      <parentid>609569140</parentid>
      <timestamp>2014-05-21T19:46:35Z</timestamp>
      <contributor>
        <username>Excirial</username>
        <id>5499713</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/121.74.147.132|121.74.147.132]] ([[User talk:121.74.147.132|talk]]) ([[WP:HG|HG 3]])</comment>
      <text xml:space="preserve" bytes="13342">{{redirect3|²|Its literal meaning is the numeral &quot;[[2 (number)|2]]&quot; in [[superscript]]}}
[[Image:Five Squared.svg|thumb|right|168px&lt;!-- at 160px and 200px lines render with unequal widths 
--&gt;|{{math|5&amp;sdot;5}}, or {{math|5&lt;sup&gt;2&lt;/sup&gt;}} (5 squared), can be shown graphically using a [[square]]. Each block represents one unit, {{math|1&amp;sdot;1}}, and the entire square represents {{math|5&amp;sdot;5}}, or the area of the square.]]
In mathematics, a '''square''' is the result of [[multiplication|multiplying]] a [[number]] by itself. The verb &quot;to square&quot; is used to denote this operation. Squaring is the same as [[exponentiation|raising to]] the power&amp;nbsp;[[2 (number)|2]], and is denoted by a [[superscript]] 2; for instance, the square of 3 may be written as 3&lt;sup&gt;2&lt;/sup&gt;, which is the number 9.
In some cases when superscripts are not available, as for instance in [[programming language]]s or [[plain text]] files, the notations &lt;tt&gt;''x''^2&lt;/tt&gt; or &lt;tt&gt;''x''**2&lt;/tt&gt; may be used in place of &lt;tt&gt;''x''&lt;sup&gt;2&lt;/sup&gt;&lt;/tt&gt;.

The adjective which corresponds to squaring is ''[[wikt:quadratic|quadratic]]''.

The square of an [[integer]] may also be called a [[square number]] or a perfect square. In [[algebra]], the operation of squaring is often generalized to [[polynomial]]s, other [[expression (mathematics)|expressions]], or values in systems of mathematical values other than the numbers. For instance, the square of the [[linear function (calculus)|linear polynomial]] {{math|''x'' + 1}} is the [[quadratic polynomial]] {{math|''x''&lt;sup&gt;2&lt;/sup&gt; + 2''x'' + 1}}.

One of the important properties of squaring, for numbers as well as in many other mathematical systems, is that (for all numbers {{mvar|x}}), the square of {{mvar|x}} is the same as the square of its [[additive inverse]] {{math|&amp;minus;''x''}}. That is,  the square function satisfies the identity {{math|1=''x''&lt;sup&gt;2&lt;/sup&gt; = (−''x'')&lt;sup&gt;2&lt;/sup&gt;}}. This can also be expressed by saying that the squaring function is an [[even function]].

== In real numbers ==
[[Image:Parabola2.svg|thumb|240px|{{math|1=''y'' = ''x''&lt;sup&gt;2&lt;/sup&gt;}}. The graph of a [[quadratic function]] has a [[parabola|parabolic]] shape. The squares of numbers make a [[power law]].]]
The squaring function preserves the order of positive numbers: larger numbers have larger squares. In other words, squaring is a [[monotonic function]] on the interval {{closed-open|0, +∞}}. On the negative numbers, numbers with greater absolute value have greater squares, so squaring is a monotonically decreasing function on {{open-closed|−∞,0}}. Hence, [[zero]] is its global [[minimum]].
The only cases where the square&amp;nbsp;{{math|''x''&lt;sup&gt;2&lt;/sup&gt;}} of a number is less than {{mvar|x}} occur when {{math|0 &lt; ''x'' &lt; 1}}, that is, when {{mvar|x}} belongs to an [[open interval]] {{open-open|0,1}}. This implies that the square of an integer is never less than the original number.

Every positive [[real number]] is the square of exactly two numbers, one of which is strictly positive and the other of which is strictly negative. Zero is the square of only one number, itself. For this reason, it is possible to define the [[square root]] function, which associates with a non-negative real number the non-negative number whose square is the original number.

No square root can be taken of a negative number within the system of [[real number]]s, because squares of all real numbers are [[non-negative]]. The lack of real square roots for the negative numbers can be used to expand the real number system to the [[complex number]]s, by postulating the [[imaginary unit]] {{math|''i''}}, which is one of the square roots of&amp;nbsp;&amp;minus;1.

The property &quot;every non negative real number is a square&quot; has been generalized to the notion of a [[real closed field]], which is an [[ordered field]] such that every non negative element is a square. The real closed fields can not be distinguished from the field of real numbers by their algebraic properties: every property of the real numbers, which may be expressed in [[first-order logic]] (that is expressed by a formula in which the variables that are quantified by ∀ or ∃ represent elements, not sets), is true for every real closed field, and conversely every property of the first-order logic, which is true for a specific real closed field is also true for the real numbers.

== In geometry ==
There are several major uses of the squaring function in geometry.

The name of the squaring function shows its importance in the definition of the [[area]]:  it comes from the fact that the area of a [[square]] with sides of length &amp;nbsp;{{mvar|l}} is equal to {{math|''l''&lt;sup&gt;2&lt;/sup&gt;}}. The area depends quadratically on the size: the area of a shape {{mvar|n}}&amp;nbsp;times larger is {{math|''n''&lt;sup&gt;2&lt;/sup&gt;}}&amp;nbsp;times greater. This holds for areas in three dimensions as well as in the plane: for instance, the surface area of a [[sphere]] is proportional to the square of its radius, a fact that is manifested physically by the [[inverse-square law]] describing how the strength of physical forces such as gravity varies according to distance.

{{anchor|r²}}[[Image:Zonenplatte Cosinus.png|thumb|right|Fresnel's [[zone plate]]s have rings with [[arithmetic progression|equally spaced]] squared distances to the center]]
The squaring function is related to [[distance]] through the [[Pythagorean theorem]] and its generalization, the [[parallelogram law]]. [[Euclidean geometry|Euclidean]] distance is not a [[smooth function]]: the [[three-dimensional graph]] of distance from a fixed point forms a [[cone]], with a non-smooth point at the tip of the cone. However, the square of the distance (denoted {{math|''d''&lt;sup&gt;2&lt;/sup&gt;}} or {{math|''r''&lt;sup&gt;2&lt;/sup&gt;}}), which has a [[paraboloid]] as its graph, is a smooth and [[analytic function]]. The [[dot product]] of a [[Euclidean vector]] with itself is equal to the square of its length: {{math|1='''v'''&amp;sdot;'''v''' = v&lt;sup&gt;2&lt;/sup&gt;}}. This is further generalised to [[quadratic form]]s in [[linear space]]s. The [[inertia tensor]] in [[mechanics]] is an example of a quadratic form. It demonstrates a quadratic relation of the [[moment of inertia]] to the size ([[length]]).

==In abstract algebra and number theory==
The squaring function is defined in any [[field (mathematics)|field]] or [[ring (mathematics)|ring]]. An element in the image of this function is called a ''square'', and the inverse images of a square are called ''[[square root]]s''.

The notion of squaring is particularly important in the [[finite field]]s  '''Z'''/''p'''''Z''' formed by the numbers modulo an odd [[prime number]] {{mvar|p}}. A non-zero element of this field is called a [[quadratic residue]] if it is a square in '''Z'''/''p'''''Z''', and otherwise, it is called a quadratic non-residue. Zero, while a square, is not considered to be a quadratic residue. Every finite field of this type has exactly {{math|(''p'' − 1)/2}} quadratic residues and exactly {{math|(''p'' − 1)/2}} quadratic non-residues. The quadratic residues form a [[group (mathematics)|group]] under multiplication. The properties of quadratic residues are widely used in [[number theory]].

More generally, in rings, the squaring function may have different properties that are sometimes used to classify rings.

Zero may be the square of some non-zero elements. A [[commutative ring]] such that the square of a non zero element is never zero is called a [[reduced ring]]. More generally, in a commutative ring, a [[radical ideal]] is an ideal&amp;nbsp;{{mvar|I}} such that &lt;math&gt;x^2 \in I&lt;/math&gt; implies &lt;math&gt;x \in I&lt;/math&gt;. Both notions are important in [[algebraic geometry]], because of [[Hilbert's Nullstellensatz]].

An element of a ring that is equal to its own square is called an [[idempotent]]. In any ring, 0 and 1 are idempotents. {{anchor|integral domains}}There are no other idempotents in fields and more generally in [[integral domain]]s. However, 
the ring of the integers [[modular arithmetic|modulo]]&amp;nbsp;{{mvar|n}} has [[power of two|{{math|2&lt;sup&gt;''k''&lt;/sup&gt;}}]] idempotents, where {{mvar|k}} is the number of distinct [[integer factorization|prime factors]] of&amp;nbsp;{{mvar|n}}.
A commutative ring in which every element is equal to its square (every element is idempotent) is called a [[Boolean ring]]; an example from [[computer science]] is the ring whose elements are [[binary number]]s, with [[Bitwise operation|bitwise AND]] as the multiplication operation and bitwise XOR as the addition operation.

In a [[supercommutative algebra]] ([[away from 2]]), the square of any ''odd'' element equals to zero.

==In complex numbers and related algebras over the reals==
{{anchor|In complex analysis}}
[[Image:Conformz2.jpg|thumb|right|256px|The [[function composition|composition]] of the tiling [[:Image:ConformId.jpg]] (understood as a function on the complex plane) with the complex square function]]
{{see also|Exponentiation #Powers of complex numbers}}
The [[complex numbers|complex]] square function&amp;nbsp;{{math|''z''&lt;sup&gt;2&lt;/sup&gt;}} is a twofold cover of the [[complex plane]], such that each non-zero complex number has exactly two square roots. This map is related to [[parabolic coordinates]].&lt;!-- unfortunately, incompatible coefficients and orientation conventions hinder a simple explanation such as σ+iτ → (σ,τ)-parabolic --&gt;

{{anchor|{{!}}z{{!}}²}}Another, more well known, function is the square of the [[absolute value]] {{math|1=&amp;#124; ''z'' &amp;#124;&lt;sup&gt;2&lt;/sup&gt; = ''z'' [[complex conjugate|{{conjugate|''z''}}]]|class=nounderlines}}, which is real-valued. It is very important for [[quantum mechanics]]: see [[probability amplitude]] and [[Born rule]]. Complex numbers form one of [[Hurwitz's theorem (composition algebras)|four possible Euclidean Hurwitz algebras]] that are defined with a real quadratic form {{mvar|q}}; here {{math|1=''q''(''z'') = &amp;#124; ''z'' &amp;#124;&lt;sup&gt;2&lt;/sup&gt;}}. In a Euclidean Hurwitz algebra this {{mvar|q}} equals to the square of the distance to 0 discussed [[#r²|above]], and the absolute value {{math|&amp;#124; ''z'' &amp;#124;}} can be defined as the (arithmetical) square root of {{math|''q''(''z'')}}. Multiplicativity of {{mvar|q}} in these algebras explains (or relies upon) certain algebraic identities (see [[#Related identities|below]]).

== Other uses ==
Squares are ubiquitous in algebra, more generally, in almost every branch of mathematics, and also in [[physics]] where many [[unit of measurement|units]] are defined using squares and [[multiplicative inverse|inverse]] squares: see [[#Related physical quantities|below]].

[[Least squares]] is the standard method used with [[overdetermined system]]s.

Squaring is used in [[statistics]] and [[probability theory]] in determining the [[standard deviation]] of a set of values, or a [[random variable]]. The deviation of each value&amp;nbsp;{{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} from the [[mean]]&amp;nbsp;&lt;math&gt;\overline{x}&lt;/math&gt; of the set is defined as the difference &lt;math&gt;x_i - \overline{x}&lt;/math&gt;. These deviations are squared, then a mean is taken of the new set of numbers (each of which is positive). This mean is the [[variance]], and its square root is the standard deviation. In [[finance]], the [[volatility (finance)|volatility]] of a financial instrument is the standard deviation of its values.

==See also==
&lt;!--
* [[von Neumann regular ring]], a ring such that every ideal is generated by an idempotent
 too remote from the topic? --&gt;
* [[Exponentiation by squaring]]
* [[Polynomial SOS]], the representation of a non-negative polynomial as the sum of squares of polynomials
* [[Hilbert's seventeenth problem]], for the representation of [[positive polynomial]]s as a sum of squares of [[rational function]]s
* [[Square-free polynomial]]
* [[Cube (algebra)]]
* [[Metric tensor]]
* [[Quadratic equation]]
* [[Polynomial ring]]

=== Related identities ===
; Algebraic&lt;span style=&quot;font-weight:400&quot;&gt; (need a [[commutative ring]])&lt;/span&gt;:
* [[Difference of two squares]]
* [[Brahmagupta–Fibonacci identity]], related to complex numbers [[#.7Cz.7C²|in the sense discussed above]]
* [[Euler's four-square identity]], related to [[quaternion]]s in the same way
* [[Degen's eight-square identity]], related to [[octonion]]s in the same way
* [[Lagrange's identity]]
; Other
* [[Pythagorean trigonometric identity]]
* [[Parseval's identity]]

=== Related physical quantities ===
* [[acceleration]], length per square time
* [[cross section (physics)]], an area-dimensioned quantity
* [[coupling constant]] (has square charge in the denominator, and may be expressed with square distance in the numerator)
* [[kinetic energy]] (quadratic dependence on velocity)
* [[specific energy]], a (square velocity)-dimensioned quantity

== Footnotes ==
{{reflist}}

==Further reading==
* Marshall, Murray Positive polynomials and sums of squares. Mathematical Surveys and Monographs, 146. American Mathematical Society, Providence, RI, 2008. xii+187 pp. ISBN 978-0-8218-4402-1, ISBN 0-8218-4402-4
* {{cite book | title=Squares | volume=171 | series=London Mathematical Society Lecture Note Series | first=A. R. | last=Rajwade | publisher=[[Cambridge University Press]] | year=1993 | isbn=0-521-42668-5 | zbl=0785.11022 }}

[[Category:Exponentials|2]]
[[Category:Algebra]]
[[Category:Elementary arithmetic]]</text>
      <sha1>73h8o8vtc2j7dxx7tv4zl0lul3349p8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
</mediawiki>
